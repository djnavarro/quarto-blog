---
title: "The Metropolis-Hastings algorithm"
author:
  - name: Danielle Navarro
    url: https://djnavarro.net
    affiliation: I'm on smoko
    affiliation-url: https://www.youtube.com/watch?v=j58V2vC9EPc
    orcid: 0000-0001-7648-6578
description: "A note that I wrote for a computer science class I taught all the way back in 2010"
date: "2023-04-12"
categories: [Statistics, MCMC]
---

<!--------------- my typical setup ----------------->

```{r setup, include=FALSE}
wide <- 136
narrow <- 76
options(width = narrow)
cache_images <- TRUE
set.seed(1)
```

<!--------------- post begins here ----------------->

This morning I received an email from a stranger, writing to say thank you for a document I wrote almost 13 years ago. It's a weird feeling every time I get one of those,^[Bizarrely, this actually happens to me a lot. It's totally surreal.] but a pleasant one. This time around, the document in question was a [note on the Metropolis-Hastings algorithm](https://compcogsci-3016.djnavarro.net/technote_metropolishastings.pdf) that I threw together in a rush for a computer science class I taught back in 2010.^[Another surreal experience that I've had quite a bit lately is getting rejected from data science jobs because I don't have a computer science degree and my qualifications are technically in psychology. Apparently I'm considered skilled enough to *teach* computational statistics to university computer science students, but still considered less skilled at those tools than the students that I taught? I mean, it's either that or tech company recruiters don't actually read the résumés that they get sent... but that couldn't possibly be true, right? Efficient market hypothesis and all that...] While drinking my second coffee of the morning and feeling the usual sense of dread I feel when I know that today I have to put some effort into looking for a job, yet again, I arrived at an excellent procrastination strategy...

Why don't I start rescuing some of the content that I wrote all those years ago and currently have hidden away in pdf files in the dark corners of the internet, and put them up on my blog? It won't get me a job, but it feels less demeaning than yet again trying to get tech company recruiters to believe that yes actually the middle aged lady with a psychology PhD does in fact know how to code and analyse data. 

Anyway. Without any further self-pity, here's a quick primer on the [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm). The target audience for this is someone who has a little bit of probability theory, can write code in R (or similar), but doesn't have any background in Markov chain Monte Carlo methods. It doesn't dive deep into the mathematics (i.e., you *won't* find any discussions of detailed balance, ergodicity etc), but it does try to go deep enough to give a beginner some formal understanding of what the algorithm is doing.

## The problem to be solved

The Metropolis-Hastings algorithm is perhaps the most popular example of a [Markov chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) (MCMC) method in statistics. The basic problem that it solves is to provide a method for sampling from some generic distribution, $P(x)$. The idea is that in many cases, you know
how to write out the equation for the probability $P(x)$, but you don’t know how to generate
a random number from this distribution, $x \sim P(x)$. This is the situation where MCMC is
handy. In fact, for the Metropolis-Hastings algorithm we don’t even need to know how to
calculate $P(x)$ completely. For example, suppose I’ve become interested -- for reasons known but to the gods -- in the probability distribution shown below: 

```{r echo=FALSE}
x <- seq(-3, 3, .01)
y <- exp(-x ^ 2) * (2 + sin(5 * x) + sin(2 * x))
y <- y / sum(y) / .01
plot(x, y, "l", axes = FALSE, xlab = "Value, x", ylab = "Probability density, p(x)")
axis(1)
axis(2)
```

The probability density function^[Some asides: my experience teaching this class is that it's quite common for people new to statistics to struggle with the concept of [probability density](https://en.wikipedia.org/wiki/Probability_density_function). It's not super important for the purposes of this post, and to a first approximation it's totally okay to think of $p(x)$ as "the probability of observing $x$ (sort of)", and the [distribution function](https://en.wikipedia.org/wiki/Cumulative_distribution_function) $P(x)$ as "the probability of observing a value no larger than $x$". These sorts of notational and technical details do matter once you start doing mathematical statistics, but this post is absolutely not that! I promise I was never so cruel as to inflict that stuff on my undergraduate computer science students! Never did I ever refer to sigma algebras in my undergrad teaching, honest. Though I did once inflict Kolmogorov complexity on a class, and apologised profusely for my error.] $p(x)$ for this distribution is given by the following equation:

$$
p(x) = \frac{\exp(-x^2) \left(2 + \sin(5x) + \sin(2x)\right)}{\int_{-\infty}^\infty \exp(-u^2) \left(2 + \sin(5u) + \sin(2u)\right) \ du}
$$

My problem is that I either don't know *how* to solve the integral in the denominator, or (equally plausible) I'm simply too lazy to try. So this means in truth, I only know the distribution "up to some unknown constant". That is, I know that:

$$
p(x) \propto \exp(-x^2) \left(2 + \sin(5x) + \sin(2x)\right)
$$
How can I generate samples from this distribution?

## The Metropolis-Hastings algorithm

The basic idea behind MCMC is very simple. The idea is to define a [Markov chain](https://en.wikipedia.org/wiki/Markov_chain)^[At this point in the class students had most certainly encountered Markov chains!] over possible $x$ values, in such a way that the stationary distribution of the Markov chain is in fact $P(x)$. That is, what we’re going to do is use a Markov chain to generate a sequence of $x$ values, denoted $(x_0, x_1, x_2, \ldots, x_n)$, in such a way that as $n \rightarrow \infty$, we can guarantee that $x_n \sim P(x)$. There are many different ways of setting up a Markov chain that has this property, one of which is the Metropolis-Hastings algorithm.

Here's how it works. Suppose that the current state of the Markov chain is $x_n$, and
we want to generate $x_{n+1}$. In the Metropolis-Hastings algorithm, the generation of $x_{n+1}$ is a two-stage process. 

### The proposal step

The first stage is to generate a *candidate*, which we'll denote $x^∗$. The value of $x^∗$ is generated from the proposal distribution that we already know how to sample from. We denote this proposal distribution $Q(x^∗ | x_n)$, and the corresponding density function as $q(x^∗ | x_n)$. Notice that the distribution we sample from depends on the current state of the Markov chain, $x_n$. There are some technical constraints on what you can use as a proposal distribution, but for the most part it can be anything you like. A very typical way to do this is to use a normal distribution centered on the current state $x_n$. More formally, we write this as:

$$
x^* | x_n \sim \mbox{Normal}(x_n, \sigma^2)
$$
for some standard deviation $\sigma$ that we select in advance (more on this later!)

### The accept-reject step

The second stage is the accept-reject step. Firstly, what you need to do is calculate
the *acceptance probability*, denoted $A(x_n \rightarrow x_∗)$, which is given by:^[Okay yeah I suppose I could have denoted the acceptance probability as $P(x_{n+1} = x^* | x_{n}, x^*)$ to properly capture the dependency on the current state and the proposal value, be clear about the position in the sequence and the fact that this is a probability mass function not a probability density function, but honestly does all that notation actually help anyone? If I follow this line of thought to its conclusion I'll end up talking about measures defined on Borel sets or something equally unhelpful to new learners.]

$$
A(x_n \rightarrow x_∗) = \min \left(1, \frac{p(x^*)}{p(x^n)} \times \frac{q(x_n | x^*)}{q(x^* | x_n)} \right)
$$
There are two things to pay attention to here. Firstly, notice that the ratio $\frac{p(x^*)}{p(x^n)}$ doesn't depend on the normalising constant for the distribution. Or, to put it in a more helpful way, that integral in the first equation is completely irrelevant and we can ignore it. As a consequence, for our toy problem we can write this:

$$
\frac{p(x^*)}{p(x^n)} = \frac{\exp(-{x^*}^2) \left(2 + \sin(5x^*) + \sin(2x^*)\right)}{\exp(-{x_n}^2) \left(2 + \sin(5x_n) + \sin(2x_n)\right)}
$$
That's a nice simple thing to compute with no need for any numerical integration or, gods forbid, solving the integral analytically. 

The second thing to pay attention to is the behaviour of the other term, $\frac{q(x_n | x^*)}{q(x^* | x_n)}$. What this term does is correct for any biases that the proposal distribution might induce. In this expression, the denominator $q(x^∗ | x_n)$ describes the probability with which you'd choose $x^*$ as the candidate if the current state of the Markov chain is $x_n$. The numerator, however, describes the probability of a transition that goes the other way: that is, if the current state had actually been $x^∗$, what is the probability that you would have generated $x^n$ as the candidate value? If the proposal distribution is symmetric, then these two probabilities will turn out to be equal. For example, if the proposal distribution is normal, then:

$$
\begin{array}{rcl}
q(x^* | x_n) & = & \frac{1}{\sqrt{2 \pi} \sigma} \exp \left( -\frac{1}{2 \sigma^2} \left(x_n - x^*\right)^2 \right) \\
q(x_n | x^*) & = & \frac{1}{\sqrt{2 \pi} \sigma} \exp \left( -\frac{1}{2 \sigma^2} \left(x^* - x_n \right)^2 \right)
\end{array}
$$

Clearly, $q(x^* | x_n) = q(x_n | x^*)$ for all choices of $x_n$ and $x^*$, and as a consequence the ratio $\frac{q(x_n | x^*)}{q(x^* | x_n)}$ is always 1 in this case. 

This special case of the Metropolis-Hastings algorithm, in which the proposal distribution is symmetric, is referred to as the *Metropolis algorithm*.

Okay. Having proposed the candidate $x^∗$ and calculated the acceptance probability,
$A(x_n \rightarrow x^∗)$, we now either decide to "accept" the candidate and set $x_{n+1} = x^∗$ or we "reject" the candidate and set $x_{n+1} = x_n$. To make this decision, we generate a uniformly distributed random number between 0 and 1, denoted $u$. Then:

$$
x_{n+1} = \left\{ 
\begin{array}{rl}
x^* & \mbox{ if } u \leq A(x_n \rightarrow x^∗) \\
x_n & \mbox{ otherwise}
\end{array}
\right.
$$
In essence, this is the entirety of the Metropolis-Hastings algorithm! True, there are quite a
few technical issues that attach to this, and if you're interested in using the algorithm for
practical purposes I strongly encourage you to do some further reading to make sure you
understand the traps in detail, but for now I'll just give you some examples of things that
work and things that don't, to give you a bit of a feel for how it works in practice.

## Some examples

Firstly, let’s have a look at some R code implementing the Metropolis-Hastings algorithm for the toy problem (i.e., sample from the distribution shown in the original figure). Notice that in addition to the parameter `sigma`, we also need to specify the total number of samples that we are intending to draw `nsamp`, a start point for the chain `x0`, and two additional variables `burnin` and `lag` that I'll explain shortly. For the moment, however, let's start the chain near the middle of the distribution, but not quite there: we'll use `x0 = −1`. Then, let’s watch what happens to our sampler for three different values of `sigma`, where we run the sampler for 1000 iterations. In one case, we’ll set the value too low (`sigma = .025`) and in another we'll set it too high (`sigma = 50`), but in a third case we’ll get it about right (`sigma = 1`). 

The results are shown in Figure 3. For all three values of `sigma`, we have
two plots. The top one shows the true target distribution, along with a histogram showing
the distribution of samples obtained using the Metropolis sampler. The lower panel plots
the actual Markov chain: the sequence of generated values. In the leftmost plots, we see
what happens when we choose a good proposal distribution: the chain shown in the lower
panel moves rapidly across the whole distribution, without getting stuck in any one place
(the acceptance rate here is 47.5%). In the far right panel, we see what happens when the
proposal distribution is too wide: the chain gets stuck in one spot for long periods of time. 
It does manage to make big jumps, covering the whole range, but because the acceptance
rate is so low (2.5%) the distribution is highly irregular. Finally, in the middle panel, if we
set the proposal distribution to be too narrow, the acceptance rate is very high (97.7%) so
the chain doesn’t get stuck in any one spot, but it doesn’t cover a very wide range.
This simple example should give you an intuition for why you need to "play around"
with the choice of proposal distribution. A good proposal distribution can make a huge
difference! However, it’s also important to realise that even if you don’t get it quite right,
you can always solve the problem with brute force. For example, Figure 4 what happens
when you run the same three chains for 50,000 iterations rather than just 1000.

Now, at this point I haven’t really explained what the `burnin` and `lag` parameters
are there for. I don't plan to go into details, but here's the basic idea. First, let's think
about the burn-in issue. Suppose you started the sampler at a very bad location. In the
examples that I’ve shown here `x0 = -1` is bad, but it's not too bad. So let's pick something
nastier: we’ll start at `x0 = -3`. Also, to make the issue visually obvious, we’ll use a proposal
distribution that is a bit too narrow, say `sigma = .2`. Figure 5 shows 3 runs of this sampler.
As you can see, the sampler spends the first 200 or so iterations slowly moving towards
the main body of the distribution. Once it gets there, the samples start to look okay, but
notice that the histograms are biased towards the left (i.e., towards the bad start location).
A simple way to fix this problem is to let the algorithm run for a while before starting to
collect actual samples. The length of time that you spend doing this is called the burn in
period. To illustrate how it helps, Figure 6 shows what happens when you use a burn in
period of 200 iterations for the same sampler.

Finally, I’ll mention in passing the role played by the lag parameter. In some situa-
tions you can be forced into using a proposal distribution that has a very low acceptance
rate. When that happens, you’re left with an awkward Markov chain that gets stuck in one location for long periods of time. One thing that people often do in that situation is allow
several iterations of the sampler to elapse in between successive samples. This is the `lag`
between samples. The effect of this is illustrated in Figure 7.


## A word of warning

The discussion in this note is *heavily* oversimplified. There are a lot of subtle issues
associated with MCMC methods, and my original plan when teaching this class was to gradually write a series of notes that expanded on this simplified discussion. Sadly, that never actually happened. I ended up too busy every year and never really had the time to write it up. Maybe one day!

<!--------------- appendices go here ----------------->



