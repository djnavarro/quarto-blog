---
title: "Pins and needles"
description: "Learning my lessons the hard way"
date: "2023-06-12"
categories: ["R"]
---

<!--------------- my typical setup ----------------->

```{r setup, include=FALSE}
wide <- 136
narrow <- 76
options(width = narrow)
cache_images <- TRUE
set.seed(1)
```

<!--------------- post begins here ----------------->

## Creating and configuring the storage buckets

I'm familiar enough with the gcloud command line that I feel comfortable creating the bucket there. There's some [relevant documentation](https://cloud.google.com/storage/docs/creating-buckets#storage-create-bucket-cli) to help out too, which is nice. Everything in google cloud takes place within a project, specified by project ID (in this case `pins-389407`). For my own purposes I have two buckets in my pins project: `djnavarro-pins` is where I store publicly accessible data, and `djnavarro-private-pins` is where I keep private pins. Rather than set defaults, I have a tendency to do everything explicitly, so the `--project` flag is set in each command, as is the `--location` flag used to specify that I want my data to be stored in Sydney (also known as `australia-southeast1`). Anyway:

```bash
gcloud storage buckets create gs://djnavarro-pins/ \
  --project pins-389407 \
  --location australia-southeast1 \
  --uniform-bucket-level-access
```

The `--uniform-bucket-level-access` flag is used to indicate that I'm not doing fancy file-specific access control. I'm too lazy or simple-minded for that: I want one bucket to be public, and another bucket to be private. To make all files in the bucket publicly readable ([relevant documentation](https://cloud.google.com/storage/docs/access-control/making-data-public#command-line_1)) the command I want is this:

```bash
gcloud storage buckets add-iam-policy-binding gs://djnavarro-pins/ \
  --member=allUsers \
  --role=roles/storage.objectViewer
```

## A terrible horrible no good very bad hack

Let's say I want to pin the `diamonds` data from the ggplot2 package as a csv file. The first thing I need is an authentication token, which I can obtain with the help of the gargle package:

```{r}
#| eval: false
library(gargle)
scope <- "https://www.googleapis.com/auth/cloud-platform"
token <- token_fetch(scopes = scope)
```

This workflow is designed for interactive use so there's a confirmation process to follow. Once that's done I can authenticate with the googleCloudStorageR package, and I can confirm it's working by listing the contents of the bucket:

```{r}
#| eval: false
library(googleCloudStorageR)
gcs_auth(token = token)
gcs_list_objects(bucket = "gs://djnavarro-pins")
```

::: {.column-page-inset-right}

```
                                                      name      size             updated
1                                               _pins.yaml 161 bytes 2023-06-12 07:33:04
2              warfpk_data/20230610T142554Z-b8888/data.txt 190 bytes 2023-06-10 14:26:11
3       warfpk_data/20230610T142554Z-b8888/warfpk_data.csv      8 Kb 2023-06-10 14:26:11
4             warfpk_draws/20230610T142202Z-5bd80/data.txt 200 bytes 2023-06-10 14:23:05
5     warfpk_draws/20230610T142202Z-5bd80/warfpk_draws.csv  281.4 Mb 2023-06-10 14:25:26
6           warfpk_summary/20230610T083635Z-340c1/data.txt 200 bytes 2023-06-10 08:38:44
7 warfpk_summary/20230610T083635Z-340c1/warfpk_summary.csv    1.2 Mb 2023-06-10 08:38:44
```

:::

Now we can use the pins package as a more convenient interface:

```{r}
#| eval: false
library(pins)
board <- board_gcs("gs://djnavarro-pins")
```

I can write pins directly to the board like so:

```{r}
#| eval: false
pin_write(
  board, 
  ggplot2::diamonds, 
  name = "diamonds", 
  type = "csv"
)
```

Under the hood the work is done by googleCloudStorageR, which whines a little about needing `predefinedAcl = "bucketLevel"`, but does get the job done. In principle I could pass this argument to `pin_write()` via the dots, but unfortunately `pin_write()` is a bit trigger happy and throws an error if I do, incorrectly guessing that the argument is misspelled. We can verify that the files have been written as follows:

```{r}
#| eval: false
gcs_list_objects(bucket = "gs://djnavarro-pins")
```

::: {.column-page-inset-right}

```
                                                      name      size             updated
1                                               _pins.yaml 161 bytes 2023-06-12 07:33:04
2                 diamonds/20230612T073405Z-c9e9b/data.txt 189 bytes 2023-06-12 07:34:05
3             diamonds/20230612T073405Z-c9e9b/diamonds.csv    2.6 Mb 2023-06-12 07:34:14
4              warfpk_data/20230610T142554Z-b8888/data.txt 190 bytes 2023-06-10 14:26:11
5       warfpk_data/20230610T142554Z-b8888/warfpk_data.csv      8 Kb 2023-06-10 14:26:11
6             warfpk_draws/20230610T142202Z-5bd80/data.txt 200 bytes 2023-06-10 14:23:05
7     warfpk_draws/20230610T142202Z-5bd80/warfpk_draws.csv  281.4 Mb 2023-06-10 14:25:26
8           warfpk_summary/20230610T083635Z-340c1/data.txt 200 bytes 2023-06-10 08:38:44
9 warfpk_summary/20230610T083635Z-340c1/warfpk_summary.csv    1.2 Mb 2023-06-10 08:38:44
```

:::

That works: the files for the diamonds pin have been written, but notice that the `_pins.yaml` manifest file is still 161 bytes in size. It hasn't been update to add an entry for the diamonds data:

```{r}
#| eval: false
gcs_get_object("gs://djnavarro-pins/_pins.yaml") |> 
  yaml::as.yaml() |>
  cat()
```

```
✔ Downloaded and parsed _pins.yaml into R object of class: character
|
  warfpk_data:
  - warfpk_data/20230610T142554Z-b8888/
  warfpk_draws:
  - warfpk_draws/20230610T142202Z-5bd80/
  warfpk_summary:
  - warfpk_summary/20230610T083635Z-340c1/
```

There's a bit of a painful thing that follows because pins doesn't currently have working `pin_list()` method for google cloud storage. This in turn means that I can't currently use `write_board_manifest()` to write a manifest file for my board. That's a huge pain. A little browsing on github issues reassures that the devs are well aware of the problem, and addressing this is indeed on the to-do list. Unfortunately that doesn't solve my problem in the here-and-now, so while I'm waiting I decided to put together some helper functions that will be good enough for my purposes:

```{r}
#| eval: false
pin_list_gcs <- function(board, ...) {
  googleCloudStorageR::gcs_list_objects(bucket = board$bucket)$name |> 
    grep(pattern = "/", x = _, value = TRUE) |> 
    gsub(pattern = "/.*", replacement = "", x = _) |>
    unique()
}
```

Now a little bit of evil, in which I do the thing you should never ever do:

```{r}
#| eval: false
unlockBinding(as.symbol("pin_list.pins_board_gcs"), getNamespace("pins"))
assignInNamespace("pin_list.pins_board_gcs", pin_list_gcs, "pins")
```

Next write an S3 method that allows me to write the manifest file:

```{r}
#| eval: false
write_board_manifest_yaml.pins_board_gcs <- function(board, manifest, ...) {
  temp_file <- withr::local_tempfile()
  yaml::write_yaml(manifest, file = temp_file)
  googleCloudStorageR::gcs_upload(
    file = temp_file, 
    bucket = board$bucket, 
    type = "text/yaml",
    name = "_pins.yaml"
  )
}
```

Now this works:

```{r}
#| eval: false
write_board_manifest_yaml(board, manifest = pins:::make_manifest(board))
```

We can verify that we've written the updated file. For reasons that escape me, this won't work unless you start a new session (and reauthenticate in the process), or specify the `generation` argument to `gcs_get_object()`. Otherwise you just get the old version of the manifest file. Anyway here's the result:

```{r}
#| eval: false
gcs_get_object("gs://djnavarro-pins/_pins.yaml") |> 
  yaml::as.yaml() |>
  cat()
```

```
✔ Downloaded and parsed _pins.yaml into R object of class: character
|
  diamonds:
  - diamonds/20230612T071111Z-c9e9b/
  warfpk_data:
  - warfpk_data/20230610T142554Z-b8888/
  warfpk_draws:
  - warfpk_draws/20230610T142202Z-5bd80/
  warfpk_summary:
  - warfpk_summary/20230610T083635Z-340c1/
```

Similarly, if we now list the contents of the bucket we can see that it's all been updated:

```{r}
#| eval: false
gcs_list_objects(bucket = "gs://djnavarro-pins")
```

::: {.column-page-inset-right}

```
                                                      name      size             updated
1                                               _pins.yaml 206 bytes 2023-06-12 07:39:53
2                 diamonds/20230612T073405Z-c9e9b/data.txt 189 bytes 2023-06-12 07:34:05
3             diamonds/20230612T073405Z-c9e9b/diamonds.csv    2.6 Mb 2023-06-12 07:34:14
4              warfpk_data/20230610T142554Z-b8888/data.txt 190 bytes 2023-06-10 14:26:11
5       warfpk_data/20230610T142554Z-b8888/warfpk_data.csv      8 Kb 2023-06-10 14:26:11
6             warfpk_draws/20230610T142202Z-5bd80/data.txt 200 bytes 2023-06-10 14:23:05
7     warfpk_draws/20230610T142202Z-5bd80/warfpk_draws.csv  281.4 Mb 2023-06-10 14:25:26
8           warfpk_summary/20230610T083635Z-340c1/data.txt 200 bytes 2023-06-10 08:38:44
9 warfpk_summary/20230610T083635Z-340c1/warfpk_summary.csv    1.2 Mb 2023-06-10 08:38:44
```

:::

## The read-only workflow

```{r}
#| echo: false
library(pins)
```

```{r}
read_only_board <- board_url(
  "https://storage.googleapis.com/djnavarro-pins/_pins.yaml"
)
pin_search(read_only_board)
```

```{r}
pin_read(read_only_board, "diamonds") |>
  tibble::as_tibble()
```

## Epilogue

As a general long-term strategy, this workflow is *terrible* and I have no intention whatsoever of relying on it. Under no circumstances is it a good idea to rely on a method that fucks around with the internals of a package. This is only a temporary hack I'm relying on while waiting for the pins package to support google cloud buckets more completely.

