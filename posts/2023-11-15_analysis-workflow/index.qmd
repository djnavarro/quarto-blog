---
title: "analysis-workflow"
description: "This is a subtitle"
date: "2023-11-15"
--- 

<!--------------- my typical setup ----------------->

```{r}
#| label: setup
#| include: false
very_wide <- 500
wide <- 136
narrow <- 76
options(width = narrow)
cache_images <- TRUE
set.seed(1)
```

<!--------------- post begins here ----------------->

## Version control without git

One of the things I've noticed recently is that over the years I've become extremely reliant on git and GitHub. If you're working in the tech sector that's entirely to be expected: those are the standard tools^[Yes I'm aware that there are alternatives to both git and GitHub, some of which are "better" under some definition of the term. But let's be realistic: version control with git/GitHub is the most typical workflow in a typical tech company.] for version control among software developers. However, analysis code is not software, and in my experience it's only a minority of data analysts who use git for version control. When you're working in a large team of analysts, you have to be practical: the project cannot be put on hold while a dozen people go through the painful process of learning git.^[That's assuming they even *want* to learn git, and assuming that organisation policies will permit the use of GitHub. Neither of those two things are necessarily true] It is grossly typical, then, that code collaboration for analysis code relies on tools like SharePoint, Dropbox, etc. Those of us with some experience in software design might groan loudly about the fact these are *not* good tools for collaborating on code,^[Actually, in my experience *everyone* complains about SharePoint. I'm yet to find a single person who uses is and likes it.] but very often you simply have to work with the tools you have.   

Okay, so from the perspective of a practical analyst, what are the problems with collaboration via SharePoint that need to be addressed? 

The biggest problem I've encountered is that "version control" in SharePoint and Dropbox doesn't come with useful metadata. It's really just a history of file snapshots that were taken at apparently-arbitrary moments in time. Compare this to git. In git, every time I make a meaningful change to my code, I *manually* create a commit, and write a short message describing the change. As a consequence, the version history in the git repository tends to contain copies of the code taken at useful moments in time, and every code update has a little message that tells you what changed. These messages are *extremely* useful when trying to trace back the history of the code. Better yet, if you're browsing the history on GitHub you automatically get the "diffs": you can see which lines of code have changed from one moment in time to the next. You can also see the "blame"^[I detest the fact that git uses the word "blame" here. It's an emotionally loaded, pejorative word. The actual word you were looking for, git designers, is "authorship". Using "blame" to attribute authorship of code... well, it is an interesting choice, to say the least.] and find out who wrote a particular piece of code. That makes it possible for me to message the author and ask questions if I don't understand something. 

Without access to those features, it's almost a necessity for analysts to fall back on "low tech" tricks for versioning code.

## Explicit versions, updated frequently

The approach I've started taking has a few features. The big one is that every analysis folder has an `archive` folder, which contains one subfolder per "version" of the code. So 
