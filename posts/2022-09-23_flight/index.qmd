---
title: "How to build an Arrow Flight server"
description: "The waitress thing makes sense, honest"
date: "2022-09-23"
categories: [Apache Arrow, Networking, R, Python]
image: "img/hennie-stander-aWwFbn0ZW6A-unsplash.jpg"
---

<!-- 
rendering from terminal:
  cd ~/GitHub/sites/quarto-blog/posts/2022-09-23_flight
  
-->

<!-- 
cover image: https://unsplash.com/photos/aWwFbn0ZW6A
credit: Hennie Stander
licence: open via unsplash licence
-->

<!--------------- my typical setup ----------------->

```{r setup, include=FALSE}
long_slug <- "2022-08-23_visualising-a-billion-rows"
#renv::use(lockfile = "renv.lock")
```

<!--------------- post begins here ----------------->


## The what and why of Arrow Flight


Let's start with a little background. If I want you to read through a whole ass post on Arrow Flight, I'd better explain why this is worth your while. What exactly *is* Arrow Flight (henceforth just "flight"), and what is it *for*? Why might you as a data scientist or data engineer care about flight?

The central idea behind flight is deceptively simple: it provides a standard protocol for transferring Arrow data over a network. But to understand why this is a Big Forking Deal, you need to have a good sense of what the Arrow ecosystem is all about. For that, I found it suuuuper helpful to go all the way back^[All the way back to October 2019, which is like ancient history by Arrow standards. Sigh. This project moves too damned fast to keep pace with it all.] to the [original announcement of flight by Wes McKinney](https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight/). I'm going to straight up quote Wes here, because his description is really good:^[Shocking, right? Wes McKinney has a really deep understanding of Arrow. What next? Hadley Wickham understands ggplot2?????] 

> **Many people have experienced the pain associated with accessing large datasets over a network.** There are many different transfer protocols and tools for reading datasets from remote data services, such as ODBC and JDBC. Over the last 10 years, file-based data warehousing in formats like CSV, Avro, and Parquet has become popular, but this also presents challenges as raw data must be transferred to local hosts before being deserialized.
>
>The work we have done since the beginning of Apache Arrow holds exciting promise for accelerating data transport in a number of ways. The Arrow columnar format has key features that can help us:
>
- It is an “on-the-wire” representation of tabular data that does not require deserialization on receipt
- Its natural mode is that of “streaming batches”, larger datasets are transported a batch of rows at a time (called “record batches” in Arrow parlance). In this post we will talk about “data streams”, these are sequences of Arrow record batches using the project’s binary protocol
- The format is language-independent and now has library support in 11 languages and counting.
>
> Implementations of standard protocols like ODBC generally implement their own custom on-wire binary protocols that must be marshalled to and from each library’s public interface. The performance of ODBC or JDBC libraries varies greatly from case to case.
>
> **Our design goal for Flight is to create a new protocol for data services that uses the Arrow columnar format as both the over-the-wire data representation as well as the public API presented to developers. In doing so, we reduce or remove the serialization costs associated with data transport and increase the overall efficiency of distributed data systems.** Additionally, two systems that are already using Apache Arrow for other purposes can communicate data to each other with extreme efficiency. [Emphasis added]

The bolded sections here are key. Arrow was originally introduced to provide an efficient and language-agnostic standard for representing tabular data in-memory, but as the project has grown it has necessarily expanded in scope. Storing data in-memory is not super useful if you can't manipulate it, so Arrow now supplies a powerful compute engine (now referred to as Acero) that underpins the arrow package in R and the pyarrow library in Python (and others!). In other words, the compute engine solves a practical data science problem, and solves it in a way that makes the data engineers on the team breath a sigh of relief. 

Flight is analogous to Acero in that way. We live in a networked world (duh) and it is hard to avoid situations where the data to be analysed are stored on a different machine than the one that does the analysis. In earlier posts [LINK] I talked about how to efficiently share access to a data set between *languages* (R and Python were my examples), but it was implicitly assumed throughout that the R process and the Python process were running on the same *machine*. The moment we have processes running on different machines, those tricks don't work anymore... 

... and at this point every data scientist's heart sinks in his, her, or their chest. Am I going to have to roll my own transport protocol? Like, should I just try to email the bloody files over or something???^[Sorry. I'm just cackling at the thought of trying to email 100Gb of data...] The terror sets in. 

Flight is designed to solve this problem. It's not a fancypants protocol with lots of different parts. To paraphrase David Li, one of the developers, 

> it's just a dumb pipe

It exists for one purpose: it makes it super easy to transfer Arrow-formatted data. That's it. It's flexible, and you can build other stuff on top of flight (more on that later), but the design of flight is deliberately simple. It's *meant* to be pretty minimal, so you can "just use it" without having to think too hard or do any of the obnoxious implementation work yourself. 


### Prerequisites

There are a couple of prerequisites for this post. Specifically I'll assume you have the arrow and reticulate packages installed in your R environment, and similarly that your Python environment has pyarrow installed. If you're only interested in the Python side, you probably don't need either of the R packages, but R users do need the pyarrow installation because the R flight implementation builds on pyarrow.



## An R example

The implementation of Arrow Flight varies a little across languages. In this post I'm going to focus on the two languages I use most -- R and Python -- but there's nothing stopping you from using other languages. For example, the book [In-Memory Analytics with Apache Arrow](https://www.packtpub.com/product/in-memory-analytics-with-apache-arrow/9781801071031) by [Matt Topol](https://twitter.com/zeroshade) has worked examples using C++ and Go, in addition to Python. 

For the purposes of this post I'm going to start with R because the arrow package in R exposes a "high-level" interface that will allow us to start using a flight server without having to dive deeply into how it all works. However, as we'll see, there are some limitations to this approach -- not least of which is the fact that the R implementation turns out to secretly be a Python implementation under the hood -- and as the post progresses I'll pivot to Python in order to unpack some of the lower-level functionality. 

To do this I'll need access to the arrow and reticulate packages, and I'll need to make certain that the Python environment is one that has pyarrow installed:

```{r}
library(arrow)
library(reticulate)
use_miniconda("base")
```

### Starting the demo server

Okay, so let's get started by thinking about the simplest possible scenario for using a flight server. In this set up all we want the server to do is to act as a "cache" for Arrow tables. Clients can upload tables to the server, download tables from the server, and so on. That's all we're really trying to accomplish, and happily for us this use case is supported out of the box in R. 

Here's how it works. As I mentioned earlier, R doesn't actually implement the flight protocol itself: it's just a wrapper around the Python tools. What that means is the underlying flight server is actually written in Python, and if we want to start that server running from R we have to call the `load_flight_server()` function that will allow us access to this server from R. Conveniently, the arrow R package comes bundled with a "demo" server that already provides the server side functionality that we want, and I can import it like this:

```{r, filename = "[R code]", eval=FALSE}
server_class <- load_flight_server("demo_flight_server")
```

When I do this, all I've done is obtain access to the relevant Python code. I haven't created a server yet and I haven't started it running either. Create an instance of the "demo server", I call the `DemoFlightServer()` method attached to the `server_class()` object:

```{r, filename = "[R code]", eval=FALSE}
server <- server_class_object$DemoFlightServer(port = 8089)
```

We have now defined a server that, once started, will run on port 8089. The `server` object has a `serve()` method that I can call to start it running:

```{r, filename = "[R code]", eval=FALSE}
server$serve()
```

I've written a short script called [start_demo_server.R](./start_demo_server.R) that bundles all these operations together, and the easiest way to start a server running in its very own R process (i.e., an R session that isn't the one you're currently working in) would be to type this at the terminal:

```{bash, filename="[terminal]", eval=FALSE}
Rscript start_demo_server.R &
```

This will start an R process as a background job that will create a server and start it running. As an alternative, if you're comfortable with using the [callr](https://callr.r-lib.org/) package, you can use `callr::r_bg()` to create a child R process from your current one. The child process will run in the background, and  we can start start the server within that R session without blocking the current one. Here's some code that will do exactly that:

```{r, filename="[R code]", eval=FALSE}
r_process <- callr::r_bg(function() {
  reticulate::use_miniconda("base")  
  demo <- arrow::load_flight_server("demo_flight_server")
  server <- demo$DemoFlightServer(port = 8089)
  server$serve()
})
```

Regardless of what method you've chosen, I'll now assume that the demo server is now running in the background on port 8089.

<br>

### Connecting with a client

Now that I have this server running quietly in the background on port 8089, I can define a flight client in my current R session that can interact with it. To do that, I call  `flight_connect()`:

```{r, filename="[R console]"}
client <- flight_connect(port = 8089)
```

Perhaps unsurprisingly, the R object `client` is a wrapper around a Python flight client. It comes with various methods that implement low-level flight operations, but I'm going to hold off talking about those for a moment because we won't need to use the low-level interface in this initial example.

Let's start by using the client to as ask a simple question: what is stored on the server? The way that data source are conceptualised in Arrow flight is as a set of "flights". Each individual "flight" corresponds to a data stream from which the client can download data. The precise implementation of this idea (e.g., what data structures are stored in a single flight) varies from server to server, but in both examples in this post each flight stores a single Arrow table. 

In any case, to find out what flights are currently available on our server, we can call the `list_flights()` function:

```{r, filename="[R console]"}
list_flights(client)
```

Hm, okay, there's nothing there. That makes sense because I haven't actually uploaded anything to the server yet! Okay, well, let's suppose I want to store a copy of the `airquality` data as an Arrow table on my server. As R users are probably aware, this is a data set that comes bundled with R, but just so we're all on the same page here's the first few rows of the data set:

```{r, filename="[R code]"}
head(airquality)
```

This object is a regular data frame in R, not an Arrow table, and strictly speaking what we want our client to do is send an Arrow table version of this data set to the server. Happily for us, the `flight_put()` function supplied by the arrow package will take care of the conversion for us, and we can cache a copy of the data as an Arrow table on the server in one line of code: 

```{r, filename="[R console]"}
flight_put(client, data = airquality, path = "pollution_data")
```

In this code, the `flight_put()` function uses the `client` object to communicate with the server, it uses the `data` argument to find the local copy of the data set, and the `path` argument is used to provide a name for the data set on the server (i.e., on the server this data set will be called `pollution_data`). Now that we've done the upload, if we try calling `list_flights()` again we get this as the result:

```{r, filename="[R console]"}
list_flights(client)
```

Yay! 

Now, just to prove to you that I'm not cheating, let's check to make sure that there is no object called `pollution_data` stored locally within my R session:^[One of my favourite things about having a quarto blog is that every post is a notebook. It's technically possible to "cheat" by including hidden code chunks that execute different code than that shown in the post, but it's something I do very sparingly and only when there's some weirdness involved. I'm not doing that here. When this post is rendered, it does start a new instance of the demo server in a different R session: every flight server demonstrated here is in fact running in the background so that the post renders, and server side data are all stored by those other processes. There really is no copy of the `pollution_data` object in the R session used to render this post. It's somewhere else, as it bloody well should be.]

```{r, filename="[R console]", error=TRUE}
pollution_data
```

Clearly there is no object called `pollution_data` available in my current R session. That data set is stored only on the server. To get access to the data cached on the server, I use the `flight_get()` function:

```{r, filename="[R console]", message=FALSE}
flight_get(client, "pollution_data")
```

What has just happened is our client contacted the server and downloaded a copy of the Arrow table stored remotely. It works! 

At the very least we have proof-of-concept that we can start a flight server and use it to upload and download data. But there's a lot that hasn't really been explained properly here. The time has come to start digging a little deeper, so we can really get a sense of what's going on under the hood and how this simple example can be extended.

<br>

## The flight protocol

One thing that I like about the flight functionality exposed through `flight_connect()`, `flight_put()`, `flight_get()`, etc is that it operates at a high level of abstraction. In my day-to-day data analysis work I really don't want to spend my time thinking about low-level operations. When I tell R to "put" a data set onto the server I *want* it to happen with one line of code. This high level API is super useful to me on an everyday basis, but it also masks some of the details about how flight works. To give you a sense of what's being hidden, we can take a closer look at the `client` object. Here's a list of some of the methods that are available through the object itself:

``` r
client$do_put()
client$do_get()
client$do_action()
client$list_action()
client$list_flights()
client$get_flight_info()
```

Each of these methods describes a low level operation available to the flight client, and because the R implementation is built on top of the Python version, these are all Python methods that are available in R thanks to the magic of [reticulate](https://rstudio.github.io/reticulate/). What we're seeing exposed here are the actual Arrow flight methods, and it's helpful to unpack things a little so we can see how these methods are related to the functions I've been calling in R. 

As you might expect, the `do_put()` method for the client is very closely related to the `flight_put()` function that I called earlier! However, they aren't the same. The `do_put()` method doesn't actually upload any data, it merely opens a connection to the server, which we can then use to stream data from the client to the server. If you were calling the `do_put()` method directly, you would need to take care of that yourself. But it's tiresome to write that code over and over, so the  `flight_put()` function provides a convenient high-level wrapper that abstracts over all that. 

If you're the analyst working with the data, this is fabulous. But if you're looking to implement your very own flight server, you probably need to understand what these low level operations are. So that's where we're headed next...

<br>

### Unpacking flight_put()

Let's start by taking a look at what happens when we call the R function `flight_put()`. Our goal is to transmit the data to the server, and there's an Arrow Flight method called `do_put()` that can do this for us. Here's how the transaction unfolds:

![](img/do_put.png)

When the client calls `do_put()` it passes two arguments: a "flight descriptor" used to identify the data (more on that later), and the schema for the flight data.^[Optionally, you can also pass a third "options" argument.]  Somewhat simplified, here's the signature on the client side:

``` python
do_put(descriptor, schema)
```

Passing the schema allows the client to create "stream writer" and "stream reader" objects that are returned to the client-side user, *and* passed to the server. On the server-side, the `do_put()` method expects three inputs: the flight descriptor, the writer, and the reader. Here's the signature on the server side: 

``` python
do_put(descriptor, reader, writer)
```

Now that the client and server agree on the description of the data, as well as the streaming reader and writer we can move to step two, in which the client streams the data to the server. The server stores the data along with the appropriate descriptor so that it can be found later, and sends a response to the client. 

<br>

### Unpacking flight_get()

Next let's look at `flight_get()`. When I called this function earlier, it triggered two separate interactions between the client and server. First, the client calls the `get_flight_info()` method, and the server responds with some information about the data source that includes -- among other things -- a "ticket". Now in possession of this ticket, the client can call `do_get()` to request that the server send the data that matches the ticket, which the server then streams. So the whole exchange looks like this:

![](img/do_get.png)

So, in the previous example when I called `flight_get()`, the process looked like this. On the client side, we used the `"pollution_data"` path to construct a descriptor object and the client used `get_flight_info()` to request that information about this "flight" from the server:

``` python
get_flight_info(descriptor)
```

On the server side, once the descriptor is received, a flight info object is constructed. The flight info object consists of five parts: 

- The schema for the data stored by the flight, 
- The descriptor
- A list of one or more endpoints that specify where the data are available for streaming. Each end point includes a location from which to stream, and the associated ticket for that location
- The total number of records (i.e. rows) stored 
- The total number of bytes to be streamed (i.e., the size of the data)

This object is then returned to the client. It may seem like this is overly elaborate: why does the client need this much information if only the ticket is needed? In the example I've used here, it's not really needed, but in a more sophisticated setup this would make it possible to stream in parallel, with different endpoints streaming different subsets of the data.

In any case, once this flight information has been received by the client, we can extract the ticket that we need from the relevant endpoint (there's only one in our case). The client now calls:

``` r
do_get(ticket)
```

This returns a stream reader object that the client can use to receive the stream of data from the server.

<br>

## A Python example

<br> 

### Building a tiny flight server

> |
It's a subtle suffocation, a tiny little fix <br>
They're playing with fire and I'm playing with matchsticks <br>
&nbsp;&nbsp; -- [Sprints](https://music.youtube.com/watch?v=v2u3Yk0b_M0)

Now that we have a basic understanding of what is happening at a lower level, we can build a server. At this point it's handy to switch to Python. R doesn't currently have an independent method to build the server: it's actually a Python flight server under the hood, so we might as well write our server using Python.

```{r, include=FALSE, cache=FALSE}
knitr::read_chunk("tiny_flight.py")
```

```{python imports, filename="tiny_flight.py", eval = FALSE}
```

Define the server class...

```{python tiny-server, filename = "tiny_flight.py", eval = FALSE}
```

Blah blah

Conveniently, the [tiny_flight.py](tiny_flight.py) script is bundled with this post, so I can import it as a module:

```{python, filename="[Python commands]"}
import threading
import tiny_flight as tiny

server = tiny.TinyServer(port = 9001)
thread = threading.Thread(target=lambda: server.serve(), daemon=True)
thread.start()
```

<br>

### A tiny server needs a tiny client

Okay, now we need to define the client:

```{python tiny-client, filename="tiny_flight.py", eval = FALSE}
```

Now let's start our client...


```{python, filename="[Python commands]"}
client = tiny.TinyClient(port = 9001)
```

Let's create some tables on the client side and cache them on the server:

```{python, filename="[Python commands]"}
import pyarrow
mario = pyarrow.table([["Mario", "Luigi", "Peach"]], names=["Character"])
riots = pyarrow.table([["Stonewall", "Comptons", "Mardi Gras"]], names=["Riot"])

client.put_table("mario", mario)
client.put_table("riots", riots)
```

Now let's ask the server to tell us what it's storing:

```{python, filename="Python commands]"}
client.list_tables()
```

Wait, we don't want the mario table anymore:

```{python, filename="Python commands]"}
client.drop_table("mario")
client.list_tables()
```

<br>

## Further down the rabbit hole

A little bit of detail about RPC vs REST, about gRPC and protobuf. Not a lot is needed here, just enough to understand what flight is built on top of


```{r, echo=FALSE, results='hide'}
client$do_action("shutdown")
```


## Where to next?

Compared to the rest of the Apache Arrow project, it's not so easy to find tutorials and documentation about flight. It's still a little piecemeal. With that in mind, here's an annotated reading list that will be helpful if you want to explore flight further:

- [The original announcement of flight](https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight/) by Wes McKinney on the Apache Arrow blog gives a very good overview of the motivation for why flight was introduced. 
- [Data transfer at the speed of flight](https://voltrondata.com/news/data-transfer-at-the-speed-of-flight/) by Tom Drabas, Fernanda Foertter, and David Li. This is a blog post on the Voltron Data blog that provides a concrete example of a working flight server written in Python. The Python code I've discussed in this post is an elaboration of the content in that post. It's a good starting point
- [Apache Arrow Flight: A Primer](https://voltrondata.com/news/apache-arrow-flight-primer/) by David Li and Tom Drabas. This is another blog post on the Voltron Data website. This one doesn't have any working code for you to look at, but it provides a good summary of the technologies that Arrow Flight is built upon. It's a little intense for novices but is pretty handy for intermediate level users who want to take a peek under the hood. 
- [The Python documentation flight vignette](https://arrow.apache.org/docs/python/flight.html) is pretty readable and goes into a moderate amount of detail, but be aware it implicitly assumes some familiarity with remote procedure calls.
- [The Python cookbook for Arrow](https://arrow.apache.org/cookbook/py/flight.html) contains the most thorough worked example I've seen anywhere. It's a little dense for novice users, but it's still the one of the most comprehensive resources I've seen, and the only one that talks about issues like authentication (which I have not discussed at all here!) 
- [The R documentation flight vignette](https://arrow.apache.org/docs/r/articles/flight.html) has a succinct overview of how you can use the high-level interface provided by `flight_put()`, `flight_get()`, etc. What it doesn't do (yet?) is discuss the low-level features. At the moment you won't find a discussion of say `client$do_get()` and how it relates to `flight_get()`.
- Along similar lines there are some examples in the [R cookbook](https://arrow.apache.org/cookbook/r/flight.html), but they are also quite minimal.
- It's not publicly accessible without purchase, but if you're willing to spend some money I thoroughly recommend the chapter on Arrow flight in Matt Topol's book [In-Memory Analytics with Apache Arrow](https://www.packtpub.com/product/in-memory-analytics-with-apache-arrow/9781801071031). I found it really helpful for cementing my own understanding. In addition to the worked examples in  Python, C++, and Go, the chapter provides some historical context for understanding the difference between RPC frameworks and REST frameworks, and is also the only resource I'm aware of that goes into detail about how more sophisticated network architectures are supported by flight.

Additionally, when digging around in source code, I found it handy to take a look at these parts of the code base:

- [Source code for the R demo server](https://github.com/apache/arrow/blob/master/r/inst/demo_flight_server.py)
- [A Python example server](https://github.com/apache/arrow/blob/master/python/examples/flight/server.py)
- [Source code for the pyarrow flight implementation](https://github.com/apache/arrow/blob/master/python/pyarrow/_flight.pyx)

Finally, while neither one is ideal as a place to start, once I started getting the hang of what I was doing, I have found it handy to browse through the [Python flight API reference pages](https://arrow.apache.org/docs/python/api/flight.html), and to occasionally dip into the official [Arrow flight RPC specification](https://arrow.apache.org/docs/format/Flight.html). Regarding the latter, my experience was that the images showing how each of the flight methods operates were handy, and the [comments](https://arrow.apache.org/docs/format/Flight.html#protocol-buffer-definitions) shown in the in the "protocol buffer definitions" are nice because they're maybe the clearest verbal description of what each of the flight methods expects as input and what objects they will return.

Happy hunting! 



<!--------------- appendices go here ----------------->


