---
title: "How to build an Arrow Flight server"
description: "The waitress thing makes sense, honest"
date: "2022-09-23"
categories: [Apache Arrow, Networking, R, Python]
image: "img/hennie-stander-aWwFbn0ZW6A-unsplash.jpg"
---

<!-- 
cover image: https://unsplash.com/photos/aWwFbn0ZW6A
credit: Hennie Stander
licence: open via unsplash licence
-->

<!--------------- my typical setup ----------------->

```{r setup, include=FALSE}
long_slug <- "2022-08-23_visualising-a-billion-rows"
#renv::use(lockfile = "renv.lock")
```

<!--------------- post begins here ----------------->


## Background: The what and why of Arrow Flight

Let's start with a little background. If I want you to read through a whole ass post on Arrow Flight, I'd better explain why this is worth your while. What exactly *is* Arrow Flight (henceforth just "flight"), and what is it *for*? Why might you as a data scientist or data engineer care about flight?

The central idea behind flight is deceptively simple: it provides a standard protocol for transferring Arrow data over a network. But to understand why this is a Big Forking Deal, you need to have a good sense of what the Arrow ecosystem is all about. For that, I found it suuuuper helpful to go all the way back^[All the way back to October 2019, which is like ancient history by Arrow standards. Sigh. This project moves too damned fast to keep pace with it all.] to the [original announcement of flight by Wes McKinney](https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight/). I'm going to straight up quote Wes here, because his description is really good:^[Shocking, right? Wes McKinney has a really deep understanding of Arrow. What next? Hadley Wickham understands ggplot2?????] 

> **Many people have experienced the pain associated with accessing large datasets over a network.** There are many different transfer protocols and tools for reading datasets from remote data services, such as ODBC and JDBC. Over the last 10 years, file-based data warehousing in formats like CSV, Avro, and Parquet has become popular, but this also presents challenges as raw data must be transferred to local hosts before being deserialized.
>
>The work we have done since the beginning of Apache Arrow holds exciting promise for accelerating data transport in a number of ways. The Arrow columnar format has key features that can help us:
>
- It is an “on-the-wire” representation of tabular data that does not require deserialization on receipt
- Its natural mode is that of “streaming batches”, larger datasets are transported a batch of rows at a time (called “record batches” in Arrow parlance). In this post we will talk about “data streams”, these are sequences of Arrow record batches using the project’s binary protocol
- The format is language-independent and now has library support in 11 languages and counting.
>
> Implementations of standard protocols like ODBC generally implement their own custom on-wire binary protocols that must be marshalled to and from each library’s public interface. The performance of ODBC or JDBC libraries varies greatly from case to case.
>
> **Our design goal for Flight is to create a new protocol for data services that uses the Arrow columnar format as both the over-the-wire data representation as well as the public API presented to developers. In doing so, we reduce or remove the serialization costs associated with data transport and increase the overall efficiency of distributed data systems.** Additionally, two systems that are already using Apache Arrow for other purposes can communicate data to each other with extreme efficiency. [Emphasis added]

The bolded sections here are key. Arrow was originally introduced to provide an efficient and language-agnostic standard for representing tabular data in-memory, but as the project has grown it has necessarily expanded in scope. Storing data in-memory is not super useful if you can't manipulate it, so Arrow now supplies a powerful compute engine (now referred to as Acero) that underpins the arrow package in R and the pyarrow library in Python (and others!). In other words, the compute engine solves a practical data science problem, and solves it in a way that makes the data engineers on the team breath a sigh of relief. 

Flight is analogous to Acero in that way. We live in a networked world (duh) and it is hard to avoid situations where the data to be analysed are stored on a different machine than the one that does the analysis. In earlier posts [LINK] I talked about how to efficiently share access to a data set between *languages* (R and Python were my examples), but it was implicitly assumed throughout that the R process and the Python process were running on the same *machine*. The moment we have processes running on different machines, those tricks don't work anymore... 

... and at this point every data scientist's heart sinks in his, her, or their chest. Am I going to have to roll my own transport protocol? Like, should I just try to email the bloody files over or something???^[Sorry. I'm just cackling at the thought of trying to email 100Gb of data...] The terror sets in. 

Flight is designed to solve this problem. It's not a fancypants protocol with lots of different parts. To paraphrase David Li, one of the developers, 

> it's just a dumb pipe

It exists for one purpose: it makes it super easy to transfer Arrow-formatted data. That's it. It's flexible, and you can build other stuff on top of flight (more on that later), but the design of flight is deliberately simple. It's *meant* to be pretty minimal, so you can "just use it" without having to think too hard or do any of the obnoxious implementation work yourself. 

So let's take flight for a spin, yeah? Later on in the post I will talk a little about what's actually happening under the hood, but let's set that aside for the moment...

## A minimal example with R

We'll start with R because the implementation in R operates at a higher level of abstraction than other languages. Makes it easier to understand the basic ideas, but be warned: we'll need to switch languages later. 

## Unpacking the operations

Like I said, the R functions operate at a high level of abstractions. The `flight_put()` function, for example, is called by the client and executes a whole exchange with the server. If we really want to understand flight, we really want to unpack that and look at what's happening at a lower level. 

## Building a flight server in Python

Now that we understand what is happening at a lower level, we can build a server. Here it's handy to switch to Python, because R doesn't currently have an independent method to build the server in R: it's actually a Python flight server under the hood. Our lives will be easier if we just switch to Python here:

## Using our server

Examples...

## Further down the rabbit hole

A little bit of detail about RPC vs REST, about gRPC and protobuf. Not a lot is needed here, just enough to understand what flight is built on top of

## Can we do everything natively in R?

Not sure if I want to write this section but it is tempting





<!--------------- appendices go here ----------------->


