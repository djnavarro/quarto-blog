---
title: "How to build an Arrow Flight server"
description: "The waitress thing makes sense, honest"
date: "2022-09-23"
categories: [Apache Arrow, Networking, R, Python]
image: "img/hennie-stander-aWwFbn0ZW6A-unsplash.jpg"
---

<!-- 
rendering from terminal:
  cd ~/GitHub/sites/quarto-blog/posts/2022-09-23_flight
  
-->

<!-- 
cover image: https://unsplash.com/photos/aWwFbn0ZW6A
credit: Hennie Stander
licence: open via unsplash licence
-->

<!--------------- my typical setup ----------------->

```{r setup, include=FALSE}
long_slug <- "2022-08-23_visualising-a-billion-rows"
#renv::use(lockfile = "renv.lock")
```

<!--------------- post begins here ----------------->


## Background: The what and why of Arrow Flight

> You look nice as, all dressed up <br>
A classy bloke with a half full cup <br>
But I came out just for you <br>
I got you


Let's start with a little background. If I want you to read through a whole ass post on Arrow Flight, I'd better explain why this is worth your while. What exactly *is* Arrow Flight (henceforth just "flight"), and what is it *for*? Why might you as a data scientist or data engineer care about flight?

The central idea behind flight is deceptively simple: it provides a standard protocol for transferring Arrow data over a network. But to understand why this is a Big Forking Deal, you need to have a good sense of what the Arrow ecosystem is all about. For that, I found it suuuuper helpful to go all the way back^[All the way back to October 2019, which is like ancient history by Arrow standards. Sigh. This project moves too damned fast to keep pace with it all.] to the [original announcement of flight by Wes McKinney](https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight/). I'm going to straight up quote Wes here, because his description is really good:^[Shocking, right? Wes McKinney has a really deep understanding of Arrow. What next? Hadley Wickham understands ggplot2?????] 

> **Many people have experienced the pain associated with accessing large datasets over a network.** There are many different transfer protocols and tools for reading datasets from remote data services, such as ODBC and JDBC. Over the last 10 years, file-based data warehousing in formats like CSV, Avro, and Parquet has become popular, but this also presents challenges as raw data must be transferred to local hosts before being deserialized.
>
>The work we have done since the beginning of Apache Arrow holds exciting promise for accelerating data transport in a number of ways. The Arrow columnar format has key features that can help us:
>
- It is an “on-the-wire” representation of tabular data that does not require deserialization on receipt
- Its natural mode is that of “streaming batches”, larger datasets are transported a batch of rows at a time (called “record batches” in Arrow parlance). In this post we will talk about “data streams”, these are sequences of Arrow record batches using the project’s binary protocol
- The format is language-independent and now has library support in 11 languages and counting.
>
> Implementations of standard protocols like ODBC generally implement their own custom on-wire binary protocols that must be marshalled to and from each library’s public interface. The performance of ODBC or JDBC libraries varies greatly from case to case.
>
> **Our design goal for Flight is to create a new protocol for data services that uses the Arrow columnar format as both the over-the-wire data representation as well as the public API presented to developers. In doing so, we reduce or remove the serialization costs associated with data transport and increase the overall efficiency of distributed data systems.** Additionally, two systems that are already using Apache Arrow for other purposes can communicate data to each other with extreme efficiency. [Emphasis added]

The bolded sections here are key. Arrow was originally introduced to provide an efficient and language-agnostic standard for representing tabular data in-memory, but as the project has grown it has necessarily expanded in scope. Storing data in-memory is not super useful if you can't manipulate it, so Arrow now supplies a powerful compute engine (now referred to as Acero) that underpins the arrow package in R and the pyarrow library in Python (and others!). In other words, the compute engine solves a practical data science problem, and solves it in a way that makes the data engineers on the team breath a sigh of relief. 

Flight is analogous to Acero in that way. We live in a networked world (duh) and it is hard to avoid situations where the data to be analysed are stored on a different machine than the one that does the analysis. In earlier posts [LINK] I talked about how to efficiently share access to a data set between *languages* (R and Python were my examples), but it was implicitly assumed throughout that the R process and the Python process were running on the same *machine*. The moment we have processes running on different machines, those tricks don't work anymore... 

... and at this point every data scientist's heart sinks in his, her, or their chest. Am I going to have to roll my own transport protocol? Like, should I just try to email the bloody files over or something???^[Sorry. I'm just cackling at the thought of trying to email 100Gb of data...] The terror sets in. 

Flight is designed to solve this problem. It's not a fancypants protocol with lots of different parts. To paraphrase David Li, one of the developers, 

> it's just a dumb pipe

It exists for one purpose: it makes it super easy to transfer Arrow-formatted data. That's it. It's flexible, and you can build other stuff on top of flight (more on that later), but the design of flight is deliberately simple. It's *meant* to be pretty minimal, so you can "just use it" without having to think too hard or do any of the obnoxious implementation work yourself. 

So let's take flight for a spin, yeah? Later on in the post I will talk a little about what's actually happening under the hood, but let's set that aside for the moment...

### Prerequisites

There are a couple of prerequisites for this post. Specifically I'll assume you have the arrow and reticulate packages installed in your R environment, and similarly that your Python environment has pyarrow installed. If you're only interested in the Python side, you probably don't need either of the R packages, but R users do need the pyarrow installation because the R flight implementation builds on pyarrow.



## A demonstration with R

The implementation status of Arrow Flight varies a little across languages. For example, in his excellent book [In-Memory Analytics with Apache Arrow](https://www.packtpub.com/product/in-memory-analytics-with-apache-arrow/9781801071031), my Voltron Data colleague [Matt Topol](https://twitter.com/zeroshade) shows the reader how to build an Arrow Flight server in Python, C++, and Go. Later in this post I'll talk about Python too, but I'm going to start with R for two reasons. The first is personal: I use R as my primary language, and it's just where my mind goes first. The second is practical: at the moment, the flight implementation in R is a more high-level than the Python implementation -- in fact, the R implementation is built on top of the Python library -- and this makes it a little easier to to show you the big picture. So we'll start in R for simplicity, then pivot to Python when a little more complexity is needed. 

### Starting the demo server

> This feels like it's heaven sent <br>
First impressions made a dent <br>
We come out, and we get rowdy <br>
Listen here, and do not doubt me

Let's get started. If all you want is a simple flight server to handle uploads and downloads of Arrow tables, there is almost nothing you need to do on the server side. The arrow R package comes with a bundled "demo_flight_server" Python module that provides this for you, so all you need to do is start it running. The [start_demo_server.R](./start_demo_server.R) script shown below shows how to do this:

```{r, filename="start_demo_server.R", eval=FALSE}
# load R packages
library(arrow)
library(reticulate)

# use a Python environment with pyarrow
use_miniconda("base")

# load the definition of the "demo flight server" Python class
# that comes bundled with the R arrow package
server_class_object <- load_flight_server("demo_flight_server")

# create an instance of the server and start it running
server_instance <- server_class_object$DemoFlightServer(port = 8089)
server_instance$serve()
```

For the purposes of this post I think it's handy to imagine that I'm executing these R scripts from the terminal. After all, I don't really want to be running the server from my RStudio console! At the terminal I'd type this to start the server as a background job:

```{bash, filename="[terminal]", eval=FALSE}
Rscript start_demo_server.R &
```

### Connecting with a client

> I see you out, I see you out and about <br>
You say you like me but I'm starting to have my doubts

Now that I have this server running quietly in the background on port 8089, I can define a flight client at my regular R console that can interact with it. However, because this is a completely different R session, I'll need to make sure that I have the environment set up correctly: 

```{r, filename="[R console]", message=FALSE}
library(arrow)
library(reticulate)
use_miniconda("base")
```

Great, now I can call `flight_connect()`:

```{r, filename="[R console]"}
client <- flight_connect(port = 8089)
```

So what are the "flights" on this server?

```{r, filename="[R console]"}
list_flights(client)
```

Hm, nothing. Yeah that makes sense because I didn't upload anything to the server! Okay, well, let's suppose I want to store a copy of the `airquality` data as an Arrow Table on my server. The `flight_put()` function will do that for me:

```{r, filename="[R console]"}
flight_put(client, data = airquality, path = "pollution_data")
```

This command creates an Arrow Table from the `airquality` data and uploads it to the server. The `path` argument is, in effect, used as the name of the data set on the server side. So if we *now* try to list the "flights" on our server we get this:

```{r, filename="[R console]"}
list_flights(client)
```

Excellent. Now, just to prove to you that I'm not cheating, let's check to make sure that there is no object called `pollution_data` stored locally within my R session:^[One of my favourite things about having a quarto blog is that every post is a notebook. It's technically possible to "cheat" by including hidden code chunks that execute different code than that shown in the post, but it's something I do very sparingly and only when there's some weirdness involved. I'm not doing that here. When this post is rendered, it does start a new instance of the demo server in a different R session: every flight server demonstrated here is in fact running in the background so that the post renders, and server side data are all stored by those other processes. There really is no copy of the `pollution_data` object in the R session used to render this post. It's somewhere else, as it bloody well should be.]

```{r, filename="[R console]", error=TRUE}
pollution_data
```

Okay, so there's no `pollution_data` object in my R session, but presumably there is some corresponding object on the server, right? I can access that object straightforwardly courtesy of the `flight_get()` function:

```{r, filename="[R console]", message=FALSE}
flight_get(client, "pollution_data")
```

Yay! It works! 

At the very least we have proof-of-concept that we can start a flight server and use it to upload and download data. But there's a lot that hasn't really been explained properly here. The time has come to start digging a little deeper, so we can really get a sense of what's going on under the hood and how this simple example can be extended.

## Unpacking the operations

> I am who I am, and I said what I said <br>
Underneath the red light <br>
I'm drowning in ya

One thing that I like about the flight functionality exposed through `flight_connect()`, `flight_put()`, `flight_get()`, etc is that it operates at a high level of abstraction. In my day-to-day data analysis work I really don't want to spend my time thinking about low-level operations. When I tell R to "put" a data set onto the server I *want* it to happen with one line of code. This high level API is super useful to me on an everyday basis, but it also masks some of the details about how flight works. To give you a sense of what's being hidden, we can take a closer look at the `client` object. Here's a list of some of the methods that are available through the object itself:

``` r
client$do_put()
client$do_get()
client$do_action()
client$list_action()
client$list_flights()
client$get_flight_info()
```

Each of these methods describes a low level operation available to the flight client. As you might expect, the `do_put()` method for the client is very closely related to the `flight_put()` function that I called earlier! However, they aren't the same. The `do_put()` method doesn't actually upload any data, it merely opens a connection to the server, which we can then use to stream data set from the client to the server. If you were calling the `do_put()` method directly, you would need to take care of that yourself. But it's tiresome to write that code over and over, so the  `flight_put()` function provides a convenient high-level wrapper that abstracts over all that. 

If you're the analyst working with the data, this is fabulous. But if you're looking to implement your very own flight server, you probably need to understand what these low level operations are. So that's where we're headed next...

### Unpacking flight_put()

Let's start by taking a look at what happens when we call the R function `flight_put()`. Our goal is to transmit the data to the server, and there's an Arrow Flight method called `do_put()` that can do this for us. Here's how the transaction unfolds:

![](img/do_put.png)

When the client calls `do_put()` it passes two arguments: a "flight descriptor" used to identify the data (more on that later), and the schema for the flight data.^[Optionally, you can also pass a third "options" argument.]  Somewhat simplified, here's the signature on the client side:

``` python
do_put(descriptor, schema)
```

Passing the schema allows the client to create "stream writer" and "stream reader" objects that are returned to the client-side user, *and* passed to the server. On the server-side, the `do_put()` method expects three inputs: the flight descriptor, the writer, and the reader. Here's the signature on the server side: 

``` python
do_put(descriptor, reader, writer)
```

Now that the client and server agree on the description of the data, as well as the streaming reader and writer we can move to step two, in which the client streams the data to the server. The server stores the data along with the appropriate descriptor so that it can be found later, and sends a response to the client. 

### Unpacking flight_get()

Next let's look at `flight_get()`. When I called this function earlier, it triggered two separate interactions between the client and server. First, the client calls the `get_flight_info()` method, and the server responds with some information about the data source that includes -- among other things -- a "ticket". Now in possession of this ticket, the client can call `do_get()` to request that the server send the data that matches the ticket, which the server then streams. So the whole exchange looks like this:

![](img/do_get.png)

So, in the previous example when I called `flight_get()`, the process looked like this. On the client side, we used the `"pollution_data"` path to construct a descriptor object and the client used `get_flight_info()` to request that information about this "flight" from the server:

``` python
get_flight_info(descriptor)
```

On the server side, once the descriptor is received, a flight info object is constructed. The flight info object consists of five parts: 

- The schema for the data stored by the flight, 
- The descriptor
- A list of one or more endpoints that specify where the data are available for streaming. Each end point includes a location from which to stream, and the associated ticket for that location
- The total number of records (i.e. rows) stored 
- The total number of bytes to be streamed (i.e., the size of the data)

This object is then returned to the client. It may seem like this is overly elaborate: why does the client need this much information if only the ticket is needed? In the example I've used here, it's not really needed, but in a more sophisticated setup this would make it possible to stream in parallel, with different endpoints streaming different subsets of the data.

In any case, once this flight information has been received by the client, we can extract the ticket that we need from the relevant endpoint (there's only one in our case). The client now calls:

``` r
do_get(ticket)
```

This returns a stream reader object that the client can use to receive the stream of data from the server.


## Building a flight server in Python

> Little bit classy, bit of a rat <br>
I got my hair tied back <br>
in a plait at the back

Now that we have a basic understanding of what is happening at a lower level, we can build a server. At this point it's handy to switch to Python. R doesn't currently have an independent method to build the server: it's actually a Python flight server under the hood, so we might as well write our server using Python.




## Using our server

Examples...

## Further down the rabbit hole

A little bit of detail about RPC vs REST, about gRPC and protobuf. Not a lot is needed here, just enough to understand what flight is built on top of

## Can we do everything natively in R?

Not sure if I want to write this section but it is tempting


```{r, echo=FALSE}
client$do_action("shutdown")
```

<!--------------- appendices go here ----------------->


