---
title: "How to build an Arrow Flight server"
description: "The waitress thing makes sense, honest"
date: "2022-09-23"
categories: [Apache Arrow, Networking, R, Python]
image: "img/hennie-stander-aWwFbn0ZW6A-unsplash.jpg"
---

<!-- 
rendering from terminal:
  cd ~/GitHub/sites/quarto-blog/posts/2022-09-23_flight
  
-->

<!-- 
cover image: https://unsplash.com/photos/aWwFbn0ZW6A
credit: Hennie Stander
licence: open via unsplash licence
-->

<!--------------- my typical setup ----------------->

```{r setup, include=FALSE}
long_slug <- "2022-08-23_visualising-a-billion-rows"
#renv::use(lockfile = "renv.lock")
```

<!--------------- post begins here ----------------->


## The what and why of Arrow Flight


Let's start with a little background. If I want you to read through a whole ass post on Arrow Flight, I'd better explain why this is worth your while. What exactly *is* Arrow Flight (henceforth just "flight"), and what is it *for*? Why might you as a data scientist or data engineer care about flight?

The central idea behind flight is deceptively simple: it provides a standard protocol for transferring Arrow data over a network. But to understand why this is a Big Forking Deal, you need to have a good sense of what the Arrow ecosystem is all about. For that, I found it suuuuper helpful to go all the way back^[All the way back to October 2019, which is like ancient history by Arrow standards. Sigh. This project moves too damned fast to keep pace with it all.] to the [original announcement of flight by Wes McKinney](https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight/). I'm going to straight up quote Wes here, because his description is really good:^[Shocking, right? Wes McKinney has a really deep understanding of Arrow. What next? Hadley Wickham understands ggplot2?????] 

> **Many people have experienced the pain associated with accessing large datasets over a network.** There are many different transfer protocols and tools for reading datasets from remote data services, such as ODBC and JDBC. Over the last 10 years, file-based data warehousing in formats like CSV, Avro, and Parquet has become popular, but this also presents challenges as raw data must be transferred to local hosts before being deserialized.
>
>The work we have done since the beginning of Apache Arrow holds exciting promise for accelerating data transport in a number of ways. The Arrow columnar format has key features that can help us:
>
- It is an “on-the-wire” representation of tabular data that does not require deserialization on receipt
- Its natural mode is that of “streaming batches”, larger datasets are transported a batch of rows at a time (called “record batches” in Arrow parlance). In this post we will talk about “data streams”, these are sequences of Arrow record batches using the project’s binary protocol
- The format is language-independent and now has library support in 11 languages and counting.
>
> Implementations of standard protocols like ODBC generally implement their own custom on-wire binary protocols that must be marshalled to and from each library’s public interface. The performance of ODBC or JDBC libraries varies greatly from case to case.
>
> **Our design goal for Flight is to create a new protocol for data services that uses the Arrow columnar format as both the over-the-wire data representation as well as the public API presented to developers. In doing so, we reduce or remove the serialization costs associated with data transport and increase the overall efficiency of distributed data systems.** Additionally, two systems that are already using Apache Arrow for other purposes can communicate data to each other with extreme efficiency. [Emphasis added]

The bolded sections here are key. Arrow was originally introduced to provide an efficient and language-agnostic standard for representing tabular data in-memory, but as the project has grown it has necessarily expanded in scope. Storing data in-memory is not super useful if you can't manipulate it, so Arrow now supplies a powerful compute engine (now referred to as Acero) that underpins the arrow package in R and the pyarrow library in Python (and others!). In other words, the compute engine solves a practical data science problem, and solves it in a way that makes the data engineers on the team breath a sigh of relief. 

Flight is analogous to Acero in that way. We live in a networked world (duh) and it is hard to avoid situations where the data to be analysed are stored on a different machine than the one that does the analysis. In earlier posts [LINK] I talked about how to efficiently share access to a data set between *languages* (R and Python were my examples), but it was implicitly assumed throughout that the R process and the Python process were running on the same *machine*. The moment we have processes running on different machines, those tricks don't work anymore... 

... and at this point every data scientist's heart sinks in his, her, or their chest. Am I going to have to roll my own transport protocol? Like, should I just try to email the bloody files over or something???^[Sorry. I'm just cackling at the thought of trying to email 100Gb of data...] The terror sets in. 

Flight is designed to solve this problem. It's not a fancypants protocol with lots of different parts. To paraphrase David Li, one of the developers, 

> it's just a dumb pipe

It exists for one purpose: it makes it super easy to transfer Arrow-formatted data. That's it. It's flexible, and you can build other stuff on top of flight (more on that later), but the design of flight is deliberately simple. It's *meant* to be pretty minimal, so you can "just use it" without having to think too hard or do any of the obnoxious implementation work yourself. 

The implementation status of Arrow Flight varies a little across languages. For example, in his excellent book [In-Memory Analytics with Apache Arrow](https://www.packtpub.com/product/in-memory-analytics-with-apache-arrow/9781801071031), my Voltron Data colleague [Matt Topol](https://twitter.com/zeroshade) shows the reader how to build an Arrow Flight server in Python, C++, and Go; in this post I'm going to talk about R and Python. I'll start with R because it supplies a high-level interface that is easier on newcomers but is less flexible than the Python version. Later, when a little more complexity is needed I'll pivot to Python.


### Prerequisites

There are a couple of prerequisites for this post. Specifically I'll assume you have the arrow and reticulate packages installed in your R environment, and similarly that your Python environment has pyarrow installed. If you're only interested in the Python side, you probably don't need either of the R packages, but R users do need the pyarrow installation because the R flight implementation builds on pyarrow.



## An R example

> | 
Boys will be girls, girls will be boys effortlessly<br>
Come as you are, come as you want completely<br>
&nbsp; &nbsp; --[Bones UK](https://music.youtube.com/watch?v=SshwJdVy5-g)

The R flight implementation is an act of transformation: internally it's a Python server, but we won't let that stop us. 


### Starting the demo server

Let's get started. If all you want is a simple flight server to handle uploads and downloads of Arrow tables, there is almost nothing you need to do on the server side. The arrow R package comes with a bundled "demo_flight_server" Python module that provides this for you, so all you need to do is start it running. The [start_demo_server.R](./start_demo_server.R) script shown below shows how to do this:

```{r, filename="start_demo_server.R", eval=FALSE}
# load R packages
library(arrow)
library(reticulate)

# use a Python environment with pyarrow
use_miniconda("base")

# load the definition of the "demo flight server" Python class
# that comes bundled with the R arrow package
server_class_object <- load_flight_server("demo_flight_server")

# create an instance of the server and start it running
server_instance <- server_class_object$DemoFlightServer(port = 8089)
server_instance$serve()
```

For the purposes of this post I think it's handy to imagine that I'm executing these R scripts from the terminal. After all, I don't really want to be running the server from my RStudio console! At the terminal I'd type this to start the server as a background job:

```{bash, filename="[terminal]", eval=FALSE}
Rscript start_demo_server.R &
```

An alternative approach -- if I wanted to do everything from R -- would be to use the callr package here. You can use `callr::r_bg()` to create a child R process that will run in the background, and start the server within that server without blocking the current R session:

```{r, filename="[R console]", eval=FALSE}
r_process <- callr::r_bg(function() {
  reticulate::use_miniconda("base")  
  demo <- arrow::load_flight_server("demo_flight_server")
  server <- demo$DemoFlightServer(port = 8089)
  server$serve()
})
```

Regardless of what method you've chosen, I'll now assume that the demo server is now running in the background on port 8089.

<br>

### Connecting with a client

>| 
I see you out, I see you out and about <br>
You say you like me but I'm starting to have my doubts <br>
&nbsp;&nbsp; -- [Amyl and the Sniffers](https://music.youtube.com/watch?v=jgqnpr79-Bk)

Now that I have this server running quietly in the background on port 8089, I can define a flight client at my regular R console that can interact with it. However, because this is a completely different R session, I'll need to make sure that I have the environment set up correctly: 

```{r, filename="[R console]", message=FALSE}
library(arrow)
library(reticulate)
use_miniconda("base")
```

Great, now I can call `flight_connect()`:

```{r, filename="[R console]"}
client <- flight_connect(port = 8089)
```

So what are the "flights" on this server?

```{r, filename="[R console]"}
list_flights(client)
```

Hm, nothing. Yeah that makes sense because I didn't upload anything to the server! Okay, well, let's suppose I want to store a copy of the `airquality` data as an Arrow Table on my server. The `flight_put()` function will do that for me:

```{r, filename="[R console]"}
flight_put(client, data = airquality, path = "pollution_data")
```

This command creates an Arrow Table from the `airquality` data and uploads it to the server. The `path` argument is, in effect, used as the name of the data set on the server side. So if we *now* try to list the "flights" on our server we get this:

```{r, filename="[R console]"}
list_flights(client)
```

Excellent. Now, just to prove to you that I'm not cheating, let's check to make sure that there is no object called `pollution_data` stored locally within my R session:^[One of my favourite things about having a quarto blog is that every post is a notebook. It's technically possible to "cheat" by including hidden code chunks that execute different code than that shown in the post, but it's something I do very sparingly and only when there's some weirdness involved. I'm not doing that here. When this post is rendered, it does start a new instance of the demo server in a different R session: every flight server demonstrated here is in fact running in the background so that the post renders, and server side data are all stored by those other processes. There really is no copy of the `pollution_data` object in the R session used to render this post. It's somewhere else, as it bloody well should be.]

```{r, filename="[R console]", error=TRUE}
pollution_data
```

Okay, so there's no `pollution_data` object in my R session, but presumably there is some corresponding object on the server, right? I can access that object straightforwardly courtesy of the `flight_get()` function:

```{r, filename="[R console]", message=FALSE}
flight_get(client, "pollution_data")
```

Yay! It works! 

At the very least we have proof-of-concept that we can start a flight server and use it to upload and download data. But there's a lot that hasn't really been explained properly here. The time has come to start digging a little deeper, so we can really get a sense of what's going on under the hood and how this simple example can be extended.

<br>

## The flight protocol

One thing that I like about the flight functionality exposed through `flight_connect()`, `flight_put()`, `flight_get()`, etc is that it operates at a high level of abstraction. In my day-to-day data analysis work I really don't want to spend my time thinking about low-level operations. When I tell R to "put" a data set onto the server I *want* it to happen with one line of code. This high level API is super useful to me on an everyday basis, but it also masks some of the details about how flight works. To give you a sense of what's being hidden, we can take a closer look at the `client` object. Here's a list of some of the methods that are available through the object itself:

``` r
client$do_put()
client$do_get()
client$do_action()
client$list_action()
client$list_flights()
client$get_flight_info()
```

Each of these methods describes a low level operation available to the flight client. As you might expect, the `do_put()` method for the client is very closely related to the `flight_put()` function that I called earlier! However, they aren't the same. The `do_put()` method doesn't actually upload any data, it merely opens a connection to the server, which we can then use to stream data set from the client to the server. If you were calling the `do_put()` method directly, you would need to take care of that yourself. But it's tiresome to write that code over and over, so the  `flight_put()` function provides a convenient high-level wrapper that abstracts over all that. 

If you're the analyst working with the data, this is fabulous. But if you're looking to implement your very own flight server, you probably need to understand what these low level operations are. So that's where we're headed next...

<br>

### Unpacking flight_put()

Let's start by taking a look at what happens when we call the R function `flight_put()`. Our goal is to transmit the data to the server, and there's an Arrow Flight method called `do_put()` that can do this for us. Here's how the transaction unfolds:

![](img/do_put.png)

When the client calls `do_put()` it passes two arguments: a "flight descriptor" used to identify the data (more on that later), and the schema for the flight data.^[Optionally, you can also pass a third "options" argument.]  Somewhat simplified, here's the signature on the client side:

``` python
do_put(descriptor, schema)
```

Passing the schema allows the client to create "stream writer" and "stream reader" objects that are returned to the client-side user, *and* passed to the server. On the server-side, the `do_put()` method expects three inputs: the flight descriptor, the writer, and the reader. Here's the signature on the server side: 

``` python
do_put(descriptor, reader, writer)
```

Now that the client and server agree on the description of the data, as well as the streaming reader and writer we can move to step two, in which the client streams the data to the server. The server stores the data along with the appropriate descriptor so that it can be found later, and sends a response to the client. 

<br>

### Unpacking flight_get()

Next let's look at `flight_get()`. When I called this function earlier, it triggered two separate interactions between the client and server. First, the client calls the `get_flight_info()` method, and the server responds with some information about the data source that includes -- among other things -- a "ticket". Now in possession of this ticket, the client can call `do_get()` to request that the server send the data that matches the ticket, which the server then streams. So the whole exchange looks like this:

![](img/do_get.png)

So, in the previous example when I called `flight_get()`, the process looked like this. On the client side, we used the `"pollution_data"` path to construct a descriptor object and the client used `get_flight_info()` to request that information about this "flight" from the server:

``` python
get_flight_info(descriptor)
```

On the server side, once the descriptor is received, a flight info object is constructed. The flight info object consists of five parts: 

- The schema for the data stored by the flight, 
- The descriptor
- A list of one or more endpoints that specify where the data are available for streaming. Each end point includes a location from which to stream, and the associated ticket for that location
- The total number of records (i.e. rows) stored 
- The total number of bytes to be streamed (i.e., the size of the data)

This object is then returned to the client. It may seem like this is overly elaborate: why does the client need this much information if only the ticket is needed? In the example I've used here, it's not really needed, but in a more sophisticated setup this would make it possible to stream in parallel, with different endpoints streaming different subsets of the data.

In any case, once this flight information has been received by the client, we can extract the ticket that we need from the relevant endpoint (there's only one in our case). The client now calls:

``` r
do_get(ticket)
```

This returns a stream reader object that the client can use to receive the stream of data from the server.

<br>

## A Python example

<br> 

### Building a tiny flight server

> |
It's a subtle suffocation, a tiny little fix <br>
They're playing with fire and I'm playing with matchsticks <br>
&nbsp;&nbsp; -- [Sprints](https://music.youtube.com/watch?v=v2u3Yk0b_M0)

Now that we have a basic understanding of what is happening at a lower level, we can build a server. At this point it's handy to switch to Python. R doesn't currently have an independent method to build the server: it's actually a Python flight server under the hood, so we might as well write our server using Python.

```{r, include=FALSE, cache=FALSE}
knitr::read_chunk("tiny_flight.py")
```

```{python imports, filename="tiny_flight.py", eval = FALSE}
```

Define the server class...

```{python tiny-server, filename = "tiny_flight.py", eval = FALSE}
```

Blah blah

Conveniently, the [tiny_flight.py](tiny_flight.py) script is bundled with this post, so I can import it as a module:

```{python, filename="[Python commands]"}
import threading
import tiny_flight as tiny

server = tiny.TinyServer(port = 9001)
thread = threading.Thread(target=lambda: server.serve(), daemon=True)
thread.start()
```

<br>

### A tiny server needs a tiny client

Okay, now we need to define the client:

```{python tiny-client, filename="tiny_flight.py", eval = FALSE}
```

Now let's start our client...


```{python, filename="[Python commands]"}
client = tiny.TinyClient(port = 9001)
```

Let's create some tables on the client side and cache them on the server:

```{python, filename="[Python commands]"}
import pyarrow
mario = pyarrow.table([["Mario", "Luigi", "Peach"]], names=["Character"])
riots = pyarrow.table([["Stonewall", "Comptons", "Mardi Gras"]], names=["Riot"])

client.put_table("mario", mario)
client.put_table("riots", riots)
```

Now let's ask the server to tell us what it's storing:

```{python, filename="Python commands]"}
client.list_tables()
```

Wait, we don't want the mario table anymore:

```{python, filename="Python commands]"}
client.drop_table("mario")
client.list_tables()
```

<br>

## Further down the rabbit hole

A little bit of detail about RPC vs REST, about gRPC and protobuf. Not a lot is needed here, just enough to understand what flight is built on top of


```{r, echo=FALSE, results='hide'}
client$do_action("shutdown")
```


## Where to next?

Compared to the rest of the Apache Arrow project, it's not so easy to find tutorials and documentation about flight. It's still a little piecemeal. With that in mind, here's an annotated reading list that will be helpful if you want to explore flight further:

- [The original announcement of flight](https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight/) by Wes McKinney on the Apache Arrow blog gives a very good overview of the motivation for why flight was introduced. 
- [Data transfer at the speed of flight](https://voltrondata.com/news/data-transfer-at-the-speed-of-flight/) by Tom Drabas, Fernanda Foertter, and David Li. This is a blog post on the Voltron Data blog that provides a concrete example of a working flight server written in Python. The Python code I've discussed in this post is an elaboration of the content in that post. It's a good starting point
- [Apache Arrow Flight: A Primer](https://voltrondata.com/news/apache-arrow-flight-primer/) by David Li and Tom Drabas. This is another blog post on the Voltron Data website. This one doesn't have any working code for you to look at, but it provides a good summary of the technologies that Arrow Flight is built upon. It's a little intense for novices but is pretty handy for intermediate level users who want to take a peek under the hood. 
- [The Python documentation flight vignette](https://arrow.apache.org/docs/python/flight.html) is pretty readable and goes into a moderate amount of detail, but be aware it implicitly assumes some familiarity with remote procedure calls.
- [The Python cookbook for Arrow](https://arrow.apache.org/cookbook/py/flight.html) contains the most thorough worked example I've seen anywhere. It's a little dense for novice users, but it's still the one of the most comprehensive resources I've seen, and the only one that talks about issues like authentication (which I have not discussed at all here!) 
- [The R documentation flight vignette](https://arrow.apache.org/docs/r/articles/flight.html) has a succinct overview of how you can use the high-level interface provided by `flight_put()`, `flight_get()`, etc. What it doesn't do (yet?) is discuss the low-level features. At the moment you won't find a discussion of say `client$do_get()` and how it relates to `flight_get()`.
- Along similar lines there are some examples in the [R cookbook](https://arrow.apache.org/cookbook/r/flight.html), but they are also quite minimal.
- It's not publicly accessible without purchase, but if you're willing to spend some money I thoroughly recommend the chapter on Arrow flight in Matt Topol's book [In-Memory Analytics with Apache Arrow](https://www.packtpub.com/product/in-memory-analytics-with-apache-arrow/9781801071031). I found it really helpful for cementing my own understanding. In addition to the worked examples in  Python, C++, and Go, the chapter provides some historical context for understanding the difference between RPC frameworks and REST frameworks, and is also the only resource I'm aware of that goes into detail about how more sophisticated network architectures are supported by flight.

Additionally, when digging around in source code, I found it handy to take a look at these parts of the code base:

- [Source code for the R demo server](https://github.com/apache/arrow/blob/master/r/inst/demo_flight_server.py)
- [A Python example server](https://github.com/apache/arrow/blob/master/python/examples/flight/server.py)
- [Source code for the pyarrow flight implementation](https://github.com/apache/arrow/blob/master/python/pyarrow/_flight.pyx)

Finally, while neither one is ideal as a place to start, once I started getting the hang of what I was doing, I have found it handy to browse through the [Python flight API reference pages](https://arrow.apache.org/docs/python/api/flight.html), and to occasionally dip into the official [Arrow flight RPC specification](https://arrow.apache.org/docs/format/Flight.html). Regarding the latter, my experience was that the images showing how each of the flight methods operates were handy, and the [comments](https://arrow.apache.org/docs/format/Flight.html#protocol-buffer-definitions) shown in the in the "protocol buffer definitions" are nice because they're maybe the clearest verbal description of what each of the flight methods expects as input and what objects they will return.

Happy hunting! 



<!--------------- appendices go here ----------------->


