---
title: "An ODE by Stan"
author:
  - name: Danielle Navarro
    url: https://djnavarro.net
    affiliation: I'm on smoko
    affiliation-url: https://www.youtube.com/watch?v=j58V2vC9EPc
    orcid: 0000-0001-7648-6578
description: "Yeah yeah"
date: "2023-05-11"
categories: [Statistics, Pharmacokinetics]
image: stan_logo.png
---

<!--------------- my typical setup ----------------->

```{r setup, include=FALSE}
wide <- 136
narrow <- 76
options(width = narrow)
cache_images <- TRUE
set.seed(1)
```

<!--------------- post begins here ----------------->

For someone who has spent most of her professional life as a Bayesian statistician, it's strange to admit that I'm only moderately experienced with Stan. My early work in Bayesian modelling involved some Laplace approximations to Bayes factors, MCMC samplers for posterior sampling, and in a few cases I would even resort to using BIC. I hand-coded everything myself, which was super helpful for understanding the mechanics underpinning the statistical inference, but terribly inefficient. When I did start using a domain-specific language for my probabilistic inference I mostly used JAGS.^[I did try WinBUGS briefly but it was a bit awkward since I wasn't a Windows user even then.] Eventually I started hearing the whispers... 

"Have you heard the good news about [Hamiltonian Monte Carlo](https://arxiv.org/pdf/1111.4246.pdf)?" the Stan believers would ask me.

With some regret I would have to reply that I was saddled with [latent discrete parameters](https://mc-stan.org/docs/2_32/stan-users-guide/latent-discrete.html) for theoretical reasons,^[Yes, I know that sometimes there are tricks to work around them. It was more effort than it was worth.] and because the Stan stans are in fact lovely humans they would all express their sympathies, offer their thoughts and prayers, and think to themselves "there but for the grace of God go I".

Long story short, it's taken me a long time to be in a position to learn Stan.

Prolonged unemployment has a silver lining, I suppose.^[I'd offer my thanks to those responsible, but gratitude isn't really the strongest emotion I feel after several months of being bored out of my mind with nothing to do.]

## Setting up

The first thing I have to do is go through the installation process. Yes, I have used Stan before, but that was a few laptops ago and things have changed a little since then. One happy little development from my perspective is that R users now have multiple options for interacting with Stan. The last time I used Stan the accepted method was to use the RStan package,^[There's also many packages for specific modelling frameworks like brms and rstanarm but for the purposes of this post I'm only interested in packages that supply a  general-purpose interface to Stan from R.] and... there's absolutely nothing wrong with RStan. It's a great package. Really. It's just... there's a lot going on there, you know? Lots of bells and whistles. It's powerful. It makes my head hurt. 

Also, I can't get the bloody thing to install on my Ubuntu box. Fucked if I know why.

Fortunately, nowadays there is also [CmdStanR](https://mc-stan.org/cmdstanr/), a lightweight interface to Stan. It suits my style of thinking nicely because it provides [R6](https://r6.r-lib.org/) classes with methods that interact more or less directly with Stan. It does mean that you have to work harder to finesse the outputs, but I'm okay with that.^[Please please please do not take any of this as a general purpose statement: I cordially dislike the "I am better than you because my code is closer to bare metal than yours" thing. Personally I think it's kinda dickish behaviour. In this particular instance I find the lightweight interface helps me think, and I prefer it over all the convenience tooling provided by RStan because I need that clarity right now. Nothing more nor less than that is intended.] As is usually the case with the Stan folks, the documentation is really nice so I won't bother talking about the installation process. Suffice to say I've got the package working, so now I'll load it:

```{r}
library(cmdstanr)
```

The only finicky thing to talk about here lies in the fact that I'm doing all this in the context of a quarto blog post, and specifically a post that is using the knitr engine to execute the code chunks. By default, if knitr encounters a code chunk tagged as the `stan` language it will look for the RStan package to do the work.^[I mean, technically the real work of compiling the Stan code into an executable and then running the sampler is all being done by Stan itself and has sweet fuck all to do with any R package, but knitr has no way of talking to Stan, so it has to rely on an R package to do that... and by default it asks RStan.] That's not going to work for me since I don't actually have RStan installed on my machine. Thankfully the cmdstanr package makes this easy for me to fix:

```{r}
register_knitr_engine()
```

Now that this is done, quarto/knitr will use cmdstanr to handle all the stan code included below.


## The context: pharmacokinetic modelling

Next, I need a toy problem to work with. In my [last post](https://blog.djnavarro.net/posts/2023-04-26_non-compartmental-analysis/) I'd started teaching myself some pharmacokinetic modelling -- statistical analysis of drug concentrations over time -- and I'll continue that line of thinking here. In that post I wrote about [noncompartmental analysis](https://en.wikipedia.org/wiki/Pharmacokinetics#Noncompartmental_analysis) (NCA), a method for analysing pharmacokinetic data without making strong assumptions about the dynamics underpinning the biological processes of drug absorption and elimination, or the statistical properties of the measurement. NCA has its uses, but often it helps to have a model with a little structure to it. 

In compartmental modelling, the analyst adopts a simplified model of (the relevant aspects of) the body as comprised of a number of distinct "compartments" that the drug can flow between. A two-compartment model might suppose that in addition to a "central" compartment that comprises systemic circulation, there is also a "peripheral" compartment where drug concentrations accrue in other bodily tissues. The model would thus include some assumptions about the dynamics that describe how the drug is absorbed (from whatever delivery mechanism is used) into (probably) the central compartment, and how it is eliminated from (probably) the central compartment. It would also need dynamics to describe how the drug moves from the central to peripheral compartment, and vice versa. These assumptions form the **structural** component of the compartmental model.

In addition to all this, a compartmental model needs to make **statistical** assumptions. The structure of the model describes how drug concentrations change over time, but in addition to that we might need a model that describes measurement error, variation among individuals, and covariates that affect the processes. 

In other words, it's really cool. 

## A very simple one-compartment model

Okay let's start super simple. We'll have a one-compartment model, and we'll assume bolus intravenous administration.^[Or, to those of us who don't speak the language, the drug is injected into the bloodstream in a single dose.] That's convenient because we don't have to have a model for the absorption process: at time zero the entire dose goes straight into systemic circulation. Assuming we know both the dose $D$ in milligrams and the volume of distribution^[Not quite the same as the total amount of blood plasma, as I understand it, but approximately the same idea.] $V_d$, then the drug concentration in the first (and only) compartment $C(t)$ at time $t=0$ is given by $C(0) = D/V_d$. That's the only thing we need to consider on the absorption (or "influx") side.

On the elimination (or "efflux") side, there are a number of possible dynamical models we could consider. One of the simplest models assumes that the body is able to "clear" a fixed volume of blood of the drug per unit time. If this clearance rate is constant, some constant proportion $k$ of the current drug concentration will be eliminated during each such time interval. Expressed as a differential equation this gives us:

$$
\frac{dC(t)}{dt} = - kC(t)
$$
Unlike many differential equations, this one is easy to solve^[So easy even I could do it.] and yields an exponential concentration-time curve:

$$
C(t) = C(0) e^{-kt}
$$

That completes the structural side to our model. Now the statistical side. Again, we'll keep it simple: I'm going to assume independent and identically normally distributed errors, no matter how unlikely that is in real life. Reflecting the fact that from a statistics point of view we're now talking about a discrete set of time points and discrete set of measured drug concentrations, I'll refer to the $n$ time points as $t_1, t_2, \ldots, t_n$ and the corresponding observed concentrations as $c_1, c_2, \ldots, c_n$. In this notation our statistical model is expressed:

$$
c_i = c_0 e^{-kt_i} + \epsilon_i 
$$

where

$$
\epsilon_i \sim \mbox{Normal}(0, \sigma)
$$

Our model therefore has two unknowns, the scale parameter $\sigma$ and the elimination rate parameter $k$. Since we are being Bayesians for the purposes of this post I'll place some priors over these parameters. However, since we are also being *lazy* Bayesians for the purposes of this post I'm not even going to pretend I've thought much about these priors. I've made them up because my actual goal here is to familiarise myself with the mechanics of pharmacokinetic modelling in Stan. The real world practicalities -- critically important though they are -- can wait! Anyway, some arbitrarily chosen priors:

$$
\begin{array}{rcl}
\sigma & \sim & \mbox{Cauchy}(0, 5) \\
k & \sim & \mbox{Exponential}(1)
\end{array}
$$

Again, I cannot stress this enough: I literally did not think *at all* about these choices. Never ever adopt such an appalling practice in real life, boys and girls and enby kids!

### Implementation in Stan

Moving along, let's have a look at how this model would be implemented in Stan. The code for the model is shown below, and -- in case you're not familiar with Stan code -- I'll quickly outline the structure. Stan is a declarative language, not an imperative one: you specify the model, it takes care of the inference. Your code is an abstract description of the model, not a sequence of instructions. In my code below, you can see it's organised into three blocks:

- The **data** block defines quantities that the user needs to supply. Some of those correspond to empirical data like concentrations, others are design variables for the study like measurement times.
- The **parameters** block defines quantities over which inference must be performed. In this case that's $k$ and $\sigma$.
- The **model** block specifies the model itself, which in this case includes both the structural and statistical components to our model, and does not distinguish between "likelihood" and "prior". From the Stan point of view Bayesian inference it's not really about "priors and likelihood" it's more of a Doctor Who style "modelly-wobbelly joint probability distribution" kind of thing. 

Anyway here it is:

```{stan}
#| filename: bolus
#| output.var: "bolus"
#| code-line-numbers: true
data {
  int<lower=1> n_obs;
  real<lower=0> dose;
  real<lower=0> vol_d;
  vector[n_obs] t_obs;
  vector<lower=0>[n_obs] c_obs;
}

parameters {
  real<lower=0.01> sigma;
  real<lower=0.01> k;
  real<lower=1, upper=12> vol_d_true;
}

model {
  k ~ normal(0, 5) T[0.01, ];
  sigma ~ normal(0, 1) T[0.01, ];
  vol_d_true ~ normal(vol_d, 1);
  c_obs ~ normal(dose / vol_d_true * exp(-k * t_obs), sigma);
}
```

Some additional things to note:

- Stan is strongly typed with (for example) `int` and `real` scalar types, and `vector` types containing multiple reals.
- Variable declarations allow you to specify lower and upper allowable values for variables. It is always good to include those if you know them
- There's some fanciness going on under the hood in how Stan "thinks about" probability distributions in terms of log-probability functions, but I'm glossing over that because Now Is Not The Time
- I've set my lower bounds on the parameters to "something very close to zero" rather than actually zero because (a) those values are wildly implausible anyway, but (b) if the sampler tries to get too close to zero the log probability goes batshit and numerical chaos ensues

Perhaps more important from the perspective of this post, here's the important bit of quarto syntax I used when defining the code chunk above. When I defined the code chunk I specified the `output.var` option by including the following line in the yaml header to the chunk:

``` r
#| output.var: "bolus"
```

By specifying `output.var: "bolus"` I've ensured that when the quarto document is rendered there is a model object called `bolus` available in the R session. It's essentially equivalent to having the code above saved to a file called `bolus.stan` and then calling the cmdstanr function `cmdstan_model()` to compile it to C++ with the assistance of Stan:

``` r
bolus <- cmdstan_model("bolus.stan")
```

For future Stan models I'll just print the name of the output variable at the top of the code chunk so that you can tell which R variable corresponds to which Stan model. 

In any case let's take a look at our `bolus` object. Printing the object yields sensible, if not exciting, output: it shows you the source code for the underlying model: 

```{r}
bolus
```

Perhaps more helpfully for our purposes, it's useful to know that this is an object of class [`CmdStanModel`](https://mc-stan.org/cmdstanr/reference/CmdStanModel.html), and if you take a look at the documentation on the linked page, you'll find a description of the methods available for such objects. There are quite a few possibilities, but a few of particular interest from a statistical perspective are:^[I'm adopting the `$method()` convention here sometimes used when discussing R6 classes to be clear that we're talking about encapsulated OOP in which methods belong to objects (as is typical in many programming languages) rather than functional OOP in which methods belong to generic functions (as appears in the S3 and S4 systems in R, for instance).]

- [`$sample()`](https://mc-stan.org/cmdstanr/reference/model-method-sample.html) calls the posterior sampling method implemented by Stan on the model
- [`$variational()`](https://mc-stan.org/cmdstanr/reference/model-method-variational.html) calls the variational Bayes algorithms implemented by Stan on the model
- [`$optimize()`](https://mc-stan.org/cmdstanr/reference/model-method-optimize.html) estimates the posterior mode

For the purposes of this post I'll use the `$sample()` method, and in order to call it on my `bolus` object I'll need to specify some data to pass from R to the compiled Stan model. These are passed as a list:

The dose is presumably known precisely, but the volume of systemic distribution is likely an estimate based on the assumption that [blood volume](https://en.wikipedia.org/wiki/Blood_volume) is about 7-8% of body mass, so we'll assume 


```{r}
bolus_data <- list(
  dose = 50,
  vol_d = 5.5,
  n_obs = 6,
  t_obs = c(0.1, 0.5, 1.0, 1.5, 2.0, 3.0),
  c_obs = c(8.4, 3.1, 1.9, 0.6, 0.2, 0.02)
)
```

To quickly visualise these "observed data", I'll organise the relevant variables into a data frame and draw a pretty little scatterplot with ggplot2: 

```{r}
library(ggplot2)
df <- data.frame(
  time = bolus_data$t_obs,
  conc = bolus_data$c_obs
)
ggplot(df, aes(time, conc)) + 
  geom_point(size = 2) + 
  labs(x = "time", y = "concentration")
```

Delightful, truly. So neat. So clean. So obviously, obviously fictitious.

As a Bayesian^[Note for young people: This was the pre-2023 equivalent of saying "As an AI language model..."] that has observed the data, what I want to compute is the joint posterior distribution over my parameters $k$ and $\sigma$. Or, since this has been an unrealistic expectation ever since the death of the [cult of conjugacy](https://en.wikipedia.org/wiki/Conjugate_prior),^[I can still remember quite viscerally the moment that conjugacy died for me as a potentially useful statistical guideline: it was the day I learned that  a Dirichlet Process prior is conjugate to i.i.d. sampling from an arbitrary(ish) unknown distribution. DP priors are... well, I don't want to say "worthless" because I try to see the good in things, but having worked with them a lot over the years I am yet to discover a situation where the thing I actually want is a Dirichlet Process. It's one of those interesting inductive cases about the projectibility of different properties. What "should" have happened is that the DP-conjugacy property made me more willing to use the DP. Instead, what "actually" happened is that learning about DP-conjugacy made me less willing to trust conjugacy. I *knew* in my bones that the DP was useless, so I revised my belief about conjugacy. Someone really should come up with a formal language to describe this kind of belief revision... I imagine it would be quite handy.] what I'll settle for are samples from that joint posterior that I can use to numerically estimate whatever it is that I'm interested in. To do this for our `bolus` model with the help of cmdstanr, we call `bolus$sample()`:

```{r}
bolus_fitted <- bolus$sample(
  data = bolus_data, 
  seed = 451, 
  chains = 4,
  refresh = 1000
)
```

This is an object of class [CmdStanMCMC](https://mc-stan.org/cmdstanr/reference/CmdStanMCMC.html) and again you can look at the linked page to see what methods are defined for it. I'll keep things simple for now and call the [`$summary()`](https://mc-stan.org/cmdstanr/reference/fit-method-summary.html) method, which returns a tibble containing summary statistics associated with the MCMC chains:

```{r}
bolus_fitted$summary()
```

Evidently the estimated posterior mean for the elimination rate $k$ is 1.79, with a 90% [credible interval](https://en.wikipedia.org/wiki/Credible_interval)^[This is "of course" (see next footnote) an equal-tailed interval rather than a highest density interval.] ^[In technical writing, the term "of course" is an expression that used to denote "something that the author is painfully aware of and some subset of the readership is equally exhausted with, and none of those people really want to talk or hear about for the rest of their living days, but is entirely unknown and a source of total confusion to another subset of the readership who have no idea why this should be obvious because in truth it absolutely is not obvious". As such it should, of course, be used with caution and with a healthy dose of self-deprecation and rolling-of-the-eyes. It is in this latter spirit that my use of the term is intended: I simply do not wish to devote any more of my life to thinking about the different kinds of credible intervals.] of [1.43, 2.19]. Similarly, the standard deviation of the measurement error $\sigma$ is estimated to have mean 0.635 and 90% interval [0.34, 1.15]. 

If we wanted to we could take this a little further by pulling out the posterior samples themselves using the [`$draws()`](https://mc-stan.org/cmdstanr/reference/fit-method-draws.html) method. Internally this method relies on the [posterior](https://mc-stan.org/posterior/) package, and supports any of the output formats allowed by that package. In this case I'll have it return a tibble because I like tibbles:

```{r}
bolus_samples <- bolus_fitted$draws(format = "draws_df")
bolus_samples
```

You could then go on to do whatever you like with these samples but I have other fish to fry so I'm going to move on.

### Generated quantities

A slight extension:

```{stan}
#| filename: bolus2
#| output.var: "bolus2"
#| code-line-numbers: true
data {
  int<lower=1> n_obs;
  int<lower=1> n_fit;
  real<lower=0> dose;
  real<lower=0> vol_d;
  vector[n_obs] t_obs;
  vector<lower=0>[n_obs] c_obs;
  vector[n_fit] t_fit;
}

parameters {
  real<lower=0.01> sigma;
  real<lower=0.01> k;
  real<lower=1, upper=12> vol_d_true;
}

model {
  k ~ normal(0, 5) T[0.01, ]; 
  sigma ~ normal(0, 1) T[0.01, ];
  vol_d_true ~ normal(vol_d, 1);
  c_obs ~ normal(dose / vol_d_true * exp(-k * t_obs), sigma);
}

generated quantities {
  vector<lower=0>[n_fit] c_fit = dose / vol_d_true * exp(-k * t_fit);
}
```

Data for this version of the model needs to specify the times for which we want to generate fitted curves:

```{r}
t_fit <- seq(0, 3, 0.05)
bolus2_data <- list(
  n_obs = 6,
  n_fit = length(t_fit),
  dose = 50,
  vol_d = 5.5,
  t_obs = c(0.1, 0.5, 1.0, 1.5, 2.0, 3.0),
  c_obs = c(8.4, 3.1, 1.9, 0.6, 0.2, 0.02),
  t_fit = t_fit
)
```

Fit the model:

```{r}
bolus2_fitted <- bolus2$sample(
  data = bolus2_data, 
  seed = 123, 
  chains = 4,
  refresh = 1000
)
```

Generate quantities:

```{r}
bolus2_generated <- bolus2$generate_quantities(
  fitted_params = bolus2_fitted,
  data = bolus2_data,
  seed = 666
)
```

This is a [CmdStanGQ](https://mc-stan.org/cmdstanr/reference/CmdStanGQ.html) object, and again it has `$draws()` and `$summary()` methods. For our purposes the `$summary()` method will suffice as it returns a tibble containing the thing I want to plot:

```{r}
bolus2_summary <- bolus2_generated$summary()
bolus2_summary
```

I'll add a column specifying the actual times:

```{r}
bolus2_summary$time <- bolus2_data$t_fit
bolus2_summary
```

Now draw the plot I really want:

```{r}
dat <- data.frame(t_obs = bolus2_data$t_obs, c_obs = bolus2_data$c_obs)
ggplot(bolus2_summary) + 
  geom_ribbon(aes(time, ymin = q5, ymax = q95), fill = "grey70") +
  geom_line(aes(time, mean)) + 
  geom_point(aes(t_obs, c_obs), data = dat, size = 2) + 
  labs(x = "time", y = "concentration")
```

I'm still learning the ropes as a pharmacometrician -- and acutely aware that there are disciplinary norms I don't yet understand -- but at least as a mathematical psychologist this was the kind of plot I really liked to have at the end of a model based inference. The solid line gives our best point estimate of the true concentration-time curve, and the shaded region shows a 90% credible interval that expresses our uncertainty about what part of the space the true curve might actually occupy.^[We could supplement it further and maybe add dotted lines even further out that show 90% credible regions for future *data* (i.e., acknowledging the role of measurement error in the future) but today I don't feel like doing that!] 


## Sadly, biology isn't always analytically tractable

Earlier when I justified the use of an exponential concentration-time function $C(t)$, I did so by assuming that the body is able to "clear" a constant volume of blood per unit time. That assumption works reasonably well in some situations. If we suppose that the kidneys^[And other organs, I guess. I've heard rumours the human body contains other organs that actually do things...] work a bit like a filter, you can imagine that the body is filtering the blood at a fixed rate, which produces the exponential curve I used earlier. But the real world is a little more complicated than that sometimes.

Suppose, instead, that the elimination process involves an [enzyme-catalysed](https://en.wikipedia.org/wiki/Enzyme_catalysis) reaction. That is, the body eliminates the drug (the substrate) by binding it to enzyme, and from this transition state it is catalysed to something else (the product). The dynamics of such a process are a little different: the enzyme concentration is often very low relative to the substrate concentration and the reaction rate saturates: once you've hit that point adding more of the drug into the blood won't speed up the elimination rate because there's no free enzyme to catalyse its conversion. Put slightly differently, if elimination involves a saturable process it won't necessarily have a constant clearance rate, and you won't see an exponential concentration-time function. 

Well, that's awkward. 

### Michaelis-Menten kinetics

Sometimes a process like this can instead be described by [Michaelis-Menten kinetics](https://en.wikipedia.org/wiki/Michaelis%E2%80%93Menten_kinetics), characterised by the following differential equation:

$$
\frac{dC(t)}{dt} = - \frac{v_{\mbox{max}}}{k_m + C(t)} C(t)$$

In this expression, $v_{\mbox{max}}$ is a constant that denoting the maximum velocity of elimination: due to the limitations imposed by the enzyme concentration, the drug concentration cannot decrease faster than this rate. The term $k_m$ is the **Michaelis constant**: when the drug concentration $C(t)$ equals $k_m$, the elimination rate $dC(t)/dt$ is exactly half its maximum rate, $v_{\mbox{max}} / 2$. Both of these properties are easy enough to demonstrate if you're willing to spend a few minutes playing around with the equation above, but it's not very interesting, so we'll move on. A more useful approach is to think about how we should expect Michaelis-Menten kinetics to behave at high and low drug concentrations:

- If the drug concentration $C(t)$ is very large, $dC(t)/dt$ will be roughly constant and very close to the satuation rate $V_m$. In other words... **at high concentrations, the concentration decreases linearly over time**. 

- If the drug concentration $C(t)$ is very small, then $V_m / (K_m + C(t))$ will be roughly constant, and $dC(t)/dt$ will be roughly proportional to the concentration $C(t)$. In other words... **at low concentrations the concentration decreases exponentially over time**.

Now that we have a sensible intuition, we should try to draw the actual curves. One teeny-tiny problem... this differential equation doesn't really have an analytic  solution.^[Apparently this is only half true. A deep dive on the relevant [wikipedia page](https://en.wikipedia.org/wiki/Michaelis%E2%80%93Menten_kinetics#Closed_form_equation) suggests that it is possible to solve it analytically, but insofar as the solution involves the [Lambert W function](https://en.wikipedia.org/wiki/Lambert_W_function) I personally would prefer to take a numerical approach.] We're going to have to do this numerically.

### What does it look like?

A little later in this post I'm going to implement Michaelis-Menten kinetics using Stan, so as to eventually give me the ability to do Bayesian statistics in a pharmacokinetic model that involves this kind of dynamics. However, I'm a big fan of doing things in small steps. For example, if I weren't planning to build all this into a probabilistic model, I wouldn't really need to use Stan at all. R has many different [tools for solving differential equations numerically](https://cran.r-project.org/web/views/DifferentialEquations.html), and I could just use one of those.

For instance, if I chose to use the [deSolve package](http://desolve.r-forge.r-project.org/), I'd begin by defining an R function `mmk()` that returns the value of the derivative at a given point in time and given specified parameters, pass it to the `ode()` solver supplied by deSolve, and then draw myself a pretty little picture using ggplot2:

```{r}
#| code-line-numbers: true
library(deSolve)
library(ggplot2)

# differential equation for MM kinetics: it returns a list because
# that's what ode() expects to receive when it calls this function
mmk <- function(t, y, parms) {
  dydt <- - y * parms["vm"] / (parms["km"] + y) 
  return(list(dydt))
}

# parameters
dose <- 120 # (in milligrams)
circulation_vol <- 5.5  # (in litres)
max_elimination <- 8 
halving_point <- 4
times <- seq(0, 8, .05) # (in hours)

# use the desolve package
out <- ode(
  y = c("conc" = dose/circulation_vol),
  times = times,
  func = mmk,
  parms = c(
    "vm" = max_elimination, 
    "km" = halving_point
  )
)

# convert matrix to tibble and plot
out <- as.data.frame(out)
ggplot(out, aes(time, conc)) + 
  geom_line() + 
  labs(x = "time", y = "concentration")
```

Consistent with the intuitions we developed earlier you can see that on the left hand side this curve looks pretty linear, but on the right hand side it looks pretty much like an exponential decay. 

### The same thing, but in Stan

Having solved the relevant differential equation in R, it's helpful -- if slightly strange -- to imagine solving the exact same problem using Stan. Although Stan is primarily a language for probabilistic inference, it does provide a toolkit for [solving differential equations](https://mc-stan.org/docs/stan-users-guide/ode-solver.html). In normal usage we'd use the Stan ODE solver in the context of a larger statistical model, but -- in order to wrap my head around how it works -- I found it helpful to try invoking the solver for a dynamical system without incorporating it into any statistical model. 

So let's do that for Michaelis-Menten kinetics. If we squint and ignore the details, the basic idea is the same in Stan as it was in the earlier example in R:

- Write a user-defined function called `mmk()` that takes arguments for the time, current state of the system (in our case concentration), and any other parameters (the maximum elimination rate and the Michaelis constant), and returns the derivatives. 

- Call the ODE solver function, in this case `ode_rk45()`, passing it the user-defined function and other required quantities. This being Stan, we'll need to ensure those quantities are defined in the data block.  

The actual code is shown below. The function is defined within the functions code block (shocking, right?), input data defined within the data block, and the variable we're trying to compute (`conc`) is defined as a generated quantity:

```{stan}
#| filename: mmk
#| output.var: "mmk"
#| code-line-numbers: true
functions {
  vector mmk(real time,
             vector state,
             real vm,
             real km) {
    vector[1] derivative;
    derivative[1] = - state[1] * vm / (km + state[1]);
    return derivative;
  }
}

data {
  int<lower=1> nt;
  vector[1] c0;
  real t0;
  array[nt] real ts;
  real vm;
  real km;
}

model {
}

generated quantities {
  array[nt] vector[1] conc = ode_rk45(mmk, c0, t0, ts, vm, km);
}
```

It may seem strange that the `mmk()` function defines the `state` variable to be a vector of length 1. At first pass it might seem a lot simpler to specify a scalar real: if we did that, then we wouldn't have the weirdness of defining `conc` as a one-dimensional length `nt` array, in which each cell is a vector of length 1, each of which contains a single real number. The layers of nesting seem... unnecessary.

However, they are necessary.

To see this, it's important to recognise that `mmk()` isn't an *arbitrary* user defined function, it is the [ODE system function](https://mc-stan.org/docs/functions-reference/functions-ode-solver.html#ode-system-function) and is designed to be passed to one of the solvers, in this case `ode_rk45()`. In this particular case our state is one-dimensional: we have a one-compartment model, and the only variable that defines the state is the drug concentration in that compartment. But dynamical systems can be -- and usually are --  multivariate: in a two-compartment model the state would probably be defined in terms of two drug concentrations, one for each compartment. In that case we would *require* a vector. 

To accommodate this cleanly, the design choice made in Stan is that the second argument to the system function must be a vector, even if the state happens to be one-dimensional. More generally, the point I'm making here is that to call the ODE solvers you need to ensure that your system function has the appropriate signature.

In any case, let's take our code for a spin. First we'll create some data that we can pass to Stan:

```{r}
times <- seq(.05, 8, .05)
mmk_data <- list(
  c0 = 21.8, 
  t0 = 0, 
  ts = times, 
  vm = 8,
  km = 4,
  nt = length(times)
)
```

Next, we "sample" from the "model". This step is, admittedly, super weird. We don't actually have a model, and there are no parameters to sample and there are no probabilistic aspects to the system at all. This is going to be the world's shortest MCMC run. The Stan model above is available in R via the `mmk` object, so I'll call its `$sample()` method, specifying `fixed_param = TRUE` so that Stan doesn't try to resample parameters that don't exist. I'm only going to run one "chain" for a single iteration, because I only need to solve the system once:

```{r}
mmk_fitted <- mmk$sample(
  data = mmk_data,
  fixed_param = TRUE, 
  chains = 1, 
  iter_sampling = 1
)
```

Because Stan has dutifully generated the generated quantities, I now have the values for the solved concentrations. I'll extract those by calling the `$draws()` method, do a tiny bit of cleanup to wrangle the data into a nice format...

```{r}
mmk_draws <- mmk_fitted$draws(format = "draws_df")
mmk_draws <- mmk_draws |>
  tidyr::pivot_longer(
    cols = tidyr::starts_with("conc"), 
    names_to = "variable", 
    values_to = "conc"
  ) |>
  dplyr::mutate(time = mmk_data$ts)

mmk_draws
```

...and draw a pretty picture:

```{r}
ggplot(mmk_draws, aes(time, conc)) + 
  geom_line() +
  labs(x = "time", y = "concentration")
```

Yup, same as before.

## One compartment bolus model with MMK elimination

Some preliminaries. For the purposes of building priors it's useful to think about what kinds of properties you can have sensible intuitions about. It's a little tricky to set a joint prior over the maximum elimination rate $v_m$ and the Michaelis constant $k_m$ without using some outside knowledge. If the data don't span a range that lets you unambiguously discover "a linear bit" and "an exponential bit" it's very easy to trade off one parameter against the other. Increasing $v_m$ and $k_m$ at the same time will slightly change the "bendiness" of the curve, but that's easily absorbed into the error terms. In other words, if you don't have sensible constraints you're going to end up with a serious identifiability problem. With that in mind, the two thoughts I had are:

- If the researcher has a reason to expect that saturable elimination processes are involved (presumably based on previous studies) the hope is that the does will be chosen so that the initial concentration $c_0$ will be substantially higher than the Michaelis constant $k_m$. If that isn't true, your data are just going to look like an exponential curve anyway, so you might as well just fit a standard bolus IV model.

- Setting a prior over the maximum elimination rate $v_m$ feels unhelpful, because the interpretation of $v_m$ is quite different depending on the value of the Michaelis constant relative to the concentrations involved in the study. I can see why it might be important to understand $v_m$ theoretically but from a data analysis perspective, the "maximum velocity" feels quite different if it's something that is attained across much of the observed time period, versus if it's something that isn't ever observed in the study because the Michaelis constant happens to be much larger than the observed concentrations. With that in mind, I think it's easier to place the prior over the elimination rate $v_0$ at time $t = 0$, noting that it's easy enough to compute $v_m$ as:

$$
v_m = v_0 \ \frac{c_0 + k_m}{c_0}
$$


### The model

```{stan}
#| filename: mmk_bolus
#| output.var: "mmk_bolus"
#| code-line-numbers: true
functions {
  vector mmk(real time,
             vector state,
             real vm,
             real km) {
    vector[1] derivative;
    derivative[1] = - state[1] * vm / (km + state[1]);
    return derivative;
  }
}

data {
  int<lower=1> n_obs;
  int<lower=1> n_fit;
  vector<lower=0>[n_obs] c_obs;
  array[n_obs] real t_obs;
  array[n_fit] real t_fit;
  real<lower=0> vol_d;
  real<lower=0> dose;
  real t0;
}

parameters {
  real<lower=0.01> sigma;
  real<lower=0.01, upper=30> v0;
  real<lower=0.01, upper=30> km;
  real<lower=1, upper=12> vol_d_true;
}

transformed parameters {
  vector[1] c0;
  c0[1] = dose / vol_d_true;
  real<lower=0.01> vm = v0 * (km + c0[1]) / c0[1];
}

model {
  array[n_obs] vector[1] mu_arr;
  vector[n_obs] mu_vec;

  v0 ~ normal(0, 10) T[0.01, ];
  km ~ normal(10, 5) T[0.01, ];
  sigma ~ normal(0, 1) T[0.01, ];
  vol_d_true ~ normal(vol_d, 1);
  
  mu_arr = ode_rk45(mmk, c0, t0, t_obs, vm, km);
  for (i in 1:n_obs) {
    mu_vec[i] = mu_arr[i, 1];
  }
  c_obs ~ normal(mu_vec, sigma);
}

generated quantities {
  array[n_fit] vector[1] c_fit = ode_rk45(mmk, c0, t0, t_fit, vm, km);
}
```

```{r}
t_obs <- 1:8
c_obs <- c(14.8, 11.0, 4.5, 1.6, 0.2, 0.01, 0, 0)
t_fit <- seq(.05, 8, .05)
mmk_bolus_data <- list(
  dose = 120,
  vol_d = 5.5,
  t0 = 0, 
  c_obs = c_obs,
  t_obs = t_obs,
  t_fit = t_fit,
  n_obs = length(t_obs),
  n_fit = length(t_fit)
)
```

```{r}
df <- data.frame(
  time = mmk_bolus_data$t_obs,
  conc = mmk_bolus_data$c_obs
)
ggplot(df, aes(time, conc)) + 
  geom_point(size = 2) + 
  lims(x = c(0, 8), y = c(0, 25)) +
  labs(x = "time", y = "concentration")
```

```{r}
mmk_bolus_fitted <- mmk_bolus$sample(
  data = mmk_bolus_data, 
  seed = 100, 
  chains = 4,
  parallel_chains = 2,  
  refresh = 1000
)
```

```{r}
mmk_bolus_fitted$summary()
```


### Pretty pictures

Parameters are not very identifiable!

```{r}
mmk_bolus_draws <- mmk_bolus_fitted$draws(format = "draws_df")
ggplot(mmk_bolus_draws, aes(km, vm)) + 
  geom_point(alpha = .5) + 
  labs(x = "Michaelis constant", y = "Maximum elimination rate")
```

In case you're interested:

```{r}
ggplot(mmk_bolus_draws, aes(km, v0)) + 
  geom_point(alpha = .5) + 
  labs(x = "Michaelis constant", y = "Elimination rate at time 0")
```

Curves are pretty recoverable though:

```{r}
mmk_bolus_generated <- mmk_bolus$generate_quantities(
  fitted_params = mmk_bolus_fitted,
  data = mmk_bolus_data,
  seed = 999
)

mmk_bolus_generated_summary <- mmk_bolus_generated$summary()
mmk_bolus_generated_summary$time <- mmk_bolus_data$t_fit

dat <- data.frame(
  t_obs = mmk_bolus_data$t_obs, 
  c_obs = mmk_bolus_data$c_obs
)

ggplot(mmk_bolus_generated_summary) + 
  geom_ribbon(aes(time, ymin = q5, ymax = q95), fill = "grey70") +
  geom_line(aes(time, mean)) + 
  geom_point(aes(t_obs, c_obs), data = dat, size = 2) + 
  labs(x = "time", y = "concentration")
```


## Multi compartment models



## Things I have not discussed

On the Stan side

- MCMC diagnostics
- Model checking
- Different kinds of ODE solvers
- Bayesian workflow more generally

On the pharmacometric side

- Logic for choosing one model structure over another
- Individual differences (i.e., population pharmacokinetics)
- Covariates
- Modelling the effect of the drug (pharmacodynamics)

## Resources 

- Holz \& Fahr (2001). Compartment modelling. *Advanced Drug Delivery Reviews*.  https://doi.org/10.1016/S0169-409X(01)00118-1 
- Choi, Rempala \& King (2017). Beyond the Michaelis-Menten equation: Accurate and efficient estimation of enzyme kinetic parameters. *Scientific Reports*.  https://doi.org/10.1038/s41598-017-17072-z 
 


<!--------------- appendices go here ----------------->



