---
title: "Unpacking the Arrow on-disk Dataset API"
description: "Kind of a post for me really."
date: "2022-11-20"
categories: [Apache Arrow]
image: "img/mammoth.png"
---

<!--------------- my typical setup ----------------->

```{r setup, include=FALSE}
long_slug <- "2022-11-20_unpacking-arrow-datasets"
#renv::use(lockfile = "renv.lock")
```


<!--------------- post begins here ----------------->

```{r}
library(arrow, warn.conflicts = FALSE)
library(dplyr, warn.conflicts = FALSE)
```


In an earlier post I wrote about [Arrays and Tables in Apache Arrow](/posts/2022-05-25_arrays-and-tables-in-arrow/index.html). The focus of that post was on the in-memory data structures that Arrow uses to store tabular data. That meant the bulk of the post was focused on Record Batch and Table objects, as well as their constituent parts, Arrays and Chunked Arrays. It didn't really talk about Datasets. Okay yeah there was a [section on Datasets](/posts/2022-05-25_arrays-and-tables-in-arrow/index.html#datasets) at the end of the post, but I was a bit evasive. I gave an example showing how to use Datasets, but I really didn't talk much about what they are. 

Okay fine, well, I forgive myself. A single blog post can't solve all the world's problems. However, I'm in the process of updating some of the Arrow R documentation at the moment and at this point I kind of do need to talk a bit more about the structure of Dataset objects.

This post is a bit of a sneak preview into where I'm headed with that. 

## Record Batches and Tables

Let's start with Record Batches. A Record Batch is tabular data structure comprised of named Arrays, and an accompanying Schema that specifies the name and data type associated with each Array. We can create one manually using `record_batch()`

```{r}
rb <- record_batch(
  strs = c("hello", "amazing", "and", "cruel", "world"), 
  ints = c(1L, NA, 2L, 4L, 8L),
  dbls = c(1.1, 3.2, 0.2, NA, 11)
)
glimpse(rb)
```

This is a Record Batch containing 5 rows and 3 columns. The command `rb[1:3, 1:2]` extracts the first three rows and the first two columns:

```{r}
glimpse(rb[1:3, 1:2])
``` 

The structure of a Record Batch is shown below:

![](./img/record_batch.png)

Record Batches are a fundamental unit for data interchange in Arrow, but are not typically used for data analysis. The reason for this is that the constituent Arrays that store columns in a Record Batch are immutable: they cannot be modified or extended without creating a new object. When data arrive sequentially Record Batches can be inconvenient, because you can't concatenate them. For that reason Tables are usually more practical.

So let's turn to Tables next. From the user perspective a Table is very similar to a Record Batch but the constituent parts are Chunked Arrays. Chunked Arrays are flexible wrappers enclosing one or more Arrays. This makes it possible to concatenate tables: 

```{r}
df1 <- arrow_table(rb)
df2 <- arrow_table(
  strs = c("I", "love", "you"), 
  ints = c(5L, 0L, 0L),
  dbls = c(7.1, -0.1, 2)
)
df <- concat_tables(df1, df2)
glimpse(df)
```



![](./img/table.png)
Okay, what about Datasets? Ah well... that's the one I haven't quite worked out. Hence the purpose of this post.


## So... Datasets?


Like Record Batch and Table objects, a Dataset is used to represent tabular data. At an abstract level, a Dataset can be viewed as an object comprised of rows and columns, and just like Record Batches and Tables, it contains an explicit Schema that specifies the name and data type associated with each column.

However, where Tables and Record Batches are data explicitly represented in-memory, a Dataset is not. Instead, a Dataset is an abstraction that refers to data stored on-disk in one or more files. Values stored in the data files are loaded into memory as a batched process. Loading takes place only as needed, and only when a query is executed against the data. In this respect Arrow Datasets are a very different kind of object to Arrow Tables, but the dplyr commands used to analyze them are essentially identical. In this section we'll talk about how Datasets are structured. 

## What does a Dataset look like on-disk?

Reduced to its simplest form, the on-disk structure of a Dataset is simply a collection of data files, each storing one subset of the data. These subsets are sometimes referred to as "fragments", and the partitioning process is sometimes referred to as "sharding". By convention, these files are organized into a folder structure called a Hive-style partition: see `hive_partition()` for details. 

To illustrate how this works, let's write a multi-file dataset to disk manually, without using any of the Arrow Dataset functionality to do the work. We'll start with three small data frames, each of which contains one subset of the data we want to store: 

```{r}
df_a <- data.frame(id = 1:5, value = rnorm(5), subset = "a")
df_b <- data.frame(id = 6:10, value = rnorm(5), subset = "b")
df_c <- data.frame(id = 11:15, value = rnorm(5), subset = "c")
```

Our intention is that each of the data frames should be stored in a separate data file. As you can see, this is a quite structured partitioning: all data where `subset = "a"` belong to one file, all data where `subset = "b"` belong to another file, and all data where `subset = "c"` belong to the third file. 

The first step is to define and create a folder that will hold all the files:

```{r, include=FALSE}
ds_dir <- "mini-dataset"
if(dir.exists(ds_dir)) {
  unlink(ds_dir, recursive = TRUE)
}
```

```{r}
ds_dir <- "mini-dataset"
dir.create(ds_dir)
```

The next step is to manually create the Hive-style folder structure:

```{r}
ds_dir_a <- file.path(ds_dir, "subset=a")
ds_dir_b <- file.path(ds_dir, "subset=b")
ds_dir_c <- file.path(ds_dir, "subset=c")

dir.create(ds_dir_a)
dir.create(ds_dir_b)
dir.create(ds_dir_c)
```

Notice that we have named each folder in a "key=value" format that exactly describes the subset of data that will be written into that folder. This naming structure is the essence of Hive-style partitions. 

Now that we have the folders, we'll use `write_parquet()` to create a single parquet file for each of the three subsets:

```{r}
write_parquet(df_a, file.path(ds_dir_a, "part-0.parquet"))
write_parquet(df_b, file.path(ds_dir_b, "part-0.parquet"))
write_parquet(df_c, file.path(ds_dir_c, "part-0.parquet"))
```

If we had wanted to, we could have further subdivided the dataset. A folder could contain multiple files (`part-0.parquet`, `part-1.parquet`, etc) if we wanted it to. Similarly, there is no particular reason to name the files `part-0.parquet` this way at all: it would have been fine to call these files `subset-a.parquet`, `subset-b.parquet`, and `subset-c.parquet` if we had wished. We could have written other file formats if we wanted, and we don't necessarily have to use Hive-style folders. You can learn more about the supported formats by reading the help documentation for `open_dataset()`, and learn about how to exercise fine grained control with `help("Dataset", package = "arrow")`. 

In any case, we have created an on-disk parquet Dataset using Hive-style partitioning. Our Dataset is defined by these files:

```{r}
list.files(ds_dir, recursive = TRUE)
```

To verify that everything has worked, let's open the data with `open_dataset()` and call `glimpse()` to inspect its contents:

```{r}
ds <- open_dataset(ds_dir)
glimpse(ds)
```

As you can see, the `ds` Dataset object aggregates the three separate data files. In fact, in this particular case the Dataset is so small that values from all three files appear in the output of `glimpse()`.

It should be noted that in everyday data analysis work, you wouldn't need to do write the data files manually in this fashion. The example above is entirely for illustrative purposes. The exact same dataset could be created with the following command:

```{r, eval=FALSE}
ds |> 
  group_by(subset) |>
  write_dataset("mini-dataset")
```

In fact, even if `ds` happens to refer to a data source that is larger than memory, this command should still work because the Dataset functionality is written to ensure that during a pipeline such as this the data is loaded piecewise in order to avoid exhausting memory. 

## What is stored in-memory by the Dataset object?

In the previous section we examined the on-disk structure of a Dataset. We now turn to the in-memory structure of the Dataset object itself (i.e., `ds` in the previous example). When the Dataset object is created, arrow searches the dataset folder looking for appropriate files, but does not load the contents of those files. Paths to these files are stored in an active binding `ds$files`:

```{r}
ds$files 
```

The other thing that happens when `open_dataset()` is called is that an explicit Schema for the Dataset is constructed and stored as `ds$schema`:

```{r}
ds$schema
```

By default this Schema is inferred by inspecting the first file only, though it is possible to construct a unified schema after inspecting all files. To do this, set `unify_schemas = TRUE` when calling `open_dataset()`. It is also possible to use the `schema` argument to `open_dataset()` to specify the Schema explicitly (see the `schema()` function for details). 

It's a bit of an oversimplification because both `ds$files` and `ds$schema` are actually active bindings that invoke function calls to the underlying Arrow C++ library but honestly I don't see a need to care. In most situations I think it's reasonable to use this as the mental model of what the `ds` object contains:

![](./img/dataset.png)

The act of reading the data is performed by a Scanner object. When analyzing a Dataset using the dplyr interface you never need to construct a Scanner manually, but for explanatory purposes we'll do it here:

```{r}
scan <- Scanner$create(dataset = ds)
```

Calling the `ToTable()` method will materialize the Dataset (on-disk) as a Table (in-memory):

```{r}
scan$ToTable()
```

This scanning process is multi-threaded by default, but if necessary threading can be disabled by setting `use_threads = FALSE` when calling `Scanner$create()`.

## How does a Dataset query work?

When a query is executed against a Dataset a new scan is initiated and the results pulled back into R. As an example, consider the following dplyr expression:

```{r}
ds |>
  filter(value > 0) |>
  mutate(new_value = round(100 * value)) |>
  select(id, subset, new_value) |>
  collect()
```

We can replicate this using the low-level Dataset interface by creating a new scan by specifying the `filter` and `projection` arguments to `Scanner$create()`. To use these arguments you need to know a little about Arrow Expressions, for which you may find it helpful to read the help documentation in `help("Expression", package = "arrow")`. 

The scanner defined below mimics the dplyr pipeline shown above,

```{r}
scan <- Scanner$create(
  dataset = ds, 
  filter = Expression$field_ref("value") > 0,
  projection = list(
    id = Expression$field_ref("id"),
    subset = Expression$field_ref("subset"),
    new_value = Expression$create("round", 100 * Expression$field_ref("value"))
  )
)
```

and if we were to call `as.data.frame(scan$ToTable())` it would produce the same result as the dplyr version, though the rows may not appear in the same order. 

To get a better sense of what happens when the query executes, what we'll do here is call `scan$ScanBatches()`. Much like the `ToTable()` method, the `ScanBatches()` method executes the query separately against each of the files, but it returns a list of Record Batches, one for each file. In addition, we'll convert these Record Batches to data frames individually:

```{r}
lapply(scan$ScanBatches(), as.data.frame)
```

If we return to the dplyr query we made earlier, and use `compute()` to return a Table rather use `collect()` to return a data frame, we can see the evidence of this process at work. The Table object is created by concatenating the three Record Batches produced when the query executes against three data files, and as a consequence of this the Chunked Array that defines a column of the Table mirrors the partitioning structure present in the data files:

```{r}
tbl <- ds |>
  filter(value > 0) |>
  mutate(new_value = round(100 * value)) |>
  select(id, subset, new_value) |>
  compute()

tbl$subset
```


<!--------------- appendices go here ----------------->


