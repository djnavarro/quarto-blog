{
  "hash": "8cc9d6a30cb23d4b7a98f7b984cc0eb5",
  "result": {
    "markdown": "---\ntitle: \"Faster data processing with Apache Arrow and Parquet\"\nauthor:\n  name: \"Tom Drabas, Weston Pace, Danielle Navarro\"\ndescription: \"An illustration of the advantages of parquet files over CSV files in R and Python, using Apache Arrow to transform CSV data into the more efficient parquet format\"\ndate: \"2022-10-30\"\ncategories: [Apache Arrow, Parquet, R, Python]\n---\n\n\n\n\n**Note:** This post is an extended version of a post previously published on the Voltron Data blog\n\nReading and writing data across different systems requires a file format that is understood by both. File formats like CSV or JSON are highly portable as they are text-based. However, they lack a schema definition thus requiring either the user to specify the schema upfront or the system to infer the data types of columns. Apache Parquet or ORC are column-oriented file formats that support compression and provide schema which makes reading the data much more efficient.\n\nIn a recent post, [François Michonneau](https://voltrondata.com/news/creating-an-arrow-dataset/) talks about creating an Arrow dataset and explores various file formats supported by the Arrow ecosystem and shows the efficiency of the parquet format for storing and reading the data. In this post, we'll continue that thought and showcase how to seamlessly convert large CSV files to parquet so you can take advantage of the space and time benefits.\n\n## Apache Parquet\n\n[Apache Arrow](https://arrow.apache.org/) and [Apache Parquet](https://parquet.apache.org/) go hand in hand. Apache Parquet is a columnar format and Parquet formatted files can be read into Arrow arrays and, vice versa, Arrow arrays written back to Parquet files. Apache Parquet format also carries metadata so each column read back will retain the proper data type. The same, unfortunately, cannot be said for CSV files.\n\nThe best gift you can give yourself as a data scientist or engineer is to ditch CSVs and move to Parquet! Yes that does mean you will have to read those CSV files one more time, but once you transcode them to Parquet format you can step away and watch your data pipelines start flowing smoothly again!\n\nHere's how.\n\n## Preliminaries\n\nOur first step is to use the right tools: in Python we'll begin with our imports, in R we'll load some packages.\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pyarrow as pa\nimport pyarrow.csv as csv\nimport pyarrow.parquet as pq\nimport pyarrow.dataset as ds\nimport pandas as pd\nimport numpy as np\nimport statistics\nimport timeit\nimport glob\nimport os\n```\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(arrow)\nlibrary(bench)\nlibrary(here)\nlibrary(fs)\n```\n:::\n\n\n:::\n\nLet's also create some helper variables to keep track of all the files we're going to use as this post progresses. This step isn't strictly needed but it will help keep our code tidy later.\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# data directory\ndata_dir = 'pydata'\nif not os.path.isdir(data_dir):\n    os.mkdir(data_dir)\n\n# csv files\ncsv_file = f'{data_dir}/large_dataset.csv'\ncsv_file_huge = f'{data_dir}/huge_dataset.csv'\n\n# parquet files\nparquet_file_snappy = f'{data_dir}/large_dataset_snappy.parquet'\nparquet_file_gzip = f'{data_dir}/large_dataset_gzip.parquet'\n\n# folder for multi-file dataset\ndataset_dir = f'{data_dir}/huge_dataset'\n```\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# data directory\ndata_dir <- \"rdata\"\nif(!dir_exists(data_dir)) {\n  dir_create(data_dir)\n}\n\n# csv files\ncsv_file <- path(\"rdata\", \"large_dataset.csv\")\ncsv_file_huge <- path(\"rdata\", \"huge_dataset.csv\")\n\n# parquet files\nparquet_file_snappy <- path(\"rdata\", \"large_dataset_snappy.parquet\")\nparquet_file_gzip <- path(\"rdata\", \"large_dataset_gzip.parquet\")\n\n# folder for multi-file dataset\ndataset_dir <- path(\"rdata\", \"huge_dataset\")\n```\n:::\n\n\n:::\n\nHelper function to report the median time taken to execute a function:\n\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef median_time(fun, repetitions = 7):\n  time = timeit.repeat(fun, number = 1, repeat = repetitions)\n  return(statistics.median(time))\n```\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmedian_time <- function(fun, repetitions = 7) {\n  benchmarks <- mark(fun(), iterations = repetitions, check = FALSE)\n  return(benchmarks$median)\n}\n```\n:::\n\n\n:::\n\n\n\n## Generate a large random data set\n\nTo demonstrate how it all works, we'll need some data. For this post, what we'll do is create a table that will occupy roughly 2GB in memory. If we have 45 columns of double precision numeric data in our table, how many rows should our table have?\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ncolumns = 45\nmemory_size = 2*1024**3\ndata_type = np.dtype('float64')\nrows = memory_size / columns / data_type.itemsize\n\nprint(f'Size in memory: {memory_size / (1024**3)}GB')\nprint(f'Number of rows: {int(rows):,}')\nprint(f'Number of columns: {columns}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSize in memory: 2.0GB\nNumber of rows: 5,965,232\nNumber of columns: 45\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbytes_float <- 8\nbytes_total <- 2 * 1024 ^ 3\nn_columns <- 45\nn_rows <- round(bytes_total / (n_columns * bytes_float)) \n\ncat(\"Size in memory:\", round(bytes_total / (1024 ^ 3), digits = 1), \"GB\\n\")\ncat(\"Number of rows:\", n_rows, \"\\n\")\ncat(\"Number of columns:\", n_columns, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSize in memory: 2 GB\nNumber of rows: 5965232 \nNumber of columns: 45 \n```\n:::\n:::\n\n\n:::\n\nWe'll need about 6 million rows. Armed with this knowledge let's create a data set of the appropriate size and write the results to a CSV file: \n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nif not os.path.isfile(csv_file):\n    dataset = pa.table(\n        {\n            f'col_{i}': np.random.randn(int(rows))\n            for i in range(columns)\n        }\n    )\n    options = csv.WriteOptions(include_header=True)\n    csv.write_csv(dataset, csv_file, options)\n```\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif(!file_exists(csv_file)) {\n  matrix(\n    data = rnorm(n_rows * n_columns),\n    nrow = n_rows, \n    ncol = n_columns\n  ) |>\n    as.data.frame() |>\n    write_csv_arrow(csv_file)\n}\n```\n:::\n\n\n:::\n\nWhen stored in memory, this object -- by design -- is about 2GB in size. How large is it on disk as a CSV file?\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef file_size(file):\n    n_bytes = os.path.getsize(file)\n    print(f'{(n_bytes / (1024**3)):.2f}GB')\n\nfile_size(csv_file)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4.91GB\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile_size(csv_file)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4.91G\n```\n:::\n:::\n\n\n:::\n\nIt's almost 5GB: it's more than doubled in size when we save to CSV. That's not surprising because CSV isn't designed to be efficient, but it's not something any data scientist or engineer wants to see.\n\n## Transcoding CSV to Parquet\n\nApache Parquet format is a preferred way to store columnar data: it requires much less compute cycles and it supports compression, rendering much smaller file sizes. By default the compression used is snappy but we will also try GZip.\n\n### Snappy compression\n\nLet’s time how long it takes to transcode our CSV file to the parquet with Apache Arrow.\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell hash='index_cache/html/py-csv-to-parquet-snappy_25db444f9a10c3ee56d62f42d747a2db'}\n\n```{.python .cell-code}\ndef transcode_snappy():\n  arrow_table = pa.csv.read_csv(csv_file)\n  pq.write_table(arrow_table, parquet_file_snappy);\n\nmedian_time(transcode_snappy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n11.038464039000246\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell hash='index_cache/html/r-csv-to-parquet-snappy_50a01f2da78810446307bc333ef2c7db'}\n\n```{.r .cell-code}\ntranscode_snappy <- function() {\n  read_csv_arrow(csv_file) |> write_parquet(parquet_file_snappy)\n}\n\nmedian_time(transcode_snappy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 11.3s\n```\n:::\n:::\n\n\n:::\n\nSo, end-to-end it takes maybe 8 to 10 seconds to read CSV and save it to a parquet file. The size of the file is less than half of what it used to be in CSV:\n  \n::: {.panel-tabset}\n\n## Python\n  \n\n::: {.cell}\n\n```{.python .cell-code}\nfile_size(parquet_file_snappy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.01GB\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile_size(parquet_file_snappy)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.01G\n```\n:::\n:::\n\n\n:::\n\n### GZip compression\n\nNow let’s try with GZIP.\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell hash='index_cache/html/py-csv-to-parquet-gzip_dd8f2ce9e68edc08bc5d8a076c53ea8c'}\n\n```{.python .cell-code}\ndef transcode_gzip():\n  arrow_table = pa.csv.read_csv(csv_file)\n  pq.write_table(arrow_table, parquet_file_gzip, compression='gzip');\n\nmedian_time(transcode_gzip)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n94.39236859499943\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell hash='index_cache/html/r-csv-to-parquet-gzip_31541d11850343e9c6d526e66dafba42'}\n\n```{.r .cell-code}\ntranscode_gzip <- function() {\n  read_csv_arrow(csv_file) |> \n    write_parquet(parquet_file_gzip, compression = \"gzip\")\n}\n\nmedian_time(transcode_gzip)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.5m\n```\n:::\n:::\n\n\n:::\n\nOverall it takes about 10 times as long to complete the transcoding with gzip than it takes when using snappy, but you do get end up with a slightly more compressed file at the end. Is it worth the wait? You need to decide.\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfile_size(parquet_file_gzip)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.93GB\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile_size(parquet_file_gzip)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.93G\n```\n:::\n:::\n\n\n:::\n\n\n## Reading Parquet Files\n\nHaving transcoded our data set from CSV to Apache Parquet, let's compare read times for the two file formats. We'll look at the CSV files first. To make things simple we'll read the data to an Arrow Table, but we would still see the same performance difference between CSV and Parquet if we read the data to a panda in Python or a data frame in R. \n\n::: {.panel-tabset}\n\n## Python\n  \n\n::: {.cell hash='index_cache/html/py-arrow-read-csv_760db68a8e08f45a535be1773974dcea'}\n\n```{.python .cell-code}\ndef read_csv_data():\n  csv.read_csv(csv_file)\n  \nmedian_time(read_csv_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4.780269909999333\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell hash='index_cache/html/r-arrow-read-csv_8489e0cdc8882ee288810271a995ad9e'}\n\n```{.r .cell-code}\nread_csv_data <- function() {\n  read_csv_arrow(csv_file, as_data_frame = FALSE) \n}\n\nmedian_time(read_csv_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.28s\n```\n:::\n:::\n\n\n:::\n\nNow let's try reading the Parquet files:\n\n::: {.panel-tabset}\n\n## Python\n  \n\n::: {.cell hash='index_cache/html/pyarrow-read-parquet_d75ee729ff5ca708530ccbb35885a32a'}\n\n```{.python .cell-code}\ndef read_parquet_arrow():\n  pq.read_table(parquet_file_snappy)\n\nmedian_time(read_parquet_arrow)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.2946250789973419\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell hash='index_cache/html/table-read-parquet_751250e4e48099cb66c4ba4c59f09878'}\n\n```{.r .cell-code}\nread_parquet_arrow <- function() {\n  read_parquet(parquet_file_snappy, as_data_frame = FALSE)\n}\n\nmedian_time(read_parquet_arrow)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 390ms\n```\n:::\n:::\n\n\n\n:::\n\nAbout one third of a second. Not too bad! \n\n<!--\n\nOne takeaway lesson is to, if you really need to read CSV and then process the data using pandas, read the data with Arrow’s CSV reader and then convert the Arrow Table to pandas DataFrame.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef read_pandas_arrow():\n  pa.csv.read_csv(csv_file).to_pandas()\n\ntimeit.timeit(read_pandas_arrow, number = 1)  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n5.283127844999399\n```\n:::\n:::\n\n\nStill slower than reading a parquet file but significantly faster than reading CSV with pandas.\n\n-->\n\n## Comparing Parquet to Compressed CSV\n\nAt this point you might be wondering how Parquet files compare to compressed CSV. A common solution to the shortcomings of CSV files it to use `gzip` or similar utility to compress them. This can lead to substantially reduced file sizes. For example, the data set we've used in this blog post is about the same size when stored as a gzipped CSV file (2.4GB) as it is when stored as a snappy-compressed Parquet file (2.2GB). The reduced file size might lead to faster load times, but in a local setting these speed-ups are mostly a function of the file size: they don't tell us much about how many CPU cycles reading these files consumes. On linux you can collect some performance statistics using the `perf` utility:\n\n<!-- \nfor future reference, this is how you modify the perf_event_paranoid settings\n\nsudo sh -c 'echo kernel.perf_event_paranoid=1 > /etc/sysctl.d/local.conf \n\nreset to 4 to be completely paranoid :-)\n-->\n\n::: {.panel-tabset}\n\n## Python\n\nHere are the performance statistics for reading the gzipped CSV file:\n  \n``` bash\nperf stat python -c \"from pyarrow import csv; csv.read_csv('pydata/large_dataset.csv.gz')\"\n```\n\n```\n Performance counter stats for 'python -c from pyarrow import csv; csv.read_csv('pydata/large_dataset.csv.gz')':\n\n         45,946.29 msec task-clock                #    2.458 CPUs utilized          \n            97,475      context-switches          #    2.121 K/sec                  \n            37,060      cpu-migrations            #  806.594 /sec                   \n         2,379,661      page-faults               #   51.792 K/sec                  \n   163,823,235,007      cycles                    #    3.566 GHz                    \n   266,069,641,526      instructions              #    1.62  insn per cycle         \n    40,294,887,901      branches                  #  877.000 M/sec                  \n     1,787,412,162      branch-misses             #    4.44% of all branches        \n   603,296,698,280      slots                     #   13.130 G/sec                  \n   136,118,412,337      topdown-retiring          #     19.2% retiring              \n   468,702,274,904      topdown-bad-spec          #     66.3% bad speculation       \n    67,807,557,217      topdown-fe-bound          #      9.6% frontend bound        \n    34,659,537,044      topdown-be-bound          #      4.9% backend bound         \n\n      18.692289542 seconds time elapsed\n\n      42.868337000 seconds user\n       6.071931000 seconds sys\n```\n\nTo compare, this is what we get when reading the snappy-compressed parquet file:\n\n``` bash\nperf stat python -c \"from pyarrow import parquet; parquet.read_table('pydata/large_dataset_snappy.parquet')\"\n```\n\n```\n Performance counter stats for 'python -c from pyarrow import parquet; parquet.read_table('pydata/large_dataset_snappy.parquet')':\n\n          4,052.57 msec task-clock                #    2.507 CPUs utilized          \n             8,212      context-switches          #    2.026 K/sec                  \n             2,898      cpu-migrations            #  715.102 /sec                   \n         1,105,237      page-faults               #  272.725 K/sec                  \n    12,740,967,348      cycles                    #    3.144 GHz                    \n    11,629,653,540      instructions              #    0.91  insn per cycle         \n     1,953,615,539      branches                  #  482.069 M/sec                  \n        15,002,905      branch-misses             #    0.77% of all branches        \n    38,990,127,780      slots                     #    9.621 G/sec                  \n    11,304,974,170      topdown-retiring          #     26.6% retiring              \n    13,387,988,026      topdown-bad-spec          #     31.5% bad speculation       \n     7,239,172,422      topdown-fe-bound          #     17.0% frontend bound        \n    10,599,009,432      topdown-be-bound          #     24.9% backend bound         \n\n       1.616350503 seconds time elapsed\n\n       1.370856000 seconds user\n       2.709456000 seconds sys\n\n```\n\nReading the parquet file consumes 12.7 billion CPU cycles; for the gzipped CSV the figure is 163.8 billion.\n\n\n## R\n\nHere are the performance statistics for reading the gzipped CSV file:\n  \n``` bash\nperf stat Rscript -e \"arrow::read_csv_arrow('rdata/large_dataset.csv.gz', as_data_frame = FALSE)\"\n```\n\n```\n Performance counter stats for 'Rscript -e arrow::read_csv_arrow('rdata/large_dataset.csv.gz', as_data_frame = FALSE)':\n\n         49,814.86 msec task-clock                #    2.343 CPUs utilized          \n           104,662      context-switches          #    2.101 K/sec                  \n            39,809      cpu-migrations            #  799.139 /sec                   \n         2,444,561      page-faults               #   49.073 K/sec                  \n   176,418,525,767      cycles                    #    3.541 GHz                    \n   277,797,226,186      instructions              #    1.57  insn per cycle         \n    43,508,214,462      branches                  #  873.398 M/sec                  \n     1,777,773,657      branch-misses             #    4.09% of all branches        \n   656,019,283,120      slots                     #   13.169 G/sec                  \n   128,143,770,282      topdown-retiring          #     16.2% retiring              \n   534,939,317,766      topdown-bad-spec          #     67.8% bad speculation       \n    65,978,499,316      topdown-fe-bound          #      8.4% frontend bound        \n    60,111,173,500      topdown-be-bound          #      7.6% backend bound         \n\n      21.263720897 seconds time elapsed\n\n      47.243353000 seconds user\n       5.763587000 seconds sys\n```\n\nTo compare, this is what we get when reading the snappy-compressed parquet file:\n\n``` bash\nperf stat Rscript -e \"arrow::read_parquet('rdata/large_dataset_snappy.parquet', as_data_frame = FALSE)\"\n```\n\n```\n Performance counter stats for 'Rscript -e arrow::read_parquet('rdata/large_dataset_snappy.parquet', as_data_frame = FALSE)':\n\n          2,675.35 msec task-clock                #    3.220 CPUs utilized          \n               535      context-switches          #  199.974 /sec                   \n                38      cpu-migrations            #   14.204 /sec                   \n           628,615      page-faults               #  234.966 K/sec                  \n     9,932,298,420      cycles                    #    3.713 GHz                    \n    11,800,309,617      instructions              #    1.19  insn per cycle         \n     2,155,219,064      branches                  #  805.585 M/sec                  \n        12,280,294      branch-misses             #    0.57% of all branches        \n    30,850,755,460      slots                     #   11.532 G/sec                  \n    11,784,389,574      topdown-retiring          #     36.8% retiring              \n     4,243,149,218      topdown-bad-spec          #     13.2% bad speculation       \n     6,247,234,332      topdown-fe-bound          #     19.5% frontend bound        \n     9,768,481,508      topdown-be-bound          #     30.5% backend bound         \n\n       0.830735747 seconds time elapsed\n\n       1.609327000 seconds user\n       1.073598000 seconds sys\n\n```\n\nReading the parquet file consumes 9.9 billion CPU cycles; for the gzipped CSV the figure is 176.4 billion.\n\n:::\n\nThese results are of course specific to the machine we used for the tests -- a 2022 Dell XPS 13 laptop running Ubuntu 20.04 -- but so are all the other numbers reported here. The fine details are less important than the big picture result:\n\nEven when a compressed CSV file is similar in size to a Parquet file, it will consume far more time and system resources to read. Any time that you have to pay for time or CPU cycles, deploying a solution using Parquet files is likely to be much more cost-effective than using compressed CSV data.\n\n\n<!-- A savvy user, who may want to store and use that data in the cloud, might want to test the I/O bandwidth because of the cloud latencies. Thus, reading these two files on a desktop machine might as well show that reading a compressed CSV is faster as the file size is smaller: we don’t need to read as many bytes as with parquet. However, on a system that is distributed (like, e.g. S3), reads are frequently done in parallel, resulting in a much higher bandwidth even with the penalty of having a more excessive latency compared to a local setting.  -->\n\n\n## Transcoding Larger-Than-Memory Datasets\n\nIn all our examples so, the data set has been small enough that the CSV file could fit easily into the memory of a modern computer with 8GB of RAM. Plenty of times, however, you might find yourself with a data set that is much larger than the RAM available. This makes transcoding a little trickier, but Apache Arrow can help with this.\n\nFirst, with a simple bash script, we can create a huge data set by concatenating eight copies of our `large_dataset.csv` together, creating a data file that is roughly 40GB on disk. Our bash script might look like this:\n\n\n::: {.cell filename='make-huge-csv.sh'}\n\n```{.bash .cell-code}\n#!/bin/bash \nlarge_csv=\"${1}/large_dataset.csv\"\nhuge_csv=\"${1}/huge_dataset.csv\" \nfor i in {0..7}\n  do\n    if [[ $i -eq 0 ]] ; then\n      head -1 $large_csv > $huge_csv\n    fi\n    tail -n +2 $large_csv >> $huge_csv\n  done\n```\n:::\n\n\nAt the terminal we'd use a command like one of thee two, depending on whether we are looking in the R data folder or the Python data folder:\n\n``` bash\n./make-huge-csv.sh \"../pydata\"  # Python\n./make-huge-csv.sh \"../rdata\"   # R\n```\n\nHaving run this script, let's check the size of the huge CSV file we have created:\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfile_size(csv_file_huge)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n39.26GB\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile_size(csv_file_huge)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n39.3G\n```\n:::\n:::\n\n\n:::\n\nThat's close to 40GB. This won't fit in RAM for a typical machine. However, using Arrow's dataset interface we can use the \"backpressure\" functionality to slow down the read of a CSV file as we write out its parquet counterpart. The backpressure feature simply prevents the system from running out-of-memory when reading the data: when Arrow's writer cannot keep up with the amount of incoming bytes from the reader it sends a backpressure message to the reader to slow down the reading. \n\nIt shows well on this graph.\n\n![](./img/backpressure.png)\n\nInitially, there's plenty of RAM available so reading and writing rates are high. However, once the disk cache gets full Arrow slows down, maintaining a relatively stable amount of occupied memory until it is done reading the file. At this point, Arrow will start releasing the memory and the writing rate will peak up a bit until done.\n\nThe backpressure feature has been automatically available in Arrow since version 6.0.0 and is used automatically when using the dataset API. Let's put it to work:\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell hash='index_cache/html/py-write-dataset_9aa7f8666213f30b52a194206bc083c3'}\n\n```{.python .cell-code}\ndef transcode_huge_csv():\n  dataset = ds.dataset(csv_file_huge, format='csv')\n  ds.write_dataset(dataset, dataset_dir, format='parquet')\n\nmedian_time(transcode_huge_csv, repetitions = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n184.2342482189997\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell hash='index_cache/html/r-write-dataset_1bc5a55a562fc7715ac4630d4dbd46a1'}\n\n```{.r .cell-code}\ntranscode_huge_csv <- function() {\n  csv_file_huge |>\n    open_dataset(format = \"csv\") |>\n    write_dataset(path = dataset_dir)\n}\n\nmedian_time(transcode_huge_csv, repetitions = 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.16m\n```\n:::\n:::\n\n\n:::\n\nThis time around we only ran each test once because it's a little time consuming, but it's enough to give you a pretty good sense of how long it takes to read and convert a 40GB CSV file to parquet format on a laptop: about 2-3 minutes. \n\nWhat about the file size? As you can see from the results below, if we use snappy compression the file size for the parquet encoded data is roughly half:\n\n::: {.panel-tabset}\n\n## Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nn_bytes = 0\nfilenames = glob.glob(f'{dataset_dir}/*') \nfor fn in filenames:\n  n_bytes += os.path.getsize(fn)\nprint(f'{(n_bytes / (1024**3)):.2f}GB')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n19.31GB\n```\n:::\n:::\n\n\n## R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndataset_dir |>\n  list.files(full.names = TRUE) |>\n  file_size() |>\n  sum()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n19.3G\n```\n:::\n:::\n\n\n:::\n\n## Summary\n\nUsing Apache Arrow to transcode data to Apache Parquet format makes a lot of sense and will save you time and boost your productivity in the future. To get started you can install the PyArrow library following the instructions found at [arrow.apache.org/install](https://arrow.apache.org/install/).\n\nLearn how a [Voltron Data Enterprise Support subscription](https://voltrondata.com/subscription/) can help accelerate your success with Apache Arrow. \n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}