{
  "hash": "ac5aa84978ae09e8fa3e5bc482316cfa",
  "result": {
    "markdown": "---\ntitle: \"Building a population pharmacokinetic model with Stan\"\ndescription: \"In which the author works her way through the first part of an online tutorial, rewrites some NONMEM code in Stan, and takes notes\"\ndate: \"2023-06-10\"\ncategories: [\"Statistics\", \"Pharmacokinetics\", \"R\", \"Stan\", \"NONMEM\", \"Bayes\"]\nimage: cover.png\nimage-alt: \"A 4x4 grid of scatter plots, each showing data that rise and then fall, with grey lines showing model predictions\"\n---\n\n\n<!--------------- my typical setup ----------------->\n\n\n\n\n\n<!--------------- post begins here ----------------->\n\nThe [Population Approach Group of Australia and New Zealand](https://www.paganz.org/) host some useful [resources](https://www.paganz.org/resources/) for folks interested in pharmacometric modelling. Specifically they have a series of workshops are pretty handy. There's a beginner workshop in 2019 that covers the core approach, and then two intermediate workshops in 2021 and 2022. I'll work through the [2019 workshop materials](https://www.paganz.org/wp-content/uploads/2016/06/PAWs-Beginners-2019.zip) in this blog post, translating the code from NONMEM to Stan and R as needed.\n\n## The warfarin data set\n\n### Parsing the data\n\nLoad the data. The csv file uses `\".\"` to specify missing values, which I'll need to state explicitly when reading the data into R, thereby ensuring numeric variables end up as numeric:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwarfpk <- readr::read_csv(\"warfpk.csv\", na = \".\", show_col_types = FALSE)\nwarfpk\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 289 × 10\n   `#ID`  time    wt   age   sex   amt  rate  dvid    dv   mdv\n   <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 0       0    66.7    50     1   100    -2     0  NA       1\n 2 0       0.5  66.7    50     1    NA    NA     1   0       0\n 3 0       1    66.7    50     1    NA    NA     1   1.9     0\n 4 0       2    66.7    50     1    NA    NA     1   3.3     0\n 5 0       3    66.7    50     1    NA    NA     1   6.6     0\n 6 0       6    66.7    50     1    NA    NA     1   9.1     0\n 7 0       9    66.7    50     1    NA    NA     1  10.8     0\n 8 0      12    66.7    50     1    NA    NA     1   8.6     0\n 9 0      24    66.7    50     1    NA    NA     1   5.6     0\n10 0      36    66.7    50     1    NA    NA     1   4       0\n# ℹ 279 more rows\n```\n:::\n:::\n\n\nThe first column name is a little awkward for R, and ordinarily I'd use `janitor::clean_names()` to tidy them, but in this case it's just one column to rename so I'll use dplyr:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwarfpk <- warfpk |> dplyr::rename(id = `#ID`)\nwarfpk\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 289 × 10\n   id     time    wt   age   sex   amt  rate  dvid    dv   mdv\n   <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 0       0    66.7    50     1   100    -2     0  NA       1\n 2 0       0.5  66.7    50     1    NA    NA     1   0       0\n 3 0       1    66.7    50     1    NA    NA     1   1.9     0\n 4 0       2    66.7    50     1    NA    NA     1   3.3     0\n 5 0       3    66.7    50     1    NA    NA     1   6.6     0\n 6 0       6    66.7    50     1    NA    NA     1   9.1     0\n 7 0       9    66.7    50     1    NA    NA     1  10.8     0\n 8 0      12    66.7    50     1    NA    NA     1   8.6     0\n 9 0      24    66.7    50     1    NA    NA     1   5.6     0\n10 0      36    66.7    50     1    NA    NA     1   4       0\n# ℹ 279 more rows\n```\n:::\n:::\n\n\nThere's one slightly puzzling thing here: the `id` column looks like it's supposed to be a numeric id for the study participants, but it's been parsed as a character vector. That's usually a sign that there's a problem somewhere in the data. A bit of digging reveals there's something peculiar going on with subject 12:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwarfpk |> dplyr::filter(id |> stringr::str_detect(\"12\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 11 × 10\n   id     time    wt   age   sex   amt  rate  dvid    dv   mdv\n   <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 12      0    75.3    32     1   113    -2     0  NA       1\n 2 12      1.5  75.3    32     1    NA    NA     1   0.6     0\n 3 #12     3    75.3    32     1    NA    NA     1   2.8     0\n 4 12      6    75.3    32     1    NA    NA     1  13.8     0\n 5 12      9    75.3    32     1    NA    NA     1  15       0\n 6 12     24    75.3    32     1    NA    NA     1  10.5     0\n 7 12     36    75.3    32     1    NA    NA     1   9.1     0\n 8 12     48    75.3    32     1    NA    NA     1   6.6     0\n 9 12     72    75.3    32     1    NA    NA     1   4.9     0\n10 12     96    75.3    32     1    NA    NA     1   2.4     0\n11 12    120    75.3    32     1    NA    NA     1   1.9     0\n```\n:::\n:::\n\n\nIt's easy enough to remove the `#` character and convert the id variable to numeric:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwarfpk <- warfpk |> \n  dplyr::mutate(\n    id = id |> \n      stringr::str_remove_all(\"#\") |> \n      as.numeric()\n  )\nwarfpk\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 289 × 10\n      id  time    wt   age   sex   amt  rate  dvid    dv   mdv\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1     0   0    66.7    50     1   100    -2     0  NA       1\n 2     0   0.5  66.7    50     1    NA    NA     1   0       0\n 3     0   1    66.7    50     1    NA    NA     1   1.9     0\n 4     0   2    66.7    50     1    NA    NA     1   3.3     0\n 5     0   3    66.7    50     1    NA    NA     1   6.6     0\n 6     0   6    66.7    50     1    NA    NA     1   9.1     0\n 7     0   9    66.7    50     1    NA    NA     1  10.8     0\n 8     0  12    66.7    50     1    NA    NA     1   8.6     0\n 9     0  24    66.7    50     1    NA    NA     1   5.6     0\n10     0  36    66.7    50     1    NA    NA     1   4       0\n# ℹ 279 more rows\n```\n:::\n:::\n\n\nThat said... even more digging revealed that the `#` character appears to be serving a specific function when used as a prefix in this data file. Later on, it turns out that the NONMEM control file used to specify the model for these data uses the following line to specify the data:\n\n``` fortran\n$DATA ..\\warfpk.csv IGNORE=#\n```\n\nThis instruction indicates that lines with the prefix `#` are ignored. What I'm guessing here is that this observation was dropped from the data set in the tutorial for some reason. It's not obvious to me why that was the case. It's possible, then, that what I should be doing instead is filtering out that row in the data. \n\n### Interpreting the data\n\nThe csv file doesn't say give much information about the variables. However, digging into the output files included in the workshop reveals the citations for the original papers. The data originate in papers by O'Reilly and colleagues, published in 1963 and 1968. Both papers are available online in full text, and after reading through them, we can reverse engineer (most of!) a data dictionary:\n\n- `id`: Numeric value specifying the arbitrary identifier for each person\n- `time`: Time elapsed since dose was administered (in hours)\n- `wt`: Weight of each person (in kilograms)\n- `age`: Age of each person (in years)\n- `sex`: Biological sex of each person (0 = female, 1 = male)^[Technically I'm guessing the code here, but there's a lot more 1s in the data than 0s, and a lot more of male subjects reported by O'Reilly & Aggeler, so it seems a safe bet!]\n- `amt`: Dose administered to this person at this time point (in milligrams)\n- `rate`: Uncertain what this refers to, but it has value -2 when drug is administered and missing otherwise\n- `dvid`: Appears to be a dummy variable indicating whether the dependent variable was measured at this time point (0 = false, 1 = true)\n- `dv`: Measured value of the dependent variable (plasma warfarin concentration, in mg/L)\n- `mdv`: Appears to be a dummy variable that is the reverse of `dvid`, and is presumably an indicator variable whose meaning is \"missing dependent variable\" (0 = false, 1 = true)\n\nWe can see the dosing information by filtering the data on `dvid`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwarfpk |> dplyr::filter(dvid == 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 32 × 10\n      id  time    wt   age   sex   amt  rate  dvid    dv   mdv\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1     0     0  66.7    50     1   100    -2     0    NA     1\n 2     1     0  66.7    50     1   100    -2     0    NA     1\n 3     2     0  66.7    31     1   100    -2     0    NA     1\n 4     3     0  80      40     1   120    -2     0    NA     1\n 5     4     0  40      46     0    60    -2     0    NA     1\n 6     5     0  75.3    43     1   113    -2     0    NA     1\n 7     6     0  60      36     0    90    -2     0    NA     1\n 8     7     0  90      41     1   135    -2     0    NA     1\n 9     8     0  50      27     0    75    -2     0    NA     1\n10     9     0  70      28     1   105    -2     0    NA     1\n# ℹ 22 more rows\n```\n:::\n:::\n\n\nSimilarly we can see the data from a single person by filtering on `id`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwarfpk |> dplyr::filter(id == 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 × 10\n     id  time    wt   age   sex   amt  rate  dvid    dv   mdv\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1     1     0  66.7    50     1   100    -2     0  NA       1\n2     1    24  66.7    50     1    NA    NA     1   9.2     0\n3     1    36  66.7    50     1    NA    NA     1   8.5     0\n4     1    48  66.7    50     1    NA    NA     1   6.4     0\n5     1    72  66.7    50     1    NA    NA     1   4.8     0\n6     1    96  66.7    50     1    NA    NA     1   3.1     0\n7     1   120  66.7    50     1    NA    NA     1   2.5     0\n```\n:::\n:::\n\n\n### Plotting the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nwarfpk |> \n  dplyr::filter(\n    dvid == 1, # only include measured times\n    !is.na(dv) # ignore missing dv cases\n  ) |>\n  ggplot(aes(x = time, y = dv, group = id)) + \n  geom_line(color = \"grey50\") +\n  geom_point() +\n  labs(\n    x = \"Time since dose (hours)\", \n    y = \"Plasma concentration (mg/L)\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/warfarin-data-1.png){width=672}\n:::\n:::\n\n\n## Deciphering NONMEM specifications\n\n### Notation from NONMEM\n\nThe original workshop is designed to be conducted using [NONMEM](https://www.iconplc.com/solutions/technologies/nonmem/), and I've been told to expect that notation used by NONMEM is pretty standard in the field, so I'll try my very best to stick to that notation. The two tutorial papers by Bauer (2019) were helpful for me in figuring this out, as was the older paper by Bauer et al (2007) that is a little more explicit about the statistical formulation of the models. As far as I can tell from the notation in the 2007 paper, the following structural conventions are applied:\n\n- Italicised lower case Greek symbols refer to scalar parameters: $\\theta$, $\\omega$, $\\sigma$, etc\n- Boldfaced upper case Greek symbols denote parameter vectors: $\\boldsymbol\\theta$, $\\boldsymbol\\omega$, $\\boldsymbol\\sigma$, etc\n- Boldfaced upper case Greek symbols denote parameter matrices: $\\boldsymbol\\Theta$, $\\boldsymbol\\Omega$, $\\boldsymbol\\Sigma$, etc\n\nThere is also a convention assigning meaning to the different Greek letters:\n\n- Population mean parameters are denoted $\\theta$\n- Population variance parameters are denoted $\\omega$\n- Individual departures from population mean and for $i$-th individual, $\\eta_i$\n- Standard deviation of error terms is denoted $\\sigma$ (i.e., variance $\\sigma^2$)\n- Difference between individual subject expected value and observation, $\\epsilon_{ij}$`\n\nAs an example, consider a simple one-compartment IV bolus model with first-order elimination, given dose $D$. If we let the function $f(t, k, V)$ denote the function describing how drug concentration changes as a function of time $t$, elimination rate $k$, and volume of distribution $V$. For this model, \n\n$$\nf(t, k, V, D) = \\frac{D}{V} \\exp(-kt)\n$$\n\nIn this model, the measurement time $t$ and dose $D$ (administered at $t = 0$) are both part of the study design. The other two quantities $k$ and $V$, are model parameters that can be different for every person. At a population level, then we will have a parameter vector $\\boldsymbol{\\theta} = (\\theta_1, \\theta_2)$ where I'll somewhat arbitrarily say that $\\theta_1$ is the typical value for $k$, and $\\theta_2$ is the typical value for $V$. Since these quantities can vary from person to person, we would also -- assuming for the sake of simplicity that there is no population correlation between them^[If we wanted to consider this correlation then we'd have a full variance-covariance matrix denoted $\\boldsymbol\\Omega$, but I'm not going to go there in this post] -- have a variance vector $\\boldsymbol{\\omega} = (\\omega_1, \\omega_2)$. \n\nIn this scenario, then, the parameters for the i-th participant would be some function of the typical values $\\theta$ and the random effects $\\eta$. For the moment I'll just use $g_1()$ and $g_2()$ to denote these transformation functions:\n\n$$\n\\begin{array}{rcl}\nk_i &=& g_1(\\theta_1, \\eta_{i1}) \\\\\nV_i &=& g_2(\\theta_2, \\eta_{i2})\n\\end{array}\n$$\nwhere the random effect terms $\\eta_{ik}$ are presumed to be normally distributed:\n\n$$\n\\eta_{ik} \\sim \\mbox{Normal}(0, \\omega_k) \n$$\n\nNext we have our pharmacokinetic function $f()$ that specifies how the plasma concentration changes as a function of time, dose, and the model parameters. Earlier I wrote out the specific form of this function for a particular model, but we could refer to it generically as $f(t, \\boldsymbol\\eta_i, \\boldsymbol\\theta, D_i)$.\n\nIf measurement errors are assumed to be additive (I'll come back to that in a moment), the observed concentration $y_{ij}$ for the i-th person at the j-th time point:\n\n$$\ny_{ij} = f(t_j, \\boldsymbol\\eta_i, \\boldsymbol\\theta, D_i) + \\epsilon_{ij}\n$$\n\nwhere $\\epsilon_{ij}$ is the error associated with person i and time j, and\n\n$$\n\\epsilon_{ik} \\sim \\mbox{Normal}(0, \\sigma^2) \n$$\n\nWith that as preliminary exposition, I think I can now make sense of how the model specification in NONMEM works...\n\n\n### Reading a NONMEM control file\n\nLooking at the control file for the model (i.e., the `.ctl` file), there's a bit of effort required for me -- as someone who doesn't use NONMEM -- to work out what structure of the underlying model is. The key line in the control file is the one specifying the subroutines:\n\n``` fortran\n$SUBR ADVAN2 TRANS2\n```\n\nThe workshop notes helpfully explain this. In NONMEM terminology, this refers to two different modules: ADVAN provides a library of pharmacokinetic models that are bundled with the software, and TRANS specifies parameter transformation. Of particular importance: ADVAN2 refers to a one-compartment model with a first order absorption process. Okay that's super handy because I've implemented one of those from scratch in Stan previously! \n\nThe file continues, specifying the pharmacokinetic model (PK) and the error model (ERROR):\n\n``` fortran\n$PK\n\n  ; COVARIATE MODEL\n  TVCL=THETA(1)\n  TVV=THETA(2)\n  TVKA=THETA(3)\n\n  ; MODEL FOR RANDOM BETWEEN SUBJECT VARIABILITY\n  CL=TVCL*EXP(ETA(1))\n  V=TVV*EXP(ETA(2))\n  KA=TVKA*EXP(ETA(3))\n\n  ; SCALE CONCENTRATIONS\n  S2=V\n\n$ERROR\n  Y=F+EPS(1)\n  IPRED=F\n```\n\nMy goal is to re-write this model in Stan, but to do that I need to first express that as a statistical model rather as NONMEM syntax. So let's start at the population level. We have three parameters here:\n\n- A population typical value for the clearance rate CL, denoted TVCL\n- A population typical value for the distribution volume V, denoted TVV\n- A population typical value for the absorption rate KA, denoted TVKA\n\nThe mapping here is straightfoward:\n\n$$\n\\begin{array}{rcl}\n\\mbox{TVCL} &=& \\theta_1 \\\\\n\\mbox{TVV} &=& \\theta_2 \\\\\n\\mbox{TVKA} &=& \\theta_3\n\\end{array}\n$$\n\nNow we consider the individual-subject level. At this level we have three parameters per person. For the i-th person, these parameters are:\n\n- The clearance rate CL$_i$\n- The distribution volume V$_i$\n- The absorption rate KA$_i$\n\nAs usual, the random effect terms $\\eta$ are normally distributed with mean zero and variance $\\omega$, and the $\\theta$ values are considered fixed effects. However, the population level and subject level parameters do not combine additively, they combine multiplicatively. Specifically, the $g(\\theta, \\eta)$ functions for this model are as follows:\n\n$$\n\\begin{array}{rcl}\n\\mbox{CL}_i &=& \\theta_1 \\exp(\\eta_{1i}) \\\\\n\\mbox{V}_i &=& \\theta_2 \\exp(\\eta_{2i}) \\\\\n\\mbox{KA}_i &=& \\theta_3 \\exp(\\eta_{3i})\n\\end{array}\n$$\n\nSo far, so good. This makes sense of most of the model specification, but there are a still some confusing parts that require a bit more digging around to decipher. First off, this strange invocation:\n\n``` fortran\n  ; SCALE CONCENTRATIONS\n  S2=V\n```\n\nThis doesn't make sense unless you know something about the way that NONMEM has implemented the underlying model. In the 1-compartment IV bolus model that I used as my motivating example (previous section), the pharmacokinetic function $f()$ has a closed form expression for the drug *concentration* in the central (only) compartment. However, when you implement a pharmacokinetic model using a system of ordinary differential equations (like I did in an earlier post), the values produced by solving the ODE typically refer to the *amount* of drug in the relevant compartment. To convert these amounts to concentrations you need to scale them, generally by dividing by the volume of said compartment. And thus we have our explanation of the mysterious `S2=V` instruction. The `S2` parameter is the NONMEM scaling parameter for the central compartment. We set this equal to `V`, i.e., the estimated volume parameter for each subject.^[Honestly, I wasn't 100% certain that my interpretation was correct, but eventually I managed to find copies of the NONMEM user manuals online and they explain it there.] \n\nAt last we come to the error model:\n\n``` fortran\n$ERROR\n  Y=F+EPS(1)\n  IPRED=F\n```\n\nThe relevant part here is the line specifying the relationship between the pharmacokinetic function `F`, the error terms `EPS`, and the observed data `Y`. In this case it's additive, exactly in keeping with what I assumed in my toy example:\n\n$$\ny_{ij} = f(t_j, \\boldsymbol\\eta_i, \\boldsymbol\\theta, D_i) + \\epsilon_{ij}\n$$\n\nIt doesn't have to be. In fact, the hands-on exercise in Lecture 2 of the tutorial I'm working through prepares three versions of this model, one with additive error, one with multiplicative error, and one with a hybrid error model that incorporates additive and multiplicative components. I'll get to that later, though probably in a future post.\n\nYay! At long last I think I know the model I need to implement...\n\n## Implementation in Stan\n\nLast time around I wrote my ODEs using notation that made sense to me. This time around I'll try to bring my notation a little closer to the terminology used in the NONMEM control file I was working from. There are two drug amounts that we need to keep track of, the amount $\\mbox{A}_g$ in the gut that has not yet been absorbed into systemic circulation, and the amount $\\mbox{A}_c$ currently in circulation (in the central/only compartment). The derivatives of these two quantities with respect to time form a system of differential quations:\n\n\n$$\n\\begin{array}{rcl}\n\\displaystyle \\frac{d\\mbox{A}_g}{dt} &=& -\\mbox{KA} \\times \\mbox{A}_{g} \\\\ \\\\\n\\displaystyle \\frac{d\\mbox{A}_c}{dt} &=& \\displaystyle \\mbox{KA} \\times \\mbox{A}_{g} - \\frac{\\mbox{CL}}{\\mbox{V}} \\ \\mbox{A}_{c}\n\\end{array}\n$$\n\nWhen we implement the full model in Stan what we actually want to model is the drug *concentration* in the central compartment as a function of time, and for that we'll need to solve for $\\mbox{A}_c$. \n\n### Modelling code\n\nThere's some nuances here in how we format the data for Stan. Stan doesn't permit [ragged arrays](https://mc-stan.org/docs/stan-users-guide/ragged-data-structs.html), so we'll have to pass the observations as one long `c_obs` vector that aggregates across subjects. Within the model (see below), we'll use the `n_obs` vector that records the number of observations per subject to create break points that we can use to extract data from a single person. Here's the R code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwarfpk_obs <- warfpk[warfpk$mdv == 0, ]\nwarfpk_amt <- warfpk[!is.na(warfpk$rate), ]\n\nt_fit <- c(\n  seq(.1, .9, .1),\n  seq(1, 2.75, .25),\n  seq(3, 9.5, .5),\n  seq(10, 23, 1),\n  seq(24, 120, 3)\n)\n\ndat <- list(\n  n_ids = nrow(warfpk_amt),\n  n_tot = nrow(warfpk_obs),\n  n_obs = purrr::map_int(\n    warfpk_amt$id,\n    ~ nrow(warfpk_obs[warfpk_obs$id == .x, ])\n  ),\n  t_obs = warfpk_obs$time,\n  c_obs = warfpk_obs$dv,\n  dose = warfpk_amt$amt,\n  t_fit = t_fit,\n  n_fit = length(t_fit)\n)\n```\n:::\n\n\nNow let's have a look at the Stan code. At some point I'd like to start using Torsten for these things rather than reinventing the wheel and coding a standard compartmental model from scratch. However, I'm a bottom-up kind of person^[If you believe the testimony of my ex-boyfriends, that is.] and I find it useful to do it myself a few times before I start relying on pre-built model code.  \n\n\n::: {.cell file='model1.stan' filename='model1.stan' output.var='mod'}\n\n```{.stan .cell-code  code-line-numbers=\"true\"}\nfunctions {\n  vector amount_change(real time,\n                       vector state,\n                       real KA,\n                       real CL,\n                       real V) {\n    vector[2] dadt;\n    dadt[1] = - (KA * state[1]);\n    dadt[2] = (KA * state[1]) - (CL / V) * state[2];\n    return dadt;\n  }\n}\n\ndata {\n  int<lower=1> n_ids;\n  int<lower=1> n_tot;\n  int<lower=1> n_fit;\n  array[n_ids] int n_obs;\n  vector[n_ids] dose;\n  array[n_tot] real t_obs;\n  vector[n_tot] c_obs;\n  array[n_fit] real t_fit;\n}\n\ntransformed data {\n  array[n_ids] int start;\n  array[n_ids] int stop;\n  array[n_ids] vector[2] initial_amount;\n  real initial_time = 0;\n\n  // break points within the data vector\n  start[1] = 1;\n  stop[1] = n_obs[1];\n  for(i in 2:n_ids) {\n    start[i] = start[i - 1] + n_obs[i - 1];\n    stop[i] = stop[i - 1] + n_obs[i];\n  }\n\n  // initial states for each person\n  for(i in 1:n_ids) {\n    initial_amount[i][1] = dose[i];\n    initial_amount[i][2] = 0;\n  }\n}\n\nparameters {\n  real<lower=0> theta_KA;\n  real<lower=0> theta_CL;\n  real<lower=0> theta_V;\n  real<lower=.001> omega_KA;\n  real<lower=.001> omega_CL;\n  real<lower=.001> omega_V;\n  real<lower=.001> sigma;\n  vector[n_ids] eta_KA;\n  vector[n_ids] eta_CL;\n  vector[n_ids] eta_V;\n}\n\ntransformed parameters {\n  vector<lower=0>[n_ids] KA;\n  vector<lower=0>[n_ids] CL;\n  vector<lower=0>[n_ids] V;\n  array[n_tot] vector[2] amount;\n  vector[n_tot] c_pred;\n\n  for(i in 1:n_ids) {\n    // pharmacokinetic parameters\n    KA[i] = theta_KA * exp(eta_KA[i]);\n    CL[i] = theta_CL * exp(eta_CL[i]);\n    V[i] = theta_V * exp(eta_V[i]);\n\n    // predicted drug amounts\n    amount[start[i]:stop[i]] = ode_bdf(\n      amount_change,            // ode function\n      initial_amount[i],        // initial state\n      initial_time,             // initial time\n      t_obs[start[i]:stop[i]],  // observation times\n      KA[i],                    // absorption rate\n      CL[i],                    // clearance\n      V[i]                      // volume\n    );\n\n    // convert to concentrations\n    for(j in 1:n_obs[i]) {\n      c_pred[start[i] + j - 1] = amount[start[i] + j - 1, 2] / V[i];\n    }\n  }\n}\n\nmodel {\n  // foolish priors over population parameters\n  // (parameter bounds ensure these are actually half-normals)\n  theta_KA ~ normal(0, 10);\n  theta_CL ~ normal(0, 10);\n  theta_V ~ normal(0, 10);\n  sigma ~ normal(0, 10);\n  omega_KA ~ normal(0, 10);\n  omega_CL ~ normal(0, 10);\n  omega_V ~ normal(0, 10);\n\n  // random effect terms\n  for(i in 1:n_ids) {\n    eta_KA[i] ~ normal(0, omega_KA);\n    eta_CL[i] ~ normal(0, omega_CL);\n    eta_V[i] ~ normal(0, omega_V);\n  }\n\n  // likelihood of observed concentrations\n  c_obs ~ normal(c_pred, sigma);\n}\n\ngenerated quantities {\n  array[n_ids, n_fit] vector[2] a_fit;\n  array[n_ids, n_fit] real c_fit;\n\n  for(i in 1:n_ids) {\n    // predicted drug amounts\n    a_fit[i] = ode_bdf(\n      amount_change,        // ode function\n      initial_amount[i],    // initial state\n      initial_time,         // initial time\n      t_fit,                // observation times\n      KA[i],                // absorption rate\n      CL[i],                // clearance\n      V[i]                  // volume\n    );\n\n    // convert to concentrations\n    for(j in 1:n_fit) {\n      c_fit[i, j] = a_fit[i, j][2] / V[i];\n    }\n  }\n}\n\n```\n:::\n\n\nSome things to note about this code:\n\n- In all honesty I don't think the ODE code is even needed here. With first-order absorption dynamics and first-order elimination dynamics, you can analytically calculate the pharmacokinetic curve using a [Bateman curve](https://en.wikipedia.org/wiki/Bateman_equation), which I talked about in a [previous post](https://blog.djnavarro.net/posts/2023-04-26_non-compartmental-analysis/). However, one of my goals here was to use the Stan ODE solvers in a population model, so for the purposes of this post I've chosen to do it the hard way.\n- Speaking of doing things the hard way, note that I've called `ode_bdf()` here rather than `ode_rk45()`. This is to avoid any issues of [ODE stiffness](https://en.wikipedia.org/wiki/Stiff_equation). While there is always the possibility that ODE can remain stiff in the posterior,^[...or so I'm told. Honestly, the last time I had to work so hard to keep a straight face in front of a statistical audience was writing academic papers that required me to talk about posterior Weiner processes.] I suspect the real issue here is that (a) I've chosen some absurdly diffuse priors, which means that the ODE can be quite poorly behaved during the warmup period for the sampler, and (b) earlier versions of the model had bugs that made the model do weird things. I strongly suspect that with those issues out of the way I could call `ode_rk45()` and get a considerable speedup. However, for the purposes of this post I'll leave it as is. \n\nIn any case, here's the R code to compile the model, run the sampler, and extract a summary representation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- cmdstanr::cmdstan_model(\"model1.stan\")\nout <- mod$sample(\n  data = dat,\n  chains = 4,\n  refresh = 1,\n  iter_warmup = 1000,\n  iter_sampling = 1000\n)\nout_summary <- out$summary()\n```\n:::\n\n\n### Parameter estimates\n\n\n::: {.cell}\n\n:::\n\n\nLet's start by taking a look at the summary. For the time being I'm only really interested in the population level parameters $\\boldsymbol\\theta$, $\\boldsymbol\\omega$, and $\\boldsymbol\\eta$, so I'll filter the results so that only those parameters are shown in the output:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nout_summary |> dplyr::filter(\n  variable |> stringr::str_detect(\"(theta|omega|sigma|lp)\")\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 × 10\n  variable     mean   median      sd     mad       q5     q95  rhat ess_bulk\n  <chr>       <dbl>    <dbl>   <dbl>   <dbl>    <dbl>   <dbl> <dbl>    <dbl>\n1 lp__     -104.    -103.    1.15e+1 1.12e+1 -124.    -86.2    1.00     407.\n2 theta_KA    0.789    0.669 4.99e-1 1.91e-1    0.449   1.47   1.01     301.\n3 theta_CL    0.135    0.134 7.71e-3 7.49e-3    0.123   0.148  1.00     869.\n4 theta_V     7.70     7.70  3.66e-1 3.52e-1    7.10    8.30   1.01     820.\n5 omega_KA    0.904    0.826 3.39e-1 2.46e-1    0.526   1.57   1.00     307.\n6 omega_CL    0.303    0.298 4.71e-2 4.56e-2    0.234   0.384  1.00    3257.\n7 omega_V     0.235    0.231 3.86e-2 3.68e-2    0.180   0.303  1.00    2551.\n8 sigma       1.08     1.08  5.71e-2 5.54e-2    0.994   1.18   1.00    3373.\n# ℹ 1 more variable: ess_tail <dbl>\n```\n:::\n:::\n\n\nThe R-hat values suggest that the chains are behaving themselves nicely, and all four chains are giving the same answers. Just to check, I'll plot the marginal distributions over all the population level parameters (and the log-probability) separately for each chain. To do that, first I'll extract the relevant columns from the `model1_draws.csv` file that contains all the raw samples:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws <- readr::read_csv(\"model1_draws.csv\", show_col_types = FALSE) |>\n  dplyr::select(tidyselect::matches(\"(theta|sigma|omega|lp|chain)\")) \n\ndraws\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4,000 × 9\n     lp__ theta_KA theta_CL theta_V omega_KA omega_CL omega_V sigma .chain\n    <dbl>    <dbl>    <dbl>   <dbl>    <dbl>    <dbl>   <dbl> <dbl>  <dbl>\n 1  -98.2    0.660    0.124    8.03    0.836    0.301   0.221 1.08       1\n 2  -98.2    0.442    0.132    7.89    1.16     0.249   0.225 1.00       1\n 3 -105.     0.376    0.142    7.94    1.17     0.366   0.230 1.07       1\n 4 -113.     0.620    0.150    7.57    1.11     0.342   0.243 1.12       1\n 5 -111.     0.700    0.150    7.47    1.22     0.316   0.209 0.993      1\n 6 -107.     0.831    0.152    7.62    1.21     0.345   0.251 1.02       1\n 7 -130.     0.741    0.152    7.69    1.38     0.312   0.288 0.988      1\n 8 -127.     0.750    0.157    7.36    1.24     0.272   0.280 1.21       1\n 9 -128.     0.733    0.151    7.36    1.26     0.239   0.267 1.20       1\n10 -104.     0.522    0.136    7.75    0.691    0.335   0.263 1.03       1\n# ℹ 3,990 more rows\n```\n:::\n:::\n\n::: {.cell .column-page}\n\n```{.r .cell-code}\ndraws_long <- draws |> \n  tidyr::pivot_longer(\n    cols = !tidyselect::matches(\"(id|chain)\"),\n    names_to = \"parameter\",\n    values_to = \"value\"\n  ) \n\ndraws_long |> \n  ggplot(aes(x = factor(.chain), value, fill = parameter)) +\n  geom_violin(draw_quantiles = c(.25, .5, .75), show.legend = FALSE) + \n  facet_wrap(~ parameter, scales = \"free\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/posterior-density-plots-1.png){width=1536}\n:::\n:::\n\n\nHere's a truncated version of the table showing the posterior mean and standard deviations for population parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraws_long |> \n  dplyr::group_by(parameter) |>\n  dplyr::summarise(\n    mean = mean(value),\n    sd = sd(value)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 × 3\n  parameter     mean       sd\n  <chr>        <dbl>    <dbl>\n1 lp__      -104.    11.5    \n2 omega_CL     0.303  0.0471 \n3 omega_KA     0.904  0.339  \n4 omega_V      0.235  0.0386 \n5 sigma        1.08   0.0571 \n6 theta_CL     0.135  0.00771\n7 theta_KA     0.789  0.499  \n8 theta_V      7.70   0.366  \n```\n:::\n:::\n\n\nFor comparison, here are the corresponding estimates and standard errors in the NONMEM output:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 × 3\n  parameter estimate std_error\n  <chr>        <dbl>     <dbl>\n1 omega_CL     0.285   0.0405 \n2 omega_KA     0.621   0.158  \n3 omega_V      0.222   0.0320 \n4 sigma        1.02    0.131  \n5 theta_CL     0.134   0.00724\n6 theta_KA     0.59    0.130  \n7 theta_V      7.72    0.347  \n```\n:::\n:::\n\n\nThey are mostly in agreement, though there are some modest differences in the estimates of the population mean and variability in the absorption rate KA. \n\n<!-- \nNote: I feel like there should be a .res file, but apparently there isn't one\nbundled with the tutorial. Fortunately, it looks like everything that matters \nis also in the .smr file. Here's the cut-and-paste for the relevant bits:\n\nTHETA:      POP_CL      POP_V       POP_KA      \nTHETA     = 0.134       7.72        0.59\n\nETA:        PPV_CL      PPV_V       PPV_KA      \nETASD     = 0.285       0.222       0.621\n\nEPSSD     = 1.020\n\nTHETA:se% = 5.4         4.5         22.0\nETASD:se% = 14.2        14.4        25.4\nEPSSD:se% = 12.8\n\nJudging from the \"se%\" part, the fact that these numbers are completely\nunhinged if treated as literal standard errors, and the fact that rescaling\nplaces most of the standard error estimates in alignment with the Bayesian\noutput, I'm pretty certain these are reported as percentages.\n-->\n\n\n### Model fits\n\nUnder normal circumstances I could simply use the `$summary()` method to do all the work for me in extracting parameter estimates. However, because I'm having fun playing with the raw samples data, I'm going to do it the long way...\n\n\n::: {.cell hash='index_cache/html/manual-summary_863c9471862e64e72dc76eb704513be3'}\n\n```{.r .cell-code}\nfit <- \"model1_draws.csv\" |> \n  readr::read_csv(show_col_types = FALSE) |>\n  dplyr::select(tidyselect::matches(\"c_pred\")) |>\n  tidyr::pivot_longer(\n    cols = tidyselect::matches(\"c_pred\"),\n    names_to = \"variable\",\n    values_to = \"y_hat\"\n  ) |> \n  dplyr::mutate(\n    obs_num = variable |> \n      stringr::str_remove_all(\".*\\\\[\") |>\n      stringr::str_remove_all(\"\\\\]\") |> \n      as.numeric()\n  ) |>\n  dplyr::group_by(obs_num) |>\n  dplyr::summarise(\n    yh = mean(y_hat),\n    q5 = quantile(y_hat, .05),\n    q95 = quantile(y_hat, .95)\n  )  |>\n  dplyr::arrange(obs_num) |>\n  dplyr::mutate(\n    y = dat$c_obs,\n    time = warfpk_obs$time,\n    id = warfpk_obs$id\n  )\n\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 251 × 7\n   obs_num    yh    q5   q95     y  time    id\n     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1       1  1.44  1.15  1.75   0     0.5     0\n 2       2  2.69  2.18  3.23   1.9   1       0\n 3       3  4.70  3.93  5.52   3.3   2       0\n 4       4  6.20  5.30  7.11   6.6   3       0\n 5       5  8.62  7.73  9.50   9.1   6       0\n 6       6  9.28  8.36 10.2   10.8   9       0\n 7       7  9.14  8.16 10.1    8.6  12       0\n 8       8  6.67  5.79  7.58   5.6  24       0\n 9       9  4.49  3.53  5.43   4    36       0\n10      10  3.01  2.04  4.00   2.7  48       0\n# ℹ 241 more rows\n```\n:::\n:::\n\n::: {.cell .column-page}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nggplot(fit, aes(yh, y, colour = factor(id))) +\n  geom_abline(intercept = 0, slope = 1, colour = \"grey50\") +\n  geom_point(size = 4, show.legend = FALSE) +\n  facet_wrap(~ factor(id), nrow = 4)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/simple-plot-1.png){width=1536}\n:::\n:::\n\n::: {.cell hash='index_cache/html/pk-predictions_296e0b65c2a1e440a0dd1956834d70ec'}\n\n```{.r .cell-code}\nprd <- \"model1_draws.csv\" |> \n  readr::read_csv(show_col_types = FALSE) |>\n  dplyr::select(tidyselect::matches(\"c_fit\")) |>\n  tidyr::pivot_longer(\n    cols = tidyselect::matches(\"c_fit\"),\n    names_to = \"variable\",\n    values_to = \"y_hat\"\n  ) |> \n  dplyr::group_by(variable) |>\n  dplyr::summarise(\n    yh = mean(y_hat),\n    q5 = quantile(y_hat, .05),\n    q95 = quantile(y_hat, .95)\n  ) |> \n  dplyr::mutate(\n    obs_id = variable |> \n      stringr::str_remove_all(\".*\\\\[\") |>\n      stringr::str_remove_all(\"\\\\]\") \n  ) |>\n  tidyr::separate(obs_id, into = c(\"id\", \"obs_num\"), sep = \",\") |>\n  dplyr::mutate(\n    obs_num = as.numeric(obs_num),\n    id = warfpk_amt$id[as.numeric(id)]\n  ) |>\n  dplyr::arrange(id, obs_num) |>\n  dplyr::mutate(time = rep(dat$t_fit, dat$n_ids))\n\nprd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2,496 × 7\n   variable       yh    q5   q95    id obs_num  time\n   <chr>       <dbl> <dbl> <dbl> <dbl>   <dbl> <dbl>\n 1 c_fit[1,1]  0.304 0.240 0.376     0       1   0.1\n 2 c_fit[1,2]  0.600 0.476 0.738     0       2   0.2\n 3 c_fit[1,3]  0.887 0.706 1.09      0       3   0.3\n 4 c_fit[1,4]  1.17  0.932 1.43      0       4   0.4\n 5 c_fit[1,5]  1.44  1.15  1.75      0       5   0.5\n 6 c_fit[1,6]  1.70  1.37  2.07      0       6   0.6\n 7 c_fit[1,7]  1.96  1.58  2.37      0       7   0.7\n 8 c_fit[1,8]  2.21  1.78  2.67      0       8   0.8\n 9 c_fit[1,9]  2.45  1.99  2.95      0       9   0.9\n10 c_fit[1,10] 2.69  2.18  3.23      0      10   1  \n# ℹ 2,486 more rows\n```\n:::\n:::\n\n::: {.cell .column-page}\n\n```{.r .cell-code}\nggplot() +\n  geom_ribbon(\n    data = prd, \n    mapping = aes(time, yh, ymin = q5, ymax = q95),\n    fill = \"grey80\"\n  ) +\n  geom_line(\n    data = prd,\n    mapping = aes(time, yh)\n  ) + \n  geom_point(\n    data = fit, \n    mapping = aes(time, y, colour = factor(id)), \n    size = 4, \n    show.legend = FALSE\n  ) +\n  geom_label(\n    data = tibble::tibble(\n      time = 110, \n      y = 17, \n      id = warfpk_amt$id\n    ),\n    mapping = aes(time, y, label = id)\n  )+ \n  facet_wrap(~ factor(id), nrow = 4) + \n  theme_bw() + \n  theme(\n    strip.text = element_blank(), \n    strip.background = element_blank()\n  ) +\n  labs(\n    x = \"Time (hours)\",\n    y = \"Warfarin plasma concentration (mg/L)\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/pk-profiles-1.png){width=1536}\n:::\n:::\n\n\n## Resources\n\n- Bauer, R. J., Guzy, S., & Ng, C. (2007). A survey of population analysis methods and software for complex pharmacokinetic and pharmacodynamic models with examples. *The AAPS Journal, 9*, E60-E83. [doi.org/10.1208/aapsj0901007](https://doi.org/10.1208/aapsj0901007)\n\n- Bauer, R. J. (2019). NONMEM tutorial part I: Description of commands and options, with simple examples of population analysis. *CPT: Pharmacometrics & Systems Pharmacology, 8*(8), 525-537. [doi.org/10.1002/psp4.12404](https://doi.org/10.1002/psp4.12404)\n\n- Bauer, R. J. (2019). NONMEM tutorial part II: Estimation methods and advanced examples. *CPT: Pharmacometrics & Systems Pharmacology, 8*(8), 538-556. [doi.org/10.1002/psp4.12422](https://doi.org/10.1002/psp4.12422)\n\n- Foster, D., Abuhelwa, A. & Hughes, J. (2019). *Population Analysis Using NONMEM Beginners Workshop*. Retrieved from: [www.paganz.org/resources/](https://www.paganz.org/resources/)\n\n- O'Reilly, R. A., & Aggeler, P. M. (1968). Studies on coumarin anticoagulant drugs: initiation of warfarin therapy without a loading dose. *Circulation, 38*(1), 169-177. [doi.org/10.1161/01.CIR.38.1.169](https://doi.org/10.1161/01.CIR.38.1.169)\n\n- O'Reilly, R. A., Aggeler, P. M., & Leong, L. S. (1963). Studies on the coumarin anticoagulant drugs: the pharmacodynamics of warfarin in man. *The Journal of Clinical Investigation, 42*(10), 1542-1551. [doi.org/10.1172%2FJCI104839](https://doi.org/10.1172%2FJCI104839)\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}