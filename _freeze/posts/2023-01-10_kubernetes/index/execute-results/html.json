{
  "hash": "654e379a91c13188ab2dc11be917cc0e",
  "result": {
    "markdown": "---\ntitle: \"Deploying R with kubernetes\"\nauthor:\n  - name: Danielle Navarro\n    url: https://djnavarro.net\n    affiliation: I'm on smoko\n    affiliation-url: https://www.youtube.com/watch?v=j58V2vC9EPc\n    orcid: 0000-0001-7648-6578\ndescription: \"In which it is painfully clear that the author is trying to figure it all out as she goes\"\ndate: \"2023-01-10\"\ncategories: [R, Docker, Kubernetes, plumber]\nimage: \"donut.png\"\n---\n\n\n<!--------------- my typical setup ----------------->\n\n\n\n\n\n<!--------------- post begins here ----------------->\n\n> Me: ooh I made a kubernetes app <br>\n> 10yo: I made a paper dragon <br>\n> Me: yeah... yours is cooler <br>\n> &nbsp; -- My daughter, reminding me that perspective is a thing\n\n\nStory time. There was a very weird moment in machine learning history, about 20 years ago, when the probabilistic AI folks were completely obsessed with Bayesian nonparametrics, and a disproportionate number of papers at NeurIPS had titles like \"[Cutesy prefix]: An infinite dimensional model of [something really boring]\". In most cases, you'd dig into the paper and discover that they hadn't done anything very special. All they'd done is implement a Bayesian model of [boring thing] that was ambiguous about the number of [components], and instead of thinking about what prior constraints make sense for the problem they were trying to solve, the authors used a [Chinese restaurant process](https://en.wikipedia.org/wiki/Chinese_restaurant_process) (CRP) to specify the conditional prior distribution over allocations of observations to components. The CRP has the mildly-interesting property that for any finite sample size there is a non-negligible conditional probability that the next observation belongs to a hitherto unobserved component, and asymptotically the partitions over observations it generates have a countably infinite number of components. Alas, exactly zero of these papers happened to have an infinitely large data set to train the model on, and without fail the results in the papers didn't appear to have anything \"infinite dimensional\" about them whatsoever.\n\nI say this with love and gentleness, dear reader, because I wrote quite a few of those papers myself.\n\nWhy do I tell this story in a blog post that has absolutely nothing to do with machine learning, statistics, or Bayesian inference? Because in a fit of pique, somewhere around 2006, I decided to do the damned reading myself and learned quite a lot of Bayesian nonparametrics. Not because I thought it would be useful, but because I was curious and I was getting extremely irritated at overconfident machine learning boys telling me that as a mere psychologist I couldn't possibly understand the depth of their thinking.\n\nWhich brings me, naturally enough, to [kubernetes](https://kubernetes.io/).\n\n## ggplot2 on kubernetes\n\nLet's start at the ending, shall we? The art shown below is generated at [donut.djnavarro.net](https://donut.djnavarro.net), and it is more-or-less unique. The site is designed to serve a different image every time it is accessed, using the timestamp as the seed to a generative art system written in R with ggplot2. If you refresh this page, the artwork will change:\n\n\n```{=html}\n<a href=\"https://donut.djnavarro.net\"><img width=\"100%\" src=\"https://donut.djnavarro.net\" title=\"donut.djnavarro.net\"></a>\n```\n\n\n<br>\n\nUnder the hood, the site is a kubernetes app running containerised R code with google kubernetes engine. Sounds fancy, right? \n\nWell, maybe. Shall we take a look at how it works? Perhaps, like so many other things in this world, it will turn out not to be anywhere near as complicated as it is made out to be.\n\n## Um. What is kubernetes? Do I care?\n\nThere's nothing I love more than looking at the website for a software tool and trying to work out what it does by reading how the developers have chosen to describe it. On the [kubernetes](https://kubernetes.io/) website they've gone with the headline *\"Production-Grade Container Orchestration\"*, and started with this:\n\n> Kubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.\n\nAs opening lines go it wouldn't get you a lot of attention on grindr^[Though who knows: there's a lot of strange on grindr.] but context makes a difference and it's not so terrible as a description of what kubernetes does. It's a useful tool if you need to deploy an application on a cluster^[Lots of computers working as a single unit.] and have that application run smoothly as you \"scale\" your cluster by adding more \"nodes\"^[Also known colloquially as \"computers\". A node in a cluster is one of the machines that makes up the cluster. I'm endlessly entertained by the human ability to make concepts impenetrable to outsiders by writing in jargon that is never explained to novices.] to the cluster. For the application I'm about to write, kubernetes is overkill. I don't actually need kubernetes to run something this simple, but this is a learning exercise. I'm doing it so that I can familiarise myself with core concepts. When I get to the part of the post that actually *does* something with kubernetes I'll start introducing terminology, but for now that's enough for us.\n\nShould you care as an R user? I mean, probably not. If you want a proper answer, Roel Hogervorst has an excellent blog post called [*\"WTF is Kubernetes and Should I Care as R User?\"*](https://www.r-bloggers.com/2022/04/wtf-is-kubernetes-and-should-i-care-as-r-user/) I won't duplicate content here: you should read the original post. But the short answer is that you probably won't need to run your own application using kubernetes, but you might need to contribute code to a larger application that uses it. If so, you may want to play around with kubernetes to make sure you understand what it does. \n\n## Write the R code\n\n> Likes to watch me in the glass room, bathroom <br>\n> Chateau Marmont, slippin' on my red dress, puttin' on my makeup <br>\n> Glass room, perfume, cognac, lilac fumes <br>\n> Says it feels like heaven to him <br>\n> &nbsp; -- Lana Del Rey^[Oh look it's a trans who likes Lana, how original.]\n\nLet's begin at the beginning. Anytime you want to write something, it helps to have something to say. There is nothing more tiresome than an op-ed writer trying to fill out 1000 words to make the Saturday paper deadline, but tech writing that tries to demonstrate a tool^[The author does not count.] without anything even remotely resembling an application runs a very close second. So let's at least *pretend* we have a use case for this yeah?\n\nIn real life I am an unemployed 40-something woman who smokes and drinks too much and makes very poor choices around men, but in my spare moments I make [generative art](https://art.djnavarro.net/) using R. It's an extremely unprofitable hobby^[Unless you are Thomas Lin Pedersen, but I think it is very clear that he is much taller than me.] but it's not completely without market value. Among other things the lovely folks at Posit were kind enough to pay me to put together an [\"art from code\"](https://art-from-code.netlify.app) workshop last year, and thanks to their kindness and my weird priorities there is now a nice little online tutorial that you can use to learn how to make generative art in R. What I'm going to do in this post is build a little kubernetes app that creates generative in R. It won't be very fancy, but hopefully you can see how an app like this could be expanded^[After a *staggering* amount of work, mind you...] to create a platform for \"long form generative art\" with R, not dissimilar to what [artblocks](https://www.artblocks.io/) or [fxhash](https://www.fxhash.xyz/) allow generative artists to do with javascript. The artist supplies code (in this case using R) that generates artwork, and the server uses that code to generate an arbitrary number of pieces that... idk, I guess you could sell them? Whatever. Do I look like a capitalist to you?\n\nTo build something like this we'll need some R code that creates generative art. I won't try to make anything too fancy here. In fact, I'll reuse code for the \"donuts\" system I used in my [multi-threaded task queues post](https://blog.djnavarro.net/queue/). It's a good choice for this application because the donuts system is something that is extremely easy to implement in R because the ggplot2 package provides tooling for creating data visualisations that use polar geometry^[To be honest I am still not convinced `coord_polar()` is particularly useful for visualisation because I very rarely have anything to say that is naturally expressed as a pie chart, but it's brilliant for generative art.]\n\nHere's how you build the system. I won't go into detail because this system is a very minor variation on [this one in my art-from-code tutorial](https://art-from-code.netlify.app/day-1/session-1/#composition), but here's the gist. First, the plot is going to need a colour scheme, so we'll define a function that samples a palette randomly with the assistance of the ggthemes package:\n\n\n::: {.cell filename='server.R'}\n\n```{.r .cell-code .numberLines startFrom=\"4\" code-line-numbers=\"true\"}\nsample_canva <- function(seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]]\n}\n```\n:::\n\n\nNext, we'll have a function that generates a table full of random numbers that we will later on map onto various plot aesthetics to make a pretty picture:\n\n\n::: {.cell filename='server.R'}\n\n```{.r .cell-code .numberLines startFrom=\"10\" code-line-numbers=\"true\"}\nsample_data <- function(seed = NULL, n = 100){\n  if(!is.null(seed)) set.seed(seed)\n  dat <- tibble::tibble(\n    x0 = stats::runif(n),\n    y0 = stats::runif(n),\n    x1 = x0 + stats::runif(n, min = -.2, max = .2),\n    y1 = y0 + stats::runif(n, min = -.2, max = .2),\n    shade = stats::runif(n),\n    size = stats::runif(n),\n    shape = factor(sample(0:22, size = n, replace = TRUE))\n  )\n}\n```\n:::\n\n\nNow comes the part of the system that does most of the artistic work, by defining a visual layout for any plots that are created using the system:\n\n\n::: {.cell filename='server.R'}\n\n```{.r .cell-code .numberLines startFrom=\"24\" code-line-numbers=\"true\"}\ndonut_style <- function(data = NULL, palette) {\n  ggplot2::ggplot(\n    data = data,\n    mapping = ggplot2::aes(\n      x = x0,\n      y = y0,\n      xend = x1,\n      yend = y1,\n      colour = shade,\n      linewidth = size\n    )) +\n    ggplot2::coord_polar(clip = \"off\") +\n    ggplot2::scale_y_continuous(\n      expand = c(0, 0),\n      limits = c(-1, 1),\n      oob = scales::oob_keep\n    ) +\n    ggplot2::scale_x_continuous(\n      expand = c(0, 0),\n      limits = c(0, 1),\n      oob = scales::oob_keep\n    ) +\n    ggplot2::scale_colour_gradientn(colours = palette) +\n    ggplot2::scale_linewidth(range = c(0, 6)) +\n    ggplot2::theme_void() +\n    ggplot2::theme(\n      panel.background = ggplot2::element_rect(\n        fill = palette[1], colour = palette[1]\n      )\n    ) +\n    ggplot2::guides(\n      colour = ggplot2::guide_none(),\n      linewidth = ggplot2::guide_none(),\n      fill = ggplot2::guide_none(),\n      shape = ggplot2::guide_none()\n    )\n}\n```\n:::\n\n\nThe last step is a function that puts it all together. The `donut()` function takes a single integer-valued input and returns a plot object that happens to look slightly pretty:\n\n\n::: {.cell filename='server.R'}\n\n```{.r .cell-code .numberLines startFrom=\"63\" code-line-numbers=\"true\"}\ndonut <- function(seed) {\n\n  dat <- sample_data(n = 200, seed = seed) |>\n    dplyr::mutate(y1 = y0, size = size / 3)\n\n  line_spec <- sample(c(\"331311\", \"11\", \"111115\"), 1)\n\n  pic <- donut_style(palette = sample_canva(seed = seed)) +\n    ggplot2::geom_segment(data = dat, linetype = line_spec)\n\n  if(stats::runif(1) < .5) {\n    pic <- pic +\n      ggplot2::geom_segment(\n        data = dat |> dplyr::mutate(y1 = y1 - .2, y0 = y0 - .2),\n        linetype = line_spec\n      )\n  }\n  if(stats::runif(1) < .5) {\n    pic <- pic +\n      ggplot2::geom_segment(\n        data = dat |> dplyr::mutate(y1 = y1 - .4, y0 = y0 - .4),\n        linetype = line_spec\n      )\n  }\n\n  pic\n}\n```\n:::\n\n\nHere it is in action:\n\n\n::: {.cell .column-page layout-ncol=\"3\"}\n\n```{.r .cell-code}\nfor(seed in 1:6) plot(donut(seed))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/donut-examples-1.png){width=576}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/donut-examples-2.png){width=576}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/donut-examples-3.png){width=576}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/donut-examples-4.png){width=576}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/donut-examples-5.png){width=576}\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/donut-examples-6.png){width=576}\n:::\n:::\n\n\nNot my finest work, but still pretty enough to be fun. \n\n## Expose an API\n\n> Are you posting hole on main again? <br>\n> &nbsp; -- Everyone who knows me, eventually\n\nThe next step in the process is to define a public API that specifies how visitors to the website can interact with the underlying R code. That's not something we typically do with R code because we aren't usually in the business of writing web applications in R, but thanks to endless joy that is the [plumber](https://www.rplumber.io/) this task can be accomplished with a few lines of code decoration:\n\n\n::: {.cell filename='server.R'}\n\n```{.r .cell-code .numberLines startFrom=\"91\" code-line-numbers=\"true\"}\n#* draws a donut plot\n#* @serializer svg list(width = 10, height = 10)\n#* @get /\nfunction(seed = NA) {\n  if(is.na(seed)) {\n    seed <- as.integer(Sys.time())\n  }\n  print(donut(seed))\n}\n```\n:::\n\n\nThere are a few things to note here:\n\n- The code decoration on line 93 specifies that the function defined in lines 94-99 will be called whenever an HTML `GET` request is sent to the `/` endpoint. Or, in simpler language, whenever someone visits the main page for the website that eventually ended up being hosted at [donut.djnavarro.net](https://donut.djnavarro.net). \n\n- The code decoration on line 92 how the output from the R function (an in-memory data structure) will be serialised (to a binary stream) and transmitted to the user.^[This isn't the right place for a deep dive on [serialising R objects](https://blog.djnavarro.net/serialisation-with-rds/), but I've written about it before.] In this case, the output is a plot object that would normally be handled by the R graphics device. What I've used plumber to do here, is have this output converted to an svg file. It's that svg file that the website will serve to the user.\n\n- Finally, notice that the function does take a `seed` argument.^[Arguments can be passed to a plumber endpoint through the query string. For instance, the URL [http://donut.djnavarro.net/?seed=6](http://donut.djnavarro.net/?seed=6) fixes the `seed` to 6, so the output will always be the same as the red and black donut shown in the bottom right of the output above.] I've set `NA` as the default value rather than the more conventional `NULL` because plumber won't accept a `NULL` default in this context.\n\nNoting that all the code I've presented so far belongs to a file called [`server.R`](https://github.com/djnavarro/donut/blob/main/server.R) (the link goes to the github repo for this \"donut\" side-project), I can start the web server running locally on port 3456 like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplumber::plumb(file=\"server.R\")$run(port = 3456)\n```\n:::\n\n\n\n\n## Containerise it\n\nAt this point in the process I have a perfectly functional webserver... that only runs on my machine which just happens to have the dependencies installed, and is only accessible locally from that machine. We'll need to fix both of those problems. \n\nLet's start by fixing the first one by running the website from within a docker container. Under normal circumstances I'd walk you through that process, but since I wrote a [long blog post about docker](https://blog.djnavarro.net/playing-with-docker/) just the other day, I'll jump straight to showing you the [`Dockerfile`](https://github.com/djnavarro/donut/blob/main/Dockerfile):\n\n:::{.column-page-right}\n\n\n::: {.cell filename='Dockerfile'}\n\n```{.dockerfile .cell-code  code-line-numbers=\"true\"}\nFROM rocker/r-ver:4.2.2\n\nLABEL org.opencontainers.image.source \"https://github.com/djnavarro/donut\"\nLABEL org.opencontainers.image.authors \"Danielle Navarro <djnavarro@protonmail.com>\"\nLABEL org.opencontainers.image.description DESCRIPTION\nLABEL org.opencontainers.image.licenses \"MIT\"\n\nRUN Rscript -e 'install.packages(c(\"ggplot2\", \"scales\", \"tibble\", \"dplyr\", \"plumber\", \"ggthemes\"))'\nCOPY server.R /home/server.R\nEXPOSE 80\nCMD Rscript -e 'plumber::plumb(file=\"/home/server.R\")$run(host=\"0.0.0.0\", port = 80)'\n```\n:::\n\n\n:::\n\nEvery instruction in this dockerfile is something I covered in the last post, except for the [`EXPOSE`](https://docs.docker.com/engine/reference/builder/#expose) instruction on line 10. That one tells the container to listen on port 80. It doesn't necessarily publish the output anywhere accessible from outside the container^[Here's a blog post on the difference between [exposing and publishing ports](https://nickjanetakis.com/blog/docker-tip-59-difference-between-exposing-and-publishing-ports) in docker if you need it!] but it does mean that the plumber web server running inside the container is listening on port 80 and can create a response when it receives a request. I'll deal with the publishing issue later.\n\n## Push it to the registry\n\nThe next step in the process is to host the image created from this dockerfile on a public registry.^[It doesn't have to be public: kubernetes can pull images from private registries too, but I'm not going to bother with that sort of thing here] I talked about that process in the last post too, so again I'll keep things simple. The process I followed for the donut project is essentially identical to the one I used in this [section of the docker post](https://blog.djnavarro.net/posts/2023-01-01_playing-with-docker/#hosting-images). I've created a github actions workflow that automatically builds the image on github and hosts it with the github container registry. The resulting image name is `ghcr.io/djnavarro/donut:main` and here's the [`build-image.yaml`](https://github.com/djnavarro/donut/blob/main/.github/workflows/build-image.yaml) workflow I'm using:\n\n:::{.column-page-right}\n\n\n::: {.cell filename='.github/workflows/build-image.yaml'}\n\n```{.yaml .cell-code  code-line-numbers=\"true\"}\nname: publish donut image\n\non:\n  push:\n    branches: ['main']\n\nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ${{ github.repository }}\n\njobs:\n  build-and-push-image:\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        include:\n          - dockerfile: ./Dockerfile\n            image: ghcr.io/djnavarro/donut\n\n    permissions:\n      contents: read\n      packages: write\n\n    steps:\n      - name: checkout repository\n        uses: actions/checkout@v2\n\n      - name: login to the container registry\n        uses: docker/login-action@f054a8b539a109f9f41c372932f1ae047eff08c9\n        with:\n          registry: ${{ env.REGISTRY }}\n          username: ${{ github.actor }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: extract metadata (tags, labels) for docker\n        id: meta\n        uses: docker/metadata-action@98669ae865ea3cffbcbaa878cf57c20bbf1c6c38\n        with:\n          images: ${{ matrix.image }}\n\n      - name: build and push docker image\n        uses: docker/build-push-action@ad44023a93711e3deb337508980b4b5e9bcdc5dc\n        with:\n          context: .\n          file: ${{ matrix.dockerfile }}\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n\n```\n:::\n\n\n:::\n\nAt long, long last we have all the precursors in place. We have a little web application that runs inside a docker container, and the image describing that container is hosted on a registry.^[If this were something fancier you'd probably have more than one container, but I'm keeping this as simple as possible for the sake of what's left of my sanity.] We can get started on the kubernetes side of things...\n\n## Create a kubernetes cluster\n\nAt the risk of stating the bloody obvious, if you want to use kubernetes to deploy an application on a cluster... you're probably going to need a cluster running kubernetes. You can get yourself one of these in lots of different ways but the way I'm going to do it is with GKE, the [google kubernetes engine](https://cloud.google.com/kubernetes-engine). You'll need a google account to do this, and yes this is something that they charge actual money for, but the good news is that when you sign up for google cloud services you get a few hundred dollars of credit to start with. That's pretty useful for novices: it's nice to be able to play around and learn the basics before you have to start worrying about what it's going to cost. \n\nIf you go down that path you can access your projects from the cloud console, located at [console.cloud.google.com](https://console.cloud.google.com/). Once there you can navigate to the various pages you'll need by clicking on links and menu items, but google offers a lot of different cloud services and it does take a little while for the interface to start feeling familiar, so I'll link to the pages you need directly as well.\n\nBefore you can create a cluster of your very own, you need to create a project. Pretty much everything you do with google cloud services is organised into projects so that's where we'll start. To create a project, go to [console.cloud.google.com/projectcreate](https://console.cloud.google.com/projectcreate) and follow the prompts. Give your project a fancy name that makes you sound cool: I called mine `donut-art`.\n\nNow that you have a project, you'll need to enable the specific google cloud services that your project will need access to. In this example the only thing I'll need is GKE itself, but in other situations you might need access to google cloud storage or something like that. To enable GKE on your current project go to [console.cloud.google.com/kubernetes/](https://console.cloud.google.com/kubernetes/). If it hasn't already been enabled for the project the page will ask if you want to. Even more conveniently, if you don't have a cluster running it will ask if you want to create one.^[I should mention that yes you can do all this with the `gcloud` command line tool, but I haven't reached the point in the post where I talk about that yet, and in any case the point-and-click process is actually pretty easy.] It will give you two options: an \"autopilot\" cluster is one where google will automatically manage the configuration for you, whereas for a \"standard\" cluster you'll have to be more explicit about how many nodes you want and how they are organised. There are situations where you need to use the standard cluster,^[An example would be if you plan to deploy [spark](https://spark.apache.org/) on kubernetes. I've been playing around with that a little and for that you really need to have google back off and not delete nodes whenever the autopilot thinks you don't need them. But that's not the case for the donut app so I'm keeping it simple.] but this is not one of those so I went with the autopilot approache because it's simpler. I didn't need to change any of the defaults: I called my cluster `donut-cluster`, and created it in the region `australia-southeast1`.^[Also known as \"Sydney\" to those of us who live here]\n\nHere's a screenshot showing you what the relevant bit of the google kubernetes engine console looks like for me now that I have a cluster up and running:\n\n![](donut-cluster.png)\n\nIf I click on the \"donut-cluster\" link it takes me to a page with a lot more detail, but you can see that some of the information is the same: \n\n<br>\n\n![](donut-cluster-detail.png)\n\n## Command line tools\n\nTime for a little digression.\n\nTake a look at the menu shown in the last screenshot. If I click on the \"connect\" button and it will reveal a command I can use to connect to the cluster from the terminal on my laptop... but it requires me to have the gcloud command line tools installed. Now, if you want to skip the step of installing all the tools you need locally by selecting the \"run in cloud shell\" option that also appears on the dialoh bog. However, I dislike the cloud shell and prefer to work from my own terminal. So, the next step is to install the command line tools. For this project, the two things I need are gcloud (to interact with google cloud services) and kubectl (to interact with kubernetes).\n\n### Installing gcloud\n\nIt turns out that installing the command line tools is relatively straightforward, and is made a lot easier thanks to the [gcloud installation instructions](https://cloud.google.com/sdk/docs/install) which are detailed and not too hard to follow. In addition to the basic tools, I installed the \"gke-gcloud-auth-plugin\" which are needed for authentication. Once the command line tools are installed, authentication from your terminal is a one-line command:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ngcloud auth login\n```\n:::\n\n\nI can now interact with my google cloud projects from my command line.\n\n### Installing kubectl\n\nThe second tool I need for this project is [kubectl](https://kubernetes.io/docs/reference/kubectl/), a command line tool used to control a kubernetes cluster. You can find installation instructions for different operating systems by visiting the kubernetes [install tools](https://kubernetes.io/docs/tasks/tools/) page. I'm doing this from a linux machine, so I also found it useful to enable autocompletion of kubectl commands within the bash shell. The instructions for this are included in the kubectl install page for linux.\n\n### Connect to the cluster\n\nNow that I have gcloud and kubectl running, I can connect to my cluster. The first thing to do is use gcloud to get the credentials needed to connect to my cluster:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n  export USE_GKE_GCLOUD_AUTH_PLUGIN=True\n  gcloud container clusters get-credentials donut-cluster \\\n    --zone australia-southeast1 \\\n    --project donut-art\n```\n:::\n\n\n```\nFetching cluster endpoint and auth data.\nkubeconfig entry generated for donut-cluster.\n```\n\nThen I can use kubectl to verify that it can connect to my cluster: \n\n\n::: {.cell}\n\n```{.bash .cell-code}\nkubectl cluster-info\n```\n:::\n\n\n```\nKubernetes control plane is running at blah blah blah\nGLBCDefaultBackend is running at blah blah blah\nKubeDNS is running at blah blah blah\nKubeDNSUpstream is running at blah blah blah\nMetrics-server is running at blah blah blah\n```\n\nOkay, I may have edited the output slightly. The missing bits are the various URLs. They aren't very interesting... the main thing to notice is that yes, kubectl can connect to my cluster and the cluster is up and running.\n\n## Kubernetes terminology\n\nNot surprisingly, kubernetes has a lot of terminology. That's quite daunting when you're getting started and I'm not going to attempt a complete glossary here. Instead, let's start with these four terms, since we'll use them a lot:\n\n- Container\n- Pod\n- Deployment\n- Service\n\nConceptually, it's also helpful to know these terms at the very beginning, even though frankly I'm not going to do anything with them here:\n\n- Node\n- Control Plane\n\nMore terms will appears as we go along -- and I'll try to explain all those when they pop up -- but these are the ones that I wish I'd understood properly before I started trying to play with kubernetes clusters.\n\n## Create a deployment\n\nThe way to configure your kubernetes cluster is with manifest files that are written in [yaml](https://en.wikipedia.org/wiki/YAML) format and use the `kubectl apply` command to update your cluster using the instructions laid out in the manifest file. You can use a manifest to modify any aspect to your cluster configuration, and later in this post I'll show a few more manifests, but for now here's the [`deployment.yaml`](https://github.com/djnavarro/donut/blob/main/deployment.yaml) file I'm using to specify a deployment for the donuts application:\n\n\n::: {.cell filename='deployment.yaml'}\n\n```{.yaml .cell-code  code-line-numbers=\"true\"}\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: donut-example\n  name: donut\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: donut-example\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: donut-example\n    spec:\n      containers:\n        - name: donut\n          image: ghcr.io/djnavarro/donut:main\n          imagePullPolicy: Always\n          resources:\n            requests:\n              memory: \"64Mi\"\n              cpu: \"250m\"\n            limits:\n              memory: \"128Mi\"\n              cpu: \"500m\"\n          ports:\n            - containerPort: 80\n```\n:::\n\n\nFor the moment, let's censor the metadata and everything that uses the metadata so that we can focus on what the rest of the manifest is doing:\n\n\n::: {.cell filename='deployment.yaml'}\n\n```{.yaml .cell-code  code-line-numbers=\"true\"}\napiVersion: apps/v1\nkind: Deployment\nmetadata: \n  [some metadata]\nspec:\n  replicas: 2\n  selector:\n    [use some metadata]\n  template:\n    metadata:\n      [some metadata]\n    spec:\n      containers:\n        - name: [names are metadata]\n          image: ghcr.io/djnavarro/donut:main\n          imagePullPolicy: Always\n          resources:\n            requests:\n              memory: \"64Mi\"\n              cpu: \"250m\"\n            limits:\n              memory: \"128Mi\"\n              cpu: \"500m\"\n          ports:\n            - containerPort: 80\n```\n:::\n\n\n\nThere's a lot going on here, so for the moment let's focus on the bottom.^[Too obvious.] The section of the code from lines 17-29 is used to specify the docker containers^[Kubernetes supports other types of containers besides docker, but let's not complicate matters.] that my cluster is going to deploy. There's only one container listed in this section, and the line that reads `image: ghcr.io/djnavarro/donut:main` is the way that I've specified the docker image to use when creating the container. There are other settings I've used to set up this container: I've asked kubernetes to allocate memory and cpu resources to the container, and I've exposed container port 80 (which, if you can remember back that far, is where the plumber web API is running inside the container). I've also set `imagePullPolicy: Always` to ensure that every time I update this deployment kubernetes will pull the image from the registry afresh. I did that because more often than not while I was writing code for the kubernetes deployment I was tweaking the image too, and I wanted to make sure that I was always trying to deploy the most recent version of the image.\n\nOkay, now that we know what's going on in that section of the code, let's collapse that part and think about the manifest file like this:\n\n\n::: {.cell filename='deployment.yaml'}\n\n```{.yaml .cell-code  code-line-numbers=\"true\"}\napiVersion: apps/v1\nkind: Deployment\nmetadata: \n  [some metadata]\nspec:\n  replicas: 2\n  selector:\n    [use some metadata]\n  template:\n    metadata:\n      [some metadata]\n    spec:\n      [details of one or more containers]\n```\n:::\n\n\nOkay, so now let's think about the bottom part of this code too. Lines 13 through 25 of this condensed pseudo-manifest describe some kind of template. But a template for what? Well, as clearly stated on line 25 in human(ish) language, it's a template for \"one or more containers\". In kubernetes terminology, a deployable thing that holds one or more containers is a pod... so this section of the code is describing a [pod template](https://kubernetes.io/docs/concepts/workloads/pods/#pod-templates). It's an instruction to kubernetes that says... \"hey, when you create a pod as part of this deployment, here's the template you should use\". So we can simplify again:\n\n\n::: {.cell filename='deployment.yaml'}\n\n```{.yaml .cell-code  code-line-numbers=\"true\"}\napiVersion: apps/v1\nkind: Deployment\nmetadata: \n  [some metadata]\nspec:\n  replicas: 2\n  selector:\n    [use some metadata]\n  template:\n    [details of the pod template]\n```\n:::\n\n\nIn this condensed form we can see that lines 5-10 provide a specification of the deployment itself. I've given it a pod template that tells kubernetes what the pods should look like, and I've specified the number of \"replicas\". How many copies of this pod do I want it to run in this deployment: for no good reason at all I decided to run two (i.e., two replicas). \n\nIf we simplify the manifest yet again, we can see the top-level description:\n\n\n::: {.cell filename='deployment.yaml'}\n\n```{.yaml .cell-code  code-line-numbers=\"true\"}\napiVersion: apps/v1\nkind: Deployment\nmetadata: \n  [some metadata]\nspec:\n  [details of the deployment]\n```\n:::\n\n\nThe only other things to point out right now is that the  \"kind\" field is used to tell kubernetes what type of object to create (a Deployment), and the \"apiVersion\" field is used to specify which version of the kubernetes API to use when interpreting the manifest. That's handy to note because later on I'll be using APIs that are a bit more specific to the google kubernetes engine.\n\nOkay, so now that we have some sense of what's going on in the `deployment.yaml` file (ignoring the fact that I've glossed over the metadata bits), let's actually apply it to our cluster:^[The `-f` argument in `kubectl apply` here tells it to use the manifest **f**ile.]\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nkubectl apply -f deployment.yaml\n```\n:::\n\n\nTo see if it's working we can use `kubectl get deployments`:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nkubectl get deployments\n```\n:::\n\n\n```\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\ndonut   2/2     2            2           19h\n```\n\nIt is alive.\n\nFor more details on this part of the process, check out the kubernetes documentation on [deploying a stateless application](https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/). You may also want to look at the page on [managing container resources](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) at this point. \n\n## Expose the deployment (no https)\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nkubectl expose deployment donut --type=LoadBalancer --name=donut-service\n```\n:::\n\n\nthen over in my website create a DNS record that points donut.djnavarro.net at the IP address\n\nmore detail:\n\nhttps://kubernetes.io/docs/concepts/services-networking/service/\n\nhttps://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/\n\n\n## Expose the deployment (with https)\n\n> I like the kick in the face <br>\n> And the things you do to me <br>\n> I love the way that it hurts <br>\n> I don't miss you, I miss the misery <br>\n> &nbsp; -- Halestorm\n\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ngcloud compute addresses create donut-ip-address --global --project donut-art\n```\n:::\n\n\n```\nCreated [https://www.googleapis.com/compute/v1/projects/donut-art/global/addresses/donut-ip-address]\n```\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ngcloud compute addresses describe donut-ip-address --global --project donut-art\n```\n:::\n\n\nThis prints out the IP address and some other details.\n\n\nNow create the managed certificate:\n\n\n::: {.cell filename='managed-cert.yaml'}\n\n```{.yaml .cell-code  code-line-numbers=\"true\"}\napiVersion: networking.gke.io/v1\nkind: ManagedCertificate\nmetadata:\n  name: managed-cert\nspec:\n  domains:\n    - donut.djnavarro.net\n```\n:::\n\n\nApply it to your cluster:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nkubectl apply -f managed-cert.yaml\n```\n:::\n\n\nSometimes when they say \"it might take an hour to...\" it only takes like 20 seconds. Yeah nah this is one where it actually took about an hour. We'll come back to it.\n\nNodePort: https://kubernetes.io/docs/concepts/services-networking/service/\n\nImportant bit: notice that under `spec.selector` I'm referring to the name of my deployment (donut-example). That's what the kubernetes docs tell you to do when setting up a NodePort service, but the help docs on the corresponding GKE page that I linked to at the start of the page are misleading: they make it look like you're supposed to use the name of the service not the app. \n\nProbably less important bit: I'm doing everything on port 80 regardless of what the GKE docs suggest because there was an issue at some point with hardcoding port 80 somewhere. Pretty certain that's been properly resolved now but it was one of the tweaks I made on the way to figuring out the problem with spec.selector and fuck it this works so I'm not changing it\n\n\n::: {.cell filename='mc-service.yaml'}\n\n```{.yaml .cell-code  code-line-numbers=\"true\"}\napiVersion: v1\nkind: Service\nmetadata:\n  name: mc-service\nspec:\n  selector:\n    app.kubernetes.io/name: donut-example\n  type: NodePort\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n```\n:::\n\n\nApply it to your cluster:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nkubectl apply -f mc-service.yaml\n```\n:::\n\n\nPoint the DNS record to the right place then...\n\nCreate the ingress:\n\n\n::: {.cell filename='mc-ingress.yaml'}\n\n```{.yaml .cell-code  code-line-numbers=\"true\"}\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: mc-ingress\n  annotations:\n    kubernetes.io/ingress.global-static-ip-name: donut-ip-address\n    networking.gke.io/managed-certificates: managed-cert\n    kubernetes.io/ingress.class: \"gce\"\nspec:\n  defaultBackend:\n    service:\n      name: mc-service\n      port:\n        number: 80\n```\n:::\n\n::: {.cell}\n\n```{.bash .cell-code}\nkubectl apply -f mc-ingress.yaml \n```\n:::\n\n\nNow let's get the results:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nkubectl get ingress\n```\n:::\n\n\n```\nNAME         CLASS    HOSTS   ADDRESS         PORTS   AGE\nmc-ingress   <none>   *       34.149.195.33   80      98s\n```\n\nMight have to wait a bit for provisioning to finish:\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nkubectl describe managedcertificate managed-cert\n```\n:::\n\n\nRelevant bit of the output once it's all working:\n\n```\nSpec:\n  Domains:\n    donut.djnavarro.net\nStatus:\n  Certificate Name:    mcrt-b2204ff4-ad92-4811-a56d-f007190bb659\n  Certificate Status:  Active\n  Domain Status:\n    Domain:     donut.djnavarro.net\n    Status:     Active\n```\n\n\n\n\n## Where next?\n\n- storage: the app generates a new image every time it is called. that's wasteful, especially if you're going to reuse images. enable google cloud storage and have the plumber app check for the relevant file on gcs before trying to generate a new one https://cloud.google.com/kubernetes-engine/docs/concepts/volumes  https://cloud.google.com/kubernetes-engine/docs/how-to/volumes\n- fancier things like spark-on-kubernetes already have helm charts. so you'd want to install helm and learn how to create deployments from that https://bitnami.com/stack/spark/helm\n\n<!--------------- appendices go here ----------------->\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}