{
  "hash": "351ddd1983697622dc49620b8b80bb75",
  "result": {
    "markdown": "---\ntitle: \"The Metropolis-Hastings algorithm\"\nauthor:\n  - name: Danielle Navarro\n    url: https://djnavarro.net\n    affiliation: I'm on smoko\n    affiliation-url: https://www.youtube.com/watch?v=j58V2vC9EPc\n    orcid: 0000-0001-7648-6578\ndescription: \"A note that I wrote for a computer science class I taught all the way back in 2010\"\ndate: \"2023-04-12\"\ncategories: [Statistics, MCMC]\n---\n\n\n<!--------------- my typical setup ----------------->\n\n\n\n\n\n<!--------------- post begins here ----------------->\n\nThis morning I received an email from a stranger, writing to say thank you for a document I wrote almost 13 years ago. It's a weird feeling every time I get one of those,^[Bizarrely, this actually happens to me a lot. It's totally surreal.] but a pleasant one. This time around, the document in question was a [note on the Metropolis-Hastings algorithm](https://compcogsci-3016.djnavarro.net/technote_metropolishastings.pdf) that I threw together in a rush for a computer science class I taught back in 2010.^[Another surreal experience that I've had quite a bit lately is getting rejected from data science jobs because I don't have a computer science degree and my qualifications are technically in psychology. Apparently I'm considered skilled enough to *teach* computational statistics to university computer science students, but still considered less skilled at those tools than the students that I taught? I mean, it's either that or tech company recruiters don't actually read the résumés that they get sent... but that couldn't possibly be true, right? Efficient market hypothesis and all that...] While drinking my second coffee of the morning and feeling the usual sense of dread I feel when I know that today I have to put some effort into looking for a job, yet again, I arrived at an excellent procrastination strategy...\n\nWhy don't I start rescuing some of the content that I wrote all those years ago and currently have hidden away in pdf files in the dark corners of the internet, and put them up on my blog? It won't get me a job, but it feels less demeaning than yet again trying to get tech company recruiters to believe that yes actually the middle aged lady with a psychology PhD does in fact know how to code and analyse data. \n\nAnyway. Without any further self-pity, here's a quick primer on the [Metropolis-Hastings algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm). The target audience for this is someone who has a little bit of probability theory, can write code in R (or similar), but doesn't have any background in Markov chain Monte Carlo methods. It doesn't dive deep into the mathematics (i.e., you *won't* find any discussions of detailed balance, ergodicity etc), but it does try to go deep enough to give a beginner some formal understanding of what the algorithm is doing.\n\n## The problem to be solved\n\nThe Metropolis-Hastings algorithm is perhaps the most popular example of a [Markov chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) (MCMC) method in statistics. The basic problem that it solves is to provide a method for sampling from some generic distribution, $P(x)$. The idea is that in many cases, you know\nhow to write out the equation for the probability $P(x)$, but you don’t know how to generate\na random number from this distribution, $x \\sim P(x)$. This is the situation where MCMC is\nhandy. In fact, for the Metropolis-Hastings algorithm we don’t even need to know how to\ncalculate $P(x)$ completely. For example, suppose I’ve become interested -- for reasons known but to the gods -- in the probability distribution shown below: \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nThe probability density function^[Some asides: my experience teaching this class is that it's quite common for people new to statistics to struggle with the concept of [probability density](https://en.wikipedia.org/wiki/Probability_density_function). It's not super important for the purposes of this post, and to a first approximation it's totally okay to think of $p(x)$ as \"the probability of observing $x$ (sort of)\", and the [distribution function](https://en.wikipedia.org/wiki/Cumulative_distribution_function) $P(x)$ as \"the probability of observing a value no larger than $x$\". These sorts of notational and technical details do matter once you start doing mathematical statistics, but this post is absolutely not that! I promise I was never so cruel as to inflict that stuff on my undergraduate computer science students! Never did I ever refer to sigma algebras in my undergrad teaching, honest. Though I did once inflict Kolmogorov complexity on a class, and apologised profusely for my error.] $p(x)$ for this distribution is given by the following equation:\n\n$$\np(x) = \\frac{\\exp(-x^2) \\left(2 + \\sin(5x) + \\sin(2x)\\right)}{\\int_{-\\infty}^\\infty \\exp(-u^2) \\left(2 + \\sin(5u) + \\sin(2u)\\right) \\ du}\n$$\n\nMy problem is that I either don't know *how* to solve the integral in the denominator, or (equally plausible) I'm simply too lazy to try. So this means in truth, I only know the distribution \"up to some unknown constant\". That is, I know that:\n\n$$\np(x) \\propto \\exp(-x^2) \\left(2 + \\sin(5x) + \\sin(2x)\\right)\n$$\nHow can I generate samples from this distribution?\n\n## The Metropolis-Hastings algorithm\n\nThe basic idea behind MCMC is very simple. The idea is to define a [Markov chain](https://en.wikipedia.org/wiki/Markov_chain)^[At this point in the class students had most certainly encountered Markov chains!] over possible $x$ values, in such a way that the stationary distribution of the Markov chain is in fact $P(x)$. That is, what we’re going to do is use a Markov chain to generate a sequence of $x$ values, denoted $(x_0, x_1, x_2, \\ldots, x_n)$, in such a way that as $n \\rightarrow \\infty$, we can guarantee that $x_n \\sim P(x)$. There are many different ways of setting up a Markov chain that has this property, one of which is the Metropolis-Hastings algorithm.\n\nHere's how it works. Suppose that the current state of the Markov chain is $x_n$, and\nwe want to generate $x_{n+1}$. In the Metropolis-Hastings algorithm, the generation of $x_{n+1}$ is a two-stage process. \n\n### The proposal step\n\nThe first stage is to generate a *candidate*, which we'll denote $x^∗$. The value of $x^∗$ is generated from the proposal distribution that we already know how to sample from. We denote this proposal distribution $Q(x^∗ | x_n)$, and the corresponding density function as $q(x^∗ | x_n)$. Notice that the distribution we sample from depends on the current state of the Markov chain, $x_n$. There are some technical constraints on what you can use as a proposal distribution, but for the most part it can be anything you like. A very typical way to do this is to use a normal distribution centered on the current state $x_n$. More formally, we write this as:\n\n$$\nx^* | x_n \\sim \\mbox{Normal}(x_n, \\sigma^2)\n$$\nfor some standard deviation $\\sigma$ that we select in advance (more on this later!)\n\n### The accept-reject step\n\nThe second stage is the accept-reject step. Firstly, what you need to do is calculate\nthe *acceptance probability*, denoted $A(x_n \\rightarrow x_∗)$, which is given by:^[Okay yeah I suppose I could have denoted the acceptance probability as $P(x_{n+1} = x^* | x_{n}, x^*)$ to properly capture the dependency on the current state and the proposal value, be clear about the position in the sequence and the fact that this is a probability mass function not a probability density function, but honestly does all that notation actually help anyone? If I follow this line of thought to its conclusion I'll end up talking about measures defined on Borel sets or something equally unhelpful to new learners.]\n\n$$\nA(x_n \\rightarrow x_∗) = \\min \\left(1, \\frac{p(x^*)}{p(x^n)} \\times \\frac{q(x_n | x^*)}{q(x^* | x_n)} \\right)\n$$\nThere are two things to pay attention to here. Firstly, notice that the ratio $\\frac{p(x^*)}{p(x^n)}$ doesn't depend on the normalising constant for the distribution. Or, to put it in a more helpful way, that integral in the first equation is completely irrelevant and we can ignore it. As a consequence, for our toy problem we can write this:\n\n$$\n\\frac{p(x^*)}{p(x^n)} = \\frac{\\exp(-{x^*}^2) \\left(2 + \\sin(5x^*) + \\sin(2x^*)\\right)}{\\exp(-{x_n}^2) \\left(2 + \\sin(5x_n) + \\sin(2x_n)\\right)}\n$$\nThat's a nice simple thing to compute with no need for any numerical integration or, gods forbid, solving the integral analytically. \n\nThe second thing to pay attention to is the behaviour of the other term, $\\frac{q(x_n | x^*)}{q(x^* | x_n)}$. What this term does is correct for any biases that the proposal distribution might induce. In this expression, the denominator $q(x^∗ | x_n)$ describes the probability with which you'd choose $x^*$ as the candidate if the current state of the Markov chain is $x_n$. The numerator, however, describes the probability of a transition that goes the other way: that is, if the current state had actually been $x^∗$, what is the probability that you would have generated $x^n$ as the candidate value? If the proposal distribution is symmetric, then these two probabilities will turn out to be equal. For example, if the proposal distribution is normal, then:\n\n$$\n\\begin{array}{rcl}\nq(x^* | x_n) & = & \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left( -\\frac{1}{2 \\sigma^2} \\left(x_n - x^*\\right)^2 \\right) \\\\\nq(x_n | x^*) & = & \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left( -\\frac{1}{2 \\sigma^2} \\left(x^* - x_n \\right)^2 \\right)\n\\end{array}\n$$\n\nClearly, $q(x^* | x_n) = q(x_n | x^*)$ for all choices of $x_n$ and $x^*$, and as a consequence the ratio $\\frac{q(x_n | x^*)}{q(x^* | x_n)}$ is always 1 in this case. \n\nThis special case of the Metropolis-Hastings algorithm, in which the proposal distribution is symmetric, is referred to as the *Metropolis algorithm*.\n\nOkay. Having proposed the candidate $x^∗$ and calculated the acceptance probability,\n$A(x_n \\rightarrow x^∗)$, we now either decide to \"accept\" the candidate and set $x_{n+1} = x^∗$ or we \"reject\" the candidate and set $x_{n+1} = x_n$. To make this decision, we generate a uniformly distributed random number between 0 and 1, denoted $u$. Then:\n\n$$\nx_{n+1} = \\left\\{ \n\\begin{array}{rl}\nx^* & \\mbox{ if } u \\leq A(x_n \\rightarrow x^∗) \\\\\nx_n & \\mbox{ otherwise}\n\\end{array}\n\\right.\n$$\nIn essence, this is the entirety of the Metropolis-Hastings algorithm! True, there are quite a\nfew technical issues that attach to this, and if you're interested in using the algorithm for\npractical purposes I strongly encourage you to do some further reading to make sure you\nunderstand the traps in detail, but for now I'll just give you some examples of things that\nwork and things that don't, to give you a bit of a feel for how it works in practice.\n\n## Some examples\n\nFirstly, let’s have a look at some R code implementing the Metropolis-Hastings algorithm for the toy problem (i.e., sample from the distribution shown in the original figure). Notice that in addition to the parameter `sigma`, we also need to specify the total number of samples that we are intending to draw `nsamp`, a start point for the chain `x0`, and two additional variables `burnin` and `lag` that I'll explain shortly. For the moment, however, let's start the chain near the middle of the distribution, but not quite there: we'll use `x0 = −1`. Then, let’s watch what happens to our sampler for three different values of `sigma`, where we run the sampler for 1000 iterations. In one case, we’ll set the value too low (`sigma = .025`) and in another we'll set it too high (`sigma = 50`), but in a third case we’ll get it about right (`sigma = 1`). \n\n\n::: {.cell}\n\n```{.r .cell-code}\ntarget <- function(x) {\n  exp(-x^2) * (2 + sin(5 * x) + sin(2 * x))\n}\n\nmetropolis_step <- function(x, sigma) {\n  proposed_x <- rnorm(1, mean = x, sd = sigma)\n  accept_prob <- min(1, target(proposed_x) / target(x))\n  u <- runif(1)\n  if(u <= accept_prob) {\n    value <- proposed_x\n    accepted <- TRUE\n  } else {\n    value <- x\n    accepted <- FALSE\n  }\n  out <- data.frame(value = value, accepted = accepted)\n  out\n}\n\nmetropolis_sampler <- function(initial_value, \n                               n = 1000, \n                               sigma = 1, \n                               burnin = 0, \n                               lag = 1) {\n  \n  results <- list()\n  current_state <- initial_value\n  for(i in 1:burnin) {\n    out <- metropolis_step(current_state, sigma)\n    current_state <- out$value\n  }\n  for(i in 1:n) {\n    for(j in 1:lag) {\n      out <- metropolis_step(current_state, sigma)\n      current_state <- out$value\n    }\n    results[[i]] <- out\n  }\n  results <- do.call(rbind, results)\n  results\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nout <- metropolis_sampler(initial_value = 0)\nout[1:10, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        value accepted\n1   0.3143686    FALSE\n2   0.3500985     TRUE\n3   0.3500985    FALSE\n4   0.3500985    FALSE\n5   0.3500985    FALSE\n6  -0.1665712     TRUE\n7  -0.9428251     TRUE\n8   0.4271852     TRUE\n9   0.3168997     TRUE\n10  0.3352164     TRUE\n```\n:::\n:::\n\n\nThe results are shown in Figure 3. For all three values of `sigma`, we have\ntwo plots. The top one shows the true target distribution, along with a histogram showing\nthe distribution of samples obtained using the Metropolis sampler. The lower panel plots\nthe actual Markov chain: the sequence of generated values. In the leftmost plots, we see\nwhat happens when we choose a good proposal distribution: the chain shown in the lower\npanel moves rapidly across the whole distribution, without getting stuck in any one place\n(the acceptance rate here is 47.5%). In the far right panel, we see what happens when the\nproposal distribution is too wide: the chain gets stuck in one spot for long periods of time. \nIt does manage to make big jumps, covering the whole range, but because the acceptance\nrate is so low (2.5%) the distribution is highly irregular. Finally, in the middle panel, if we\nset the proposal distribution to be too narrow, the acceptance rate is very high (97.7%) so\nthe chain doesn’t get stuck in any one spot, but it doesn’t cover a very wide range.\nThis simple example should give you an intuition for why you need to \"play around\"\nwith the choice of proposal distribution. A good proposal distribution can make a huge\ndifference! However, it’s also important to realise that even if you don’t get it quite right,\nyou can always solve the problem with brute force. For example, Figure 4 what happens\nwhen you run the same three chains for 50,000 iterations rather than just 1000.\n\nNow, at this point I haven’t really explained what the `burnin` and `lag` parameters\nare there for. I don't plan to go into details, but here's the basic idea. First, let's think\nabout the burn-in issue. Suppose you started the sampler at a very bad location. In the\nexamples that I’ve shown here `x0 = -1` is bad, but it's not too bad. So let's pick something\nnastier: we’ll start at `x0 = -3`. Also, to make the issue visually obvious, we’ll use a proposal\ndistribution that is a bit too narrow, say `sigma = .2`. Figure 5 shows 3 runs of this sampler.\nAs you can see, the sampler spends the first 200 or so iterations slowly moving towards\nthe main body of the distribution. Once it gets there, the samples start to look okay, but\nnotice that the histograms are biased towards the left (i.e., towards the bad start location).\nA simple way to fix this problem is to let the algorithm run for a while before starting to\ncollect actual samples. The length of time that you spend doing this is called the burn in\nperiod. To illustrate how it helps, Figure 6 shows what happens when you use a burn in\nperiod of 200 iterations for the same sampler.\n\nFinally, I’ll mention in passing the role played by the lag parameter. In some situa-\ntions you can be forced into using a proposal distribution that has a very low acceptance\nrate. When that happens, you’re left with an awkward Markov chain that gets stuck in one location for long periods of time. One thing that people often do in that situation is allow\nseveral iterations of the sampler to elapse in between successive samples. This is the `lag`\nbetween samples. The effect of this is illustrated in Figure 7.\n\n\n## A word of warning\n\nThe discussion in this note is *heavily* oversimplified. There are a lot of subtle issues\nassociated with MCMC methods, and my original plan when teaching this class was to gradually write a series of notes that expanded on this simplified discussion. Sadly, that never actually happened. I ended up too busy every year and never really had the time to write it up. Maybe one day!\n\n<!--------------- appendices go here ----------------->\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}