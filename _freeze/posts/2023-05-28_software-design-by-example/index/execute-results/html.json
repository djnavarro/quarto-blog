{
  "hash": "47c8cb6df36f7c98668b89ba5aaddddb",
  "result": {
    "markdown": "---\ntitle: \"Software design by example\"\ndescription: \"A book review, sort of. Not really.\"\ndate: \"2023-05-28\"\n---\n\n\n<!--------------- my typical setup ----------------->\n\n\n\n\n\n<!--------------- post begins here ----------------->\n\nThe book I'm currently reading is [Software Design by Example: A Tool-Based Introduction with JavaScript](https://www.routledge.com/Software-Design-by-Example-A-Tool-Based-Introduction-with-JavaScript/Wilson/p/book/9781032330235) by [Greg Wilson](https://third-bit.com/). Greg was kind enough to send me a review copy a little while back, and I've been slowly working my way through it. \n\nIn some ways I'm not the target audience for the book: it's a book about software engineering that uses javascript for the worked examples, not a book designed to teach you javascript. I'm not the worst javascript coder in the world, but I'm not the strongest either, so it's harder work for me than maybe it would have been if javascript were my primary language. That said, I'm finding the book rewarding. The glossary at the end is particularly helpful. I never received a formal education in programming, and I commonly have the experience in conversation that software engineers use terms that -- though they are rarely complicated ideas -- no-one has ever explained to me. \n\nAs an example, here's a section from the glossary on the top of page 318:\n\n> |\n- **query string**. The portion of a **URL** after the question mark ? that specfies extra parameters for the **HTTP request** as name-value pairs\n- **race condition**. A situation in which a result depend on the order in which two or more concurrent operations are carried out.\n- **raise (an exception)**. To signal that something unexpected or unusual has happened in a program by creating an **exception** and handling it to the **error-handling** system, which then tries to find a point in the program that will **catch** it.\n- **read-eval-print-loop (REPL)**. An interactive program that reads a command typed in by a user, executes it, prints the result, and then waits patiently for the next command. REPLs are often used to explore new ideas, or for debugging.\n\nI can honestly say that at no point in my life has someone explained to me what a \"race condition\" is or what a REPL does. When I read entries like this in the glossary I find myself going \"oh, right, I did already know this... but now I know what the name for it is\". Race conditions are not unfamiliar to me (I encounter them quite a bit) but because software engineers have a tendency to talk to \"race conditions\" without ever saying what the term means, I've sat in a lot of very confusing conversations in the past that would have made perfect sense had I known the nomenclature.\n\nI think that's likely to be true for a lot of self-taught programmers who never studied computer science, but instead had to learn to code in order to solve practical problems. The mere act of reading a concise definition of each thing has the effect of making my mental model more precise. It's a helpful way to learn the culture and avoid getting caught out by the various [shibboleths](https://en.wikipedia.org/wiki/Shibboleth) that pervade the tech industry. \n\nThere are other examples of this sort of thing throughout the book, historical anecdotes and other tidbits that make it a little easier for an outsider to make sense of the culture of software engineering. As an example, this little passage on p145 makes sense of something I've never understood:\n\n> The coordinate systems for screens puts (0, 0) in the upper left corner instead of the lower left. X increases to the right as usual, but Y increases as we go down, rather than up [The book has a little picture here]. This convention is a holdover from the days of teletype terminals that printed lines on rolls of paper\n\nThese historical asides are really valuable. It feels a little bit like one of those \"Magician's Secrets Revealed!\" shows. Knowing the terminology, conventions, and history behind a thing does so much of the work in making it all feel a bit more coherent. \n\n## Writing a tokenizer\n\n### Preliminaries\n\n\n::: {.cell file='token_class.R' filename='token_class.R'}\n\n```{.r .cell-code  code-line-numbers=\"true\"}\ntoken <- function(kind, loc, value = NULL) {\n  structure(\n    list(kind = kind, loc = loc, value = value),\n    class = \"token\"\n  )\n}\n\ntoken_list <- function(...) {\n  structure(\n    list(...),\n    class = \"token_list\"\n  )\n}\n\nprint.token <- function(x, ...) {\n  cat(\"<Token at \",  x$loc, \"> \", x$kind, sep = \"\")\n  if(!is.null(x$value)) {\n    cat(\":\", x$value)\n  }\n  cat(\"\\n\")\n  return(invisible(x))\n}\n\nprint.token_list <- function(x, ...) {\n  if(length(x) == 0) {\n    cat(\"<Empty token list>\\n\")\n  } else {\n    for(token in x) {\n      print(token)\n    }\n  }\n  return(invisible(x))\n}\n```\n:::\n\n\n\n### Version 1\n\n\n::: {.cell file='tokenizer_1.R' filename='tokenizer_1.R'}\n\n```{.r .cell-code  code-line-numbers=\"true\"}\nsimple <- list(\n  \"*\" = \"Any\",\n  \"|\" = \"Or\",\n  \"(\" = \"GroupStart\",\n  \")\" = \"GroupEnd\"\n)\n\ntokenize <- function(text) {\n  result <- token_list()\n  n <- 0\n  for (i in 1:nchar(text)) {\n    chr <- substr(text, start = i, stop = i)\n\n    # simple cases are always added as non-literal tokens\n    if (chr %in% names(simple)) {\n      result[[n + 1]] <- token(simple[[chr]], i)\n      n <- n + 1\n\n    # the ^ character is non-literal if position is 1\n    } else if (chr == \"^\" & i == 1) {\n      result[[n + 1]] <- token(\"Start\", i)\n      n <- n + 1\n\n    # the $ character is non-literal if it's the last character\n    } else if (chr == \"$\" & i == nchar(text)) {\n      result[[n + 1]] <- token(\"End\", i)\n      n <- n + 1\n\n    # literals that follow a non-literal create a new token\n    } else if (n > 0 && result[[n]]$kind != \"Literal\"){\n      result[[n + 1]] <- token(\"Literal\", i, value = chr)\n      n <- n + 1\n\n    # literals that follow a literal are combined with it\n    } else {\n      result[[n]]$value <- paste0(result[[n]]$value, chr)\n    }\n  }\n  return(result)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"token_class.R\")\nsource(\"tokenizer_1.R\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntokenize(\"^(cat|dog)$\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 1> Start\n<Token at 2> GroupStart\n<Token at 3> Literal: cat\n<Token at 6> Or\n<Token at 7> Literal: dog\n<Token at 10> GroupEnd\n<Token at 11> End\n```\n:::\n:::\n\n\nMerging a sequence of literal characters into a single multi-character literal makes the output readable, and in this case is very convenient if I later wanted to write a regular expression matcher that builds on top of this tokenizer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrepl(\"^(cat|dog)$\", c(\"cat\", \"dog\", \"dag\", \"ca\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  TRUE  TRUE FALSE FALSE\n```\n:::\n:::\n\n\nIf my tokenizer represent the literals in `^(cat|dog)$` as the two multicharacter literals `\"cat\"` and `\"dog\"` then it's going to be easier for me to write a regex matcher that mirrors the behaviour of `grepl()`.\n\nUnfortunately, there's a problem with this tokenizer. It's trying to be clever, by grouping multiple literals together, and it ends up being too greedy sometimes. Consider the regular expression `\"^caa*t$\"`. This is a pattern that should match against `\"cat\"` and `\"caaaaat\"` but should not match `\"caacaat\"`, as illustrated below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrepl(\"^caa*t$\", c(\"cat\", \"caaaaat\", \"caacaat\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  TRUE  TRUE FALSE\n```\n:::\n:::\n\n\nHowever, let's look at the tokens that are produced by this version of our `tokenizer()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntokenize(\"^caa*t$\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 1> Start\n<Token at 2> Literal: caa\n<Token at 5> Any\n<Token at 6> Literal: t\n<Token at 7> End\n```\n:::\n:::\n\n\nThat's the wrong way to tokenize this input: in this regular expression the `*` operator should be applied only to the preceding character, so it's not correct to treat `\"caa\"` as a single literal string. What we should have done is split this into literals, `\"ca\"` and `\"a\"`. The `\"ca\"` literal is required and must appear at the beginning of the string (because it follows the start operator `^`), whereas `\"a\"` is optional and may appear zero or more times (because it precedes the any operator `*`). A tokenizer that collapses these into a single literal `\"caa\"` will make life hell for any regular expression parser that tries to work with these tokens. \n\nIn essence, this tokenizer fails because it's trying to be a parser as well as a tokenizer, and as a consequence it solves both problems badly.\n\n### Version 2\n\nOkay so that doesn't work. The second approach considered in the chapter simplifies the code a little and produces a token list where every literal character is treated as a distinct token:\n\n\n::: {.cell file='tokenizer_2.R' filename='tokenizer_2.R'}\n\n```{.r .cell-code  code-line-numbers=\"true\"}\nsimple <- list(\n  \"*\" = \"Any\",\n  \"|\" = \"Or\",\n  \"(\" = \"GroupStart\",\n  \")\" = \"GroupEnd\"\n)\n\ntokenize <- function(text) {\n  result <- token_list()\n  n <- 0\n  for (i in 1:nchar(text)) {\n    chr <- substr(text, start = i, stop = i)\n\n    # simple cases are always added as non-literal tokens\n    if (chr %in% names(simple)) {\n      result[[n + 1]] <- token(simple[[chr]], i)\n      n <- n + 1\n\n    # the ^ character is non-literal if position is 1\n    } else if (chr == \"^\" & i == 1) {\n      result[[n + 1]] <- token(\"Start\", i)\n      n <- n + 1\n\n    # the $ character is non-literal if it's the last character\n    } else if (chr == \"$\" & i == nchar(text)) {\n      result[[n + 1]] <- token(\"End\", i)\n      n <- n + 1\n\n    # literals always create a new token\n    } else {\n      result[[n + 1]] <- token(\"Literal\", i, value = chr)\n      n <- n + 1\n    }\n  }\n  return(result)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"token_class.R\")\nsource(\"tokenizer_2.R\")\n```\n:::\n\n\nHere's the sort of output we get from this version of the tokenizer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntokenize(\"^caa*t$\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 1> Start\n<Token at 2> Literal: c\n<Token at 3> Literal: a\n<Token at 4> Literal: a\n<Token at 5> Any\n<Token at 6> Literal: t\n<Token at 7> End\n```\n:::\n:::\n\n\nThis output is entirely correct, as long as our goal is *only* to extract and label the tokens in our regular expression. The output correctly labels each literal as a literal and assigns the appropriate label to the non-literals. The problem, however, is that doesn't solve the parsing problem. We've lost the grouping structure that we had in our original version. The tokenizer has no way of expressing the idea that `\"ca\"` is a syntactically coherent unit in the expression `^caa*t$`, or that `\"cat\"` is similarly coherent within `^(cat|dog)$`. It's a tokenizer, but not a parser. \n\nFortunately, we can build a parser on top of this tokenizer. It requires a little thought, but it's achieveable because the tokenizer is reliable *as* a tokenizer.\n\n### Post-mortem\n\nAt this point any competent software engineer is probably screaming internally, because I have not written any unit tests for my code. This is terribly bad practice, and something I would never do when writing actual software. Suffice it to say *Software Design by Example* is at great pains to emphasize the importance of unit tests, and if you were actually following the chapter step by step you'd see that Greg does in fact introduce tests as he develops this example. I've been lazy in this blog post because... well, it's a blog post. It's neither intended to be software nor a chapter in a book on software engineering. \n\nAnyway... let's return to the development of ideas in the chapter, yes?\n\n## Parsing the tokens\n\n### Version 1\n\n\n::: {.cell file='parser_1.R' filename='parser_1.R'}\n\n```{.r .cell-code  code-line-numbers=\"true\"}\nlist_reverse <- function(x) {\n  x[length(x):1]\n}\n\nhandle <- function(result, token) {\n\n  # create a subtree with no child nodes\n  if(token$kind %in% c(\"Literal\", \"Start\", \"End\", \"GroupStart\")) {\n    result[[length(result) + 1]] <- subtree(token)\n  }\n\n  # when end group is encountered, find corresponding start group and\n  # create a subtree with parent of kind group with the appropriate children\n  if (token$kind == \"GroupEnd\") {\n    children <- list()\n    while(TRUE) {\n      last <- result[[length(result)]]\n      result <- result[-length(result)]\n      if(last$parent$kind == \"GroupStart\") {\n        break\n      }\n      children[[length(children) + 1]] <- last\n    }\n    result[[length(result) + 1]] <- subtree(\n      parent = token(\"Group\", last$parent$loc),\n      children = list_reverse(children)\n    )\n  }\n\n  # when an any token is encountered, create a subtree with the any token\n  # as the parent, and the preceding subtree as the child\n  if (token$kind == \"Any\") {\n    last <- result[[length(result)]]\n    result[[length(result)]] <- subtree(\n      parent = token,\n      children = list(last)\n    )\n  }\n\n  # when an or token is encountered, create a subtree with the or token\n  # as the parent, and two children: one from the preceding subtree, and\n  # one \"missing\" token that will be filled later\n  if (token$kind == \"Or\") {\n    last <- result[[length(result)]]\n    result[[length(result)]] <- subtree(\n      parent = token,\n      children = list(last, subtree(token(kind = \"Missing\", loc = 0)))\n    )\n  }\n\n  return(result)\n}\n\nparse <- function(text) {\n  result <- token_list()\n  tokens <- tokenize(text)\n  for(i in 1:length(tokens)) {\n    token <- tokens[[i]]\n    result <- handle(result, token)\n  }\n  class(result) <- \"token_list\"\n  return(result)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"token_class.R\")\nsource(\"parse_class.R\")\nsource(\"tokenizer_2.R\")\nsource(\"parser_1.R\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#parse(\"^caa*t$\")\n#parse(\"aa|b\")\n#parse(\"(ab)|c\")\n#parse(\"^(caa*t)|(dog)$\")\n#parse(\"ab|((cd*)|ef)|g\")\nparse(\"x|(y|z)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n- <Token at 2> Or\n[\n     - <Token at 1> Literal: x\n     - <Token at 0> Missing\n]\n- <Token at 3> Group\n[\n     - <Token at 5> Or\n     [\n          - <Token at 4> Literal: y\n          - <Token at 0> Missing\n     ]\n     - <Token at 6> Literal: z\n]\n```\n:::\n:::\n\n\n\n### Version 2\n\n\n::: {.cell file='parser_2.R' filename='parser_2.R'}\n\n```{.r .cell-code  code-line-numbers=\"true\"}\nlist_reverse <- function(x) {\n  x[length(x):1]\n}\n\nhandle <- function(result, token) {\n\n  # create a subtree with no child nodes\n  if(token$kind %in% c(\"Literal\", \"Start\", \"End\", \"GroupStart\")) {\n    result[[length(result) + 1]] <- subtree(token)\n  }\n\n  # when end group is encountered, find corresponding start group and\n  # create a subtree with parent of kind group with the appropriate children\n  if (token$kind == \"GroupEnd\") {\n    children <- list()\n    while(TRUE) {\n      last <- result[[length(result)]]\n      result <- result[-length(result)]\n      if(last$parent$kind == \"GroupStart\") {\n        break\n      }\n      children[[length(children) + 1]] <- last\n    }\n    result[[length(result) + 1]] <- subtree(\n      parent = token(\"Group\", last$parent$loc),\n      children = list_reverse(children)\n    )\n  }\n\n  # when an any token is encountered, create a subtree with the any token\n  # as the parent, and the preceding subtree as the child\n  if (token$kind == \"Any\") {\n    last <- result[[length(result)]]\n    result[[length(result)]] <- subtree(\n      parent = token,\n      children = list(last)\n    )\n  }\n\n  # when an or token is encountered, create a subtree with the or token\n  # as the parent, and two children: one from the preceding subtree, and\n  # one \"missing\" token that will be filled later\n  if (token$kind == \"Or\") {\n    last <- result[[length(result)]]\n    result[[length(result)]] <- subtree(\n      parent = token,\n      children = list(last, subtree(token(kind = \"Missing\", loc = 0)))\n    )\n  }\n\n  return(result)\n}\n\nchildless <- function(x) {\n  is.null(x$children) || length(x$children) == 0\n}\n\ncompress <- function(raw) {\n  if(length(raw) == 1) {\n    return(raw)\n  }\n  loc <- length(raw)\n  while(loc > 1) {\n    el_1 <- raw[[loc - 1]]\n    el_2 <- compress(raw[[loc]])\n    if(!childless(el_2)) {\n      el_2$children <- compress(el_2$children)\n    }\n    if(!childless(el_1)) {\n      n_children <- length(el_1$children)\n      last <- el_1$children[[n_children]]\n      if(last$parent$kind == \"Missing\") {\n        el_1$children[[n_children]] <- el_2\n        raw[[loc]] <- NULL\n      }\n      el_1 <- compress(el_1)\n      el_1$children <- compress(el_1$children)\n      raw[[loc - 1]] <- compress(el_1)\n    }\n    loc <- loc - 1\n  }\n  return(raw)\n}\n\nparse <- function(text) {\n  result <- token_list()\n  tokens <- tokenize(text)\n  for(i in 1:length(tokens)) {\n    token <- tokens[[i]]\n    result <- handle(result, token)\n  }\n  result <- compress(result)\n  class(result) <- \"token_list\"\n  return(result)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"token_class.R\")\nsource(\"parse_class.R\")\nsource(\"tokenizer_2.R\")\nsource(\"parser_2.R\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#parse(\"^caa*t$\")\n#parse(\"aa|b\")\n#parse(\"(ab)|c\")\n#parse(\"^(caa*t)|(dog)$\")\n#parse(\"ab|((cd*)|ef)|g\")\n#parse(\"x|(y|z)z\")\nparse(\"x|(y|z)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n- <Token at 2> Or\n[\n     - <Token at 1> Literal: x\n     - <Token at 3> Group\n     [\n          - <Token at 5> Or\n          [\n               - <Token at 4> Literal: y\n               - <Token at 6> Literal: z\n          ]\n     ]\n]\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}