{
  "hash": "c86c27288c108cacbe3ef0d426f89ff5",
  "result": {
    "markdown": "---\ntitle: \"Software design by example\"\ndescription: \"A book review, sort of. Not really. Look, Greg sent me a copy and I had fun reading it. okay?\"\ndate: \"2023-05-28\"\ncategories: [\"Software Design\", \"Javascript\", \"R\", \"Regular Expressions\"]\n---\n\n\n<!--------------- my typical setup ----------------->\n\n\n\n\n\n<!--------------- post begins here ----------------->\n\nThe book I'm currently reading is [Software Design by Example: A Tool-Based Introduction with JavaScript](https://www.routledge.com/Software-Design-by-Example-A-Tool-Based-Introduction-with-JavaScript/Wilson/p/book/9781032330235) by [Greg Wilson](https://third-bit.com/). Greg was kind enough to send me a review copy a little while back, and I've been slowly working my way through it. \n\nIn some ways I'm not the target audience for the book: it's a book about software engineering that uses javascript for the worked examples, not a book designed to teach you javascript. I'm not the worst javascript coder in the world, but I'm not the strongest either, so it's harder work for me than maybe it would have been if javascript were my primary language. \n\n![](cover.jpg){fig-align=\"center\"}\n\n## Who is the book for?\n\nSome years ago I took a very good course that Greg ran on how to teach technical concepts,^[Something that you'd think I'd have been taught back when I was an academic and had to do it for a living. Universities, however, are utterly useless at this kind of thing. They tend to throw professors in the deep end with this expectation that someone who has made a career as a good researcher will automatically work out how to be a good teacher. Suffice it to say, this widespread practice has not been the best thing for higher education.] and one thing he emphasised in that course is the importance of designing teaching materials with *specific* learning personas in mind. Having a small number of representative examples in mind when you write the content is incredibly useful when teaching, so it is no surprise that -- literally on page 1 -- the book states explicitly what the learner personas used to write the book were.\n\nI'm going to reproduce them in full in this blog post as a reminder to myself that this is the right way to construct learner personas. \n\n> |\n- Aïsha started writing VB macros for Excel in an accounting course and never looked back. After spending three years doing front-end JavaScript work she now wants to learn how to build back-end applications. This material will fill in some gaps in her programming knowledge and teach her some common design patterns\n- Rupinder is studying computer science at college. He has learned a lot about the theory of algorithms, and while he uses Git and unit testing tools in his assignments, he doesn't feel he understands how they work. This material will give him a better understanding of these tools and how to design new ones.\n- Yim builds mobile apps for a living but also teaches two college courses: one on full-stack web development using JavaScript and Node and another titled \"Software Design\". They are happy with the former, but frustrated that so many books about the latter subject talk about it in the abstract and use examples that their students can't relat to. This material will fill those gaps and give them starting points for a wide variety of course assignments.\n\nThey're detailed enough to make the teacher engage with the personas during the writing process, diverse enough to help catch things the writer might not have thought of, and provide a *strong* indication to the learner about whether the book is written for them. In my case, for instance, it's pretty clear from the outset that I'm likely to struggle with the level of JavaScript involved. Indeed, when I look at the list of skills that the reader is expected to have (on pages 1-2) I'm fine on most things but I suspect I have a little less practical experience with JavaScript. I'm not terrible at it, I just don't spend enough time with it to feel fluent. \n\nThat's okay, of course. It's often a good experience as a learner to read content that pushes you outside your comfort zone. What matters is that you *know in advance* which aspects are going to be hard work for you. Thankfully, *Software Design by Example* does that very well at the beginning. \n\n## Rewarding the cursory reader\n\nDespite knowing from the outset that I'm ever-so-slightly outside the target audience for the book, I'm sitll finding the it very rewarding. To understand why, it's worth taking a moment to look at how the book uses the glossary as the end to help readers like myself who have never received a formal education in programming... I'm basically the Aïsha persona, except I with R playing the same role for me that JavaScript plays for her.  \n\nBecause I don't have a formal background and -- to put it gently -- don't belong to a demographic that can easily acquire the \"culture\" of software engineering, I very commonly have the experience in conversation that software engineers will use terms that they simply assume that \"everybody knows\", and never take the time to explain them. The actual concepts are often very simple things, and often I've had experiene with them without knowing the names, but no-one ever tells you what the words mean and -- sadly -- many people in the field have a tendency to make you feel like an idiot if you ask. \n\nWith that as the real world backdrop, I'm finding the glossary to be worth its weight in gold. Here's a little snippet from the top of page 318:\n\n> |\n- **query string**. The portion of a **URL** after the question mark ? that specfies extra parameters for the **HTTP request** as name-value pairs\n- **race condition**. A situation in which a result depend on the order in which two or more concurrent operations are carried out.\n- **raise (an exception)**. To signal that something unexpected or unusual has happened in a program by creating an **exception** and handling it to the **error-handling** system, which then tries to find a point in the program that will **catch** it.\n- **read-eval-print-loop (REPL)**. An interactive program that reads a command typed in by a user, executes it, prints the result, and then waits patiently for the next command. REPLs are often used to explore new ideas, or for debugging.\n\nI can honestly say that at no point in my life has someone explained to me what a \"race condition\" is or what a REPL actually is. Seriously. I've worked in tech companies and people use the terms all the time but never explain them. Very frustrating. So when I read entries like this in the glossary I find myself going \"oh, *that's* what <blah> means... okay, yes, I did already know this... but now I know what the name for it is\". I mean, race conditions are not at all unfamiliar to me -- I encounter them quite a bit -- but because software engineers have a tendency to refer to \"race conditions\" without ever saying what the term means, I've sat in a lot of very confusing conversations over the years that would have made a loy more bloody sense had I known the nomenclature or been in a position to \"just ask\" without being made to feel stupid. \n\nI think that's likely to be true for a lot of self-taught programmers who never studied computer science, but instead had to learn to code in order to solve practical problems. The mere act of reading a concise definition of each thing has the effect of making my mental model more precise, and better aligned with the mental models that other people in the field adopt. It's a helpful way to learn the culture and avoid getting caught out by the various [shibboleths](https://en.wikipedia.org/wiki/Shibboleth) that are sadly pervasive the tech industry.\n\nThere are other examples of this sort of thing throughout the book, historical anecdotes and other tidbits that make it a little easier for an outsider to make sense of the culture of software engineering. As an example, this little passage on p145 makes sense of something I've never understood:\n\n> The coordinate systems for screens puts (0, 0) in the upper left corner instead of the lower left. X increases to the right as usual, but Y increases as we go down, rather than up [The book has a little picture here]. This convention is a holdover from the days of teletype terminals that printed lines on rolls of paper\n\nThese historical asides are really valuable. It feels a little bit like one of those \"Magician's Secrets Revealed!\" shows. Knowing the terminology, conventions, and history behind a thing does so much of the work in making it all feel a bit more coherent. \n\nAnyway, let's dive a little deeper, shall we?\n\n## A worked example\n\n> My mum always liked Delia Smith <br>\nAnd I drank, drank, drank just to deal with my shit <br>\nI learned to tell little white lies <br>\nWhen I feel inadequate almost all the time <br>\n<br>\nI'd like to think I'm deep <br>\nI'd like to think I'm deep <br>\nI'd like to think I'm deep <br>\nBut I just skim the pages, so I can fake my speech  <br>\n&nbsp; &nbsp; - [Sprints](https://youtu.be/zHHjDihVwiQ)\n\nA little honesty when writing blog posts is important, I feel. When reading the book I did not, in fact, attempt all the exercises or work through all the code examples. I tinkered with a few of the examples, read some parts thoroughly, and skimmed other parts. That's pretty representative of how I read technical books, really. I'll pick a few parts that I want to understand properly and do a deep dive in those sections, and read the rest of it in a much more superficial way.\n\nThe parts that I did read fairly deeply are Chapters 7 and 8, which talk about query matching and expression parsing. Especially Chapter 8, which shows you how to write a tokenizer and parser for a restricted form of regular expression syntax. For that chapter, I worked through the examples and translated the code in R. In the rest of the post I'll show the code that I wrote, tie it back to the structure of Chapter 8 in the book, and at the end I'll say a little about what I learned from this exercise.\n\n## Writing the tokenizer\n\nThe subset of regular expression syntax that we're going to consider uses the following characters:\n\n| Token Kind  | Meaning                                    | Characters used    |\n|:------------|:-------------------------------------------|:-------------------:\n| Literal     | A literal character                        | `a`, `b`, `c`, etc |\n| Start       | Beginning of string                        | `^`                |\n| End         | End of string                              | `$`                |\n| Any         | Zero or more of the previous thing         | `*`                |\n| Or          | Either the thing before or the thing after | `|`                |\n| Group       | Collection of tokens to treat as one thing | `(` and `)`        |\n\nIt's a very small subset of the regular expression grammar available for pattern matching in R, javascript, and pretty much every language these days, but it's a handy one. It's certainly rich enough to make it an interesting exercise to write a parser. Not that it's particularly important to write my own parser: the purpose of doing so is to wrap my head around the basics of how parsers work and nothing more. As Greg helpfully reminds us on p99, if a format is commonly known there will be good tools already, and if you find yourself wanting to roll your own parser to interpret some new file format you just invented for something... there's a good chance you shouldn't do it. The world has enough file formats already.\n\n### Preliminaries\n\n\n::: {.cell file='token_class.R' filename='token_class.R'}\n\n```{.r .cell-code  code-line-numbers=\"true\"}\ntoken <- function(kind, loc, value = NULL) {\n  structure(\n    list(kind = kind, loc = loc, value = value),\n    class = \"token\"\n  )\n}\n\ntoken_list <- function(...) {\n  structure(\n    list(...),\n    class = \"token_list\"\n  )\n}\n\nprint.token <- function(x, ...) {\n  cat(\"<Token at \",  x$loc, \"> \", x$kind, sep = \"\")\n  if(!is.null(x$value)) {\n    cat(\":\", x$value)\n  }\n  cat(\"\\n\")\n  return(invisible(x))\n}\n\nprint.token_list <- function(x, ...) {\n  if(length(x) == 0) {\n    cat(\"<Empty token list>\\n\")\n  } else {\n    for(token in x) {\n      print(token)\n    }\n  }\n  return(invisible(x))\n}\n```\n:::\n\n\n\n### Version 1\n\n\n::: {.cell file='tokenizer_1.R' filename='tokenizer_1.R'}\n\n```{.r .cell-code  code-line-numbers=\"true\"}\nsimple <- list(\n  \"*\" = \"Any\",\n  \"|\" = \"Or\",\n  \"(\" = \"GroupStart\",\n  \")\" = \"GroupEnd\"\n)\n\ntokenize <- function(text) {\n  result <- token_list()\n  n <- 0\n  for (i in 1:nchar(text)) {\n    chr <- substr(text, start = i, stop = i)\n\n    # simple cases are always added as non-literal tokens\n    if (chr %in% names(simple)) {\n      result[[n + 1]] <- token(simple[[chr]], i)\n      n <- n + 1\n\n    # the ^ character is non-literal if position is 1\n    } else if (chr == \"^\" & i == 1) {\n      result[[n + 1]] <- token(\"Start\", i)\n      n <- n + 1\n\n    # the $ character is non-literal if it's the last character\n    } else if (chr == \"$\" & i == nchar(text)) {\n      result[[n + 1]] <- token(\"End\", i)\n      n <- n + 1\n\n    # literals that follow a non-literal create a new token\n    } else if (n > 0 && result[[n]]$kind != \"Literal\"){\n      result[[n + 1]] <- token(\"Literal\", i, value = chr)\n      n <- n + 1\n\n    # literals that follow a literal are combined with it\n    } else {\n      result[[n]]$value <- paste0(result[[n]]$value, chr)\n    }\n  }\n  return(result)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"token_class.R\")\nsource(\"tokenizer_1.R\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntokenize(\"^(cat|dog)$\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 1> Start\n<Token at 2> GroupStart\n<Token at 3> Literal: cat\n<Token at 6> Or\n<Token at 7> Literal: dog\n<Token at 10> GroupEnd\n<Token at 11> End\n```\n:::\n:::\n\n\nMerging a sequence of literal characters into a single multi-character literal makes the output readable, and in this case is very convenient if I later wanted to write a regular expression matcher that builds on top of this tokenizer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrepl(\"^(cat|dog)$\", c(\"cat\", \"dog\", \"dag\", \"ca\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  TRUE  TRUE FALSE FALSE\n```\n:::\n:::\n\n\nIf my tokenizer represent the literals in `^(cat|dog)$` as the two multicharacter literals `\"cat\"` and `\"dog\"` then it's going to be easier for me to write a regex matcher that mirrors the behaviour of `grepl()`.\n\nUnfortunately, there's a problem with this tokenizer. It's trying to be clever, by grouping multiple literals together, and it ends up being too greedy sometimes. Consider the regular expression `\"^caa*t$\"`. This is a pattern that should match against `\"cat\"` and `\"caaaaat\"` but should not match `\"caacaat\"`, as illustrated below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrepl(\"^caa*t$\", c(\"cat\", \"caaaaat\", \"caacaat\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  TRUE  TRUE FALSE\n```\n:::\n:::\n\n\nHowever, let's look at the tokens that are produced by this version of our `tokenizer()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntokenize(\"^caa*t$\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 1> Start\n<Token at 2> Literal: caa\n<Token at 5> Any\n<Token at 6> Literal: t\n<Token at 7> End\n```\n:::\n:::\n\n\nThat's the wrong way to tokenize this input: in this regular expression the `*` operator should be applied only to the preceding character, so it's not correct to treat `\"caa\"` as a single literal string. What we should have done is split this into literals, `\"ca\"` and `\"a\"`. The `\"ca\"` literal is required and must appear at the beginning of the string (because it follows the start operator `^`), whereas `\"a\"` is optional and may appear zero or more times (because it precedes the any operator `*`). A tokenizer that collapses these into a single literal `\"caa\"` will make life hell for any regular expression parser that tries to work with these tokens. \n\nIn essence, this tokenizer fails because it's trying to be a parser as well as a tokenizer, and as a consequence it solves both problems badly.\n\n### Version 2\n\nOkay so that doesn't work. The second approach considered in the chapter simplifies the code a little and produces a token list where every literal character is treated as a distinct token:\n\n\n::: {.cell file='tokenizer_2.R' filename='tokenizer_2.R'}\n\n```{.r .cell-code  code-line-numbers=\"true\"}\nsimple <- list(\n  \"*\" = \"Any\",\n  \"|\" = \"Or\",\n  \"(\" = \"GroupStart\",\n  \")\" = \"GroupEnd\"\n)\n\ntokenize <- function(text) {\n  result <- token_list()\n  n <- 0\n  for (i in 1:nchar(text)) {\n    chr <- substr(text, start = i, stop = i)\n\n    # simple cases are always added as non-literal tokens\n    if (chr %in% names(simple)) {\n      result[[n + 1]] <- token(simple[[chr]], i)\n      n <- n + 1\n\n    # the ^ character is non-literal if position is 1\n    } else if (chr == \"^\" & i == 1) {\n      result[[n + 1]] <- token(\"Start\", i)\n      n <- n + 1\n\n    # the $ character is non-literal if it's the last character\n    } else if (chr == \"$\" & i == nchar(text)) {\n      result[[n + 1]] <- token(\"End\", i)\n      n <- n + 1\n\n    # literals always create a new token\n    } else {\n      result[[n + 1]] <- token(\"Literal\", i, value = chr)\n      n <- n + 1\n    }\n  }\n  return(result)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"token_class.R\")\nsource(\"tokenizer_2.R\")\n```\n:::\n\n\nHere's the sort of output we get from this version of the tokenizer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntokenize(\"^caa*t$\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 1> Start\n<Token at 2> Literal: c\n<Token at 3> Literal: a\n<Token at 4> Literal: a\n<Token at 5> Any\n<Token at 6> Literal: t\n<Token at 7> End\n```\n:::\n:::\n\n\nThis output is entirely correct, as long as our goal is *only* to extract and label the tokens in our regular expression. The output correctly labels each literal as a literal and assigns the appropriate label to the non-literals. The problem, however, is that doesn't solve the parsing problem. We've lost the grouping structure that we had in our original version. The tokenizer has no way of expressing the idea that `\"ca\"` is a syntactically coherent unit in the expression `^caa*t$`, or that `\"cat\"` is similarly coherent within `^(cat|dog)$`. It's a tokenizer, but not a parser. \n\nFortunately, we can build a parser on top of this tokenizer. It requires a little thought, but it's achieveable because the tokenizer is reliable *as* a tokenizer.\n\n### Post-mortem\n\nAt this point any competent software engineer is probably screaming internally, because I have not written any unit tests for my code. This is terribly bad practice, and something I would never do when writing actual software. Suffice it to say *Software Design by Example* is at great pains to emphasize the importance of unit tests, and if you were actually following the chapter step by step you'd see that Greg does in fact introduce tests as he develops this example. I've been lazy in this blog post because... well, it's a blog post. It's neither intended to be software nor a chapter in a book on software engineering. \n\nAnyway... let's return to the development of ideas in the chapter, yes?\n\n## Parsing the tokens\n\n### Version 1\n\n\n::: {.cell file='parser_1.R' filename='parser_1.R'}\n\n```{.r .cell-code  code-line-numbers=\"true\"}\nlist_reverse <- function(x) {\n  x[length(x):1]\n}\n\nupdate_tree <- function(tree, token) {\n\n  # For some kinds of token, we simply append them to the tree\n  if(token$kind %in% c(\"Literal\", \"Start\", \"End\", \"GroupStart\")) {\n    tree[[length(tree) + 1]] <- subtree(token)\n  }\n\n  # When GroupEnd is encountered, find the most recent GroupStart and\n  # make the tokens between them the children of a Group\n  if (token$kind == \"GroupEnd\") {\n    children <- list()\n    while(TRUE) {\n      last <- tree[[length(tree)]]\n      tree <- tree[-length(tree)]\n      if(last$parent$kind == \"GroupStart\") {\n        break\n      }\n      children[[length(children) + 1]] <- last\n    }\n    tree[[length(tree) + 1]] <- subtree(\n      parent = token(\"Group\", last$parent$loc),\n      children = list_reverse(children)\n    )\n  }\n\n  # When Any is encountered, make the preceding token (or subtree)\n  # the child of the Any token\n  if (token$kind == \"Any\") {\n    last <- tree[[length(tree)]]\n    tree[[length(tree)]] <- subtree(\n      parent = token,\n      children = list(last)\n    )\n  }\n\n  # When Or is encountered, create a subtree with two children. The\n  # first (or left) child is taken by moving it from the previous\n  # token/subtree in our list. The second child is tagged as \"Missing\"\n  # and will be filled in later\n  if (token$kind == \"Or\") {\n    last <- tree[[length(tree)]]\n    tree[[length(tree)]] <- subtree(\n      parent = token,\n      children = list(last, subtree(token(kind = \"Missing\", loc = 0)))\n    )\n  }\n\n  return(tree)\n}\n\nparse <- function(text) {\n  tree <- list()\n  tokens <- tokenize(text)\n  for(token in tokens) {\n    tree <- update_tree(tree, token)\n  }\n  class(tree) <- \"token_list\" # allows pretty printing\n  return(tree)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"token_class.R\")\nsource(\"parse_class.R\")\nsource(\"tokenizer_2.R\")\nsource(\"parser_1.R\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nparse(\"^caa*t$\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 1> Start\n<Token at 2> Literal: c\n<Token at 3> Literal: a\n<Token at 5> Any\n     <Token at 4> Literal: a\n<Token at 6> Literal: t\n<Token at 7> End\n```\n:::\n\n```{.r .cell-code}\nparse(\"aa|b\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 1> Literal: a\n<Token at 3> Or\n     <Token at 2> Literal: a\n     <Token at 0> Missing\n<Token at 4> Literal: b\n```\n:::\n\n```{.r .cell-code}\nparse(\"(ab)|c\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 5> Or\n     <Token at 1> Group\n          <Token at 2> Literal: a\n          <Token at 3> Literal: b\n     <Token at 0> Missing\n<Token at 6> Literal: c\n```\n:::\n\n```{.r .cell-code}\nparse(\"^(caa*t)|(dog)$\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 1> Start\n<Token at 9> Or\n     <Token at 2> Group\n          <Token at 3> Literal: c\n          <Token at 4> Literal: a\n          <Token at 6> Any\n               <Token at 5> Literal: a\n          <Token at 7> Literal: t\n     <Token at 0> Missing\n<Token at 10> Group\n     <Token at 11> Literal: d\n     <Token at 12> Literal: o\n     <Token at 13> Literal: g\n<Token at 15> End\n```\n:::\n\n```{.r .cell-code}\nparse(\"ab|((cd*)|ef)|g\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 1> Literal: a\n<Token at 3> Or\n     <Token at 2> Literal: b\n     <Token at 0> Missing\n<Token at 14> Or\n     <Token at 4> Group\n          <Token at 10> Or\n               <Token at 5> Group\n                    <Token at 6> Literal: c\n                    <Token at 8> Any\n                         <Token at 7> Literal: d\n               <Token at 0> Missing\n          <Token at 11> Literal: e\n          <Token at 12> Literal: f\n     <Token at 0> Missing\n<Token at 15> Literal: g\n```\n:::\n\n```{.r .cell-code}\nparse(\"x|(y|z)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 2> Or\n     <Token at 1> Literal: x\n     <Token at 0> Missing\n<Token at 3> Group\n     <Token at 5> Or\n          <Token at 4> Literal: y\n          <Token at 0> Missing\n     <Token at 6> Literal: z\n```\n:::\n:::\n\n\n\n### Version 2\n\n\n::: {.cell file='parser_2.R' filename='parser_2.R'}\n\n```{.r .cell-code  code-line-numbers=\"true\"}\nlist_reverse <- function(x) {\n  x[length(x):1]\n}\n\nupdate_tree <- function(tree, token) {\n\n  # For some kinds of token, we simply append them to the tree\n  if(token$kind %in% c(\"Literal\", \"Start\", \"End\", \"GroupStart\")) {\n    tree[[length(tree) + 1]] <- subtree(token)\n  }\n\n  # When GroupEnd is encountered, find the most recent GroupStart and\n  # make the tokens between them the children of a Group\n  if (token$kind == \"GroupEnd\") {\n    children <- list()\n    while(TRUE) {\n      last <- tree[[length(tree)]]\n      tree <- tree[-length(tree)]\n      if(last$parent$kind == \"GroupStart\") {\n        break\n      }\n      children[[length(children) + 1]] <- last\n    }\n    tree[[length(tree) + 1]] <- subtree(\n      parent = token(\"Group\", last$parent$loc),\n      children = list_reverse(children)\n    )\n  }\n\n  # When Any is encountered, make the preceding token (or subtree)\n  # the child of the Any token\n  if (token$kind == \"Any\") {\n    last <- tree[[length(tree)]]\n    tree[[length(tree)]] <- subtree(\n      parent = token,\n      children = list(last)\n    )\n  }\n\n  # When Or is encountered, create a subtree with two children. The\n  # first (or left) child is taken by moving it from the previous\n  # token/subtree in our list. The second child is tagged as \"Missing\"\n  # and will be filled in later\n  if (token$kind == \"Or\") {\n    last <- tree[[length(tree)]]\n    tree[[length(tree)]] <- subtree(\n      parent = token,\n      children = list(last, subtree(token(kind = \"Missing\", loc = 0)))\n    )\n  }\n\n  return(tree)\n}\n\nhas_children <- function(x) {\n  !is.null(x$children) && length(x$children) > 0\n}\n\ncompress_or_skip <- function(tree, location) {\n  if (has_children(tree[[location - 1]])) {\n    n <- length(tree[[location - 1]]$children)\n    if (tree[[location - 1]]$children[[n]]$parent$kind == \"Missing\") {\n      tree[[location - 1]]$children[[n]] <- tree[[location]]\n      tree[[location]] <- NULL\n    }\n  }\n  return(tree)\n}\n\ncompress_tree <- function(tree) {\n  if (length(tree) <= 1) {\n    return(tree)\n  }\n\n  # Compress branches of the top-level tree\n  loc <- length(tree)\n  while (loc > 1) {\n    tree <- compress_or_skip(tree, loc)\n    loc <- loc - 1\n  }\n\n  # Recursively compress branches of children subtrees\n  for (loc in 1:length(tree)) {\n    tree[[loc]]$children <- compress_tree(tree[[loc]]$children)\n  }\n\n  return(tree)\n}\n\nparse <- function(text) {\n  tree <- list()\n  tokens <- tokenize(text)\n  for(token in tokens) {\n    tree <- update_tree(tree, token)\n  }\n  tree <- compress_tree(tree)\n  class(tree) <- \"token_list\" # allows pretty printing\n  return(tree)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"token_class.R\")\nsource(\"parse_class.R\")\nsource(\"tokenizer_2.R\")\nsource(\"parser_2.R\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nparse(\"^caa*t$\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 1> Start\n<Token at 2> Literal: c\n<Token at 3> Literal: a\n<Token at 5> Any\n     <Token at 4> Literal: a\n<Token at 6> Literal: t\n<Token at 7> End\n```\n:::\n\n```{.r .cell-code}\nparse(\"aa|b\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 1> Literal: a\n<Token at 3> Or\n     <Token at 2> Literal: a\n     <Token at 4> Literal: b\n```\n:::\n\n```{.r .cell-code}\nparse(\"(ab)|c\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 5> Or\n     <Token at 1> Group\n          <Token at 2> Literal: a\n          <Token at 3> Literal: b\n     <Token at 6> Literal: c\n```\n:::\n\n```{.r .cell-code}\nparse(\"^(caa*t)|(dog)$\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 1> Start\n<Token at 9> Or\n     <Token at 2> Group\n          <Token at 3> Literal: c\n          <Token at 4> Literal: a\n          <Token at 6> Any\n               <Token at 5> Literal: a\n          <Token at 7> Literal: t\n     <Token at 10> Group\n          <Token at 11> Literal: d\n          <Token at 12> Literal: o\n          <Token at 13> Literal: g\n<Token at 15> End\n```\n:::\n\n```{.r .cell-code}\nparse(\"ab|((cd*)|ef)|g\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 1> Literal: a\n<Token at 3> Or\n     <Token at 2> Literal: b\n     <Token at 14> Or\n          <Token at 4> Group\n               <Token at 10> Or\n                    <Token at 5> Group\n                         <Token at 6> Literal: c\n                         <Token at 8> Any\n                              <Token at 7> Literal: d\n                    <Token at 11> Literal: e\n               <Token at 12> Literal: f\n          <Token at 15> Literal: g\n```\n:::\n\n```{.r .cell-code}\nparse(\"x|(y|z)z\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 2> Or\n     <Token at 1> Literal: x\n     <Token at 3> Group\n          <Token at 5> Or\n               <Token at 4> Literal: y\n               <Token at 6> Literal: z\n<Token at 8> Literal: z\n```\n:::\n\n```{.r .cell-code}\nparse(\"x|(y|z)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Token at 2> Or\n     <Token at 1> Literal: x\n     <Token at 3> Group\n          <Token at 5> Or\n               <Token at 4> Literal: y\n               <Token at 6> Literal: z\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}