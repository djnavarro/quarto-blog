{
  "hash": "9e770c37e50e60b1d7bdb1582ca0570d",
  "result": {
    "markdown": "---\ntitle: \"How to visualise a billion rows of data in R with Apache Arrow\"\ndescription: \"In which the author grapples with the awkward question of what data visualisation really means when you have a staggering amount of data to work with\"\ndate: \"2022-08-23\"\ncategories: [Apache Arrow, Data Visualisation]\nimage: \"img/cover.jpg\"\n---\n\n\n<!--------------- my typical setup ----------------->\n\n\n\n\n\n<!--------------- post begins here ----------------->\n\nIt's been a couple of months since I published anything on this blog. In my defence, I've been busy: I spent the month of June developing a workshop and website on [larger than memory workflows in R with Apache Arrow](https://arrow-user2022.netlify.app/) for the useR! conference, and I spent July doing the same thing for my [art from code](https://art-from-code.netlify.app/) workshop at rstudio::conf. It was a little intense because both workshops had to be prepared from scratch, and there is very little overlap in content. The first one was all about how to use the {arrow} package to analyse very large data sets in R, whereas the second was was all about techniques for making generative art in R. There wasn't a lot of room for art in my Arrow workshop, nor for Arrow in my art workshop. It's a bit of a shame: ever since I started learning Arrow in late 2021 I've been wondering how to bring the two worlds. It feels like Arrow represents an untapped wellspring of creative possibilities for generative art, and I really want to develop generative art systems built on top of Arrow. Sadly, I am not quite there yet, but I've started the journey with something a little less ambitious: data visualisation with Arrow.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow)\nlibrary(dplyr)\nlibrary(tictoc)\nlibrary(tidyr)\nlibrary(ggplot2)\n```\n:::\n\n\n:::{.column-margin}\nThe post is adapted from the [NYC taxi scatter GitHub repository](https://github.com/djnavarro/arrow-nyc-taxi-scatter) that I put together to chat about on [The Data Thread](https://www.youtube.com/c/TheDataThread/about) live interviews series, *Pulling the Thread*\n:::\n\n\n## The NYC taxi data\n\nAt this point in my life I have used the \"NYC Taxi Data\" for so many illustrative examples I feel like I don't need to explain it: doesn't \"everyone\" know about this data by now? Yeah, no dice sweetie. That's a terrible intuition. Most people don't know the data, and those that do can just skip this section. So here's a quick summary of the data set. In its full form, the data set takes the form of one very large table with about 1.7 billion rows and 24 columns. Each row corresponds to a single taxi ride sometime between 2009 and 2022. There's a complete [data dictionary for the NYC taxi data](https://arrow-user2022.netlify.app/packages-and-data.html#data) on the useR workshop site, but the columns that will be relevant for us are as follows:\n\n- `pickup_longitude` (double): Longitude data for the pickup location \n- `pickup_latitude` (double): Latitude data for the pickup location\n- `dropoff_longitude` (double): Longitude data for the dropoff location \n- `dropoff_latitude` (double): Latitude data for the dropoff location \n\nOn my laptop I have a copy of both the full data set, located at `\"~/Datasets/nyc-taxi\"` on my machine, and a much smaller \"tiny\" data set that contains 1 out of every 1000 records from the original, located at `\"~/Datasets/nyc-taxi-tiny/\"`. This tiny version has a mere 1.7 million rows of data, and as such is small enough that it will fit in memory. [Instructions for downloading both data sets](https://arrow-user2022.netlify.app/packages-and-data.html#data) are available at the same location as the data dictionary.\n\n## Loading the data\n\nSince I have local copies of the data, I'll use the `open_dataset()` function from the {arrow} package to connect to both versions of the NYC taxi data:^[It's worth noting that you can connect to remote data sets as well as local ones, but that's a bit beyond the scope of this post.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnyc_taxi <- open_dataset(\"~/Datasets/nyc-taxi/\")\nnyc_taxi_tiny <- open_dataset(\"~/Datasets/nyc-taxi-tiny/\")\n```\n:::\n\n\nStarting with Arrow 9.0.0 it's been possible to use the {dplyr} `glimpse()` function to take a look at the data sets, so let's do that:\n\n\n::: {.cell hash='index_cache/html/glimpse-data_9a1e42d8540622b23300148ccf5b5bca'}\n\n```{.r .cell-code}\nglimpse(nyc_taxi)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFileSystemDataset with 158 Parquet files\n1,672,590,319 rows x 24 columns\n$ vendor_name             <string> \"VTS\", \"VTS\", \"VTS\", \"DDS\", \"DDS\", \"DDS\", \"DD…\n$ pickup_datetime  <timestamp[ms]> 2009-01-04 13:52:00, 2009-01-04 14:31:00, 200…\n$ dropoff_datetime <timestamp[ms]> 2009-01-04 14:02:00, 2009-01-04 14:38:00, 200…\n$ passenger_count          <int64> 1, 3, 5, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, …\n$ trip_distance           <double> 2.63, 4.55, 10.35, 5.00, 0.40, 1.20, 0.40, 1.…\n$ pickup_longitude        <double> -73.99196, -73.98210, -74.00259, -73.97427, -…\n$ pickup_latitude         <double> 40.72157, 40.73629, 40.73975, 40.79095, 40.71…\n$ rate_code               <string> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ store_and_fwd           <string> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ dropoff_longitude       <double> -73.99380, -73.95585, -73.86998, -73.99656, -…\n$ dropoff_latitude        <double> 40.69592, 40.76803, 40.77023, 40.73185, 40.72…\n$ payment_type            <string> \"Cash\", \"Credit card\", \"Credit card\", \"Credit…\n$ fare_amount             <double> 8.9, 12.1, 23.7, 14.9, 3.7, 6.1, 5.7, 6.1, 8.…\n$ extra                   <double> 0.5, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, …\n$ mta_tax                 <double> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ tip_amount              <double> 0.00, 2.00, 4.74, 3.05, 0.00, 0.00, 1.00, 0.0…\n$ tolls_amount            <double> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_amount            <double> 9.40, 14.60, 28.44, 18.45, 3.70, 6.60, 6.70, …\n$ improvement_surcharge   <double> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ congestion_surcharge    <double> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ pickup_location_id       <int64> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ dropoff_location_id      <int64> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ year                     <int32> 2009, 2009, 2009, 2009, 2009, 2009, 2009, 200…\n$ month                    <int32> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n```\n:::\n:::\n\n\nIf you've used `glimpse()` before this output will look very familiar. Each line in the output show the name of one column in the data, followed by the first few entries in that column. There are a few small hints that the underlying data structure is different though. For instance, the data types associated with each column refer to Arrow data types (e.g., timestamp, int32, int64, etc) rather than R data types. I'm not going to talk about those here, but if you're looking for information about this topic, there's a short [summary of Arrow data types](https://arrow-user2022.netlify.app/advanced.html#how-are-scalar-types-mapped) on the workshop website, and a [longer blog post on Arrow data types](https://blog.djnavarro.net/posts/2022-03-04_data-types-in-arrow-and-r/) on this blog. Nevertheless, those minor differences notwithstanding, the output of `glimpse()` is more or less exactly what you'd expect to see if `nyc_taxi` were a data frame. However, when you look at the size of the data set, you might begin to suspect that some magic is going on. Behind the scenes there are 1.7 billion rows of data in one huge table, and this is just too big to load into memory. Fortunately, the {arrow} package allows us to work with it anyway!\n\n## Plotting a million rows\n\nOkay, let's start with a data visualisation problem that wouldn't be too difficult to manage on a small data set. I want to draw an image that plots the pickup location for every taxi ride in the data set. Here's how I might go about that. First, I'll do a minimal amount of data wrangling in {arrow}. Specifically, I'll use the {dplyr} `select()` and `filter()` functions to limit the amount of data I have to `collect()` into R:\n\n\n::: {.cell hash='index_cache/html/filtering_105a395a3b69129b1deb402371e596aa'}\n\n```{.r .cell-code}\ntic()\nnyc_pickups <- nyc_taxi_tiny |>\n  select(pickup_longitude, pickup_latitude) |>\n  filter(\n    !is.na(pickup_longitude),\n    !is.na(pickup_latitude)\n  ) |>\n  collect()\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.16 sec elapsed\n```\n:::\n:::\n\n\nAt this point I have a regular R data frame, `nyc_pickups`, that contains only the data I need: the pickup locations for all those taxi rides (in the *tiny* taxi data set) that actually contain longitude and latitude data. Let's use `glimpse()` again:\n\n\n::: {.cell hash='index_cache/html/glimpse-pickups_777a2e321317d21e8bd17c1d8b4b682e'}\n\n```{.r .cell-code}\nglimpse(nyc_pickups)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 1,249,107\nColumns: 2\n$ pickup_longitude <dbl> -73.95557, -73.97467, -73.78190, -73.97872, -73.97400…\n$ pickup_latitude  <dbl> 40.76416, 40.76222, 40.64478, 40.75371, 40.77901, 0.0…\n```\n:::\n:::\n\n\nCompared to the full NYC taxi data, this is a relatively small data set. Drawing a scatter plot from 1.2 million observations isn't a trivial task, to be sure, but it is achievable. In fact the {ggplot2} package handles this task surprisingly well:\n\n\n::: {.cell hash='index_cache/html/ggplot2-image_72f14eb74a16f2be7164916ef14192b5'}\n\n```{.r .cell-code}\nx0 <- -74.05 # minimum longitude to plot\ny0 <- 40.6   # minimum latitude to plot\nspan <- 0.3  # size of the lat/long window to plot\n\ntic()\npic <- ggplot(nyc_pickups) +\n  geom_point(\n    aes(pickup_longitude, pickup_latitude), \n    size = .2, \n    stroke = 0, \n    colour = \"#800020\"\n  ) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_void() +\n  coord_equal(\n    xlim = x0 + c(0, span), \n    ylim = y0 + c(0, span)\n  )\npic\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/ggplot2-image-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n3.365 sec elapsed\n```\n:::\n:::\n\n\nIt's not lightning fast or anything, but it's still pretty quick!\n\nAs neat as this visualisation is, if we look at it for a while we can see some problems with it. I'm not talking about the fact that there's no legend or explanatory text: although those are real failures of data visualisation, they're easily fixable. {ggplot2} has lots of tools that would allow us to fix those problems. The bigger issue here is that the plot manages to be too dense and too sparse at the same time. There are so many data points in Manhattan (particularly in the midtown area) that it's impossible to discern any fine detail: that part of the map is just one solid block of colour. Yet, at the same time, for most places outside of Manhattan there the data are too sparse. There are so few data points in surrounding areas of the city that it's hard to see a lot of detail.\n\n\n::: {.cell hash='index_cache/html/annotated-ggplot_75189ae42604d08a128a4c5c2067bf46'}\n::: {.cell-output-display}\n![](index_files/figure-html/annotated-ggplot-1.png){width=672}\n:::\n:::\n\n\nThis combination of problems is a little tricky: if we want more detail in the plot, we need to include a lot more data. In one sense this is fine: we have 1000x the volume of data in the *full* NYC taxi data, but there is so much data there that we can't even load it into R, much less ask {ggplot2} to create a scatter plot from it. Plus, even if we did somehow manage to accomplish this, just imagine how utterly unreadable the high-density parts of the plot would look. The entirety of Manhattan would just be one enormous blur. \n\nIf we want to make good use of our large data set, we have to rethink our data visualisation technique. Perhaps scatter plots are not quite the right approach?\n\n## Scaling to a billion rows\n\nA natural solution to the \"some parts are too dense, others are too sparse\" problem is to stop thinking of the desired visualisation as a scatter plot, and instead think of it as a heat map. Instead of drawing a small plot marker at the location of each pickup, we colour each pixel in the image based on the number of pickups within the geographic region spanned by that pixel. Normally, this approach wouldn't work very well: the number of observations within each pixel would probably be too small, and the resulting heat map wouldn't be much of an improvement over a scatter plot.\n\nBut these aren't normal circumstances. We have 1.2 billion rows of data here. Even if I create a fairly high resolution 4000x4000 image, there are \"only\" 16 million pixels. The expected number of pickups within each pixel is 75. Binning 1.2 billion observations into 16 million categories isn't an *easy* thing to do, but it's exactly the kind of task that Arrow excels at. \nSo let's do that: we'll use the {arrow} package to count the number of pickups that fall within the region spanned by each pixel in the image. This does almost all of the work for us, because we reduce billions of records to a much smaller table of pixels and counts, one that is small enough to pull into R:\n\n\n::: {.cell hash='index_cache/html/compute-pixels_47854146bcf35cbd0a57c1d3a15eb7da'}\n\n```{.r .cell-code}\ntic()\npixels <- 4000\npickup <- nyc_taxi |>\n  filter(\n    !is.na(pickup_longitude),\n    !is.na(pickup_latitude),\n    pickup_longitude > x0,\n    pickup_longitude < x0 + span,\n    pickup_latitude > y0,\n    pickup_latitude < y0 + span\n  ) |>\n  mutate(\n    unit_scaled_x = (pickup_longitude - x0) / span,\n    unit_scaled_y = (pickup_latitude - y0) / span,\n    x = as.integer(round(pixels * unit_scaled_x)), \n    y = as.integer(round(pixels * unit_scaled_y))\n  ) |>\n  count(x, y, name = \"pickup\") |>\n  collect()\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n31.101 sec elapsed\n```\n:::\n:::\n\n\nHalf a minute to solve the binning problem is pretty decent, really. As before, I'll use `glimpse()` to take a peek at the object we've just created. It's a data frame containing `x` and `y` columns specifying the horizontal and vertical index of a pixel, and a `pickup` column that counts the number of pickups that fall within the geographical region covered by that pixel:\n\n\n::: {.cell hash='index_cache/html/glimpse-pickup_fae4536072ae01f505509c9d4d61bb91'}\n\n```{.r .cell-code}\nglimpse(pickup)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 4,677,864\nColumns: 3\n$ x      <int> 1058, 1024, 1162, 3525, 865, 794, 856, 705, 647, 762, 802, 1207…\n$ y      <int> 2189, 2040, 2265, 552, 1983, 1646, 2018, 1590, 1723, 2010, 1645…\n$ pickup <int> 6514, 5030, 3818, 67, 2408, 2415, 932, 3607, 2664, 1024, 2207, …\n```\n:::\n:::\n\n\nOne thing to note about this output is that the pixels aren't arranged in a meaningful order, and only those pixels with at least one pickup (a little under 30% of all pixels) are included in data. \n\nNow that we have a smaller data set to work with, visualising this condensed data can be done in a number of ways. The \"laziest\" way to it is to repeat the exact same approach I took earlier. That is, I could draw a scatter plot:\n\n\n::: {.cell hash='index_cache/html/ggplot2-image-2_b70a8ab656a5717a6cb46fb50a9f0cb9'}\n\n```{.r .cell-code}\ntic()\nggplot(pickup) +\n  geom_point(\n    aes(x, y, colour = log10(pickup)), \n    size = .01, \n    stroke = 0, \n    show.legend = FALSE\n  ) +\n  scale_colour_gradient(low = \"white\", high = \"#800020\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_void() +\n  coord_equal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/ggplot2-image-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n12.159 sec elapsed\n```\n:::\n:::\n\n\nAs you can see, {ggplot2} has no problems drawing a scatter plot from a few million observations, and it's not too slow in producing an output either. It's also much clearer than our first attempt using the tiny taxi data. The \"too dense but also too sparse\" problem no longer plagues us, and we're better able to see how the distribution of pickups spreads along the streets of New York. \n\nHowever, there's a better way to do this. Instead of trying to draw a scatter plot of all the points listed in the `pickup` data frame, we can use it to populate a bitmap. We'll create a 4000x4000 matrix, and fill in the cells with the pickup counts at the corresponding pixel. It's a two part process. First, we use `expand_grid()` to initialise a \"grid like\" tibble that contains every valid combination of `x` and `y` values. Then we use `left_join()` to populate a column containing the pickup counts:\n\n\n::: {.cell hash='index_cache/html/expand-to-grid_9e7b0810461cde4fb35da44cb352ee62'}\n\n```{.r .cell-code}\ntic()\ngrid <- expand_grid(x = 0:pixels, y = 0:pixels) |>\n  left_join(pickup, by = c(\"x\", \"y\")) |>\n  mutate(pickup = replace_na(pickup,  0))\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n7.98 sec elapsed\n```\n:::\n:::\n\n\nHaving done this, we can explicitly convert this to a matrix. It's super-easy to do this because `expand_grid()` orders the elements of `grid$pickup` so that they can be passed straight to `matrix()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\npickup_grid <- matrix(\n  data = grid$pickup,\n  nrow = pixels + 1,\n  ncol = pixels + 1\n)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.746 sec elapsed\n```\n:::\n:::\n\n\nThis is our bitmap. It's a matrix whose values correspond to the pixel intensities to be plotted. Now all we have to do is write the image. Because the data are already stored as a bitmap, we don't even need {ggplot2}: we can use `image()` to draw the bitmap directly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrender_image <- function(mat, cols = c(\"white\", \"#800020\")) {\n  op <- par(mar = c(0, 0, 0, 0))\n  shades <- colorRampPalette(cols)\n  image(\n    z = log10(t(mat + 1)),\n    axes = FALSE,\n    asp = 1,\n    col = shades(256),\n    useRaster = TRUE\n  )\n  par(op)\n}\n\ntic()\nrender_image(pickup_grid)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/taxi-scatter-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2.425 sec elapsed\n```\n:::\n:::\n\n\nThis method is slightly faster the {ggplot2} scatter plot approach, but the real advantage isn't speed -- it's clarity. This version of the map is the clearest one so far. There's less blurring in the denser parts of the plot (midtown Manhattan), and there's also more clarity in the sparser areas (e.g., the Brooklyn streets are sharper).\n\nWe can push it slightly further by tweaking the colour palette. Plotting the logarithm of the number of pickups ensures that all the streets are visible (not just the extremely common ones), but it does have the downside that it's hard to tell the difference between moderately popular pickup locations and extremely popular ones. A well-chosen diverging palette helps rectify this a little:     \n\n\n::: {.cell}\n\n```{.r .cell-code}\nrender_image(pickup_grid, cols = c(\"#002222\", \"white\", \"#800020\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/taxi-scatter-2-1.png){width=672}\n:::\n:::\n\n\nAt long last we have a visualisation that shows all the billion rows of data, crisply delineates all the streets on which taxi pickups are at least moderately frequent, *and* does a reasonable job of highlighting those locations where taxi pickups are extremely common. \n\nIn real life we'd now want to go on to add legends, explanatory text, and other annotations, but I'm not going to bother with that here. After all, the point of this post was to give an example of how to draw the heat map itself!\n\n## Lessons learned?\n\nTo wrap this post up, I think it's useful to reflect on the process I went through in constructing this image. In one sense, the process I've gone through here isn't actually much different to what we do when creating any other data visualisation in R. For example, if you're working in {tidyverse}, a typical work flow is to use {dplyr} to wrangle the data into an appropriate format and then use {ggplot2} to plot the data. What I've done here isn't that different: okay yes, my {dplyr} code only works because it's backed by the {arrow} engine, and in the end I decided to use base graphics rather than {ggplot2} to draw the final image, but I don't think those differences constitute a major departure from my usual approach.\n\nThat being said, I think there are two key principles I've taken away from this. When trying to visualise very large data sets in R, the things I'm going to try to keep in mind are:\n\n- Push as much of the computational work onto {arrow} as possible. The {arrow} package is designed specifically to handle these kinds of data manipulation problems, and things go much more smoothly when I don't make {ggplot2} do the computational heavy lifting.\n\n- Think carefully about the data representation. The reason why the final plot drawn with `image()` is so much nicer than the earlier ones drawn with {ggplot2} has nothing at all to do with the \"base R vs tidyverse\" issue. Instead, it's because the data structure I created (i.e., `pickup_grid`) is the exact bitmap that needed to be rendered, and that's exactly what `image()` is good for. \n\nOddly, the second point is something that came up in my rstudio::conf workshop when discussing [raster representations in generative art](https://art-from-code.netlify.app/day-2/session-1/#even-faster-chaos-with-raster-representation). Here's what I had to say in that context:\n\n> If you want your image rendering to go faster, well, maybe you should store the data in a format that mirrors the output you want? I mean, at this point we’re storing a data frame with 10 million coordinates, and then plotting circles in an abstract canvas that ggplot2 constructs with the help of the grid graphics system, and then… aren’t you tired already?\n>\n>If you want a bitmap that stores pixel values at the end of your generative process, why not start with the data in exactly the same format at the beginning? Don’t draw circles-as-polygons-around-a-coordinate. Just store the damned pixel values from the outset.\n\nThis is not to say that {ggplot2} isn't useful in generative art, and it's not to say that we don't need it when visualising large data sets. That's not even remotely the point! It's a powerful tool that serves us well. The point is much more low level: don't make your plotting library fight *against* your data representation. If you've created a bitmap (as I did here), use a tool that writes bitmaps (so I used `image()`), but if you've created something else you should use something else. The main thing is to spend some time thinking about \"what is it that I'm trying to draw?\" and as a consequence \"what data format is optimal for helping me draw that?\" \n\n<br><br>\n\n\n\n<!--------------- appendices go here ----------------->\n\n\n::: {.cell}\n\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}