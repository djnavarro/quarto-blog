{
  "hash": "270f162ba0bcf9c0ab14875a5d5955a0",
  "result": {
    "markdown": "---\ntitle: \"Arrow tables and record batches\"\ndate: \"2022-04-30\"\ncategories: [Apache Arrow]\nimage: \"../../blank_preview.jpg\"\n---\n\n<!--------------- my typical setup ----------------->\n\n\n\n<!--------------- post begins here ----------------->\n\n>\n| \"The time has come,\" the Walrus said,\n|    \"To talk of many things:\n| Arrays (in Arrow) and Record Batches,\n|    Of Tables, Chunks and things—\n| Why IPC's great for streaming blocks—\n|    But for saving, Parquet wins.\" \n|\n| (My sincere apologies to [Lewis Carroll](https://poets.org/poem/walrus-and-carpenter))\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'arrow'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:utils':\n\n    timestamp\n```\n:::\n:::\n\nThis post has been rolling around in my head since the start of April. I've known from the beginning what I want to say -- approximately! -- but not how I want to say it. It is annoying me, and at a certain point I decided I just needed to put words to metaphorical paper and see what happens. It's another post about Apache Arrow, the fourth in a series I've been writing.\n\nMy goal in this one is... well, I suppose it is partly to do my job since I am employed to write these things, but it's a little more than that. My goal is to clarify something that has been a source of confusion about Arrow for me, in the hope that once I've sorted it out in my own head I can perhaps contribute something to the official documentation that helps other people who might have the same problem that I have. \n\nSo here's the thing that has been giving me grief. If you go to the Get Started page for the **arrow** R package, one of the first things you encounter is a table telling you that Arrow has classes for zero-dimensional data (scalars), one-dimensional data (arrays and other vector-like data), and two-dimensional data (tabular or data frame-like data). I'll reproduce the entire table in full because it's actually super important...\n\n\n| Dim | Class          | Description                               | How to create an instance                                                                             |\n| --- | -------------- | ----------------------------------------- | ------------------------------------------------------------------------------------------------------|\n| 0   | `Scalar`       | single value and its `DataType`           | `Scalar$create(value, type)`                                                                          |\n| 1   | `Array`        | vector of values and its `DataType`       | `Array$create(vector, type)`                                                                          | \n| 1   | `ChunkedArray` | vectors of values and their `DataType`    | `ChunkedArray$create(..., type)` or alias `chunked_array(..., type)`                                  |\n| 2   | `RecordBatch`  | list of `Array`s with a `Schema`          | `RecordBatch$create(...)` or alias `record_batch(...)`                                                |\n| 2   | `Table`        | list of `ChunkedArray` with a `Schema`    | `Table$create(...)`, alias `arrow_table(...)`, or `arrow::read_*(file, as_data_frame = FALSE)`        |\n| 2   | `Dataset`      | list of `Table`s  with the same `Schema`  | `Dataset$create(sources, schema)` or alias `open_dataset(sources, schema)`                            |\n\n...but I'm going to be honest with you, dear reader. When I first started learning Arrow, I had no idea what any of this meant. This whole table was completely intimidating. I looked at it and thoughts roughly along the following lines went through my head:\n\n\n> Oh... f**k me. I'm completely out of my depth, I am too stupid to understand any of this. I should quit now and find a new job before everyone realises I'm a total fraud. They made a terrible mistake hiring me and... blah blah blah\n\n\nThe self-pity went on for a while, but I'll spare you the details. \n\nEventually I remembered that this is my impostor syndrome talking and that I am in fact quite good at learning technical concepts. The problem I'm encountering here is that this table is not in any way self-explanatory, but it's placed in a part of the documentation where new users will easily encounter it and get confused the same way I did. The placement itself isn't the problem: the content of the table is actually pretty important for any Arrow user to grasp, but all the explanatory scaffolding is missing. \n\nTo a new user, most of this is is incomprehensible. What exactly is a `ChunkedArray` and how is it different from an `Array`? Why are these necessary as distinct concepts? While we are at it, what the heck is a `RecordBatch`, a `Table` and a `Dataset`, and what makes them different from one another? Unless someone takes the time to explain it all to you, it does look like Arrow is unnecessarily complicated, doesn't it? And yet, some very smart people seem to think that Arrow is a very good idea indeed so... what's the story? \n\nKick back, relax into whatever comfort you have available, and let me tell you a tale.^[In the near future, I hope that the documentation itself is going to tell this story -- and yes, I realise that by calling attention to the issue I've effectively volunteered to fix it -- but sometimes it's easier to do the same job in an informal blog post where you have the luxury of going overboard with \"authorial voice\" and \"narrative\", and all those other fancy things that writers love.] All will be revealed...\n\n## Scalars\n\nLet's start with scalars. A scalar object is simply a single value, that can be of any type. It might be an integer, a string, a timestamp, or any of the different data types that Arrow supports. I won't talk about the different types in this post because I already wrote an extremely long post on that topic. For the current purposes, what matters is that a scalar is *one* value. It is \"zero dimensional\". All higher order data structures are built on top of scalars, so they are in some sense fundamental, but there is not much I need to say about them for this post. \n\n## Arrays\n\nLet's turn our attention to arrays next. I'll start by introducing some terminology from the page describing the [Arrow specification](https://arrow.apache.org/docs/format/Columnar.html):\n\n- An **array** in Arrow is analogous to a vector in R: it is a sequence of values with known length, all of which have the same type. The array refers to the abstract data structure itself. \n- A related but different concept is that of a **buffer**, a sequential virtual address space with a given length. Any byte in the buffer can be reached via a single pointer offset less than the region’s length.\n- Finally, we have the concept of the **physical layout**, which describes how information is laid out in memory, without taking into account of how that information is interpreted. For example, a 32-bit signed integer array and 32-bit floating point array have the same layout because they have the same physical structure in memory. The meaning of the bits that make up a 32-bit float is different to the meaning of the bits that make up a 32-bit integer, but the physical layout is the same. \n\n### What's an array?\n\nOkay, for the most part, those of us who use Arrow on an everyday basis don't really need to care all that much about low level implementation details. The exact physical layout of an array in memory doesn't matter to us. But, on the other hand, in this post I'm trying to highlight the fact that Arrow does make some good choices about these details, so it makes a little sense to dive deeper than we normally would. So let's take a look at an example taken from the Arrow documentation pages. Here's a simple array of int32 values:\n\n``` python\n[1, null, 2, 4, 8]\n```\n\nWhat does this thing look like in memory? It contains two pieces of metadata, namely the length of the array (i.e. 5) and a count of the number of null values (i.e., 1), both of which are stored as 64-bit integers. Next, it contains two buffers, a **validity bitmap buffer** and a **value buffer**. The validity bitmap is binary-valued, and contains a 1 whenever the corresponding slot in the array contains a valid, non-null value. To a first approximation, you might imagine the validity bitmap for the array above containing the following five bits: `10111`. Written this way, you can see there are four non-null values (shown as `1`s) and one null (shown as a `0`) in the second slot. This maps naturally onto the visual convention I've used to show the array, but it's a little misleading in a few respects. \n\n- First, I've only shown five values here, but in reality these five values occupy part of a byte, so in actuality there are 8 bits. So I ought to have added three trailing zeros to pad it out out to a complete 8-bit sequence: that gives us `10111000`. \n\n- Second, I've written this the wrong way around! I've written it left-to-right in order to match up to the usual convention of the English language (and to mirror the ordering shown in the vector above!), but by convention that corresponds to \"big endian\" layout, and the validity bitmap is actually little endian in Arrow. So the actual binary sequence looks like this: `00011101`\n\n- Third, I've not padded it enough. As a general rule, if you want things to be efficient you want the beginnings and endings of your data structures to be **naturally aligned**, in the sense that the memory address is a multiple of the data block sizes. So on a 64-bit machine, you want the memory address for every data structure to start on a multiple of 64 bits. Apparently that makes lookup easier or something. Unfortunately, I've only specified 8 bits (i.e. 1 byte) so if I wanted to ensure that the validity bitmap is naturally aligned I'm going to need to add another 7 bytes worth of padding in order to make it to the full 64 bits. This method of aligning data structures in memory is referred to as \"8 byte alignment\". However, what Arrow usually does in this situation is 64 byte alignment.^[Strictly speaking the Arrow specification allows some flexibility here. Implementations can choose between 8 byte alignment or 64 byte alignment. Moreover, when allocating memory this is only a \"recommendation\", whereas when serialising Arrow data via IPC the alignment is required. It's also recommended that implementations use 64 byte alignment rather than 8 byte alignment. As discussed in the Arrow specification page: \"The recommendation for 64 byte alignment comes from the [Intel performance guide](https://software.intel.com/en-us/articles/practical-intel-avx-optimization-on-2nd-generation-intel-core-processors) that recommends alignment of memory to match SIMD register width. The specific padding length was chosen because it matches the largest SIMD instruction registers available on widely deployed x86 architecture (Intel AVX-512). The recommended padding of 64 bytes allows for using [SIMD](https://software.intel.com/en-us/cpp-compiler-developer-guide-and-reference-introduction-to-the-simd-data-layout-templates) instructions consistently in loops without additional conditional checks. This should allow for simpler, efficient and CPU cache-friendly code. In other words, we can load the entire 64-byte buffer into a 512-bit wide SIMD register and get data-level parallelism on all the columnar values packed into the 64-byte buffer. Guaranteed padding can also allow certain compilers to generate more optimized code directly.\" I can honestly say that I understand about 1/3 of that now. I'm learning things!] \n\nTaking all these considerations together is what gives us this diagram on the Arrow documentation page. The validity bitmap buffer for this array is represented as follows:\n\n|Byte 0 (validity bitmap) | Bytes 1-63            |\n|-------------------------|-----------------------|\n| `00011101`              | `0` (padding)         |\n\n\nOkay, now let's have a look at the value buffer. It's essentially the same logic. Again notice that its padded out to a length of 64 bytes to preserve natural alignment, but for our purposes those details don't matter too much. Here's the diagram showing the physical layout, again lifted straight from the Arrow specification page:\n\n|Bytes 0-3   | Bytes 4-7   | Bytes 8-11  | Bytes 12-15 | Bytes 16-19 | Bytes 20-63 |\n|------------|-------------|-------------|-------------|-------------|-------------|\n| `1`        | unspecified | `2`         | `4`         | `8`         | unspecified |\n\nEach integer occupies 4 bytes, as required by the int32 data type. \n\nWhat should you take away from all this? Well, firstly, fear not -- if you're an R user like me and trying to wrap your head around Arrow, you really don't have to spend much of your time thinking about what this physical layout looks like. The thing you really need to keep in mind is that **an Arrow array is constructed from buffers that occupy a single contiguous block in memory**\n\n### Peeking inside arrays\n\n::: {.cell}\n\n```{.r .cell-code}\narr <- Array$create(c(1, NA, 2, 4, 8), type = int32())\narr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArray\n<int32>\n[\n  1,\n  null,\n  2,\n  4,\n  8\n]\n```\n:::\n:::\n\nThe metadata are easily extracted:\n\n::: {.cell}\n\n```{.r .cell-code}\narr$length()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5\n```\n:::\n\n```{.r .cell-code}\narr$null_count\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\nWe can extract the validity bitmap, using the `IsValid()` method:\n\n::: {.cell}\n\n```{.r .cell-code}\nvalidity_bitmap <- function(x) {\n  slots <- 0:(x$length() - 1)\n  vapply(slots, x$IsValid, FUN.VALUE = FALSE)\n}\n\nvalidity_bitmap(arr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  TRUE FALSE  TRUE  TRUE  TRUE\n```\n:::\n:::\n\nWe can extract the pointer to tell us where in memory the Arrow array begins:\n\n::: {.cell}\n\n```{.r .cell-code}\narr$pointer()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<pointer: 0x561e827d14b0>\n```\n:::\n:::\n\nWe can count the total number of bytes allocated to the array:\n\n::: {.cell}\n\n```{.r .cell-code}\narr$nbytes()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 21\n```\n:::\n:::\n\nNotice that this doesn't count any padded bytes at the end of each buffer. The validity bitmap requires only 1 byte, and the values bitmap occupies 20 bytes (noting that the unspecified bytes corresponding to the null value do count because they are part of the contiguous region between byte 0 and byte 19)\n\n## Record Batches\n\nA record batch is table-like data structure that is semantically a sequence of fields, each a contiguous Arrow array.^[If you're familiar with Arrow data structures, it is in essence a struct with additional metadata] A struct is a nested type parameterized by an ordered sequence of types (which can all be distinct), called its fields. Each field must have a UTF8-encoded name, and these field names are part of the type metadata. A struct array does not have any additional allocated physical storage for its values. A struct array must still have an allocated validity bitmap, if it has one or more null values. Physically, a struct array has one child array for each field. The child arrays are independent and need not be adjacent to each other in memory.  \n\n\n\n\n<!--------------- appendices go here ----------------->\n\n::: {.cell}\n\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}