{
  "hash": "9df057a5c53457af5801197569d946d0",
  "result": {
    "markdown": "---\ntitle: \"Arrow tables and record batches\"\nsubtitle: \"In which the author discusses arrays, chunked arrays, record batches, tables, and datasets in the R interface to Apache Arrow. Masochism is mentioned, and the music of Florence + The Machine makes a guest appearance\"\ndate: \"2022-04-30\"\ncategories: [Apache Arrow]\nimage: \"img/cover.jpg\"\n---\n\n<!--------------- my typical setup ----------------->\n\n\n\n<!--------------- post begins here ----------------->\n\n>\n| My catholic taste in the devil\n| All gilded and golden, yes, I'm your girl\n| Hell, if it glitters, I'm going\n|   -- *Heaven is Here*, Florence + The Machine\n\nIf you've made the life choice to become a developer advocate with a focus on [Apache Arrow](https://arrow.apache.org/), you're probably not unfamiliar with masochism. Don't believe me? \n\nWell then, let's consider my past choices:\n\n- My first attempt to write about the topic didn't even talk about Arrow: the post was a descent into the madness of [data serialisation in R](https://blog.djnavarro.net/posts/2021-11-15_serialisation-with-rds/) and the RDS format. \n- My second attempt went a little better, and I managed to write a few thousand words on [getting started with Apache Arrow](https://blog.djnavarro.net/posts/2021-11-19_starting-apache-arrow-in-r/) for the R user. Much of that post was spent on answering very technical questions like _\"what is this??????\"_ and _\"why do I care????\"_ Totally normal behaviour, right? \n- My third attempt talked about how [the **arrow** package supplies **dplyr** bindings](https://blog.djnavarro.net/posts/2022-01-18_binding-arrow-to-r/), allowing R to perform data manipulation on Arrow data using familiar **dplyr** syntax. It wasn't a bad post, to be honest, but I did go down a very strange path with all the _Magicians_ gifs. \n- As for my fourth attempt, the [data types in Arrow and R](https://blog.djnavarro.net/posts/2022-03-04_data-types-in-arrow-and-r/) post... look, if you've ever had a secret desire to see what happens when I am reduced to tears and wailing at the horrors of the world and of IEEE 754, then oh my yes, this is the extremely long blog post for you.\n\nWhat I am trying to convey to you, dear reader, is that -- setting aside the superficial trappings of whips and chains and the various other devices that propelled E. L. James to great fortune -- I am intimately acquainted with pain. It is important to me that you understand this, and that when I mention the pain I encountered when trying to learn how the **arrow** R package works, I am not using the term lightly.\n\nSo let talk about my latest pain point, shall we? \n\n<br><br>\n\n## Data objects\n\nHere's the thing that has been giving me grief. Suppose you are an R user who is new to this whole Apache Arrow business. You've installed the **arrow** package, and you're now reading the [Get Started](https://arrow.apache.org/docs/r/articles/arrow.html) page in the hopes that you too will be able to, well, get started. When you visit this page, one of the very first things you encounter is a table listing a variety of data structures used by Arrow. Specifically, the table tells you that Arrow has classes for zero-dimensional data (scalars), one-dimensional data (arrays and other vector-like data), and two-dimensional data (tabular or data frame-like data). It shows you that... \n\n...actually, you know what? Instead of describing it, let's take a look at the actual table. Here's what it tells you about the hierarchy of data structures in **arrow**:\n\n<br>\n\n| Dim | Class          | Description                               | How to create an instance                                                                             |\n| --- | -------------- | ----------------------------------------- | ------------------------------------------------------------------------------------------------------|\n| 0   | `Scalar`       | single value and its `DataType`           | `Scalar$create(value, type)`                                                                          |\n| 1   | `Array`        | vector of values and its `DataType`       | `Array$create(vector, type)`                                                                          | \n| 1   | `ChunkedArray` | vectors of values and their `DataType`    | `ChunkedArray$create(..., type)` or alias `chunked_array(..., type)`                                  |\n| 2   | `RecordBatch`  | list of `Array`s with a `Schema`          | `RecordBatch$create(...)` or alias `record_batch(...)`                                                |\n| 2   | `Table`        | list of `ChunkedArray` with a `Schema`    | `Table$create(...)`, alias `arrow_table(...)`, or `arrow::read_*(file, as_data_frame = FALSE)`        |\n| 2   | `Dataset`      | list of `Table`s  with the same `Schema`  | `Dataset$create(sources, schema)` or alias `open_dataset(sources, schema)`                            |\n\n<br>\n\nNow, perhaps there are some devilishly clever R users who can look at this table and immediately decode all its mysteries. But I will be honest with you, dear reader, and confess that I am not one of these people. When I first started learning Arrow, I had no idea what any of this meant. This whole table was completely intimidating. I looked at it and thoughts roughly along the following lines went through my head:\n\n\n> Oh... f**k me. I'm completely out of my depth, I am too stupid to understand any of this. I should quit now and find a new job before everyone realises I'm a total fraud. They made a terrible mistake hiring me and... blah blah blah\n\n\nThe self-pity went on for a while and the names I called myself became quite inappropriate for a family restaurant, but I'll be kind and spare you the tiresome details. \n\nEventually I remembered that this is my impostor syndrome talking and that I am in fact quite good at learning technical concepts. The problem I'm encountering here is that this table is not in any way self-explanatory, but it's placed in a part of the documentation where new users will easily encounter it and get confused the same way I did. The placement itself isn't the problem: the content of the table is actually pretty important for any Arrow user to grasp, but all the explanatory scaffolding is missing. \n\nTo a new user, most of this is is incomprehensible. What exactly is a `ChunkedArray` and how is it different from an `Array`? Why are these necessary as distinct concepts? While we are at it, what the heck is a `RecordBatch`, a `Table` and a `Dataset`, and what makes them different from one another? Unless someone takes the time to explain it all to you, it does look like Arrow is unnecessarily complicated, doesn't it? And yet, some very smart people seem to think that Arrow is a very good idea indeed so... what's the story? \n\nKick back, relax into whatever comfort you have available, and let me tell you a tale.^[In the near future, I hope that the documentation itself is going to tell this story -- and yes, I realise that by calling attention to the issue I've effectively volunteered to fix it -- but sometimes it's easier to do the same job in an informal blog post where you have the luxury of going overboard with \"authorial voice\" and \"narrative\", and all those other fancy things that writers love.] All will be revealed...\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow)\nlibrary(spotifyr)\nlibrary(dplyr)\nlibrary(tibble)\noptions(scipen = 20)\n```\n:::\n\n::: {.cell}\n\n:::\n\n::: {.cell .column-screen}\n::: {.cell-output-display}\n[![](img/heaven_is_here.png)](https://www.youtube.com/watch?v=Q4Ez9pitRJ0)\n:::\n:::\n\n<br><br>\n\n## Scalars\n\nLet's start with scalars. A scalar object is simply a single value, that can be of any type. It might be an integer, a string, a timestamp, or any of the different data types that Arrow supports. I won't talk about the different types in this post because I already wrote [an extremely long post on that topic](https://blog.djnavarro.net/posts/2022-03-04_data-types-in-arrow-and-r/). For the current purposes, what matters is that a scalar is *one* value. It is \"zero dimensional\". All higher order data structures are built on top of scalars, so they are in some sense fundamental, but there is not much I need to say about them for this post. \n\n\n<br><br>\n\n## Arrays\n\n>\n| All the gods have been domesticated\n| And heaven is now overrated\n|   -- *Cassandra*, Florence + The Machine\n\n\nLet's turn our attention to arrays next. I'll start by introducing some terminology from the page describing the [Arrow specification](https://arrow.apache.org/docs/format/Columnar.html):\n\n- An **array** in Arrow is analogous to a vector in R: it is a sequence of values with known length, all of which have the same type. Coming to Arrow from R, one thing I found a little difficult to wrap my head around is the concept of an array as an **immutable** object. Once an Arrow array has been initialised, it cannot be modified. \n- A related but different concept is that of a **buffer**, a sequential virtual address space with a given length. Any byte in the buffer can be reached via a single pointer offset less than the region’s length.\n- Finally, we have the concept of the **physical layout**, which describes how information is laid out in memory, without taking into account of how that information is interpreted. For example, a 32-bit signed integer array and 32-bit floating point array have the same layout because they have the same physical structure in memory. The meaning of the bits that make up a 32-bit float is different to the meaning of the bits that make up a 32-bit integer, but the physical layout is the same. \n\n\n<br>\n\n### Structure of arrays\n\nOkay, for the most part, those of us who use Arrow on an everyday basis don't really need to care all that much about low level implementation details. The exact physical layout of an array in memory doesn't matter to us. But, on the other hand, in this post I'm trying to highlight the fact that Arrow does make some good choices about these details, so it makes a little sense to dive deeper than we normally would. So let's take a look at an example taken from the Arrow documentation pages. Here's a simple array of integer values:\n\n::: {.cell}\n\n```{.r .cell-code}\narr <- Array$create(c(1L, NA, 2L, 4L, 8L))\narr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArray\n<int32>\n[\n  1,\n  null,\n  2,\n  4,\n  8\n]\n```\n:::\n:::\n\nWhat precisely is this thing? Well that's a mess of different questions. In one sense, the answer is straightforward. It's an Arrow array, and the values contained within the array are all stored as signed 32 bit integers:\n\n::: {.cell}\n\n```{.r .cell-code}\narr$type\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInt32\nint32\n```\n:::\n:::\n\nBut that's not a very satisfying answer at some level. What does this thing look like in memory? How is the information structured? In other words, what is the **physical layout** of this object? \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](img/array_layout.svg)\n:::\n:::\n\n\nThe Arrow documentation page helps us answer that. Our array contains two pieces of metadata, namely the length of the array (i.e. 5) and a count of the number of null values (i.e., 1), both of which are stored as 64-bit integers. The **arrow** package makes it easy to extract these values, because the Array object has fields and methods that will return them:\n\n::: {.cell}\n\n```{.r .cell-code}\narr$length()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5\n```\n:::\n\n```{.r .cell-code}\narr$null_count\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\nOkay, that seems reasonable. What about the data itself? Where is that stored? In Arrow, these are stored within buffers, a contiguous block of memory assigned to the array. The number of buffers associated with an array depends on the exact type of data being stored. For an integer array such as `arr`, there are two buffers, a **validity bitmap buffer** and a **data value buffer**. A little later in the post I'll talk about how you can access the raw content of these buffers, but for now let's talk about what each of these buffers contains. \n\nThe validity bitmap is binary-valued, and contains a 1 whenever the corresponding slot in the array contains a valid, non-null value. Setting aside some very tiresome technicalities we can imagine that the validity bitmap is a buffer that contains the following five bits: \n\n``` python\n10111\n```\n\nExcept... this isn't really true, for three reasons. First, memory is allocated in byte-size units, so we have to pad it out to the full 8 bits. That gives us the bitmap `10111000`. Second, that's still a little inaccurate because -- assuming you read left to right -- you're looking it with the \"most significant bit\" first (i.e., big endian format), and the bits are actually organised with the least significant bit first (i.e., little endian format) so the bits in this byte should be shown in the reverse order, `00011101`. Third, this is still misleading because I've not padded it enough. For reasons that make a lot of sense if you start diving into the Arrow specifications at a low level, you have to imagine another 503 trailing zeros.^[Quick explanation: As a general rule, if you want things to be efficient you want the beginnings and endings of your data structures to be **naturally aligned**, in the sense that the memory address is a multiple of the data block sizes. So on a 64-bit machine, you want the memory address for every data structure to start on a multiple of 64 bits. Apparently that makes lookup easier or something. Unfortunately, I've only specified 8 bits (i.e. 1 byte) so if I wanted to ensure that the validity bitmap is naturally aligned I'm going to need to add another 7 bytes worth of padding in order to make it to the full 64 bits. This method of aligning data structures in memory is referred to as \"8 byte alignment\". However, what Arrow does in this situation is *64 byte alignment*, so each data structure has to be 64 bytes long at a minimum. This design feature exists to allow efficient use of modern hardware, and if you want to know more, it's discussed on the Arrow website.] So that the nice and neat `10111` I've shown above actually looks like this in memory:\n\n|Byte 0 (validity bitmap) | Bytes 1-63            |\n|-------------------------|-----------------------|\n| `00011101`              | `0` (padding)         |\n\n\nI know, I know. Boring. Let's move on.\n\nOkay, now let's have a look at the value buffer. It's essentially the same logic. Again notice that its padded out to a length of 64 bytes to preserve natural alignment, but for our purposes those details don't matter too much. Here's the diagram showing the physical layout, again lifted straight from the Arrow specification page:\n\n|Bytes 0-3   | Bytes 4-7   | Bytes 8-11  | Bytes 12-15 | Bytes 16-19 | Bytes 20-63 |\n|------------|-------------|-------------|-------------|-------------|-------------|\n| `1`        | unspecified | `2`         | `4`         | `8`         | unspecified |\n\nEach integer occupies 4 bytes, as required by the int32 data type. Just to give you a sense of what that looks like, I'll take a peek at the bits that make up the integer `8` (i.e., the contents of Bytes 16-19). I could do this using base R with a command like `intToBits(8L)` but the `bits()` function from the **pryr** package produces a more compact output so I'll do this:\n\n::: {.cell}\n\n```{.r .cell-code}\npryr::bits(8L)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"00000000 00000000 00000000 00001000\"\n```\n:::\n:::\n\nOr, to condense it even further, we can express each byte as a hexadecimal value. That's what the `pryr::bytes()` function does, so this is an equivalent way of writing out the content of those four bytes:\n\n::: {.cell}\n\n```{.r .cell-code}\npryr::bytes(8L)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"00 00 00 08\"\n```\n:::\n:::\n\nPutting all this together you can imagine -- roughly speaking -- what the contents of the data buffer should look like. Ignoring all the trailing bytes used for padding, and not worrying too much about what's going on with the command I've used to extract them, the 20 bytes worth of actual data in the data buffer look like this:\n\n::: {.cell}\n\n```{.r .cell-code}\narr$data()$buffers[[2]]$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 01 00 00 00 00 00 00 80 02 00 00 00 04 00 00 00 08 00 00 00\n```\n:::\n:::\n\nNotice that the last four bytes shown are written `08 00 00 00` (little endian), whereas the bytes produced by `pryr::bytes()` are shown in the other direction. It's the same thing, just a different display convention! \n\nOkay, so what should you take away from all this? Well, firstly, fear not -- if you're an R user like me and trying to wrap your head around Arrow, you don't *really* have to spend much of your time thinking about what this physical layout looks like. All you need to take away from it is that an Arrow array is an immutable object with this particular \"metadata + buffers\" structure that is designed to be efficient, and that each buffer is a contiguous bloc of memory on your machine. \n\n<br>\n\n### Peeking inside arrays\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndance_fever <- read_csv_arrow(\"dance_fever_tracks.csv\")\ndance_fever\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 14 × 3\n   track_number title             duration\n          <int> <chr>                <int>\n 1            1 King                   280\n 2            2 Free                   234\n 3            3 Choreomania            213\n 4            4 Back in Town           236\n 5            5 Girls Against God      280\n 6            6 Dream Girl Evil        227\n 7            7 Prayer Factory          73\n 8            8 Cassandra              258\n 9            9 Heaven Is Here         111\n10           10 Daffodil               214\n11           11 My Love                231\n12           12 Restraint               48\n13           13 The Bomb               165\n14           14 Morning Elvis          262\n```\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nduration <- Array$create(dance_fever$duration)\nduration\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArray\n<int32>\n[\n  280,\n  234,\n  213,\n  236,\n  280,\n  227,\n  73,\n  258,\n  111,\n  214,\n  231,\n  48,\n  165,\n  262\n]\n```\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](img/array_layout_with_nulls.svg)\n:::\n:::\n\nArray objects come with a variety of methods you can use to examine their properties. For instance, you can use the `length()`, `null_count()` and `nbytes()` to calculate the length of the array, the number of missing values it contains, and the number of bytes it occupies in memory. To find out how many bytes have been allocated the `duration` array, we would do this:\n\n::: {.cell}\n\n```{.r .cell-code}\nduration$nbytes()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 56\n```\n:::\n:::\n\nIf you really want to dive in and take a look at how information is structured in an array, you can use the `data()` method to return an ArrayData object:\n\n::: {.cell}\n\n```{.r .cell-code}\nduration$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArrayData\n```\n:::\n:::\n\nThis output is a little underwhelming because at the moment the print method for an `ArrayData` object doesn't do anything except print the class name. However, it's an R6 object which means everything is stored in an environment. So let's use the `env_print()` function from the **rlang** package to take a look at it as an environment:\n\n::: {.cell}\n\n```{.r .cell-code}\nrlang::env_print(duration$data())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<environment: 0x5563b225bfa8> [L]\nParent: <environment: empty>\nClass: ArrayData, ArrowObject, R6\nBindings:\n• .__enclos_env__: <env>\n• buffers: <active>\n• offset: <active>\n• null_count: <active>\n• length: <active>\n• type: <active>\n• `.:xp:.`: <externalptr>\n• clone: <fn> [L]\n• print: <fn> [L]\n• set_pointer: <fn> [L]\n• pointer: <fn> [L]\n• initialize: <fn> [L]\n```\n:::\n:::\n\nThere's a lot going on here. There are various fields used to store metadata about the array, but most of those are things we've seen before and you can access the same information in other ways (e.g., `duration$data()$length` stores the same value returned by `duration$length()`). The one we're actually interested in here is `buffers`:\n\n::: {.cell}\n\n```{.r .cell-code}\nduration$data()$buffers\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\nNULL\n\n[[2]]\nBuffer\n```\n:::\n:::\n\nOkay, so there are two buffers here. If we look back up to the earlier discussion we might (correctly!) guess that the first one is the validity bitmap buffer, and the second one is the data buffer. As before, the buffer is an R6 object and guess what? It also has a `data()` method. If we peek into *that*, at long last we uncover the thing we were looking for:\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_buffer <- duration$data()$buffers[[2]]$data()\ndata_buffer\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 18 01 00 00 ea 00 00 00 d5 00 00 00 ec 00 00 00 18 01 00 00 e3 00 00 00 49\n[26] 00 00 00 02 01 00 00 6f 00 00 00 d6 00 00 00 e7 00 00 00 30 00 00 00 a5 00\n[51] 00 00 06 01 00 00\n```\n:::\n:::\n\nThe contents of `data_buffer` are the raw bytes stored ihe data buffer that Arrow created for the `duration` array. They're not easy to read in this format, so I'll use the `readBin()` function from base R to parse the binary data as integers, and just like that...\n\n::: {.cell}\n\n```{.r .cell-code}\nreadBin(data_buffer, what = \"integer\", n = 14)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 280 234 213 236 280 227  73 258 111 214 231  48 165 262\n```\n:::\n:::\n\n...we have reconstructed the values from the data buffer. \n\n\n<br>\n\n### A nicer way\n\nThe print method for `ArrayData` objects isn't very helpful, so I'll write an `array_layout()` function that shows you the metadata and buffer contents associated with an Arrow array (source code [here](display_array_layout.R)) that works for the array types I'm using in this post. When applied to the `duration` array it produces this output:\n\n::: {.cell}\n\n```{.r .cell-code}\narray_layout(duration)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n── Metadata \n• length : 14\n• null count : 0\n\n── Buffers \n• validity : null\n• data : 280 234 213 236 280 227 73 258 111 214 231 48 165 262\n```\n:::\n:::\n\nThe output here is divided into two sections, structured to mirror how the Arrow columnar specification is described on the website (and also to mirrot the diagrams in the post). There is one section showing the metadata variables stored: array length, and a count of the number of null values. Underneath that we have a section listing all the buffers associated with an array. For an integer array like `duration` there are two buffers, the validity bitmap buffer and the data values buffer.\n\nThe `array_layout()` function also works for string arrays and produces similar output. However, character data in Arrow are stored using three buffers rather than two. As before the first buffer stores the validity bitmap. The second buffer is a vector of offsets specifying the locations for each of the substrings. The third buffer contains the character data itself. Here's an example of that:\n\n::: {.cell}\n\n```{.r .cell-code}\ndance_fever$title |>\n  Array$create() |>\n  array_layout()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n── Metadata \n• length : 14\n• null count : 0\n\n── Buffers \n• validity : null\n• offset : 0 4 8 19 31 48 63 77 86 100 108 115 124 132 145\n• data : KingFreeChoreomaniaBack in TownGirls Against GodDream Girl EvilPrayer\nFactoryCassandraHeaven Is HereDaffodilMy LoveRestraintThe BombMorning Elvis\n```\n:::\n:::\n\nYou wouldn't want to use my code in the wild though because it doesn't even try to be efficient or truncate the output in any way. I only wrote it so that I had a way of showing you the content of an Arrow array in R at a slightly lower level.\n\n::: {.cell .column-screen}\n::: {.cell-output-display}\n[![](img/hunger.png)](https://www.youtube.com/watch?v=5GHXEGz3PJg)\n:::\n:::\n\n\n<br><br>\n\n## Chunked arrays\n  \n>\n| I need my golden crown of sorrow, my bloody sword to swing\n| I need my empty halls to echo with grand self-mythology\n| 'Cause I am no mother, I am no bride\n| I am king\n|   -- *King*, Florence + The Machine\n\n\n::: {.cell}\n\n```{.r .cell-code}\nking <- chunked_array(\n  c(\"I\", \"am\", \"no\", \"mother\"),\n  c(\"I\", \"am\", NA, \"bride\"),\n  c(\"I\", \"am\", \"king\")\n)\nking\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChunkedArray\n[\n  [\n    \"I\",\n    \"am\",\n    \"no\",\n    \"mother\"\n  ],\n  [\n    \"I\",\n    \"am\",\n    null,\n    \"bride\"\n  ],\n  [\n    \"I\",\n    \"am\",\n    \"king\"\n  ]\n]\n```\n:::\n:::\n\nA chunked array has some structure to it:\n\n::: {.cell}\n::: {.cell-output-display}\n![](img/chunked_array_layout.svg)\n:::\n:::\n\n\nWe can check:\n\n::: {.cell}\n\n```{.r .cell-code}\nking$chunk(0) |> array_layout()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n── Metadata \n• length : 4\n• null count : 0\n\n── Buffers \n• validity : null\n• offset : 0 1 3 5 11\n• data : Iamnomother\n```\n:::\n\n```{.r .cell-code}\nking$chunk(1) |> array_layout()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n── Metadata \n• length : 4\n• null count : 1\n\n── Buffers \n• validity : 1 1 0 1\n• offset : 0 1 3 3 8\n• data : Iambride\n```\n:::\n:::\n\n\n::: {.cell .column-screen}\n::: {.cell-output-display}\n[![](img/king.png)](https://www.youtube.com/watch?v=L62LtChAwww)\n:::\n:::\n\n\n<br><br>\n\n## Record batches\n\nA record batch is table-like data structure that is semantically a sequence of fields, each a contiguous Arrow array.^[If you're familiar with Arrow data structures, it is in essence a struct with additional metadata] A struct is a nested type parameterized by an ordered sequence of types (which can all be distinct), called its fields. Each field must have a UTF8-encoded name, and these field names are part of the type metadata. A struct array does not have any additional allocated physical storage for its values. A struct array must still have an allocated validity bitmap, if it has one or more null values. Physically, a struct array has one child array for each field. The child arrays are independent and need not be adjacent to each other in memory. To illustrate, let's return to our `dance_fever` data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndance_fever\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 14 × 3\n   track_number title             duration\n          <int> <chr>                <int>\n 1            1 King                   280\n 2            2 Free                   234\n 3            3 Choreomania            213\n 4            4 Back in Town           236\n 5            5 Girls Against God      280\n 6            6 Dream Girl Evil        227\n 7            7 Prayer Factory          73\n 8            8 Cassandra              258\n 9            9 Heaven Is Here         111\n10           10 Daffodil               214\n11           11 My Love                231\n12           12 Restraint               48\n13           13 The Bomb               165\n14           14 Morning Elvis          262\n```\n:::\n:::\n\nNext, let's bundle these into a record batch:\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- record_batch(dance_fever)\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecordBatch\n14 rows x 3 columns\n$track_number <int32>\n$title <string>\n$duration <int32>\n```\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](img/record_batch_layout.svg)\n:::\n:::\n\n\n<br>\n\n### Serialising a record batch\n\n[discuss the IPC protocol for record batches here. Mention that feather is the same thing but as a file format rather than an input stream]\n\nOkay, so here's where we're at. The Arrow specification gives a precise statement of what an array should look like and how it should be laid out in memory. It also gives a precise statement about how arrays can be organised into record batches. Not only that, it provides a specification for how a record batch should be serialised via the IPC protocol. The IPC protocol is designed so that the structure of the serialised record batch is essentially identical to the physical layout of an in-memory record batch, which minimises the amount of computation required. \n\n::: {.cell}\n\n```{.r .cell-code}\ndf_batch_0 <- record_batch(dance_fever[1:4,])\ndf_batch_0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecordBatch\n4 rows x 3 columns\n$track_number <int32>\n$title <string>\n$duration <int32>\n```\n:::\n\n```{.r .cell-code}\ndf_batch_1 <- record_batch(dance_fever[5:14,])\ndf_batch_1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecordBatch\n10 rows x 3 columns\n$track_number <int32>\n$title <string>\n$duration <int32>\n```\n:::\n:::\n\nSuppose I want to share this. The IPC format is designed to lay this object out as a stream of bytes that (almost) exactly mirrors the structure of the object as it is represented in memory. \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](img/record_batch_serialised.svg)\n:::\n:::\n\n\nEach column in the record batch gets converted to a \"flattened field\". The data header for contains the length and null count for each flattened field. It also contains the memory offset (i.e., the start point) and length of every buffer that is stored in the message body.\n\n\nThere are three functions you can use if you want to do this. To send the data directly to an output stream (e.g., for communicating to some other process) use `write_ipc_stream()`. When writing data in this IPC format to a static file on disk (with a few minor differences), it is referred to as a \"feather\" formatted file, and you use `write_feather()`. Finally, if you want the same sequence of bytes but you'd like to keep them in R as raw vectors, you can use `write_to_raw()`. That's what I'll do here so I can show you the byte stream:\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_ipc_0 <- write_to_raw(df_batch_0)\ndf_ipc_0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] ff ff ff ff f0 00 00 00 10 00 00 00 00 00 0a 00 0c 00 06 00 05 00 08 00 0a\n [26] 00 00 00 00 01 04 00 0c 00 00 00 08 00 08 00 00 00 04 00 08 00 00 00 04 00\n [51] 00 00 03 00 00 00 7c 00 00 00 3c 00 00 00 04 00 00 00 a0 ff ff ff 00 00 01\n [76] 02 10 00 00 00 1c 00 00 00 04 00 00 00 00 00 00 00 08 00 00 00 64 75 72 61\n[101] 74 69 6f 6e 00 00 00 00 8c ff ff ff 00 00 00 01 20 00 00 00 d4 ff ff ff 00\n[126] 00 01 05 10 00 00 00 1c 00 00 00 04 00 00 00 00 00 00 00 05 00 00 00 74 69\n[151] 74 6c 65 00 00 00 04 00 04 00 04 00 00 00 10 00 14 00 08 00 06 00 07 00 0c\n[176] 00 00 00 10 00 10 00 00 00 00 00 01 02 10 00 00 00 28 00 00 00 04 00 00 00\n[201] 00 00 00 00 0c 00 00 00 74 72 61 63 6b 5f 6e 75 6d 62 65 72 00 00 00 00 08\n[226] 00 0c 00 08 00 07 00 08 00 00 00 00 00 00 01 20 00 00 00 00 00 00 00 ff ff\n[251] ff ff f8 00 00 00 14 00 00 00 00 00 00 00 0c 00 16 00 06 00 05 00 08 00 0c\n[276] 00 0c 00 00 00 00 03 04 00 18 00 00 00 58 00 00 00 00 00 00 00 00 00 0a 00\n[301] 18 00 0c 00 04 00 08 00 0a 00 00 00 8c 00 00 00 10 00 00 00 04 00 00 00 00\n[326] 00 00 00 00 00 00 00 07 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n[351] 00 00 00 00 00 00 00 00 00 00 10 00 00 00 00 00 00 00 10 00 00 00 00 00 00\n[376] 00 00 00 00 00 00 00 00 00 10 00 00 00 00 00 00 00 14 00 00 00 00 00 00 00\n[401] 28 00 00 00 00 00 00 00 1f 00 00 00 00 00 00 00 48 00 00 00 00 00 00 00 00\n[426] 00 00 00 00 00 00 00 48 00 00 00 00 00 00 00 10 00 00 00 00 00 00 00 00 00\n[451] 00 00 03 00 00 00 04 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 04 00 00\n[476] 00 00 00 00 00 00 00 00 00 00 00 00 00 04 00 00 00 00 00 00 00 00 00 00 00\n[501] 00 00 00 00 01 00 00 00 02 00 00 00 03 00 00 00 04 00 00 00 00 00 00 00 04\n[526] 00 00 00 08 00 00 00 13 00 00 00 1f 00 00 00 00 00 00 00 4b 69 6e 67 46 72\n[551] 65 65 43 68 6f 72 65 6f 6d 61 6e 69 61 42 61 63 6b 20 69 6e 20 54 6f 77 6e\n[576] 00 18 01 00 00 ea 00 00 00 d5 00 00 00 ec 00 00 00 ff ff ff ff 00 00 00 00\n```\n:::\n:::\n\nNote that by default the `write_to_raw()` function will produce output in \"stream\" format: what you're looking at above is the same sequence of bytes that would get transmitted if I'd called `write_ipc_stream()`. If you want `write_to_raw()` to mimic the behaviour of `write_feather()`, you can set the `format` argument:\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_to_raw(df_batch_0, format = \"file\")\n```\n:::\n\nThis would produce a somewhat longer output, due to the fact that the feather format includes additional information. For more detail, see the Arrow specification page.\n\n::: {.cell}\n\n```{.r .cell-code}\nread_ipc_stream(df_ipc_0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 × 3\n  track_number title        duration\n         <int> <chr>           <int>\n1            1 King              280\n2            2 Free              234\n3            3 Choreomania       213\n4            4 Back in Town      236\n```\n:::\n:::\n\nThe same works for feather:\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_feather(df_batch_1, \"df_ipc_1.feather\")\nread_feather(\"df_ipc_1.feather\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 3\n   track_number title             duration\n          <int> <chr>                <int>\n 1            5 Girls Against God      280\n 2            6 Dream Girl Evil        227\n 3            7 Prayer Factory          73\n 4            8 Cassandra              258\n 5            9 Heaven Is Here         111\n 6           10 Daffodil               214\n 7           11 My Love                231\n 8           12 Restraint               48\n 9           13 The Bomb               165\n10           14 Morning Elvis          262\n```\n:::\n:::\n\n::: {.cell .column-screen}\n::: {.cell-output-display}\n[![](img/delilah.png)](https://www.youtube.com/watch?v=zZr5Tid3Qw4)\n:::\n:::\n\n<br><br>\n\n## Tables\n\n>\n| Tell me where to put my love\n| Do I wait for time to do what it does?\n| I don't know where to put my love\n|   -- *My Love*, Florence + The Machine\n\n\nDraw an analogous figure for tables, showing how a table is a collection of chunked arrays, and each chunked array is a collection of arrays. Talk a little about why that helps with sequentially arriving data: a new record batch arrives, it's already organised in the format needed for memory, so you can wrap those parts of memory in an Arrow buffer without moving it again. Then the various chunked arrays for the Table can be linked to the newly arrived Arrays without in disturbing any of the existing Arrays. Etc.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](img/table_layout.svg)\n:::\n:::\n\n\nCan't concatenate record batches directly. Arrays are immutable, and we can't touch them. They need to be wrapped into chunked arrays. In other words we need to convert each record batch into a table. I could use the `as_arrow_table()` function to do this conversion:\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_table_0 <- as_arrow_table(df_batch_0)\ndf_table_1 <- as_arrow_table(df_batch_1)\n```\n:::\n\nThis works, but it's also not something you will need to do very often in real life. The reader functions in **arrow** tend to convert record batches to tables by default. For instance, in the previous section I serialised two record batches, one to a file and one to a raw vector. Let's look at what happens when I try to deserialise (a.k.a. \"read\") them: \n\n::: {.cell}\n\n```{.r .cell-code}\ndf_table_0 <- read_ipc_stream(\n  file = df_ipc_0, \n  as_data_frame = FALSE\n)\ndf_table_0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTable\n4 rows x 3 columns\n$track_number <int32>\n$title <string>\n$duration <int32>\n```\n:::\n:::\n\nThat's the same data as before, but it's a table not a record batch. Each column is a chunked array, not an array. The same happens when I read from the file:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_table_1 <- read_feather(\n  file = \"df_ipc_1.feather\", \n  as_data_frame = FALSE\n)\ndf_table_1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTable\n10 rows x 3 columns\n$track_number <int32>\n$title <string>\n$duration <int32>\n```\n:::\n:::\n\nIn general, you won't get a record batch in **arrow** unless you explicitly ask for one. Tables are the default tabular data structure, which is usually what you want anyway\n\nOkay, so now I have the two fragments of my data set represented as tables. The difference between the table version and the record batch version is that the columns are all represented as chunked arrays. Each array from the original record batch is now one chunk in the corresponding chunked array in the table:\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_batch_0$title\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArray\n<string>\n[\n  \"King\",\n  \"Free\",\n  \"Choreomania\",\n  \"Back in Town\"\n]\n```\n:::\n\n```{.r .cell-code}\ndf_table_0$title\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChunkedArray\n[\n  [\n    \"King\",\n    \"Free\",\n    \"Choreomania\",\n    \"Back in Town\"\n  ]\n]\n```\n:::\n:::\n\nIt's the same underlying object, just with a new, flexible wrapper. The wrapper is what allows us to concatenate tables:\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- concat_tables(\n  df_table_0,\n  df_table_1\n)\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTable\n14 rows x 3 columns\n$track_number <int32>\n$title <string>\n$duration <int32>\n```\n:::\n:::\n\nLooking at the columns we see that the chunking are preserved. That's not arbitrary. It's because those are the original arrays, still located at the same spot in memory and not contiguous:\n\n::: {.cell}\n\n```{.r .cell-code}\ndf$title\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChunkedArray\n[\n  [\n    \"King\",\n    \"Free\",\n    \"Choreomania\",\n    \"Back in Town\"\n  ],\n  [\n    \"Girls Against God\",\n    \"Dream Girl Evil\",\n    \"Prayer Factory\",\n    \"Cassandra\",\n    \"Heaven Is Here\",\n    \"Daffodil\",\n    \"My Love\",\n    \"Restraint\",\n    \"The Bomb\",\n    \"Morning Elvis\"\n  ]\n]\n```\n:::\n:::\n\nAn important thing to note about chunks is that they aren't considered to be a part of the data structure for the purpose of testing equality. Here's the same data with a different chunking:\n\n::: {.cell}\n\n```{.r .cell-code}\ntitle_rechunked <- chunked_array(\n  df$title[1:8],\n  df$title[9:12],\n  df$title[13:14]\n)\ntitle_rechunked\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChunkedArray\n[\n  [\n    \"King\",\n    \"Free\",\n    \"Choreomania\",\n    \"Back in Town\"\n  ],\n  [\n    \"Girls Against God\",\n    \"Dream Girl Evil\",\n    \"Prayer Factory\",\n    \"Cassandra\"\n  ],\n  [\n    \"Heaven Is Here\",\n    \"Daffodil\",\n    \"My Love\",\n    \"Restraint\"\n  ],\n  [\n    \"The Bomb\",\n    \"Morning Elvis\"\n  ]\n]\n```\n:::\n:::\n\nAlthough these two chunked arrays partition the data in different ways, the values stored in each slot are the same. When I test for equality, the scalar values in each slot are compared to one another without regard for which chunk they are assigned to. Since all those values are the same, every comparison returns `true`:\n\n::: {.cell}\n\n```{.r .cell-code}\ntitle_rechunked == df$title\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChunkedArray\n[\n  [\n    true,\n    true,\n    true,\n    true,\n    true,\n    true,\n    true,\n    true,\n    true,\n    true,\n    true,\n    true,\n    true,\n    true\n  ]\n]\n```\n:::\n:::\n\n\n::: {.cell .column-screen}\n::: {.cell-output-display}\n[![](img/my_love.png)](https://www.youtube.com/watch?v=h9CNGPy11Jc)\n:::\n:::\n\n\n<br><br>\n\n## Datasets\n\n>\n| What kind of man loves like this?\n| To let me dangle at a cruel angle\n| Oh, my feet don't touch the floor\n| Sometimes you're half in and then you're half out\n| Buy you never close the door\n|   -- *What Kind Of Man*, Florence + The Machine\n\n\nSo what about datasets? They're the last item on that table, and you might be wondering where they fall in all this. I'm not going to dive into the details on datasets in this post, because they're a whole separate topic and they deserve their own blog post. However, it's a little unsatisfying to write all this and not say anything about them, so I'll give a very quick overview here. \n\nUp to this point I've talked about tabular data sets that are contained entirely in memory. When such data are written to disk, they are typically written to a single file. For larger-than-memory data sets, a different strategy is needed. Only a subset of the data can be stored in memory at any point in time, and as a consequence it becomes convenient to write the data to disk by partitioning it into many smaller files. This functionality is supported in Arrow via Datasets. \n\nI'll give a simple example here, using a small data set. Let's suppose I've downloaded the entire Florence + The Machine discography using the **spotifyr** package:\n\n::: {.cell}\n\n```{.r .cell-code}\nflorence <- get_discography(\"florence + the machine\")\nflorence\n```\n:::\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 414 × 41\n# Groups:   album_name [18]\n   artist_name       artist_id album_id album_type album_images album_release_d…\n   <chr>             <chr>     <chr>    <chr>      <list>       <chr>           \n 1 Florence + The M… 1moxjboG… 0uGwPmq… album      <df [3 × 3]> 2022-05-18      \n 2 Florence + The M… 1moxjboG… 0uGwPmq… album      <df [3 × 3]> 2022-05-18      \n 3 Florence + The M… 1moxjboG… 0uGwPmq… album      <df [3 × 3]> 2022-05-18      \n 4 Florence + The M… 1moxjboG… 0uGwPmq… album      <df [3 × 3]> 2022-05-18      \n 5 Florence + The M… 1moxjboG… 0uGwPmq… album      <df [3 × 3]> 2022-05-18      \n 6 Florence + The M… 1moxjboG… 0uGwPmq… album      <df [3 × 3]> 2022-05-18      \n 7 Florence + The M… 1moxjboG… 0uGwPmq… album      <df [3 × 3]> 2022-05-18      \n 8 Florence + The M… 1moxjboG… 0uGwPmq… album      <df [3 × 3]> 2022-05-18      \n 9 Florence + The M… 1moxjboG… 0uGwPmq… album      <df [3 × 3]> 2022-05-18      \n10 Florence + The M… 1moxjboG… 0uGwPmq… album      <df [3 × 3]> 2022-05-18      \n# … with 404 more rows, and 35 more variables: album_release_year <dbl>,\n#   album_release_date_precision <chr>, danceability <dbl>, energy <dbl>,\n#   key <int>, loudness <dbl>, mode <int>, speechiness <dbl>,\n#   acousticness <dbl>, instrumentalness <dbl>, liveness <dbl>, valence <dbl>,\n#   tempo <dbl>, track_id <chr>, analysis_url <chr>, time_signature <int>,\n#   artists <list>, available_markets <list>, disc_number <int>,\n#   duration_ms <int>, explicit <lgl>, track_href <chr>, is_local <lgl>, …\n```\n:::\n:::\n\nThe `florence` data frame is of course quite small, and I have no real need to use Arrow Datasets: it's small enough that I can store it natively in R as a tibble! But it will suffice to illustrate concepts that come in handy when working with large datasets.\n\nLet's suppose I want to partition this in into many data files, using the album release year as the basis for the partitioning. To do this I'll use the `write_dataset()` function, specifying `partitioning = \"album_release_year\"` to ensure that files are created after splitting the data set by release year. By default, the `write_dataset()` function writes individual data files in the parquet format, which is in general a very good default choice for large tabular data sets. However, because I have not talked about [Apache Parquet](https://parquet.apache.org/) in this post, I'll make a different choice and write the data files in the feather format that we've seen earlier in this post. I can do that by setting `format = \"feather\"`. Finally, I'll set `path = \"spotify_florence\"` to ensure that all the files are stored in a folder by that name. That gives this command:\n\n::: {.cell}\n\n```{.r .cell-code}\nflorence |> \n  select(where(~!is.list(.))) |>  # drop list columns\n  as_arrow_table() |>             # convert to an arrow table\n  write_dataset(                  # write to multi-file storage\n    path = \"spotify_florence\",\n    format = \"feather\",\n    partitioning = \"album_release_year\"\n  )\n```\n:::\n\nThe result is that the following files are written to disk:\n\n::: {.cell}\n\n```{.r .cell-code}\nlist.files(\"spotify_florence\", recursive = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"album_release_year=2009/part-0.feather\"\n[2] \"album_release_year=2010/part-0.feather\"\n[3] \"album_release_year=2011/part-0.feather\"\n[4] \"album_release_year=2012/part-0.feather\"\n[5] \"album_release_year=2015/part-0.feather\"\n[6] \"album_release_year=2018/part-0.feather\"\n[7] \"album_release_year=2022/part-0.feather\"\n```\n:::\n:::\n\nThese file names are written in \"Hive partitioning\" format. It looks a little weird the first time you encounter it, because `=` is a character most coders instinctively avoid including in file names because it has such a strong meaning in programming contexts. However, when files are named in Hive partitioning format, the intended interpretation is exactly the one you implicitly expect as a coder: it's a `field_name=value` statement, so you will often encounter files with names like \n\n```\n/year=2019/month=2/data.parquet\n```\n\nFor more information see the help documentation for the `hive_partitioning()` function in the **arrow** package.  \n\nIn any case, the key thing is that I've now written the data to disk in a fashion that splits it across multiple files. For the Florence + The Machine discography data this is is really not needed because the entire `spotify_florence` folder occupies a mere 320kB on my hard drive. However, elsewhere on my laptop I have a copy of the infamous New York City Taxi data set, and that one occupies a rather more awkward 69GB of storage. For that one, it really does matter that I have it written to disk in a sensible format! \n\nHaving a data set stored in a distributed multi-file format is nice, but it's only useful if I can open it and work with it as if it were the same as a regular tabular data set. The `open_dataset()` function allows me to do exactly this. Here's what happens when I open the file:\n\n::: {.cell}\n\n```{.r .cell-code}\nflorence_dataset <- open_dataset(\"spotify_florence\", format = \"feather\")\nflorence_dataset\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFileSystemDataset with 7 Feather files\nartist_name: string\nartist_id: string\nalbum_id: string\nalbum_type: string\nalbum_release_date: string\nalbum_release_date_precision: string\ndanceability: double\nenergy: double\nkey: int32\nloudness: double\nmode: int32\nspeechiness: double\nacousticness: double\ninstrumentalness: double\nliveness: double\nvalence: double\ntempo: double\ntrack_id: string\nanalysis_url: string\ntime_signature: int32\ndisc_number: int32\nduration_ms: int32\nexplicit: bool\ntrack_href: string\nis_local: bool\ntrack_name: string\ntrack_preview_url: bool\ntrack_number: int32\ntype: string\ntrack_uri: string\nexternal_urls.spotify: string\nalbum_name: string\nkey_name: string\nmode_name: string\nkey_mode: string\ntrack_n: double\nalbum_release_year: int32\n\nSee $metadata for additional Schema metadata\n```\n:::\n:::\n\nOkay yes, the output makes clear that I have loaded *something* and it has registered the existence of the 7 constituent files that comprise the dataset as a whole. But can I work with it? One of the big selling points to the **arrow** package is that it supplies a **dplyr** backend that lets me work with `Tables` as if they were R data frames, using familiar syntax. Can I do the same thing with `Datasets`? \n\n::: {.cell}\n\n```{.r .cell-code}\ndanceability <- florence_dataset |> \n  select(album_name, track_name, danceability) |>\n  distinct() |>\n  arrange(desc(danceability)) |> \n  head(n = 10) |> \n  compute()\n\ndanceability\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTable\n10 rows x 3 columns\n$album_name <string>\n$track_name <string>\n$danceability <double>\n\nSee $metadata for additional Schema metadata\n```\n:::\n:::\n\nYes. Yes I can. Because I called `compute()` at the end of this pipeline rather than `collect()`, the results have been returned to me as a `Table` rather than a data frame. I did that so that I can show that the `danceability` output is no different to the `Table` objects we've seen earlier, constructed from `ChunkedArray` objects:\n\n::: {.cell}\n\n```{.r .cell-code}\ndanceability$track_name\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChunkedArray\n[\n  [\n    \"Heaven Is Here\",\n    \"King\",\n    \"King\",\n    \"Hunger\",\n    \"My Love - Acoustic\",\n    \"Ghosts - Demo\",\n    \"What The Water Gave Me - Demo\",\n    \"What The Water Gave Me - Demo\",\n    \"South London Forever\",\n    \"What The Water Gave Me - Demo\"\n  ]\n]\n```\n:::\n:::\n\nIf I want to I can convert this to a tibble, and discover that \"Dance Fever\" does indeed contain the most danceable F+TM tracks, at least according to Spotify:\n\n::: {.cell}\n\n```{.r .cell-code}\nas.data.frame(danceability)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 3\n   album_name                   track_name                    danceability\n   <chr>                        <chr>                                <dbl>\n 1 Dance Fever                  Heaven Is Here                       0.852\n 2 Dance Fever                  King                                 0.731\n 3 Dance Fever (Deluxe)         King                                 0.73 \n 4 High As Hope                 Hunger                               0.729\n 5 Dance Fever (Deluxe)         My Love - Acoustic                   0.719\n 6 Lungs (Deluxe Version)       Ghosts - Demo                        0.681\n 7 Ceremonials                  What The Water Gave Me - Demo        0.68 \n 8 Ceremonials (Deluxe Edition) What The Water Gave Me - Demo        0.68 \n 9 High As Hope                 South London Forever                 0.679\n10 Ceremonials (Deluxe Edition) What The Water Gave Me - Demo        0.678\n```\n:::\n:::\n\n::: {.cell .column-screen}\n::: {.cell-output-display}\n[![](img/big_god.png)](https://www.youtube.com/watch?v=_kIrRooQwuk)\n:::\n:::\n\n\n<!--------------- appendices go here ----------------->\n\n::: {.cell}\n\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}