{
  "hash": "29df0c2e18fff17da4daecc05ae91384",
  "result": {
    "markdown": "---\ntitle: \"Unpacking the Arrow on-disk Dataset API\"\ndescription: \"Kind of a post for me really.\"\ndate: \"2022-11-20\"\ncategories: [Apache Arrow]\nimage: \"img/mammoth.png\"\n---\n\n\n<!--------------- my typical setup ----------------->\n\n\n\n\n\n\n<!--------------- post begins here ----------------->\n\nHello again lovely people. I am, once again, blogging about Apache Arrow and I'm not even sorry. Oh well. \n\nIn an earlier post I wrote about [Tables and other in-memory data structures](/posts/2022-05-25_arrays-and-tables-in-arrow/index.html) that Arrow uses to represent data objects. That meant the bulk of the post was focused on Record Batch and Table objects and the constituent objects used to define columns in one of these things (Arrays and Chunked Arrays).\n\nWhat I didn't *really* talk about in that post was Datasets, which are used to represent data (typically larger-than-memory data) that are stored on-disk rather than in-memory. Okay, fine, yeah. Technically I did include a [section on Datasets](/posts/2022-05-25_arrays-and-tables-in-arrow/index.html#datasets) at the end of the post, but I was a bit evasive. I gave an example showing how to use Datasets, but I really didn't talk much about what they are. \n\nI had a very good reason for this, dear reader, and that reason is this: when I wrote that post I had no f**king idea whatsoever how Datasets worked. I knew how to use them, but if you'd asked me questions about how the magic works I couldn't have told you.^[Unbelievably, there are people out there who will start talking about predicate pushdown and not even give a girl a heads up? Rude. You don't see me starting conversations at the pub about metric axiom violations in human similarity judgment do you? Well, okay, you might. But that's not the point!]\n\nSince that time I've learned a few things, and because I'm an annoying person I'm going to tell you about them.^[Okay, *now* I'm a bit sorry.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow, warn.conflicts = FALSE)\nlibrary(dplyr, warn.conflicts = FALSE)\n```\n:::\n\n\n<br>\n\n:::{.column-screen}\n![](./img/subdivision_08_1731.png)\n:::\n\n## Quick recap: Record Batches and Tables\n\nAt this point I've written [quite a few posts about Arrow](https://blog.djnavarro.net/category/apache-arrow), and it's not necessarily a good idea for me to assume that you've had the misfortune to read all^[Or indeed, \"any\".] of them. So here's a quick recap of some of the key Arrow data structures that I've talked about in other posts...\n\nLet's start with Record Batches. A Record Batch is tabular data structure comprised of named Arrays,^[For the purposes of this post we are going to pretend that Arrays behave like R vectors, which... they sort of do as long as you don't try to push at the analogy too hard.] and an accompanying Schema^[The Schema is the way Arrow formalises the metadata for rectangular data structures. I'm not going to dive into the details here: it's enough for our purposes to recognise that it's basically a list of variable names and their data types.] that specifies the name and data type associated with each Array. We can create one manually using `record_batch()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrb <- record_batch(\n  strs = c(\"hello\", \"amazing\", \"and\", \"cruel\", \"world\"), \n  ints = c(1L, NA, 2L, 4L, 8L),\n  dbls = c(1.1, 3.2, 0.2, NA, 11)\n)\nglimpse(rb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecordBatch\n5 rows x 3 columns\n$ strs <string> \"hello\", \"amazing\", \"and\", \"cruel\", \"world\"\n$ ints  <int32> 1, NA, 2, 4, 8\n$ dbls <double> 1.1, 3.2, 0.2, NA, 11.0\n```\n:::\n:::\n\n\nThis is a Record Batch containing 5 rows and 3 columns. The command `rb[1:3, 1:2]` extracts the first three rows and the first two columns:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(rb[1:3, 1:2])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecordBatch\n3 rows x 2 columns\n$ strs <string> \"hello\", \"amazing\", \"and\"\n$ ints  <int32> 1, NA, 2\n```\n:::\n:::\n\n\nThe structure of a Record Batch is shown below. In addition to the three Arrays specifying the columns, it includes an explicit Schema object containing relevant metadata:\n\n![](./img/record_batch.png)\n\nRecord Batches are a fundamental unit for data interchange in Arrow, but are not typically used for data analysis. The reason for this is that the constituent Arrays that store columns in a Record Batch are immutable: they cannot be modified or extended without creating a new object.^[I mean, this is where you start asking all sort of questions about what objects are mutable in R anyway, since we're almost never doing modify-in-place operations. But whatever. This is not the post for that, and if you try to make me talk about that here I will cry.] When data arrive sequentially Record Batches can be inconvenient, because you can't concatenate them. For that reason Tables are usually more practical...\n\nSo let's turn to Tables next. From the user perspective a Table is very similar to a Record Batch but the constituent parts are Chunked Arrays. Chunked Arrays are flexible wrappers enclosing one or more Arrays.^[Again, let's just pretend that a Chunked Array behaves just like an R vector, except for the fact that it has these weird stitches from where we've sewn the individual Arrays together. It's all a bit vivisectionist in nature, sure, but this is the mechanism that allows Chunked Arrays to behave more like R vectors than simple Arrays do. Dr Frankenstein may not have been entirely wrong on all counts, I guess.] This makes it possible to concatenate tables. To quickly illustrate this, let's first convert the `rb` Record Batch to a Table using `arrow_table()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf1 <- arrow_table(rb)\n```\n:::\n\n\nNow we create a second Table with the same column names and types, again using `arrow_table()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf2 <- arrow_table(\n  strs = c(\"I\", \"love\", \"you\"), \n  ints = c(5L, 0L, 0L),\n  dbls = c(7.1, -0.1, 2)\n)\n```\n:::\n\n\nWe can concatenate these using `concat_tables()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- concat_tables(df1, df2)\nglimpse(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTable\n8 rows x 3 columns\n$ strs <string> \"hello\", \"amazing\", \"and\", \"cruel\", \"world\", \"I\", \"love\", \"you\"\n$ ints  <int32> 1, NA, 2, 4, 8, 5, 0, 0\n$ dbls <double> 1.1, 3.2, 0.2, NA, 11.0, 7.1, -0.1, 2.0\n```\n:::\n:::\n\n\nThe structure of this Table ject is similar to the structure of the Record Batch object I showed earlier, but the columns are Chunked Arrays rather than simple Arrays:\n\n\n![](./img/table.png)\nYou can see this if we print out a single column:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf$strs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChunkedArray\n<string>\n[\n  [\n    \"hello\",\n    \"amazing\",\n    \"and\",\n    \"cruel\",\n    \"world\"\n  ],\n  [\n    \"I\",\n    \"love\",\n    \"you\"\n  ]\n]\n```\n:::\n:::\n\n\nThere's a visual separation there between the different chunks, used to indicated where the boundaries between individual Arrays are. In practice though you actually don't have to care about this because it's not semantically meaningful. It's there for purely technical reasons.\n\nBut all this is background. So let's move on, shall we?\n\n<br>\n\n\n:::{.column-screen}\n![](./img/subdivision_08_1708.png)\n:::\n\n\n## So... Datasets?\n\nOkay, what about Datasets? Like Record Batch and Table objects, a Dataset is used to represent tabular data. At an abstract level, a Dataset can be viewed as an object comprised of rows and columns, and just like Record Batches and Tables, it contains an explicit Schema that specifies the name and data type associated with each column.\n\nHowever, where Tables and Record Batches are data explicitly represented in-memory, a Dataset is not. Instead, a Dataset is an abstraction that refers to data stored on-disk in one or more files. Reading the data takes place only as needed, and only when a query is executed against the data. In this respect Arrow Datasets are a very different kind of object to Arrow Tables, but the arrow package is written in a way that the dplyr commands used to analyze Tables can also be applied to Datasets. \n\n\n<br>\n\n:::{.column-screen}\n![](./img/subdivision_08_1715.png)\n:::\n\n\n## What does a Dataset look like on-disk?\n\nReduced to its simplest form, the on-disk structure of a Dataset is simply a collection of data files, each storing one subset of the data. These subsets are sometimes referred to as \"fragments\", and the partitioning process is sometimes referred to as \"sharding\". To illustrate how this works, I'll write a multi-file dataset to disk manually, without using any of the Arrow Dataset functionality to do the work. I'll keep it deliberately simple and use three small data frames, each containing one subset of the data we want to store: \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_a <- data.frame(id = 1:5, value = rnorm(5), subset = \"a\")\ndf_b <- data.frame(id = 6:10, value = rnorm(5), subset = \"b\")\ndf_c <- data.frame(id = 11:15, value = rnorm(5), subset = \"c\")\n```\n:::\n\n\nOur intention is that each of the data frames should be stored in a separate data file. As you can see, this is a quite structured partitioning: all data where `subset = \"a\"` belong to one file, all data where `subset = \"b\"` belong to another file, and all data where `subset = \"c\"` belong to the third file. \n\nThe first step is to define and create a folder that will hold all the files:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds_dir <- \"mini-dataset\"\ndir.create(ds_dir)\n```\n:::\n\n\nThe next step is to manually create a \"Hive-style\"^[The name comes from Apache Hive: [hive.apache.org](https://hive.apache.org).] folder structure:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds_dir_a <- file.path(ds_dir, \"subset=a\")\nds_dir_b <- file.path(ds_dir, \"subset=b\")\nds_dir_c <- file.path(ds_dir, \"subset=c\")\n\ndir.create(ds_dir_a)\ndir.create(ds_dir_b)\ndir.create(ds_dir_c)\n```\n:::\n\n\nNotice that we have named each folder in a \"key=value\" format that exactly describes the subset of data that will be written into that folder. This naming structure is the essence of Hive-style partitions. \n\nNow that we have the folders, we'll use `write_parquet()` to create a single [parquet file](https://parquet.apache.org)^[One of these days I am going to write a proper blog post on parquet files for R users, I promise. I just don't seem to have found the time yet. Not sure where all the time goes...] for each of the three subsets:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_parquet(df_a, file.path(ds_dir_a, \"part-0.parquet\"))\nwrite_parquet(df_b, file.path(ds_dir_b, \"part-0.parquet\"))\nwrite_parquet(df_c, file.path(ds_dir_c, \"part-0.parquet\"))\n```\n:::\n\n\nIf I'd wanted to, I could have further subdivided the dataset. A folder can contain multiple files (`part-0.parquet`, `part-1.parquet`, etc) if we would like it to, though there's no point whatsoever in doing that with such a tiny dataset. Similarly, there is no requirement to name the files `part-0.parquet` this way at all: it would have been fine to call these files `subset-a.parquet`, `subset-b.parquet`, and `subset-c.parquet` if I'd wanted to do that. I only chose `part-0.parquet` because that's the default filename that the `write_dataset()` function in the arrow package generates!\n\nAlong the same lines, it isn't *necessary* to use Hive-style partitions to use Arrow Datasets. The default behaviour of `write_dataset()` is to construct Hive-style partitions, and the default in `open_dataset()` is to look for Hive-style partitions, but it isn't required. \n\nIn any case, I've created an on-disk parquet Dataset using Hive-style partitioning. My Dataset is defined by these files:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist.files(ds_dir, recursive = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"subset=a/part-0.parquet\" \"subset=b/part-0.parquet\"\n[3] \"subset=c/part-0.parquet\"\n```\n:::\n:::\n\n\nThis is exciting, right? I mean, I'm excited. How could anyone not be completely enthralled by this thrilling exposition?\n\nAaaaanyway.... to verify that everything has worked, I'll now try to open the data with `open_dataset()` and call `glimpse()` to inspect its contents:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds <- open_dataset(ds_dir)\nglimpse(ds)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFileSystemDataset with 3 Parquet files\n15 rows x 3 columns\n$ id      <int32> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n$ value  <double> -0.71197051, 0.63132302, -0.86527378, -0.60997280, 0.04286651,…\n$ subset <string> \"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c…\nCall `print()` for full schema details\n```\n:::\n:::\n\n\nAs you can see, the `ds` Dataset object aggregates the three separate data files. In fact, in this particular case the Dataset is so small that values from all three files appear in the output of `glimpse()`.\n\nNow, it's pretty obvious that I wouldn't use this workflow in my everyday life. Manually writing individual files like this is tiresome, especially when the exact same dataset can be created with the following command:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds |> \n  group_by(subset) |>\n  write_dataset(\"mini-dataset\")\n```\n:::\n\n\nAs an aside, even if `ds` happens to refer to an on-disk Dataset that is larger than memory, and you're just wanting to rewrite it with a different file structure, this pipeline should still work without any risk of an out-of-memory error. This is thanks to the Dataset backpressure functionality^[As usual there is esoteric knowledge buried in the C++ documentation, in this case describing [backpresure control](https://arrow.apache.org/docs/cpp/api/compute.html#_CPPv4N5arrow7compute19BackpressureControlE). It's probably ancient forbidden lore and Dumbledore is going to turn me into a hobbit or something but whatever.] in which the reader will back off and slow down if the writer has fallen too far behind and the memory cache is filling up. Or something like that. Look, I almost managed to make myself care about the details, okay? \n\n<br>\n\n:::{.column-screen}\n![](./img/subdivision_08_1730.png)\n:::\n\n\n## What is stored in-memory by the Dataset object?\n\nIn the previous section we examined the on-disk structure of a Dataset. We now turn to the in-memory structure of the Dataset object itself (i.e., `ds` in the previous example). When the Dataset object is created, arrow searches the dataset folder looking for appropriate files, but does not load the contents of those files. Paths to these files are stored in an active binding `ds$files`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds$files \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"/home/danielle/GitHub/sites/quarto-blog/posts/2022-11-20_unpacking-arrow-datasets/mini-dataset/subset=a/part-0.parquet\"\n[2] \"/home/danielle/GitHub/sites/quarto-blog/posts/2022-11-20_unpacking-arrow-datasets/mini-dataset/subset=b/part-0.parquet\"\n[3] \"/home/danielle/GitHub/sites/quarto-blog/posts/2022-11-20_unpacking-arrow-datasets/mini-dataset/subset=c/part-0.parquet\"\n```\n:::\n:::\n\n\nThe other thing that happens when `open_dataset()` is called is that an explicit Schema for the Dataset is constructed and stored as `ds$schema`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds$schema\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSchema\nid: int32\nvalue: double\nsubset: string\n\nSee $metadata for additional Schema metadata\n```\n:::\n:::\n\n\nBy default this Schema is inferred by inspecting the first file only, though it is possible to construct a unified schema after inspecting all files. To do this, set `unify_schemas = TRUE` when calling `open_dataset()`. It is also possible to use the `schema` argument to `open_dataset()` to specify the Schema explicitly (see the `schema()` function for details). \n\nIt's a bit of an oversimplification because both `ds$files` and `ds$schema` are actually active bindings that invoke function calls to the underlying Arrow C++ library but honestly I don't see a need to care. In most situations I think it's reasonable to use this as the mental model of what the `ds` object contains:\n\n![](./img/dataset.png)\n\nThe act of reading the data is performed by a Scanner object. When analyzing a Dataset using the dplyr interface you never need to construct a Scanner manually, but for explanatory purposes we'll do it here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscan <- Scanner$create(dataset = ds)\n```\n:::\n\n\nCalling the `ToTable()` method will materialize the Dataset (on-disk) as a Table (in-memory):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscan$ToTable()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTable\n15 rows x 3 columns\n$id <int32>\n$value <double>\n$subset <string>\n\nSee $metadata for additional Schema metadata\n```\n:::\n:::\n\n\nThis scanning process is multi-threaded by default, but if necessary threading can be disabled by setting `use_threads = FALSE` when calling `Scanner$create()`.\n\n\n<br>\n\n:::{.column-screen}\n![](./img/subdivision_08_1707.png)\n:::\n\n\n## How does a Dataset query work?\n\nWhen a query is executed against a Dataset a new scan is initiated and the results pulled back into R. As an example, consider the following dplyr expression:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds |>\n  filter(value > 0) |>\n  mutate(new_value = round(100 * value)) |>\n  select(id, subset, new_value) |>\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  id subset new_value\n1  6      b        40\n2  7      b        99\n3 10      b         1\n4 14      c        78\n5  2      a        63\n6  5      a         4\n```\n:::\n:::\n\n\nWe can replicate this using the low-level Dataset interface by creating a new scan by specifying the `filter` and `projection` arguments to `Scanner$create()`. To use these arguments you need to know a little about Arrow Expressions, for which you may find it helpful to read the help documentation in `help(\"Expression\", package = \"arrow\")`. \n\nThe scanner defined below mimics the dplyr pipeline shown above,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscan <- Scanner$create(\n  dataset = ds, \n  filter = Expression$field_ref(\"value\") > 0,\n  projection = list(\n    id = Expression$field_ref(\"id\"),\n    subset = Expression$field_ref(\"subset\"),\n    new_value = Expression$create(\"round\", 100 * Expression$field_ref(\"value\"))\n  )\n)\n```\n:::\n\n\nand if we were to call `as.data.frame(scan$ToTable())` it would produce the same result as the dplyr version, though the rows may not appear in the same order. \n\nTo get a better sense of what happens when the query executes, what we'll do here is call `scan$ScanBatches()`. Much like the `ToTable()` method, the `ScanBatches()` method executes the query separately against each of the files, but it returns a list of Record Batches, one for each file. In addition, we'll convert these Record Batches to data frames individually:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlapply(scan$ScanBatches(), as.data.frame)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n  id subset new_value\n1  2      a        63\n2  5      a         4\n\n[[2]]\n  id subset new_value\n1  6      b        40\n2  7      b        99\n3 10      b         1\n\n[[3]]\n  id subset new_value\n1 14      c        78\n```\n:::\n:::\n\n\nIf we return to the dplyr query we made earlier, and use `compute()` to return a Table rather use `collect()` to return a data frame, we can see the evidence of this process at work. The Table object is created by concatenating the three Record Batches produced when the query executes against three data files, and as a consequence of this the Chunked Array that defines a column of the Table mirrors the partitioning structure present in the data files:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl <- ds |>\n  filter(value > 0) |>\n  mutate(new_value = round(100 * value)) |>\n  select(id, subset, new_value) |>\n  compute()\n\ntbl$subset\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChunkedArray\n<string>\n[\n  [\n    \"c\"\n  ],\n  [\n    \"a\",\n    \"a\"\n  ],\n  [\n    \"b\",\n    \"b\",\n    \"b\"\n  ]\n]\n```\n:::\n:::\n\n\n\n<!--------------- appendices go here ----------------->\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}