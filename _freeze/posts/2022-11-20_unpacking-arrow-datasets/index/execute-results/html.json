{
  "hash": "c30ab76a2782303c074ce744afecf5ad",
  "result": {
    "markdown": "---\ntitle: \"Unpacking the Arrow on-disk Dataset API\"\ndescription: \"Kind of a post for me really.\"\ndate: \"2022-11-20\"\ncategories: [Apache Arrow]\nimage: \"img/mammoth.png\"\n---\n\n\n<!--------------- my typical setup ----------------->\n\n\n\n\n\n\n<!--------------- post begins here ----------------->\n\nHello again lovely people. I am, once again, blogging about Apache Arrow and I'm not even sorry. Oh well. \n\nIn an earlier post I wrote about [Tables and other in-memory data structures](/posts/2022-05-25_arrays-and-tables-in-arrow/index.html) that Arrow uses to represent data objects. That meant the bulk of the post was focused on Record Batch and Table objects and the constituent objects used to define columns in one of these things (Arrays and Chunked Arrays).\n\nWhat I didn't *really* talk about in that post was Datasets, which are used to represent data (typically larger-than-memory data) that are stored on-disk rather than in-memory. Okay, fine, yeah. Technically I did include a [section on Datasets](/posts/2022-05-25_arrays-and-tables-in-arrow/index.html#datasets) at the end of the post, but I was a bit evasive. I gave an example showing how to use Datasets, but I really didn't talk much about what they are. \n\nI had a very good reason for this, dear reader, and that reason is this: when I wrote that post I had no f**king idea whatsoever how Datasets worked. I knew how to use them, but if you'd asked me questions about how the magic works I couldn't have told you.^[Unbelievably, there are people out there who will start talking about predicate pushdown and not even give a girl a heads up? Rude. You don't see me starting conversations at the pub about metric axiom violations in human similarity judgment do you? Well, okay, you might. But that's not the point!]\n\nSince that time I've learned a few things, and because I'm an annoying person I'm going to tell you about them.^[Okay, *now* I'm a bit sorry.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow, warn.conflicts = FALSE)\nlibrary(dplyr, warn.conflicts = FALSE)\n```\n:::\n\n\n<br>\n\n## Quick recap: Record Batches and Tables\n\nAt this point I've written [quite a few posts about Arrow](https://blog.djnavarro.net/category/apache-arrow), and it's not necessarily a good idea for me to assume that you've had the misfortune to read them. So here's a quick recap of some of the key Arrow data structures that I've talked about in other posts...\n\nLet's start with Record Batches. A Record Batch is tabular data structure comprised of named Arrays,^[For the purposes of this post we are going to pretend that Arrays behave like R vectors, which... they sort of do as long as you don't try to push at the analogy too hard.] and an accompanying Schema^[The Schema is the way Arrow formalises the metadata for rectangular data structures. I'm not going to dive into the details here: it's enough for our purposes to recognise that it's basically a list of variable names and their data types.] that specifies the name and data type associated with each Array. We can create one manually using `record_batch()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrb <- record_batch(\n  strs = c(\"hello\", \"amazing\", \"and\", \"cruel\", \"world\"), \n  ints = c(1L, NA, 2L, 4L, 8L),\n  dbls = c(1.1, 3.2, 0.2, NA, 11)\n)\nglimpse(rb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecordBatch\n5 rows x 3 columns\n$ strs <string> \"hello\", \"amazing\", \"and\", \"cruel\", \"world\"\n$ ints  <int32> 1, NA, 2, 4, 8\n$ dbls <double> 1.1, 3.2, 0.2, NA, 11.0\n```\n:::\n:::\n\n\nThis is a Record Batch containing 5 rows and 3 columns. The command `rb[1:3, 1:2]` extracts the first three rows and the first two columns:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(rb[1:3, 1:2])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecordBatch\n3 rows x 2 columns\n$ strs <string> \"hello\", \"amazing\", \"and\"\n$ ints  <int32> 1, NA, 2\n```\n:::\n:::\n\n\nThe structure of a Record Batch is shown below:\n\n![](./img/record_batch.png)\n\nRecord Batches are a fundamental unit for data interchange in Arrow, but are not typically used for data analysis. The reason for this is that the constituent Arrays that store columns in a Record Batch are immutable: they cannot be modified or extended without creating a new object.^[I mean, this is where you start asking all sort of questions about what objects are mutable in R anyway, since we're almost never doing modify-in-place operations. But whatever. This is not the post for that, and if you try to make me talk about that here I will cry.] When data arrive sequentially Record Batches can be inconvenient, because you can't concatenate them. For that reason Tables are usually more practical...\n\nSo let's turn to Tables next. From the user perspective a Table is very similar to a Record Batch but the constituent parts are Chunked Arrays. Chunked Arrays are flexible wrappers enclosing one or more Arrays.^[Again, let's just pretend that a Chunked Array behaves just like an R vector, except for the fact that it has these weird stitches from where we've sewn the individual Arrays together. It's all a bit vivisectionist in nature, sure, but this is the mechanism that allows Chunked Arrays to behave more like R vectors than simple Arrays do. Dr Frankenstein may not have been entirely wrong on all counts, I guess.] This makes it possible to concatenate tables: \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf1 <- arrow_table(rb)\ndf2 <- arrow_table(\n  strs = c(\"I\", \"love\", \"you\"), \n  ints = c(5L, 0L, 0L),\n  dbls = c(7.1, -0.1, 2)\n)\ndf <- concat_tables(df1, df2)\nglimpse(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTable\n8 rows x 3 columns\n$ strs <string> \"hello\", \"amazing\", \"and\", \"cruel\", \"world\", \"I\", \"love\", \"you\"\n$ ints  <int32> 1, NA, 2, 4, 8, 5, 0, 0\n$ dbls <double> 1.1, 3.2, 0.2, NA, 11.0, 7.1, -0.1, 2.0\n```\n:::\n:::\n\n\n\n\n![](./img/table.png)\nOkay, what about Datasets? Ah well... that's the one I hadn't quite worked out until quite recently. Hence the purpose of this post.\n\n<br>\n\n\n## So... Datasets?\n\n\nLike Record Batch and Table objects, a Dataset is used to represent tabular data. At an abstract level, a Dataset can be viewed as an object comprised of rows and columns, and just like Record Batches and Tables, it contains an explicit Schema that specifies the name and data type associated with each column.\n\nHowever, where Tables and Record Batches are data explicitly represented in-memory, a Dataset is not. Instead, a Dataset is an abstraction that refers to data stored on-disk in one or more files. Values stored in the data files are loaded into memory as a batched process. Loading takes place only as needed, and only when a query is executed against the data. In this respect Arrow Datasets are a very different kind of object to Arrow Tables, but the dplyr commands used to analyze them are essentially identical. In this section we'll talk about how Datasets are structured. \n\n\n<br>\n\n## What does a Dataset look like on-disk?\n\nReduced to its simplest form, the on-disk structure of a Dataset is simply a collection of data files, each storing one subset of the data. These subsets are sometimes referred to as \"fragments\", and the partitioning process is sometimes referred to as \"sharding\". By convention, these files are organized into a folder structure called a Hive-style partition: see `hive_partition()` for details. \n\nTo illustrate how this works, let's write a multi-file dataset to disk manually, without using any of the Arrow Dataset functionality to do the work. We'll start with three small data frames, each of which contains one subset of the data we want to store: \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf_a <- data.frame(id = 1:5, value = rnorm(5), subset = \"a\")\ndf_b <- data.frame(id = 6:10, value = rnorm(5), subset = \"b\")\ndf_c <- data.frame(id = 11:15, value = rnorm(5), subset = \"c\")\n```\n:::\n\n\nOur intention is that each of the data frames should be stored in a separate data file. As you can see, this is a quite structured partitioning: all data where `subset = \"a\"` belong to one file, all data where `subset = \"b\"` belong to another file, and all data where `subset = \"c\"` belong to the third file. \n\nThe first step is to define and create a folder that will hold all the files:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds_dir <- \"mini-dataset\"\ndir.create(ds_dir)\n```\n:::\n\n\nThe next step is to manually create the Hive-style folder structure:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds_dir_a <- file.path(ds_dir, \"subset=a\")\nds_dir_b <- file.path(ds_dir, \"subset=b\")\nds_dir_c <- file.path(ds_dir, \"subset=c\")\n\ndir.create(ds_dir_a)\ndir.create(ds_dir_b)\ndir.create(ds_dir_c)\n```\n:::\n\n\nNotice that we have named each folder in a \"key=value\" format that exactly describes the subset of data that will be written into that folder. This naming structure is the essence of Hive-style partitions. \n\nNow that we have the folders, we'll use `write_parquet()` to create a single parquet file for each of the three subsets:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_parquet(df_a, file.path(ds_dir_a, \"part-0.parquet\"))\nwrite_parquet(df_b, file.path(ds_dir_b, \"part-0.parquet\"))\nwrite_parquet(df_c, file.path(ds_dir_c, \"part-0.parquet\"))\n```\n:::\n\n\nIf we had wanted to, we could have further subdivided the dataset. A folder could contain multiple files (`part-0.parquet`, `part-1.parquet`, etc) if we wanted it to. Similarly, there is no particular reason to name the files `part-0.parquet` this way at all: it would have been fine to call these files `subset-a.parquet`, `subset-b.parquet`, and `subset-c.parquet` if we had wished. We could have written other file formats if we wanted, and we don't necessarily have to use Hive-style folders. You can learn more about the supported formats by reading the help documentation for `open_dataset()`, and learn about how to exercise fine grained control with `help(\"Dataset\", package = \"arrow\")`. \n\nIn any case, we have created an on-disk parquet Dataset using Hive-style partitioning. Our Dataset is defined by these files:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist.files(ds_dir, recursive = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"subset=a/part-0.parquet\" \"subset=b/part-0.parquet\"\n[3] \"subset=c/part-0.parquet\"\n```\n:::\n:::\n\n\nTo verify that everything has worked, let's open the data with `open_dataset()` and call `glimpse()` to inspect its contents:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds <- open_dataset(ds_dir)\nglimpse(ds)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFileSystemDataset with 3 Parquet files\n15 rows x 3 columns\n$ id      <int32> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n$ value  <double> 0.55549839, -1.36625808, 0.44427432, -1.11435957, -0.54390056,…\n$ subset <string> \"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c…\nCall `print()` for full schema details\n```\n:::\n:::\n\n\nAs you can see, the `ds` Dataset object aggregates the three separate data files. In fact, in this particular case the Dataset is so small that values from all three files appear in the output of `glimpse()`.\n\nIt should be noted that in everyday data analysis work, you wouldn't need to do write the data files manually in this fashion. The example above is entirely for illustrative purposes. The exact same dataset could be created with the following command:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds |> \n  group_by(subset) |>\n  write_dataset(\"mini-dataset\")\n```\n:::\n\n\nIn fact, even if `ds` happens to refer to a data source that is larger than memory, this command should still work because the Dataset functionality is written to ensure that during a pipeline such as this the data is loaded piecewise in order to avoid exhausting memory. \n\n<br>\n\n## What is stored in-memory by the Dataset object?\n\nIn the previous section we examined the on-disk structure of a Dataset. We now turn to the in-memory structure of the Dataset object itself (i.e., `ds` in the previous example). When the Dataset object is created, arrow searches the dataset folder looking for appropriate files, but does not load the contents of those files. Paths to these files are stored in an active binding `ds$files`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds$files \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"/home/danielle/GitHub/sites/quarto-blog/posts/2022-11-20_unpacking-arrow-datasets/mini-dataset/subset=a/part-0.parquet\"\n[2] \"/home/danielle/GitHub/sites/quarto-blog/posts/2022-11-20_unpacking-arrow-datasets/mini-dataset/subset=b/part-0.parquet\"\n[3] \"/home/danielle/GitHub/sites/quarto-blog/posts/2022-11-20_unpacking-arrow-datasets/mini-dataset/subset=c/part-0.parquet\"\n```\n:::\n:::\n\n\nThe other thing that happens when `open_dataset()` is called is that an explicit Schema for the Dataset is constructed and stored as `ds$schema`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds$schema\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSchema\nid: int32\nvalue: double\nsubset: string\n\nSee $metadata for additional Schema metadata\n```\n:::\n:::\n\n\nBy default this Schema is inferred by inspecting the first file only, though it is possible to construct a unified schema after inspecting all files. To do this, set `unify_schemas = TRUE` when calling `open_dataset()`. It is also possible to use the `schema` argument to `open_dataset()` to specify the Schema explicitly (see the `schema()` function for details). \n\nIt's a bit of an oversimplification because both `ds$files` and `ds$schema` are actually active bindings that invoke function calls to the underlying Arrow C++ library but honestly I don't see a need to care. In most situations I think it's reasonable to use this as the mental model of what the `ds` object contains:\n\n![](./img/dataset.png)\n\nThe act of reading the data is performed by a Scanner object. When analyzing a Dataset using the dplyr interface you never need to construct a Scanner manually, but for explanatory purposes we'll do it here:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscan <- Scanner$create(dataset = ds)\n```\n:::\n\n\nCalling the `ToTable()` method will materialize the Dataset (on-disk) as a Table (in-memory):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscan$ToTable()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTable\n15 rows x 3 columns\n$id <int32>\n$value <double>\n$subset <string>\n\nSee $metadata for additional Schema metadata\n```\n:::\n:::\n\n\nThis scanning process is multi-threaded by default, but if necessary threading can be disabled by setting `use_threads = FALSE` when calling `Scanner$create()`.\n\n\n<br>\n\n## How does a Dataset query work?\n\nWhen a query is executed against a Dataset a new scan is initiated and the results pulled back into R. As an example, consider the following dplyr expression:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nds |>\n  filter(value > 0) |>\n  mutate(new_value = round(100 * value)) |>\n  select(id, subset, new_value) |>\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  id subset new_value\n1  1      a        56\n2  3      a        44\n3  8      b         9\n4 11      c        39\n5 12      c       194\n6 13      c        87\n7 14      c         9\n```\n:::\n:::\n\n\nWe can replicate this using the low-level Dataset interface by creating a new scan by specifying the `filter` and `projection` arguments to `Scanner$create()`. To use these arguments you need to know a little about Arrow Expressions, for which you may find it helpful to read the help documentation in `help(\"Expression\", package = \"arrow\")`. \n\nThe scanner defined below mimics the dplyr pipeline shown above,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscan <- Scanner$create(\n  dataset = ds, \n  filter = Expression$field_ref(\"value\") > 0,\n  projection = list(\n    id = Expression$field_ref(\"id\"),\n    subset = Expression$field_ref(\"subset\"),\n    new_value = Expression$create(\"round\", 100 * Expression$field_ref(\"value\"))\n  )\n)\n```\n:::\n\n\nand if we were to call `as.data.frame(scan$ToTable())` it would produce the same result as the dplyr version, though the rows may not appear in the same order. \n\nTo get a better sense of what happens when the query executes, what we'll do here is call `scan$ScanBatches()`. Much like the `ToTable()` method, the `ScanBatches()` method executes the query separately against each of the files, but it returns a list of Record Batches, one for each file. In addition, we'll convert these Record Batches to data frames individually:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlapply(scan$ScanBatches(), as.data.frame)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n  id subset new_value\n1  1      a        56\n2  3      a        44\n\n[[2]]\n  id subset new_value\n1  8      b         9\n\n[[3]]\n  id subset new_value\n1 11      c        39\n2 12      c       194\n3 13      c        87\n4 14      c         9\n```\n:::\n:::\n\n\nIf we return to the dplyr query we made earlier, and use `compute()` to return a Table rather use `collect()` to return a data frame, we can see the evidence of this process at work. The Table object is created by concatenating the three Record Batches produced when the query executes against three data files, and as a consequence of this the Chunked Array that defines a column of the Table mirrors the partitioning structure present in the data files:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntbl <- ds |>\n  filter(value > 0) |>\n  mutate(new_value = round(100 * value)) |>\n  select(id, subset, new_value) |>\n  compute()\n\ntbl$subset\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChunkedArray\n<string>\n[\n  [\n    \"a\",\n    \"a\"\n  ],\n  [\n    \"c\",\n    \"c\",\n    \"c\",\n    \"c\"\n  ],\n  [\n    \"b\"\n  ]\n]\n```\n:::\n:::\n\n\n\n<!--------------- appendices go here ----------------->\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}