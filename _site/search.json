[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes from a data witch",
    "section": "",
    "text": "Generative art with grid\n\n\n\nArt\n\n\nR\n\n\ngrid\n\n\n\nI decided it was time to learn more about the grid package in R, and so naturally used it to make generative art\n\n\n\nDanielle Navarro\n\n\nMar 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShattered landscapes\n\n\n\nArt\n\n\nR\n\n\n\nUsing ambient and rayshader to create weird, broken landcape images in R\n\n\n\nDanielle Navarro\n\n\nMar 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFun and games with p5.js and observable.js in quarto\n\n\n\nArt\n\n\nP5\n\n\nObservable\n\n\nJavascript\n\n\nQuarto\n\n\n\nOkay it’s a short post in which I teach myself a bit of p5.js, but it does have five different donut examples which seems cool?\n\n\n\nDanielle Navarro\n\n\nJan 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying R with kubernetes\n\n\n\nR\n\n\nDocker\n\n\nKubernetes\n\n\nPlumber\n\n\n\nIn which it is painfully clear that the author is trying to figure it all out as she goes\n\n\n\nDanielle Navarro\n\n\nJan 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlaying with docker and the github container registry\n\n\n\nLinux\n\n\nR\n\n\nDocker\n\n\n\nIn which the author learns how to use docker and, in her usual fashion, goes a little bit overboard. But there are also generative art whales, so that’s nice.\n\n\n\nDanielle Navarro\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbtw I use Arch now\n\n\n\nLinux\n\n\nR\n\n\n\nThere is no reason for this.\n\n\n\nDanielle Navarro\n\n\nDec 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA very strange year\n\n\n\n\n\nA wrap up of my 2022. Even by the standards of the recent past this has been a very weird year for me. A lot has happened, both professionally and personally. I’m still…\n\n\n\nDanielle Navarro\n\n\nDec 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQueue\n\n\n\nParallel Computing\n\n\nR\n\n\nObject-Oriented Programming\n\n\n\nIt is a truth universally acknowledged, that a post about multithreading must be in want of an async trick\n\n\n\nDanielle Navarro\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCrayola crayon colours\n\n\n\nR\n\n\nData Cleaning\n\n\nData Visualisation\n\n\n\nAm I bored in the house? Yes. And am I in the house bored? Also yes. But do I have rvest and a stubborn desire not to allow the horrors of ‘data encoded in the CSS style’ to…\n\n\n\nDanielle Navarro\n\n\nDec 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnpacking Arrow Datasets\n\n\n\nApache Arrow\n\n\nR\n\n\n\nA comment on how Datasets work in Apache Arrow. I’m not really sure who the audience for this one. Am I just talking to myself? Probably.\n\n\n\nDanielle Navarro\n\n\nNov 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEverything I know about Mastodon\n\n\n\nMastodon\n\n\nSocial Media\n\n\n\nA hastily written guide for data science folks trying to navigate the fediverse.\n\n\n\nDanielle Navarro\n\n\nNov 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding an Arrow Flight server\n\n\n\nApache Arrow\n\n\nNetworking\n\n\nR\n\n\nPython\n\n\n\nA step by step walkthrough introducing the reader to Arrow Flight, with examples in R and Python\n\n\n\nDanielle Navarro\n\n\nOct 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData transfer between Python and R with rpy2 and Apache Arrow\n\n\n\nApache Arrow\n\n\nR\n\n\nPython\n\n\n\nA Pythonic approach for sharing Arrow Tables between Python and R. This is the second in a two-part series on data transfer. In this post I discuss how the rpy2 Python…\n\n\n\nDanielle Navarro\n\n\nSep 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPassing Arrow data between R and Python with reticulate\n\n\n\nApache Arrow\n\n\nR\n\n\nPython\n\n\n\nIn a multi-language ‘polyglot’ data science world, it becomes important that we are able to pass large data sets efficiently from one language to another without making…\n\n\n\nDanielle Navarro\n\n\nSep 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSudo ask me a password\n\n\n\nLinux\n\n\nCredentials\n\n\nR\n\n\n\nResolving a little quirk in managing packages with pak on linux\n\n\n\nDanielle Navarro\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to visualise a billion rows of data in R with Apache Arrow\n\n\n\nApache Arrow\n\n\nData Visualisation\n\n\nR\n\n\n\nIn which the author grapples with the awkward question of what data visualisation really means when you have a staggering amount of data to work with\n\n\n\nDanielle Navarro\n\n\nAug 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArrays and tables in Arrow\n\n\n\nApache Arrow\n\n\nR\n\n\n\nIn which the author discusses arrays, chunked arrays, record batches, tables, and datasets in the R interface to Apache Arrow. Masochism is mentioned, and the music of…\n\n\n\nDanielle Navarro\n\n\nMay 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPorting a distill blog to quarto\n\n\n\nQuarto\n\n\nBlogging\n\n\nReproducibility\n\n\n\nI have moved this blog from distill over to quarto, and taken notes. A year after starting the blog, this promises to be an interesting reproducibility test\n\n\n\nDanielle Navarro\n\n\nApr 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Amazon S3 with R\n\n\n\nAmazon S3\n\n\nR\n\n\n\nSomehow I was convinced that using Amazon S3 would be a supremely difficult thing to learn, kind of like learning git and GitHub for the first time. Thankfully, it’s…\n\n\n\nDanielle Navarro\n\n\nMar 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData types in Arrow and R\n\n\n\nApache Arrow\n\n\nR\n\n\n\nA post describing fundamental data types in R and Apache Arrow, and how data is exchanged between the two systems. It covers conversion of logicals, integers, floats…\n\n\n\nDanielle Navarro\n\n\nMar 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR scripts for twitter mutes and blocks\n\n\n\nTwitter\n\n\nR\n\n\nSocial Media\n\n\n\nSocial media safety tools like muting and blocking are often misused, but for people who are targeted for harassment they are powerful and important. This is a brief…\n\n\n\nDanielle Navarro\n\n\nFeb 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinding Apache Arrow to R\n\n\n\nApache Arrow\n\n\nR\n\n\n\nI’ve been learning how to program with Apache Arrow inside R, and also I have been watching the SyFy show “The Magicians” obsessively. For no sensible reason I wrote a blog…\n\n\n\nDanielle Navarro\n\n\nJan 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting CRAN repository options\n\n\n\nR\n\n\n\nA quick post on how to use RStudio public package manager instead of a standard CRAN mirror, and an example of why that can be useful sometimes.\n\n\n\nDanielle Navarro\n\n\nJan 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started with Apache Arrow\n\n\n\nApache Arrow\n\n\nR\n\n\n\nI’ve been wanting to learn the basics of Apache Arrow a while: this is the story of how an R user learned to stop worrying and love a standardised in-memory columnar data…\n\n\n\nDanielle Navarro\n\n\nNov 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData serialisation in R\n\n\n\nSerialisation\n\n\nR\n\n\n\nA terrifying descent into madness, or, an explanation of how R serialises an in-memory data structure to summon a sequence of bytes that can be saved or transmitted.…\n\n\n\nDanielle Navarro\n\n\nNov 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnpredictable paintings\n\n\n\nArt\n\n\nR\n\n\n\nAn example showing how to build a generative art system in R. The post walks through some of the creative and design choices that are involved, and highlights how much of a…\n\n\n\nDanielle Navarro\n\n\nNov 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative art resources in R\n\n\n\nArt\n\n\nR\n\n\n\nAn extremely incomplete (and probably biased) list of resources to help an aspiring generative artist get started making pretty pictures in R\n\n\n\nDanielle Navarro\n\n\nOct 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to mint digital art on HEN\n\n\n\nArt\n\n\n\nNot every artist wants to make cryptoart, and that’s okay. Others do, and that’s okay too. But if you want to try it out in a socially responsible way, it takes a bit of…\n\n\n\nDanielle Navarro\n\n\nSep 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising the hits of Queen Britney\n\n\n\nTidy Tuesday\n\n\nData Visualisation\n\n\nR\n\n\n\nA gentle walkthrough of a few data wrangling and visualisation tools using the Billboard 100 data for this weeks Tidy Tuesday. Pitched at beginners looking to refresh their…\n\n\n\nDanielle Navarro\n\n\nSep 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArt, jasmines, and the water colours\n\n\n\nArt\n\n\nR\n\n\n\nAn essay and tutorial covering a few useful art techniques in R\n\n\n\nDanielle Navarro\n\n\nSep 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManaging GitHub credentials from R, difficulty level linux\n\n\n\nGit\n\n\nCredentials\n\n\nLinux\n\n\nR\n\n\n\nA sick sad story in which a humble R user was forced to learn something about how linux stores passwords and, more importantly, got R to use her GitHub credentials properly\n\n\n\nDanielle Navarro\n\n\nAug 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative art in R\n\n\n\nArt\n\n\nR\n\n\n\nComments on an exhibit I contributed to as part of useR!2021\n\n\n\nDanielle Navarro\n\n\nJul 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrap cards in distill\n\n\n\nBlogging\n\n\nBootstrap\n\n\nDistill\n\n\nR\n\n\n\nHow to enable bootstrap 4 on a distill website, even though you probably don’t need to. I like it though because I get to add pretty bootstrap cards\n\n\n\nDanielle Navarro\n\n\nApr 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPretty little CLIs\n\n\n\nCommand Line\n\n\nR\n\n\n\nHow to make a gorgeous command line interface in R using the cli package. Somewhere along the way I accidentally learned about ANSI control codes, which strikes me as…\n\n\n\nDanielle Navarro\n\n\nApr 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to the jungle\n\n\n\nBlogging\n\n\nReproducibility\n\n\nDistill\n\n\n\nI have reluctantly decided to create a new blog. Some thoughts on what I hope to achieve, having tried my hand at blogging so very many times before\n\n\n\nDanielle Navarro\n\n\nApr 5, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html",
    "href": "posts/2022-04-20_porting-to-quarto/index.html",
    "title": "Porting a distill blog to quarto",
    "section": "",
    "text": "A little over a year ago I decided to start blogging again, and set up this site. At the time I made the deliberate choice to use distill as my blogging platform rather than something that would require a static site generator like hugo or jeykll, and I don’t regret that choice. However, along the way I’ve found a few things that have bothered me about using distill. It’s never been worth considering changing to something new though, because distill has so many things that I do like. Until now.\nEnter, stage left, quarto.\nNow out of stealth mode and attracting no end of attention, quarto offers the promise of being a cross-platform, cross-format, open source publishing tool based on pandoc. Intrigued, I decided to play around with it for a while, and ended up making the decision to port this blog from distill to quarto. This post outlines my process.\n(I am a little nervous: porting a blog often involves recomputing things. Will it work? Will everything turn out to be reproducible? I hope so…)"
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#getting-started",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#getting-started",
    "title": "Porting a distill blog to quarto",
    "section": "Getting started",
    "text": "Getting started\nThe very first thing I do is go read Alison Hill’s wonderful We don’t talk about quarto blog post. If you’re an R markdown user considering making the jump to quarto and haven’t already read her summary of where things are at, you won’t regret it. It’s a nice high level overview. I’d also suggest Nick Tierney’s notes on making the switch, which is very helpful also.\nAfter doing my background reading, I go to the get started page on the quarto website to download the installer file. I’m on Ubuntu, so for me that’s a .deb file. I install it in the usual way from the command line:\nsudo dpkg -i quarto-0.9.282-linux-amd64.deb\nNow that I have quarto installed, I’m able to use it to create a blog. My old distill blog exists in a project folder that I’d imaginatively named distill-blog, so I decide to keep to tradition and create the quarto version in an equally boring project folder called quarto-blog.\nThere is a page on the quarto website that walks you through the process for creating a blog, which I dutifully follow. From the terminal, I use the quarto create-project command, and a variety of files are created:\nquarto create-project quarto-blog --type website:blog\nCreating project at /home/danielle/GitHub/sites/quarto-blog:\n  - Created _quarto.yml\n  - Created index.qmd\n  - Created posts/welcome/index.qmd\n  - Created posts/post-with-code/index.qmd\n  - Created about.qmd\n  - Created styles.css\n  - Created posts/_metadata.yml\nComing from an R markdown background, this is very familiar:\n\nThe files with a .qmd extension are the quarto markdown documents. These contain source code for the blog posts (the two files in the posts folder), the home page (the index.qmd file in the project root folder) and a standalone “about me” page for the blog (the about.qmd file).\nThe files with a .yml extension are the YAML files used to configure the blog. I don’t notice this at first, but the fact that there are two of them is important. The _quarto.yml file is used for settings that will apply across the entire site, but you will often want to configure settings that only apply to your blog posts. Those can be set by editing the posts/_metadata.yml file.\nThe styles.css file can be used to specify custom CSS rules that will apply to the whole site. I’ll talk more about styles later."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#rendering-posts",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#rendering-posts",
    "title": "Porting a distill blog to quarto",
    "section": "Rendering posts",
    "text": "Rendering posts\nThere are several different ways to interact with quarto. For example, later in the post I’ll talk about the quarto command line interface which allows you to work with quarto without going through R or RStudio. However, when getting started I try to keep things simple, and go with the option that was most familiar to me: I use RStudio.\nTo do this, it’s convenient to have an RStudio project for my blog. Using the RStudio file menu, I create a new project from an existing directory (i.e., my quarto-blog folder), which supplies the quarto-blog.Rproj file and other infrastructure needed to work with my new quarto blog as an RStudio project. Once that’s done, I am able to open up a quarto file in the RStudio editor and see a familiar looking interface:\n\n\n\n\n\nA blog post written in quarto markdown open in the RStudio editor. Notice that in the place where one would normally expect to see the ‘Knit’ button for an R markdown document, there is a ‘Render’ button. It serves the same function and is mapped to the same hot keys as the ‘Knit’ button\n\n\n\n\nFrom here I can click on the “Render” button to render a single page, or alternatively I can go to the RStudio build pane and select the “Render Project” option to build the entire site. By default, the blog builds into the _site folder."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#playing-around",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#playing-around",
    "title": "Porting a distill blog to quarto",
    "section": "Playing around",
    "text": "Playing around\nProgress! I am making progress. However, before trying to do any other practical things, I have some very important business to attend to: playing around. Aimlessly exploring the functionality of a new tool is always fun, and I find it’s a good way to familiarise myself with something. I’m pretty familiar with R markdown already, and I imagine most readers of this post will be too, so for the most part there are no surprises. Still it is worth asking myself the usual questions:\n\nCan I add footnotes?1\nCan they be nested?2\nCan I add comments in the margin?\n\n\n\nA comment in the margin\nLooking at the quarto article layout documentation, I discover some nice features. You can use the :::{.class} notation to apply a CSS class to a section of output, like this:\n:::{.column-margin}\nA comment in the margin\n:::\nThe .column-margin class is handy for margin asides, but there are several other useful classes that come in handy when adding images to blog posts:\n\n.column-body spans the usual body width of the post\n.column-body-outset extends slightly outside the usual width\n.column-page spans the whole page (including both margins)\n.column-screen class spans the full width of the screen\n.column-screen-inset class stops just short of the full screen width\n\nYou can set these inside a chunk option. For example, setting column: margin as a chunk option will assign the output a .column-margin class, and any resulting figure will appear in the margin rather than below the code. Similarly, setting column: screen as the chunk option will assign the output a .column-screen class, and the output will span the full width. Here’s a simple example based pretty closely on the one used in the quarto documentation:\n\nlibrary(leaflet)\nleaflet() %>%\n  addTiles() %>%\n  addMarkers(\n    lng=151.22251, \n    lat=-33.85943, \n    label=\"Mrs Macquarie's Chair\"\n  ) %>% \n  addProviderTiles(providers$CartoDB.Positron)\n\n\n\n\n\n\nI confess. I’m a little bit in love already."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#notes-on-yaml-headers",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#notes-on-yaml-headers",
    "title": "Porting a distill blog to quarto",
    "section": "Notes on YAML headers",
    "text": "Notes on YAML headers\nThe YAML headers used for blog posts are a little different in quarto than their equivalents in distill were, and it takes me a moment to work out how to adapt the YAML headers from my old R markdown posts for the new quarto blog. Here’s a quick overview. First, some fields require almost no changes:\n\nThe title field is unchanged. That was an easy one!\nThe date field is essentially unchanged, except for the fact there seems to be a tiny bug in date parsing for blogs that I’m sure will vanish soon. If you’re using ISO-8601 date formats like date: \"2022-04-20\" it will be fine.3\nThe categories field takes a list of values, which (I think?) is no different to what it looked like before. To be honest I don’t remember because my old blog didn’t use them. I’ve started now.\n\nOther changes are superficial:\n\nThe description field that I used on my old blog still does what it used to: it provides preview text on the listing page, and a summary at the top of the file. However, there is also a subtitle field that you can use for this purpose, and the output has the same look and field as my old descriptions, so I decide to switch all my old description fields to subtitle entries.\nTo specify a preview image associated with a blog post, use the image field (e.g., something like image: thumbnail.jpg) instead of the preview field from distill.\nThere is a new license field that replaces the creative_commons field from distill. At the bottom of this post you’ll see a “Reuse” appendix that links to a license file. To generate this, I’ve included a license: \"CC BY\" line in the YAML.\n\nOther changes are a little deeper:\n\nIn distill it is possible to specify the author field in a lot of detail, mirroring the academic convention of listing an authors affiliation alongside their employer, orcid record, and contact details. Quarto supports this also, though the tags have changed slightly: orcid_id is now orcid, for example. There’s an example of this shown a little later in the post.\nSpecifying the table of contents is slightly different. Just like in distill, you can turn on the table of contents by including toc: true as a line in the YAML header, and set the toc-depth field to determine how detailed the table of contents should be. But there are new options. You can customise the text that appears above the table of contents, and the location in which it appears. I decide to be boring and go with some standard options: toc-title: Table of contents and toc-location: left.\nOne feature in distill that I like is that it generates a citation for each post. You can do that in quarto too, and you’ll see at the bottom of this post that I’ve used that feature here. However, quarto manages this in a different way to distill, and uses a YAML version of citation style language (CSL) formatting to define the citation. To see how it works, you can read through the quarto pages on citations and creating citeable articles. It’s a little more elaborate than the distill version, but much more flexible. For this blog it’s as simple as including citation: true in the YAML, but it can be more elaborate and accommodate any pattern of academic citation you like."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#creating-a-new-post",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#creating-a-new-post",
    "title": "Porting a distill blog to quarto",
    "section": "Creating a new post",
    "text": "Creating a new post\nOkay. Time to get to work at transforming the starter blog into a quarto version of my distill blog. My first step is to delete the two posts that came with the starter blog, and then create this one.\nA folder with an index.qmd file is the bare minimum I need to get started with a new post. I suppose there are other ways do to this but what I actually do is create the the folder and an empty file from the terminal (for reasons known but to god):\nmkdir posts/2022-04-20_porting-to-quarto\ntouch posts/2022-04-20_porting-to-quarto/index.qmd\nTo be honest, using the terminal was overkill. What I could have done instead, had I been looking at RStudio rather than the terminal, is use the “New File” option in the file menu and then select the “Quarto Doc” option. That creates a new untitled quarto document that you can save to the appropriate location."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#inheriting-yaml-settings",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#inheriting-yaml-settings",
    "title": "Porting a distill blog to quarto",
    "section": "Inheriting YAML settings",
    "text": "Inheriting YAML settings\nA handy feature in quarto websites is that YAML fields are inherited. For example, this post has its own YAML header that contains the following – and only the following – fields:\ntitle: \"Porting a distill blog to quarto\"\nsubtitle: | \n  I have moved this blog from distill over to quarto, and \n  taken notes. A year after starting the blog, this promises \n  to be an interesting reproducibility test\ndate: \"2022-04-20\"\ncategories: [Quarto, Blogging, Reproducibility]\nimage: \"img/preview.jpg\"\nThat’s a little peculiar, because a lot of the metadata needed to specify this post is missing. The reason it is missing is that I’ve placed some fields in the posts/_metadata.yml file. Those fields are inherited by every blog post. This is the entire contents of my post metadata file:\n# Freeze computed outputs\nfreeze: true\n\n# Enable banner style title blocks\ntitle-block-banner: true\n\n# Enable CC licence appendix\nlicense: \"CC BY\"\n\n# Default for table of contents\ntoc: true\ntoc-title: Table of contents\ntoc-location: left\n\n# Default knitr options\nexecute:\n  echo: true\n  message: true\n  warning: true\n\n# Default author\nauthor:\n  - name: Danielle Navarro\n    url: https://djnavarro.net\n    affiliation: Voltron Data\n    affiliation-url: https://voltrondata.com\n    orcid: 0000-0001-7648-6578\n\n# Default fields for citation\ncitation: true\n\n\nThe freeze option is extremely useful in the blogging context. I’d advise reading the linked documentation page!\nThat explains a lot, but if you’re looking closely you’ll realise that there’s nothing in these fields specifying the output format! In R markdown I’d have included an output field for this, but in quarto the relevant field is called format. Because the output applies to the entire site, that part of the YAML header is in the _quarto.yml file. The relevant lines of that file are:\nformat:\n  html:\n    theme: ember.scss\n    css: styles.css\nI’ll come back to this later. For now it’s enough to recognise that this indicates that all pages on this site should be rendered to HTML documents, and using the ember.scss and styles.css files specify the blog style."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#converting-my-old-posts",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#converting-my-old-posts",
    "title": "Porting a distill blog to quarto",
    "section": "Converting my old posts",
    "text": "Converting my old posts\nThe time has come for a little manual labour. Although quarto is compatible with most existing R markdown and I can probably get away with leaving them untouched, in the longer term I’m expecting that I’ll be moving across languages so it appeals to me to take this opportunity to port everything over to quarto now. Renaming all the index.Rmd files to index.qmd files is easy enough, and can be done programmatically, but most of my edits require a small amount of manual tinkering with each post. Not a lot, because it is mostly a matter of renaming a few YAML fields. Given that there are only 20 or so posts that need to be ported, I decide it is easier to do it manually than to try to write a script to automate the task. I get through it in an afternoon."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#styling-the-new-blog",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#styling-the-new-blog",
    "title": "Porting a distill blog to quarto",
    "section": "Styling the new blog",
    "text": "Styling the new blog\nUp to this point, the adoption of quarto has felt very “distill-like”. The structure of the blog feels familiar from distill, the YAML headers are similar in spirit (if different in the particulars), and so on. When it comes to customising the appearance of the blog, it’s not very similar to distill at all, and feels more similar to simple R markdown sites. Quarto websites are bootstrap based, and as discussed on the quarto theming page, they come with the same bootswatch themes that you might be familiar with from R markdown. For example, if you decide like I did that you would like a very plain white theme, you could choose the “litera” theme. To apply this to your blog, all you’d have to do is make sure your _quarto.yml file contains the following lines:\nformat:\n  html:\n    theme: litera\n    css: styles.css\nWhat this does is assert that output will be rendered as HTML objects using the litera bootswatch theme, and applying any custom CSS rules that you add in the styles.css file.\nOne very nice feature of quarto, if you’re comfortable using SASS to define styles and know something about how the bootstrap SASS files are organised,4 is that it allows you to write your own .scss file to define your blog theme more precisely, giving you access to bootstrap parameters and so on. I would strongly recommend reading about the quarto theming system before tinkering with this aspect yourself, but if you are more knowledgeable (or more foolish) than I, here’s how I set my blog up. First, instead of referring to the litera theme, the YAML in my _quarto.yml file points at my own custom .scss file:\nformat:\n  html:\n    theme: ember.scss\n    css: styles.css\nThe contents of the ember.scss file are as follows:\n/*-- scss:defaults --*/\n\n// use litera as the base\n$theme: \"litera\" !default;\n\n// import google fonts\n@import 'https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:ital,wght@0,400;0,700;1,400;1,700&display=swap';\n@import 'https://fonts.googleapis.com/css2?family=Fira+Code:wght@300;400;500;600;700&display=swap';\n\n// use Atkinson Hyperlegible font if available\n$font-family-sans-serif:  \"Atkinson Hyperlegible\", -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, \"Noto Sans\", sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\", \"Noto Color Emoji\" !default;\n\n/*-- scss:rules --*/\n\n// litera is serif by default: revert to san-serif\np {\n  font-family: $font-family-sans-serif;\n}\nAs you can see, right now my customisation really doesn’t do much other than make some very minor tweaks on the litera theme, but the potential is there to do so much more than I have in setting up this blog. I plan to tinker with this more later on!"
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#adding-an-rss-feed",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#adding-an-rss-feed",
    "title": "Porting a distill blog to quarto",
    "section": "Adding an RSS feed",
    "text": "Adding an RSS feed\nMy old distill blog had an RSS feed, and while I acknowledge that it’s increasingly an esoteric feature that most folks don’t use, I have a fondness for RSS. Quarto supports this, but it’s not enabled by default. What I need to do is edit the YAML in the index.qmd file that corresponds to your homepage, because that’s where I have my primary listing of posts. In it, I see a listing field. All I need to do is add feed: true underneath and there is now an RSS feed for the site:\ntitle: \"Notes from a data witch\"\nsubtitle: A data science blog by Danielle Navarro\nlisting:\n  feed: true\n  contents: posts\nThe quarto section on feeds has more information on this."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#deploying-the-site",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#deploying-the-site",
    "title": "Porting a distill blog to quarto",
    "section": "Deploying the site",
    "text": "Deploying the site\nPreparing the site to be deployed is relatively painless. I found it useful to read the quarto website options page before doing this, because it mentions a lot of settings to tinker with, mostly in the _quarto.yml file. For example, I choose to customise the navigation bar, the social media preview images, and so on. Eventually, I reach the point where I am happy and move on to deployment.\nHappily, as to the deployment process itself, there’s not a lot to say. The quarto deployment page discusses several options for how you can do this. Most of my websites are deployed either through GitHub Pages or through Netlify. This one is a Netlify site, so I follow the instructions there and everything goes smoothly. However, this does bring me to another topic…"
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#netlify-redirects",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#netlify-redirects",
    "title": "Porting a distill blog to quarto",
    "section": "Netlify redirects",
    "text": "Netlify redirects\nI’ve structured my blog in a particular way. Like the default quarto blog, all the posts live in the posts folder, and they’re named in a systematic way: they have an ISO-8601 formatted date first, and then a semantic slug. So the full URL for this blog post is:\nblog.djnavarro.net/posts/2022-04-20_porting-to-quarto\nThat’s convenient for archiving purposes and for keeping everything nicely organised in my project folder, but it’s also a little clunky for sharing links. In practice, the “posts” part is a bit redundant, and I’m never going to use the same slug twice, so it’s handy to set it up so that there’s also a shorter URL for the post,\nblog.djnavarro.net/porting-to-quarto\nand that this shorter URL automatically redirects to the longer one.\nSince I’m intending to deploy this blog to Netlify, what I need to do is ensure that whenever the site builds, a _redirects file is created within the _site folder. This file needs to have one line per redirect, listing the “redirect from” path first, followed by the “redirect to” path. Here’s what that line looks like for this post:\n/porting-to-quarto /posts/2022-04-20_porting-to-quarto\nI have no intention of adding these lines manually, so what I do instead is add an R chunk to the index.qmd file corresponding to the blog home page, with the following code:\n# list names of post folders\nposts <- list.dirs(\n  path = here::here(\"posts\"),\n  full.names = FALSE,\n  recursive = FALSE\n)\n\n# extract the slugs\nslugs <- gsub(\"^.*_\", \"\", posts)\n\n# lines to insert to a netlify _redirect file\nredirects <- paste0(\"/\", slugs, \" \", \"/posts/\", posts)\n\n# write the _redirect file\nwriteLines(redirects, here::here(\"_site\", \"_redirects\"))\nEvery time this site gets rebuilt – which usually involves rebuilding the home page since that’s the one that contains the post listing – the _redirects file gets refreshed. There might be a cleaner way, but this works."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#quarto-cli",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#quarto-cli",
    "title": "Porting a distill blog to quarto",
    "section": "The quarto CLI",
    "text": "The quarto CLI\nSomething I forgot to mention earlier. About half way through the process of tinkering with my old posts to be suitable for the quarto blog, I decided to stop using RStudio for the rendering, and spent a little time familiarising myself with the quarto command line interface. I haven’t made any particular decisions about what my long term workflow with quarto is going to look like, but I did find it helpful to get a feel for the concept of quarto as a standalone install. I’m not going to go into detail here, but just briefly: at the terminal I can see that I have some help options,\n\nquarto help\n\n  Usage:   quarto \n  Version: 0.9.282\n                  \n\n  Description:\n\n    Quarto CLI\n\n  Options:\n\n    -h, --help     - Show this help.                            \n    -V, --version  - Show the version number for this program.  \n\n  Commands:\n\n    render          [input] [args...]  - Render input file(s) to various document types.                                                \n    serve           [input]            - Serve an interactive document.                                                                 \n    create-project  [dir]              - Create a project for rendering multiple documents                                              \n    preview         [file] [args...]   - Render and preview a Quarto document or website project. Automatically reloads the browser when\n    convert         [input]            - Convert documents to alternate representations.                                                \n    capabilities                       - Query for current capabilities (output formats, engines, kernels etc.)                         \n    check           [target]           - Check and verify correct functioning of Quarto installation and computation engines.           \n    inspect         [path]             - Inspect a Quarto project or input path. Inspecting a project returns its config and engines.   \n    tools           [command] [tool]   - Manage the installation, update, and uninstallation of useful tools.                           \n    help            [command]          - Show this help or the help of a sub-command.\n    \nFrom there I can check the help documentation for the quarto render command by typing the following,\n\nquarto render help\n\nand so on. Browsing this documentation in addition to all the excellent content on the quarto website is a useful way of finding additional options. If I wanted to render the current post, and my terminal was currently at the project root folder (i.e., my quarto-blog folder), I can render it as follows:\n\nquarto render posts/2022-04-20_porting-to-quarto/index.qmd\n\nThe ability to do this cleanly from the terminal seems like a handy feature of quarto, though I’ll admit I am not yet sure how I’ll use it."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#epilogue",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#epilogue",
    "title": "Porting a distill blog to quarto",
    "section": "Epilogue",
    "text": "Epilogue\nWhen I started this process I wasn’t quite sure if I was going to follow through on it and actually switch the blog over to quarto. The distill blog has served me well for the last year, and I don’t like fixing things if they aren’t broken. However, the longer I played with quarto the more I liked it, and the process was far less painful than I feared it would be. I feel like it’s retained the things I like about distill, but integrated those cleanly with other features (e.g., the bootstrap grid!) that I really missed having access to from distill. Every now and then I’ve come across little quirks where some of the rough edges to quarto are still showing – it is a new tool still – but I’m enjoying it a lot."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#last-updated",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#last-updated",
    "title": "Porting a distill blog to quarto",
    "section": "Last updated",
    "text": "Last updated\n\n2022-08-23 13:02:10 AEST"
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#details",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#details",
    "title": "Porting a distill blog to quarto",
    "section": "Details",
    "text": "Details\n\nsource code, R environment"
  },
  {
    "objectID": "posts/2021-11-15_serialisation-with-rds/index.html",
    "href": "posts/2021-11-15_serialisation-with-rds/index.html",
    "title": "Data serialisation in R",
    "section": "",
    "text": "I still alive, and that’s what matters. The traumatic experience of the last week is fading, leaving a pale residue of fear and the few scraps of writing that are the sole surviving documentation of these days. It is a tale of fright, a desperate agony, and like any good tragedy it starts with the hope and naive optimism of youth…\nI’ve decided the time has come for me to do a deep dive into data serialisation in R. Serialisation is one of those terms that comes up from time to time in data science, and it’s popped up so many times on my twitter feed that I feel like I need a better grasp of how serialisation works in R. It’s a topic that folks who work with big data or have a computer science background likely understand quite well, but a lot of people who use R come from other backgrounds. If you’re a social scientist who mostly works with small CSV files, for example, there’s no particular reason why you’d have encountered this. In my case, I’ve worked as a mathematical psychologist and computational modeller for about 20 years, and until very recently I’ve never had never had to think about it in any detail. The issue only came up for me when I started reading about Apache Arrow (a topic for another post, perhaps) and realised that I needed to have a better understanding of what all this data serialisation business is about, and how R handles it.\nThis post is aimed at anyone who is in a similar situation to me!\nOh you sweet summer child. You really think you are prepared for the dark? That’s adorable."
  },
  {
    "objectID": "posts/2021-11-15_serialisation-with-rds/index.html#what-is-serialisation",
    "href": "posts/2021-11-15_serialisation-with-rds/index.html#what-is-serialisation",
    "title": "Data serialisation in R",
    "section": "What is serialisation?",
    "text": "What is serialisation?\nIn general serialisation refers to any process that takes an object stored in memory and converts into a stream of bytes that can be written to a file or transmitted elsewhere. Any time we write data to a file, we are “serialising” it according to some encoding scheme. Suppose, for instance, I have a data frame called art:\n\n\n\n\nart\n\n   resolution      series sys_id img_id   short_name format\n1        1000 watercolour  sys02  img34 teacup-ocean    jpg\n2        1000 watercolour  sys02  img34 teacup-ocean    png\n3        2000 watercolour  sys02  img34 teacup-ocean    jpg\n4        2000 watercolour  sys02  img34 teacup-ocean    png\n5        4000 watercolour  sys02  img34 teacup-ocean    jpg\n6        4000 watercolour  sys02  img34 teacup-ocean    png\n7         500 watercolour  sys02  img34 teacup-ocean    jpg\n8         500 watercolour  sys02  img34 teacup-ocean    png\n9        8000 watercolour  sys02  img34 teacup-ocean    jpg\n10       8000 watercolour  sys02  img34 teacup-ocean    png\n\n\nThis data frame is currently stored in memory on my machine, and it has structure. R represents this data frame as a list of length 6. Each element of this list is a pointer to another data structure, namely an atomic vector (e.g., numeric vector). The list is accompanied by additional metadata that tells R that this particular list is a data frame. The details of how this is accomplished don’t matter for this post. All that matters for now is that the in-memory representation of art is a structured object. It’s little more complicated than a stream of data, but if I want to save this data to a file it needs to be converted into one. The process of taking an in-memory structure and converting it to a sequence of bytes is called serialisation.\nSerialisation doesn’t have to be fancy. The humble CSV file can be viewed as a form of serialisation for a data frame, albeit one that does not store all the metadata associated with the data frame. Viewed this way, write.csv() can be viewed as a serialisation function for tabular data:\n\nwrite.csv(art, file = \"art.csv\", row.names = FALSE)\n\nWhen I call this function R uses the art object to write text onto the disk, saved as the file “art.csv”. If I were to open this file in a text editor, I’d see this:\n\n\n\"resolution\",\"series\",\"sys_id\",\"img_id\",\"short_name\",\"format\"\n1000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"jpg\"\n1000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"png\"\n2000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"jpg\"\n2000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"png\"\n4000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"jpg\"\n4000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"png\"\n500,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"jpg\"\n500,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"png\"\n8000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"jpg\"\n8000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"png\"\n\n\nAlthough this view is human-readable, it is slightly misleading. The text in shown above isn’t the literal sequence of bytes. It’s how those bytes are displayed when the have been unserialised and displayed on screen as UTF-8 plain text. To get a sense of what serialised text actually looks like we can use the charToRaw() function. The first few characters of the text file are \"resolu\" which looks like this when series of bytes:\n\ncharToRaw('\"resolu')\n\n[1] 22 72 65 73 6f 6c 75\n\n\nThe raw vector shown in the output above uses one byte to represent each character. For instance, the character \"l\" is represented with the byte 6c in the usual hexadecimal representation. We can unpack that byte into its consituent 8-bit representation using rawToBits()\n\n\"u\" |>\n  charToRaw() |>\n  rawToBits()\n\n[1] 01 00 01 00 01 01 01 00\n\n\n(Note that the base pipe |> is rendered as a triangle-shaped ligature in Fira Code)\nReturning to the “art.csv” data file, I can use file() and readBin() to define a simple helper function that opens a binary connection to the file, reads in the first 100 bytes (or whatever), closes the file, and then returns those bytes as a raw vector:\n\nread_bytes <- function(path, max_bytes = 100) {\n  con <- file(path, open = \"rb\")\n  bytes <- readBin(con, what = raw(), n = max_bytes)\n  close(con)\n  return(bytes)\n}\n\nHere are the first 100 bytes of the “art.csv” file:\n\nread_bytes(\"art.csv\")\n\n  [1] 22 72 65 73 6f 6c 75 74 69 6f 6e 22 2c 22 73 65 72 69 65 73 22 2c 22 73 79\n [26] 73 5f 69 64 22 2c 22 69 6d 67 5f 69 64 22 2c 22 73 68 6f 72 74 5f 6e 61 6d\n [51] 65 22 2c 22 66 6f 72 6d 61 74 22 0a 31 30 30 30 2c 22 77 61 74 65 72 63 6f\n [76] 6c 6f 75 72 22 2c 22 73 79 73 30 32 22 2c 22 69 6d 67 33 34 22 2c 22 74 65\n\n\nThe read.csv() function is similar to read_bytes() in spirit: when I call read.csv(\"art.csv\"), R opens a connection to the “art.csv” file. It then reads that sequence of bytes into memory, and then closes the file. However, unlike my simple read_bytes() function, it does something useful with that information. The sequence of bytes gets decoded (unserialised), and the result is that R reconstructs the original art data frame:\n\nart <- read.csv(\"art.csv\")\nart\n\n   resolution      series sys_id img_id   short_name format\n1        1000 watercolour  sys02  img34 teacup-ocean    jpg\n2        1000 watercolour  sys02  img34 teacup-ocean    png\n3        2000 watercolour  sys02  img34 teacup-ocean    jpg\n4        2000 watercolour  sys02  img34 teacup-ocean    png\n5        4000 watercolour  sys02  img34 teacup-ocean    jpg\n6        4000 watercolour  sys02  img34 teacup-ocean    png\n7         500 watercolour  sys02  img34 teacup-ocean    jpg\n8         500 watercolour  sys02  img34 teacup-ocean    png\n9        8000 watercolour  sys02  img34 teacup-ocean    jpg\n10       8000 watercolour  sys02  img34 teacup-ocean    png\n\n\nThrilling stuff.\n\nDo you feel that slow dread yet, my dear? Do you feel yourself slipping? You are on the edge of the cliff. You can still climb back to safety if you want. You don’t have to fall. The choice is still yours.\n\n\n\n\n\n\nImage by Daniel Jensen. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-15_serialisation-with-rds/index.html#how-does-rds-serialisation-work",
    "href": "posts/2021-11-15_serialisation-with-rds/index.html#how-does-rds-serialisation-work",
    "title": "Data serialisation in R",
    "section": "How does RDS serialisation work?",
    "text": "How does RDS serialisation work?\nData can be serialised in different ways. The CSV format works reasonably well for rectangular data structures like data frames, but doesn’t work well if you need to serialise something complicated like a nested list. The JSON format is a better choice for those cases, but it too has some limitations when it comes to storing R objects. To serialise an R object we need to store the metadata (classes, names, and other attributes) associated with the object, and if the object is a function there is a lot of other information relevant to its execution besides the source code (e.g., enclosing environment). Because R needs this information, it relies on the native RDS format to do the work. As it happens I have an “art.rds” file on disk that stores the same data frame in the RDS format. When I use readRDS() to unserialise the file, it recreates the same data frame:\n\nreadRDS(\"art.rds\")\n\n   resolution      series sys_id img_id   short_name format\n1        1000 watercolour  sys02  img34 teacup-ocean    jpg\n2        1000 watercolour  sys02  img34 teacup-ocean    png\n3        2000 watercolour  sys02  img34 teacup-ocean    jpg\n4        2000 watercolour  sys02  img34 teacup-ocean    png\n5        4000 watercolour  sys02  img34 teacup-ocean    jpg\n6        4000 watercolour  sys02  img34 teacup-ocean    png\n7         500 watercolour  sys02  img34 teacup-ocean    jpg\n8         500 watercolour  sys02  img34 teacup-ocean    png\n9        8000 watercolour  sys02  img34 teacup-ocean    jpg\n10       8000 watercolour  sys02  img34 teacup-ocean    png\n\n\nHowever, when I read this file using read_bytes() it’s also clear that “art.rds” contains a very different sequence of bytes to “art.csv”:\n\nread_bytes(\"art.rds\")\n\n  [1] 1f 8b 08 00 00 00 00 00 00 03 8b e0 62 60 60 60 66 60 61 64 62 60 66 05 32\n [26] 19 58 43 43 dc 74 2d 80 62 c2 40 0e 1b 10 f3 02 31 50 11 f3 0b 08 66 bf 00\n [51] c1 fc 0b 20 98 f1 0b 04 cb 3b 40 30 83 00 58 3d 0b 03 27 90 e6 2e 4f 2c 49\n [76] 2d 4a ce cf c9 2f 2d 1a 4a 42 a8 be 60 2d ae 2c 36 30 1a 18 0e 9a 4b 32 73\n\n\nThis is hardly surprising since RDS and CSV are different file formats. But while I have a pretty good mental model of what the contents of a CSV file look like, I don’t have a very solid grasp of what the format of an RDS file is. I’m curious.\n\nOh sweetie, I tried to warn you…\n\n\nThe serialize() function\nTo get a sense of how the RDS format works, it’s helpful to note that R has a serialize() function and an unserialize() function that provide low-level access to the same mechanisms that underpin saveRDS() and readRDS().\n\nbytes <- serialize(art, connection = NULL)\n\nAs you can see, this is the same sequence of bytes returned by read_bytes()…\n\nbytes[1:100]\n\n  [1] 58 0a 00 00 00 03 00 04 02 01 00 03 05 00 00 00 00 05 55 54 46 2d 38 00 00\n [26] 03 13 00 00 00 06 00 00 00 0d 00 00 00 0a 00 00 03 e8 00 00 03 e8 00 00 07\n [51] d0 00 00 07 d0 00 00 0f a0 00 00 0f a0 00 00 01 f4 00 00 01 f4 00 00 1f 40\n [76] 00 00 1f 40 00 00 00 10 00 00 00 0a 00 04 00 09 00 00 00 0b 77 61 74 65 72\n\n\n…oh wait, no it’s not. What gives???? The “art.rds” file begins with 1f 8b 08 00, whereas serialize() returns a sequence of bytes that begins with 58 0a 00 00. These are not the same at all! Why is this happening???\n\n\nRDS uses gzip compression\nAfter digging a little into the help documentation, I realised that this happens because the default behaviour of saveRDS() is to write a compressed RDS file using gzip compression. In contrast, serialize() does not employ any form of compression. The art.rds file that I have stored on disk is that gzipped version, but it’s easy enough to save an uncompressed RDS file, simply by setting compress = FALSE:\n\nsaveRDS(art, file = \"art_nozip.rds\", compress = FALSE)\n\nSo now when I inspect the uncompressed file using read_bytes(), the output is the same one I obtained when I called serialize(art) earlier:\n\nread_bytes(\"art_nozip.rds\")\n\n  [1] 58 0a 00 00 00 03 00 04 02 01 00 03 05 00 00 00 00 05 55 54 46 2d 38 00 00\n [26] 03 13 00 00 00 06 00 00 00 0d 00 00 00 0a 00 00 03 e8 00 00 03 e8 00 00 07\n [51] d0 00 00 07 d0 00 00 0f a0 00 00 0f a0 00 00 01 f4 00 00 01 f4 00 00 1f 40\n [76] 00 00 1f 40 00 00 00 10 00 00 00 0a 00 04 00 09 00 00 00 0b 77 61 74 65 72\n\n\nThat’s a relief. I was getting very anxious there, but I feel a little better now. My sanity is restored.\n\n…for now.\n\n\n\nThe unserialize() function\nThat was frustrating. Anyway getting back to the main thread, the inverse of the serialize() function is unserialize(). It’s very similar to the readRDS() function that you’d normally use to read an RDS file, but you can apply it to a raw vector like bytes. Once again we reconstruct the original data frame:\n\nunserialize(bytes)\n\n   resolution      series sys_id img_id   short_name format\n1        1000 watercolour  sys02  img34 teacup-ocean    jpg\n2        1000 watercolour  sys02  img34 teacup-ocean    png\n3        2000 watercolour  sys02  img34 teacup-ocean    jpg\n4        2000 watercolour  sys02  img34 teacup-ocean    png\n5        4000 watercolour  sys02  img34 teacup-ocean    jpg\n6        4000 watercolour  sys02  img34 teacup-ocean    png\n7         500 watercolour  sys02  img34 teacup-ocean    jpg\n8         500 watercolour  sys02  img34 teacup-ocean    png\n9        8000 watercolour  sys02  img34 teacup-ocean    jpg\n10       8000 watercolour  sys02  img34 teacup-ocean    png\n\n\nYay.\n\nYou can sense it can’t you? It will only get worse for you, my sweet. Look upon the grim visage of those that have passed this way before. Their lifeless bones are a warning.\n\n\n\n\n\n\nImage by Chelms Varthoumlien. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-15_serialisation-with-rds/index.html#serialising-to-plain-text-rds",
    "href": "posts/2021-11-15_serialisation-with-rds/index.html#serialising-to-plain-text-rds",
    "title": "Data serialisation in R",
    "section": "Serialising to plain text RDS",
    "text": "Serialising to plain text RDS\nOkay, so what I’ve learned so far is that in most cases, an RDS file is just a gzipped version of … something. It’s the gzipped version of whatever the hell it is that serialize() creates. What I don’t yet know is how the serialize() function operates. What secret magic does it use? How does it construct this sequence of bytes? What do the contents of this file actually include?\nI’ll start simple. Trying to understand how a complicated object is serialised might be painful, so I’ll set the art data frame to one side. Instead, I’ll serialise a numeric vector containing three elements, and … I guess I’ll set ascii = TRUE so that R uses UTF-8 to serialise the object to plain text format rather than … writing a binary file?\n\nClever girl. Yes, the default behaviour is binary serialization. Unless otherwise specified using the xdr argument, serialize() enforces a big-endian representation on the binary encoding. But you didn’t want to go there did you? It frightened you, didn’t it? The abyss stares back at you, sweetness, and you are beginning to attract its attention\n\n\nbytes <- serialize(\n  object = c(10.1, 2.2, 94.3), \n  connection = NULL,\n  ascii = TRUE\n)\n\nWhen I print out the bytes vector I still don’t get text though?\n\nbytes\n\n [1] 41 0a 33 0a 32 36 32 36 35 37 0a 31 39 37 38 38 38 0a 35 0a 55 54 46 2d 38\n[26] 0a 31 34 0a 33 0a 31 30 2e 31 0a 32 2e 32 0a 39 34 2e 33 0a\n\n\nI was expecting text. Where is my text??? I dig a little deeper and realise my mistake. What I’m looking at here is the sequence of bytes that correspond to the UTF-8 encoded text. If I want to see that text using actual letters, I need to use rawToChar(). When I do that I see something that looks vaguely like data:\n\nrawToChar(bytes)\n\n[1] \"A\\n3\\n262657\\n197888\\n5\\nUTF-8\\n14\\n3\\n10.1\\n2.2\\n94.3\\n\"\n\n\nIt is a little easier to read if I use cat() to print the output:\n\nbytes |>\n  rawToChar() |>\n  cat()\n\nA\n3\n262657\n197888\n5\nUTF-8\n14\n3\n10.1\n2.2\n94.3\n\n\nIt’s… not immediately obvious how this output should be interpreted? I don’t know what all these lines mean, but I recognise the last three lines: those are the three values stored in the vector I serialised. Now I just need to work out what the rest of it is all about.\nBut before I do, I’ll check that this is exactly the same text that I see if I create an RDS file using the following command and then open that file in a text editor:\n\nsaveRDS(\n  object = c(10.1, 2.2, 94.3), \n  file = \"numbers.rds\", \n  ascii = TRUE, \n  compress = FALSE\n)\n\nOkay, it checks out. My excitement can barely be contained.\n\nWilting already, aren’t you? Poor little flower, you’ve been cut from the stem. You’re dead already but you don’t even know it. All that is left is to wither away under the blistering glare of knowledge.\n\n\n\n\n\n\nImage by Daria Shevtsova. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-15_serialisation-with-rds/index.html#interpreting-the-rds-format",
    "href": "posts/2021-11-15_serialisation-with-rds/index.html#interpreting-the-rds-format",
    "title": "Data serialisation in R",
    "section": "Interpreting the RDS format",
    "text": "Interpreting the RDS format\nAll right, lets see if I can interpret the contents of an RDS file. Rather than tediously writing the file to disk using saveRDS() and then loading it again, I’ll cheat slightly and write a show_rds() function that serialises an object and prints the results directly to the R console:\n\nshow_rds <- function(object, header = TRUE) {\n  rds <- object |>\n    serialize(connection = NULL, ascii = TRUE) |>\n    rawToChar() |>\n    strsplit(split = \"\\n\") |>\n    unlist()\n  if(header == FALSE) rds <- rds[-(1:6)]\n  cat(rds, sep = \"\\n\")\n}\n\nJust to make sure it’s doing what it’s supposed to I’ll make sure it gives the output I’m expecting. Probably a good idea given how many times I’ve been surprised so far…\n\nshow_rds(object = c(10.1, 2.2, 94.3))\n\nA\n3\n262657\n197888\n5\nUTF-8\n14\n3\n10.1\n2.2\n94.3\n\n\nOkay, phew. That looks good.\nI guess my next task is to work out what all this output means. The last three lines are obvious: that’s the data! What about the line above the data? That line reads 3 and is followed by three data values. I wonder if that’s a coincidence? I’ll see what happens if I try to serialise just 2 numbers. Does that line change to 2?\n\nshow_rds(object = c(10.1, 2.2))\n\nA\n3\n262657\n197888\n5\nUTF-8\n14\n2\n10.1\n2.2\n\n\nYes. Yes it does. I am learning things.\nHere’s what I know so far:\nA\n3\n262402\n197888\n5\nUTF-8\n14\n3      # the object has length 3\n10.1   # first value is 10.1\n2.2    # second value is 2.2\n94.3   # third value is 94.3\nOkay, so what’s next? The 14 in the preceding line. What does that mean?\nI puzzled over this for a while, and ended up needing to consult an occult tome of dangerous lore – the R Internals Manual – to find a partial answer. On the very first page of the Infernals Manual there is a table listing the SEXPTYPE codes that R uses internally to specify what kind of entity is encoded by an R object. Here are a few of these SEXPTYPE codes:\n\n\n\nValue\nSEXPTYPE\nVariable type\n\n\n\n\n10\nLGLSXP\nlogical\n\n\n13\nINTSXP\ninteger\n\n\n14\nREALSXP\nnumeric\n\n\n16\nSTRSXP\ncharacter\n\n\n19\nVECSXP\nlist\n\n\n\nSo… when I serialise a plain numeric vector, the RDS file writes the number 14 to the file. In that case I will tentatively update my beliefs about the RDS file\nA\n3\n262402\n197888\n5\nUTF-8\n14     # the object is numeric\n3      # the object has length 3\n10.1   # first value is 10.1\n2.2    # second value is 2.2\n94.3   # third value is 94.3\n\nOh no dear. You have strayed so far from the light already. That 14 carries much more meaning than your fragile mind is prepared to handle. Soon you will know better. Soon you will unravel entirely. You can feel it coming, can’t you?\n\n\n\n\n\n\nImage by Roxy Aln Available by CC0 licence on unsplash.\n\n\n\n\n\nThe RDS header\nAt this point, I have annotated every part of the RDS file that corresponds to the actual object. Consulting the section of the Infernal Manual devoted to serialisation, I learn that the six lines at the beginning of the file are known as the RDS header. Reading further I learn that the first line specifies the encoding scheme (A for ASCII, X for binary big-endian). The second line specifies which version of the RDS file format is used. The third line indicates the version of R that wrote the file. Finally, the fourth line is the minimum version of R required to read the file.\nIf I annotate my RDS header to reflect this knowledge, I get this:\nA       # use ASCII encoding\n3       # use version 3 of the RDS format\n262402  # written with R version 4.1.2\n197888  # minimum R version that can read it is 3.5\n5\nUTF-8 \nI am confused. Where did those numbers come from? Why does version 4.1.2 correspond to the number 262402, and why does 3.5 get encoded as 197888? The Manual is silent, and my thoughts become bleak. Am I losing my mind? Is the answer obvious??? What mess have I gotten myself into?\nIn desperation, I look at the R source code which reveals unto me the magic formula:\n\nencode_r_version <- function(major, minor, patch) {\n  (major * 65536) + (minor * 256) + patch\n}\n\nYessss. This all makes sense now…\n\nencode_r_version(4, 1, 2)\nencode_r_version(3, 5, 0)\n\n[1] 262402\n[1] 197888\n\n\n…so much sense.\nWhat about the other two lines in the header? Prior to RDS version 3 – which was released in R version 3.5 – those two lines didn’t exist in the header. Those are now used to specify the “native encoding” of the file, according to the Manual.\n“But isn’t that ASCII????”, whispers a voice in my head. “Is that not what the A is for?”\nNot quite. The RDS file format isn’t restricted to ASCII characters. In the usual case, the RDS file can encode any UTF-8 character and the native encoding line reads UTF-8. There is another possibility though: the file may use the Latin-1 alphabet. Because of this, there is some ambiguity that needs to be resolved. The RDS file needs to indicate which character set is used for the encoding.\nMy annotated header now looks like this:\nA      # the file uses ASCII encoding\n3      # the file uses version 3 of the RDS format\n262402 # the file was written in R version 4.1.2\n197888 # the minimum R version that can read it is 3.5\n5\nUTF-8  # the file encodes UTF-8 characters not Latin-1\nOkay, that makes a certain kind of sense, but what’s the story behind that 5? What does that mean? What dark secret does it hide?\nIt took me so very long to figure this one out. As far as I can tell this line isn’t discussed in the R Internals Manual, but I worked it out by looking at the source code for serialize. That line reads 5 because it’s telling the parser that the string that follows on the next line (i.e., UTF-8) contains five characters. Presumably if I’d used Latin-1 encoding, the corresponding line would have been 7.\nThis is doing my head in, but I think I’m okay?\n\nAre you sure? Really? You don’t sound too certain\n\n\n\n\n\n\nImage by Liza Polyanskaya. Available by CC0 licence on unsplash.\n\n\n\n\n\n\nLogical, integer, and numeric vectors\nNow that I have a sense of how the RDS header works, I’ll set header = FALSE whenever I call show_rds() from now on. That way I won’t have to look at that same six lines of output over and over and they will no longer haunt my dreams.\n\nOh no my dear. Hiding won’t save you.\n\nI think the time has come to look at how RDS encodes other kinds of data. For three of the four commonly used atomic vector types (logical, integer, and numeric), the RDS format looks exactly as I expected given what I learned earlier. As shown in the table above, the SEXPTYPE code for a logical vector is 10, so a logical vector with four elements looks like this:\n\nshow_rds(\n  object = c(TRUE, TRUE, FALSE, NA), \n  header = FALSE\n)\n\n10\n4\n1\n1\n0\nNA\n\n\nTRUE values are represented by 1 in the RDS file, and FALSE values are represented by 0. Missing values are represented as NA.\nFor an integer vector, the output is again familiar. The SEXPTYPE here is 13, so a vector of four integer looks like this:\n\nshow_rds(\n  object = c(-10L, 20L, 30L, NA),\n  header = FALSE\n)\n\n13\n4\n-10\n20\n30\nNA\n\n\nNumeric vectors I’ve already seen. They have SEXPTYPE of 14, so a numeric vector of length 3 starts with 14 on the first line, 3 on the second line, and then the numbers themselves appear over the remaining three lines. However, there is a catch. There always is when dealing with real numbers. Numeric values are subject to the vagaries of floating point arithmetic when represented in memory, and the encoding is not exact. As a consequence, it is entirely possible that something like this happens:\n\nshow_rds(\n  object = c(10.3, 99.9, 100),\n  header = FALSE\n)\n\n14\n3\n10.3\n99.90000000000001\n100\n\n\nFloating point numbers always make my head hurt. It is best not to dwell too long upon them lest my grip on sanity loosen.\n\nToo late. Far, far too late.\n\n\n\n\n\n\nImage by Hoshino Ai. Available by CC0 licence on unsplash.\n\n\n\n\n\n\nCharacter vectors\nWhat about character vectors?\n\nAdorable that you think these will be safer waters in which to swim my dear. A wiser woman would turn back now and return to the shallows. Yet there you go, drifting out to sea. Fool.\n\n\nLet’s create a simple character vector. According to the table above, character vectors have SEXPTYPE 16, so I’d expect that a character vector with three elements would start with 16 on the first line and 3 on the second line, which would then be followed by the contents of each cell.\nAnd that’s… sort of true?\n\nshow_rds(\n  object = c(\"text\", \"is\", \"strange\"),\n  header = FALSE\n)\n\n16\n3\n262153\n4\ntext\n262153\n2\nis\n262153\n7\nstrange\n\n\nThe format of this output is roughly what I was expecting, except for the fact that each string occupies three lines. For instance, these three lines correspond to the word \"strange\":\n262153\n7\nstrange\nThis puzzled me at first. Eventually, I remembered that the source code for R is written in C, and C represents strings as an array. So where R treats the word \"strange\" a single object with length 1, C treats it as a string array containing 7 characters. In the R source code, the object encoding a string is called a CHARSXP. So lines two and three begin to make sense:\n262153\n7        # the string has \"length\" 7\nstrange  # the 7 characters in the string\nWhat about the first line? Given everything I’ve seen previously it’s pretty tempting to guess that it means something similar to the SEXPTYPE codes that we’ve seen earlier. Perhaps in the same way that numeric is SEXPTYPE 14 and logical is SEXPTYPE 10, maybe there’s some sense in which a single string has a “SEXPTYPE” of 262153? That can’t be right though. According to the R Internals Manual, a CHARSXP object has a SEXPTYPE code of 9, not 262153. I must be misunderstanding something? Why is it 262153?\n\nFrightened by the first wave, are you? All in good time my love. The secrets of 262153 will reveal themselves soon.\n\n\n\n\n\n\nImage by Tim Marshall Available by CC0 licence on unsplash.\n\n\n\n\n\n\nLists\nWhat about lists? Lists are more complicated than atomic vectors, because they’re just containers for other data structures that can have different lengths and types. As mentioned earlier, they have SEXPTYPE 19, so a list with three elements will of course start with 19 on the first line and 3 on the second line. Here’s an example:\n\nshow_rds(\n  object = list(\n    c(TRUE, FALSE), \n    10.2, \n    c(\"strange\", \"thing\")\n  ),\n  header = FALSE\n)\n\n19\n3\n10\n2\n1\n0\n14\n1\n10.2\n16\n2\n262153\n7\nstrange\n262153\n5\nthing\n\n\nThis output makes my brain hurt, but it does make sense if I stare at it long enough. It begins with the two lines specifying that it’s a list of length three. This is then followed by the RDS representation for the logical vector c(TRUE, FALSE), the RDS representation for the numeric vector 10.2, and finally the RDS representation for the character vector c(\"strange\", \"thing\").\nI have started using annotations and whitespace to make it clearer:\n19 # it's a list\n3  # of length 3\n\n  10  # list entry 1 is logical\n   2  # of length 2\n   \n    1       # value is TRUE\n    0       # value is FALSE\n      \n  14  # list entry 2 is numeric \n   1  # of length 1\n   \n    10.2    # value is 10.2\n    \n  16  # list entry 3 is character\n   2  # of length 2\n   \n    262153  # every string starts with this\n         7  # this string has 7 characters\n   strange  # values are: s, t, r, a, n, g, e\n   \n    262153  # every string starts with this\n         5  # this string has 5 characters\n     thing  # values are: t, h, i, n, g\nI feel so powerful! My mind is now afire with knowledge! All the secrets of RDS will be mine…\n\n…and the madness strikes at last. Pride comes before the fall, always.\n\n\n\n\n\n\nImage by Moreno Matković. Available by CC0 licence on unsplash.\n\n\n\n\n\n\nObject attributes\nOne of the key features of R is that vectors are permitted to have arbitrary metadata: names, classes, attributes. If an R object contains metadata, that metadata must be serialised too. That has some slightly surprising effects. Let’s start with this very simple numeric object with two elements:\n\nshow_rds(object = c(100, 200), header = FALSE)\n\n14\n2\n100\n200\n\n\nAs expected it has SEXPTYPE 14 (numeric), length 2, and the values it stores are 100 and 200. Nothing out of the ordinary here. But when I add a name to the object, the output is … complicated.\n\nshow_rds(object = c(a = 100, b = 200), header = FALSE)\n\n526\n2\n100\n200\n1026\n1\n262153\n5\nnames\n16\n2\n262153\n1\na\n262153\n1\nb\n254\n\n\nI … don’t know what I am looking at here. First off, I seem to be having the same problem I had with character strings. If I take the first line of this output at face value I would think that a named numeric vector has SEXPTYPE 526. That can’t be right, can it?\n\nIt isn’t. In the same way that strings don’t have a SEXPTYPE of 262153 (the actual number is 9), the 526 here is a little misleading. This is a numeric vector and like all numeric vectors it is SEXPTYPE 14. You will learn the error of your ways very soon.\n\n\nSetting that mystery aside, I notice that the RDS output is similar to the output we saw when converting a list to RDS. The output contains the numeric vector first (the data), which is then followed by a list that specifies the attributes linked to that object?\n\n Not quite. You’re so close, but it’s a pairlist, not a list. The underlying data structure is different. Don’t let it worry your mind, sweet thing. Preserve your mind for the trials still to come.\n\n\nFor this object, there’s only one attribute that needs to be stored, corresponding to the names associated with each element of the vector. If I annotate the output again, I get this:\n526     # Numeric vector \n2       # with two values\n\n   100     # value 1 \n   200     # value 2\n   \n1026    # Pairlist for attributes\n1       # with one pair of entries\n\n   262153  # The attribute is called \"names\"\n   5       # \n   names   # \n   \n   16      # The attribute has two values\n   2       # \n   \n      262153   # First value is \"a\"\n           1   #\n           a   # \n\n      262153   # Second value is \"b\"\n           1   #\n           b   #\n\n254   # end of pairlist\nThe 254 marking the end of the pairlist confused me for a little while, but it isn’t arbitrary. It represents a NULL value in the RDS format:\n\nshow_rds(NULL, header=FALSE)\n\n254\n\n\n\nYes, my dear. If you look at the relevant part of the R source code, you see that there are a collection of “administrative codes” that are used to denote special values in a SEXPTYPE-like fashion. NULL is the one you’d be most likely to encounter though. Perhaps best not to travel down that road tonight though? Wait until day. You’re getting tired.\n\n\n\n\n\n\nImage by Kelly Sikkema. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-15_serialisation-with-rds/index.html#typeflag-packing",
    "href": "posts/2021-11-15_serialisation-with-rds/index.html#typeflag-packing",
    "title": "Data serialisation in R",
    "section": "Type/flag packing",
    "text": "Type/flag packing\nThroughout this post, I’ve given the impression that when R serialises an object to RDS format, the first thing it writes is the SEXPTYPE of that object. Technically I wasn’t lying, but this is an oversimplificiation that hides something important. It’s time to unpack this, and to do that I’ll have to dive into the R source code…\n\nDecoding the SEXPTYPE\nAfter digging around in the source code I found the answer. What R actually does in that first entry is write a single integer, and packs multiple pieces of information into the bits that comprise that integer. Only the first eight bits are used to define the SEXPTYPE. Other bits are used as flags indicating other things. Earlier on, I said that a value of 526 actually corresponds to a SEXPTYPE of 14. That becomes clearer when we take a look at the binary representation of 14 and 526. The first eight bits are identical:\n\nintToBits(14)\n\n [1] 00 01 01 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n[26] 00 00 00 00 00 00 00\n\nintToBits(526)\n\n [1] 00 01 01 01 00 00 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n[26] 00 00 00 00 00 00 00\n\n\nTo extract the SEXPTYPE, what we want to do is ignore all the later bits. I could write a function that uses intToBits() to unpack an integer into its binary representation, then sets all the bits except the first eight to 0, and then converts back to an integer …but there’s no need. The thing I just described is a “bitwise AND” operation:\n\ndecode_sexptype <- function(x) bitwAnd(x, 255)\n\ndecode_sexptype(14)\n\n[1] 14\n\ndecode_sexptype(526)\n\n[1] 14\n\n\nWhen I said that those 262153 values we encounter every time a string is serialised actually correspond to a SEXPTYPE of 9, this is exactly what I was talking about:\n\ndecode_sexptype(262153)\n\n[1] 9\n\n\nThe attributes pairlist, which gave us a value of 1026 when the RDS is printed out as text?\n\ndecode_sexptype(1026)\n\n[1] 2\n\n\nThose are SEXPTYPE 2, and if we check the R internals manual again, we see that this is indeed the code for a pairlist.\nI feel triumphant, but broken.\n\nGirl, same.\n\n\n\n\n\n\nImage by Aimee Vogelsang. Available by CC0 licence on unsplash.\n\n\n\n\n\n\nWhat’s in the other bits?\nI fear that my mind is lost, but in case anyone uncover these notes and read this far, I should document what I have learned about the contents of the other bits. There are a few different things in there. The two you’d most likely encounter are the object flag (bit 9) and the attributes flag (bit 10). For example, consider the data frame below:\n\ndata.frame(\n  a = 1, \n  b = 2\n)\n\n  a b\n1 1 2\n\n\nhas an integer code of 787. Data frames are just lists with additional metadata, so it’s not surprising that when we extract the SEXPTYPE we get a value of 19:\n\ndecode_sexptype(787)\n\n[1] 19\n\n\nBut data frames are also more than lists. They have an explicit S3 class (\"data.frame\") and they have other attributes too: \"names\" and \"row.names\". If we unpack the integer code 787 into its constituent bits we see that bit 9 and bit 10 are both set to 1:\n\nintToBits(787)\n\n [1] 01 01 00 00 01 00 00 00 01 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n[26] 00 00 00 00 00 00 00\n\n\nBit 9 is the “object flag”: it specifies whether or not the R data structure has a class attribute. Bit 10 is the more general one, and is called the “attribute flag”: it specifies whether or not the object has any attributes.\n\n\nOkay but what’s up with 262153?\nWho is asking me all these questions anyway?\nIt worries me that I’m now listening to the voices in my head, but okay fine. If we unpack the integer code 262153, we see that there’s something encoded in bit 19:\n\nintToBits(262153)\n\n [1] 01 00 00 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 00 00 00\n[26] 00 00 00 00 00 00 00\n\n\nI haven’t found the part of the source code that sets this bit yet, but I’m pretty sure that the role of this bit is to flag whether or not the string should be added to the global string pool. In recent versions of R that’s true for all strings, so in practice every string has an integer code of 262153 rather than 9.\n\n\n\n\n\nImage by Pelly Benassi. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-15_serialisation-with-rds/index.html#are-we-done-yet",
    "href": "posts/2021-11-15_serialisation-with-rds/index.html#are-we-done-yet",
    "title": "Data serialisation in R",
    "section": "Are we done yet?",
    "text": "Are we done yet?\nWell that depends on what you mean by asking the question. If you mean “have we described everything there is to know about the RDS format and how data serialisation works in base R?” then no, we’re absolutely not done. I haven’t said anything about how R serialises functions or expressions:\n\nexpr <- quote(sum(a, b, c))\nfn <- function(x) x + 1 \n\nThese are both R objects and you can save them to RDS files. So of course there’s a serialisation format for those but it’s not a lot of fun. I mean, if you squint at it you can kiiiiiinnnnda see what’s going on with the expression…\n\nshow_rds(expr, header = FALSE)\n\n6\n1\n262153\n3\nsum\n2\n1\n262153\n1\na\n2\n1\n262153\n1\nb\n2\n1\n262153\n1\nc\n254\n\n\n\n\n\n…but if I do the same thing to serialise the function it gets unpleasant. This has been quite an ordeal just getting this far, and I see no need to write about the serialisation of closures. Let someone else suffer through that, because my brain is a wreck.\nSo no, we are not “done”. The RDS format keeps some secrets still.\nBut if you mean “have we reached the point where the author is losing her mind and needs to rest?” then… oh my god yes I am utterly and completely done with this subject, and wish to spend the rest of my night sobbing quietly in darkness.\n\nLet us never speak of this again.\n\n\n\n\n\n\nImage by Andrey Zvyagintsev. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html",
    "href": "posts/2023-01-01_playing-with-docker/index.html",
    "title": "Playing with docker and the github container registry",
    "section": "",
    "text": "Docker docker docker baby. This is a post about docker, and on the off chance that you’ve been living under a rock for the last several years, docker1 allows you to run your code within a “container” that isolates it from other processes running on your machine. Containers are a bit like virtual machines, but smaller, more portable, and don’t require you to have a complete copy of a second operating system running on your machine. They’re… actually, you know what? Why don’t I quote the relevant paragraphs from the docker website:\nThey even have pretty pictures on the website. I thought about reproducing their figures for this blog post but why bother? If you want to look at their pictures you can go look at the website and in any case I think we can all agree that making these cute whale graphics with ggplot2 was a much better use of my time, yes?\nAnyway. I’ve been meaning to teach myself docker for a few years now. It’s one of those “things” that has this weird aura of being difficult when it… doesn’t seem to be all that difficult? For a long time I’ve had this feeling of dread or insecurity about it, thinking that it must be “too technical” for me.5 I have no doubt that the internals to docker are complicated, and there are subtleties to using docker well that will take a while to grasp, but when I managed to set aside my fears and read the documentation it turned out that the basics were surprisingly easy."
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html#installing-docker",
    "href": "posts/2023-01-01_playing-with-docker/index.html#installing-docker",
    "title": "Playing with docker and the github container registry",
    "section": "Installing docker",
    "text": "Installing docker\nThe installation guides on the docker website are good, and have information for various operating systems. I’m doing this on my ubuntu laptop6 so I followed the ubuntu install guide. I also went a little further and followed the post-install instructions for linux so that I could run docker commands without requiring superuser privileges: that’s the reason you won’t see any sudo commands in this post. Obviously, that’s something that will be a bit different on different operating systems and I’m not trying to write a tutorial here, but if you are using this post as a resource you can check that everything is working on your own installation by running this command:\n\ndocker run hello-world\n\n\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\n2db29710123e: Pull complete \nDigest: sha256:c77be1d3a47d0caf71a82dd893ee61ce01f32fc758031a6ec4cf1389248bb833\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://hub.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/get-started/\n\nOkay that looks good. Docker7 seems to be running on my machine. As an aside, as long as you are online you don’t need to have a copy hello-world itself for this to work: docker will download it for you when you run the command."
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html#terminology",
    "href": "posts/2023-01-01_playing-with-docker/index.html#terminology",
    "title": "Playing with docker and the github container registry",
    "section": "Terminology",
    "text": "Terminology\nBefore diving in and using docker, it helps to disambiguate three terms:\n\nContainer. A container is an executable. It runs on your machine isolated from other processes, has a namespace on the kernel, etc. Setting the particulars aside, it is a computing environment.\nImage. An image is a read-only template that contains the instruction to build a container. It’s a “snapshot” of a computing environment, constructed from one or more “layers” of build steps. Images are binaries that are stored locally and hosted on various registries. More on that later!\nDockerfile. Finally, there’s the dockerfile.8 That’s a plain text file that you as the user write. It contains the instructions for how to construct an image. They supply, in a (very!) abstract sense, the source code for an image.\n\nSo it works like this. You use a dockerfile to build an image, the image contains the instructions to run a container, and the corresponding commands are quite sensibly called docker build and docker run. Or if you like diagrams with labelled arrows…\n\\[\n\\mbox{dockerfile} \\xrightarrow{\\mbox{build}} \\mbox{image} \\xrightarrow{\\mbox{run}} \\mbox{container}\n\\]\nAt any point you can get a summary of the images on your system by running docker image list. If you’re doing this with a fresh installation and you run the command after running the “hello world” example above,9 you’d get output that looks like this:\n\ndocker image list\n\nREPOSITORY    TAG       IMAGE ID       CREATED         SIZE\nhello-world   latest    feb5d9fea6a5   15 months ago   13.3kB\nYou can do the same thing for containers with docker container ls,10 which by default will show you currently-running containers. To see all containers, running or not, add the --all parameter:\n\ndocker container ls --all\n\n\nCONTAINER ID   IMAGE         COMMAND    CREATED         STATUS                     PORTS     NAMES\nefcf7186776f   hello-world   \"/hello\"   6 minutes ago   Exited (0) 6 minutes ago             bold_davinci\n\nNotice the difference in the “CREATED” time! The image for hello-world is something that someone else created 15 months ago and kindly placed online so I could pull it onto my machine without building it myself. The container is the executable that I created from that image a mere 6 minutes ago when I called docker run. They’re both currently on my laptop, but they are quite different things.\nAh, but I am rambling again, aren’t I? Sorry. Shall we have a go at this then?\n\n\n\n\n\n\nThis was my first attempt at plotting something that looks a bit like the docker whale. It’s nothing fancy: I created a data frame with coordinates corresponding to a circle and then distorted it in two different ways. One distortion produces the whale body, another makes the tail. They are rendered in ggplot2 with geom_polygon(). Later in the process I tweaked the tail a bit."
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html#motivating-problem",
    "href": "posts/2023-01-01_playing-with-docker/index.html#motivating-problem",
    "title": "Playing with docker and the github container registry",
    "section": "Motivating problem",
    "text": "Motivating problem\nIn my last post I mentioned that, btw I use arch now.11 Well. Sort of. A more accurate statement would be to say that I installed arch linux on a secondary laptop as something to play with and I’m still using ubuntu for my day to day coding. At the moment I’m still getting used to the quirks of arch and encountering odd behaviour when – for example – one of my scripts that ran perfectly well on my ubuntu machine caused RStudio to crash when I ran it on the arch box. The “it works on my machine” problem strikes again… sigh.\nIn an effort to isolate the problem I started reran the unit tests for the package that I thought might be responsible for the crash and they all passed on both machines, but since that package is my queue package and the unit test aren’t as comprehensive as I’d like I would not be at all surprised if there’s an exotic bug that makes it fail only on arch.\nAll this made me think a little about how I typically use CI.12 Like many R developers I’ll use github actions to run my unit tests on mac os, ubuntu, and windows. I’ll run the tests with multiple versions of R including R-devel. If I’m thinking about a CRAN submission I’ll expand the scope and run my tests using other services also.\nI’ve never tested on arch though.\nI’ve never tested on arch because I’ve never had an arch machine to test on before. Or… [docker enters from stage left]… I’ve never had an arch image that I can use to containerise my unit tests before…\nOoh… a side project! Why don’t I try creating some docker images with R running on arch linux? In other words, why don’t I do a really lazy, half-arsed version of the thing that the rocker project has already done to an extremely high standard with ubuntu and debian… except with arch?13\n\n\n\n\n\n\nAdding the boxes was conceptually easy: the expand_grid() function from tidyr creates the necessary data structure, and geom_tile() plots it. One thing I really like about this iteration is that the spacing of the boxes creates a Hermann grid illusion. It’s not as cool as the scintillating grid version, but I used to teach it in introductory cognitive science classes and I have a soft spot for it."
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html#minimal-example",
    "href": "posts/2023-01-01_playing-with-docker/index.html#minimal-example",
    "title": "Playing with docker and the github container registry",
    "section": "Minimal example",
    "text": "Minimal example\nSometimes the easiest way to tell a story is to begin at the ending, and – spoiler! – I did in fact succeed in my attempt,14 and I am now the proud15 maintainer of two hastily-constructed images hosted on the github container repository. Now that I have these things, it should be really easy for us to put together a simple project that will run R code using these images and – even though I’m going to be using my ubuntu laptop – have it be executed by a container that is running arch.\nOh. The. Thrill.\nBe. Still. My. Beating. Heart.\nOkay, so here it is. Accompanying this post is a project called system-check that consists of a three-line dockerfile and a two-line R script. Let’s ignore the dockerfile for a moment and focus on the R code. Here’s the script:\n\n\n\n./system-check/script.R\n\ncat(c(\"Running on:\", osVersion), sep = \"\\n  \")\ncat(c(\"With locale:\", strsplit(Sys.getlocale(), \";\")[[1]]), sep = \"\\n  \")\n\n\nIf we ignore the parts the code dedicated to making the output pretty, we can see that all it’s doing is printing the osVersion and calling Sys.getlocale(). Here’s what happens when I run the script on my ubuntu laptop, without using docker in any way:\n\nRscript ./system-check/script.R\n\nRunning on:\n  Ubuntu 22.04.1 LTS\nWith locale:\n  LC_CTYPE=en_AU.UTF-8\n  LC_NUMERIC=C\n  LC_TIME=en_AU.UTF-8\n  LC_COLLATE=en_AU.UTF-8\n  LC_MONETARY=en_AU.UTF-8\n  LC_MESSAGES=en_AU.UTF-8\n  LC_PAPER=en_AU.UTF-8\n  LC_NAME=C\n  LC_ADDRESS=C\n  LC_TELEPHONE=C\n  LC_MEASUREMENT=en_AU.UTF-8\n  LC_IDENTIFICATION=C\nThe first part of the output tells me my operating system (ubuntu), and the second part specifies the locale. I’m in Australia so for most things my locale is en_AU.UTF-8. That makes sense, but of course this output is specific to my machine: an arch user running R in the United States should expect to see something very different.\nThat’s where docker comes in.\nThe docker images that I built and am hosting on github simulate exactly that. The computing environments specified by the arch-r-base and arch-r-test images use arch linux as the operating system and have the system locale set to en_US.UTF-8. So if I were to execute this script from within a container running the arch-r-base16 image, I should expect to see different results even though my laptop is running ubuntu and my system locale is en_AU.UTF-8.\nHere’s a dockerfile specifying an image that does exactly that:\n\n\n\n./system-check/Dockerfile\n\nFROM ghcr.io/djnavarro/arch-r-base:release\nCOPY script.R /home/script.R\nCMD Rscript /home/script.R\n\n\nIt’s a sequence of three docker instructions.\n\nLike all dockerfiles, it begins with a FROM17 18 instruction that specifies the name of a preexisting docker image to use as a starting point. I’ve been very explicit here and referenced the image using a fully qualified name that consists of a container repository (ghcr.io), a username (djnavarro), the image name arch-r-base, and an optional tag (release). You don’t always need to be that precise, especially if you’re using an image that you know exists locally.\nThe second step is a COPY instruction that copies the R script to a specific file path within the image. This takes place at build time. This step is necessary because when the container starts up it will be isolated from other processes on the system. It doesn’t have access to the host file system. If you want the container to have access to a file you need to copy it at build time.19\nThe third step is a CMD instruction. Every dockerfile must have a CMD instruction (and much like highlanders there can be only one) specifying a default for what the container should do when it is launched.20\n\nLater on, when you’re starting to feel comfortable with the basic idea of writing dockerfiles, its worth reading the official guide on dockerfile best practices. Lots of little things started to make sense to me when I did that. For now, let’s just acknowledged that yes Virginia we have a dockerfile.\n\n\n\n\n\n\n\n\n\n\n\nI created the random container stacks using dplyr. The boxes are grouped by column (using their x-coordinate), a random height is generated for that column, and rows in the data frame corresponding to boxes above that height are filtered out of the data set before it is passed to ggplot2.\n\n\n\n\nBuilding the image\nOur next step is to build it to an image. The way we do that from the terminal is with the docker build command. For the purposes of this post – which I am writing in quarto and thus has a code execution engine blah blah blah – I am going to assume21 that the working directory is set to the folder containing the post, and that it contains a subfolder called system-check in which the dockerfile and the R script are stored. In other words, system-check is the directory holding the docker project.\nThe simplest way to build an image from this project is like this:\n\ndocker build system-check\n\nThis command tells docker to look for a dockerfile in the system-check folder, and make an image using whatever it finds there. That’s a perfectly fine way to do it, but my personal preference is to give the resulting image a name, using the --tag flag. So the command, which I’ve broken over a few lines to highlight its structure, now looks like this:\n\ndocker build \\\n  --tag my-system-check \\\n  system-check\n\nThe reason I’ve done this is that later on when I call the docker run command I can refer to the image by name, which does make life simpler. Under normal circumstances I’d probably have called the image system-check rather than my-system-check (why create new names when I don’t need to?) but for the purposes of this post I think it’s helpful to be clear that when I refer to the image name I’m referring to the thing I created using --tag, not the name of the folder that holds the dockerfile!\nOkay, enough talk. Let’s run it this time:\n\ndocker build \\\n  --tag my-system-check \\\n  system-check\n\n\nSending build context to Docker daemon  3.072kB\nStep 1/3 : FROM ghcr.io/djnavarro/arch-r-base:release\nrelease: Pulling from djnavarro/arch-r-base\n597018910566: Pull complete \n8150bcc6bc64: Pull complete \ne49e8a34689c: Pull complete \nc14eff78251d: Pull complete \n42b358854199: Pull complete \nbabcc0d99cfd: Pull complete \nDigest: sha256:f9ff0f7b431ed1b975823c871949ccbc15c3e3d7dce23775f793f9f64bb2779e\nStatus: Downloaded newer image for ghcr.io/djnavarro/arch-r-base:release\n ---> 0a9929e54a6b\nStep 2/3 : COPY script.R /home/script.R\n ---> b9913096b118\nStep 3/3 : CMD Rscript /home/script.R\n ---> Running in 1314ee0ff2fb\nRemoving intermediate container 1314ee0ff2fb\n ---> 489003ffb5d0\nSuccessfully built 489003ffb5d0\nSuccessfully tagged my-system-check:latest\n\nThe output here shows you that the build process unfolds as a sequence of three steps: one for each of our docker instructions. It also gives you the impression (correctly!) that the first step is considerably more complex than the other two. That makes sense: the arch-r-base image is itself constructed from a sequence of steps, and those steps have produced an image that is built from several “layers”. Each of those hexadecimal hashes refers to one of the layers.22\nWhen you run this on your own system you’ll see little progress bars as the different layers of the image are downloaded. For example, that line that says 597018910566: Pull complete? That’s referring to the very first layer in the arch-r-base image (which is arch linux itself) and that layer is about 280MB or something like that, so you get a little progress bar to let you know how its going. That’s super helpful if you ever find yourself using the arch-r-test image, because one of the layers in that image includes a texlive installation (ugh) so that layer is (I’m so sorry) about 2GB in size.\nDownloading large images is a huge pain, and generally I would try to avoid creating an image with a layer that large. Thankfully, docker is smart enough to check the local cache before trying to download anything.23 We can see this in action if we repeat the exact same command:\n\ndocker build \\\n  --tag my-system-check \\\n  system-check\n\nSending build context to Docker daemon  3.072kB\nStep 1/3 : FROM ghcr.io/djnavarro/arch-r-base:release\n ---> 0a9929e54a6b\nStep 2/3 : COPY script.R /home/script.R\n ---> Using cache\n ---> b9913096b118\nStep 3/3 : CMD Rscript /home/script.R\n ---> Using cache\n ---> 489003ffb5d0\nSuccessfully built 489003ffb5d0\nSuccessfully tagged my-system-check:latest\nThis finishes instantaneously because docker24 25 notices that I already have a copy of this image so it uses the cache for everything.\nWe can confirm that this has worked by running docker image list:\n\ndocker image list\n\n\nREPOSITORY                      TAG       IMAGE ID       CREATED          SIZE\nmy-system-check                 latest    489003ffb5d0   26 minutes ago   955MB\nghcr.io/djnavarro/arch-r-base   release   0a9929e54a6b   13 hours ago     955MB\nhello-world                     latest    feb5d9fea6a5   15 months ago    13.3kB\n\nNow, you might be wondering about those image sizes. Did I really just create two 955MB images? That seems a bit much. It’s certainly true that the image is 955MB in size: after all, the image does have to describe an entire operating system running R, so it’s not surprising that it isn’t tiny. But it looks as if I just wasted an entire GB of space by making two of them. Thankfully, docker is not that silly. The my-system-check image is almost identical to arch-r-base. In fact, it’s just one very small layer added on top of the layers that comprise the arch-r-base image. If you dig into the documentation on storage you discover that docker quite sensibly allows images to share layers, so even though arch-r-base and my-system-check are individually 955MB in size, they are also collectively 955MB in size thanks to layer sharing.\nThe sheer excitement of working with computers is just too much for me to bear sometimes.\n\n\nRun in a container\nOkay, we are ready to go baby! The image is set up, and all we have to do is run it in a container using docker run. The docker run command is quite powerful, and has a lot of arguments you can use to control how the image executes.26 I’m not going to use any of that flexibility here. This is just a vanilla command asking docker to run the my-system-check image:\n\ndocker run my-system-check\n\nRunning on:\n  Arch Linux\nWith locale:\n  LC_CTYPE=en_US.UTF-8\n  LC_NUMERIC=C\n  LC_TIME=en_US.UTF-8\n  LC_COLLATE=en_US.UTF-8\n  LC_MONETARY=en_US.UTF-8\n  LC_MESSAGES=en_US.UTF-8\n  LC_PAPER=en_US.UTF-8\n  LC_NAME=C\n  LC_ADDRESS=C\n  LC_TELEPHONE=C\n  LC_MEASUREMENT=en_US.UTF-8\n  LC_IDENTIFICATION=C\nIt’s an awfully elaborate way to say “btw I use arch”, but yes… the image does what we hoped it would. It’s executed the R script on arch linux with a en_US.UTF-8 locale. I have successfully faked it27 as an arch user."
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html#fancier-example",
    "href": "posts/2023-01-01_playing-with-docker/index.html#fancier-example",
    "title": "Playing with docker and the github container registry",
    "section": "Fancier example",
    "text": "Fancier example\nFor the next example I’ll add a little bit of extra complexity. The real reason I wanted the arch-r images in the first place was to make it easier to run unit tests for an R package on a system running arch linux. If I were going to do this properly I’d set it up in a way that could be incorporated into a CI workflow with github actions, but I’m not going to be that fancy for this blog post. Instead, I’ll set it up so that I can generate containers running arch linux that can clone a package repository from github into the container, and then run the unit tests. I’ll even give it a bit of flexibility so that the user can decide at build time28 which github repository the container points to.\nAs before the project – which I’ve called test-on-arch – consists of two files. There’s an R script that executes at run time, and the dockerfile executed at build time. Here they are:\n\n\n\n./test-on-arch/Dockerfile\n\nFROM ghcr.io/djnavarro/arch-r-test:release\n\n# copy the testing script\nCOPY clone-and-check.R /home/clone-and-check.R\n\n# pass args through environment variables\nARG user\nARG repo\nARG cran=https://cloud.r-project.org\nENV user=$user\nENV repo=$repo\nENV cran=$cran\n\n# run the testing script\nCMD Rscript /home/clone-and-check.R\n\n\n\n\n\n./test-on-arch/clone-and-check.R\n\n# get the system environment variables\nuser <- Sys.getenv(\"user\")\nrepo <- Sys.getenv(\"repo\")\ncran <- Sys.getenv(\"cran\")\n\n# define github url and a path for the local package install\nurl <- paste(\"https://github.com\", user, repo, sep = \"/\")\ndir <- paste(\"/home/project\", repo, sep=\"/\")\n\n# clone repo, install dependencies, and run checks\ngert::git_clone(url, dir, verbose = TRUE)\nremotes::install_deps(dir, dependencies = TRUE, repos = cran)\nrcmdcheck::rcmdcheck(dir)\n\n\nLike last time, the place to start is with the R script. It expects to find user, repo, and cran values as environment variables. Once it finds those, it clones the user/repo repository from github, installs any dependencies of the package from cran, and then uses rcmdcheck to check the downloaded package.\nNow let’s look at how the dockerfile sets up the computing environment to enable this script to be run on arch linux:\n\nJust like we saw in the last example, the dockerfile begins with a FROM instruction. This time around though I’m using the arch-r-test image rather than the arch-r-base image. Much like the base image, the test image runs arch linux and installs R in the environment. However, it also installs several other system dependencies and R packages that come in handy when running R CMD check, which makes it a bit more useful in this context.\nThe next step in the dockerfile is the COPY instruction that ensures that the image has a copy of the R script. There’s nothing new here so we can move on.\nThe next two steps use the ARG instruction. This is a new one for us: it’s a mechanism for allowing the user to specify arguments that will be passed to docker when building the image. That’s handy because it means I can customise the image that gets built. The obvious use here is that I can specify the user and the repo for the package that I want to check! (Later on we’ll see how this is done using the --build-arg argument to docker build)\nNext up is another ARG step, used to specify the url for the cran repository that the container should use to download any R packages. Notice, however, that this time I’ve specified a default value, so you don’t actually have to specify cran when you call docker build: if you don’t it will just use the default url\nThe ARG steps pass the user input to docker, but they don’t set any environment variables (remember, our R script is expecting to find environment variables). That’s the job of the ENV instructions that appear in the next three steps.29\nFinally, we have the CMD instruction, which specifies a default action for the container: run the script.\n\n\n\n\n\n\n\nAfter a little bit of tinkering I decided to make the tail a little fatter and use theme_minimal() with a border added as a way of subtly communicating the fact that ggplot2 is doing the work. The grid lines are almost invisible in a single whale plot like this but become more prominent in the facetted plots where there are more of them.\n\n\n\n\nBuilding the image\nSetting aside the fact that our test-on-arch project has a lot of flaws and limitations, it will serve the purposes we need it to. Let’s say I want to create an image that will check the queue package hosted at github.com/djnavarro/queue. To do that I’ll need to set user=djnavarro and repo=queue when I build the image, which I can do with the --build-arg argument:\n\ndocker build \\\n  --tag test-queue \\\n  --build-arg user=djnavarro \\\n  --build-arg repo=queue \\\n  test-on-arch\n\nNotice that I’ve chosen to call this image test-queue. A nice thing about being able to name the images independently from the dockerfile is that it’s easy to create multiple images using the same dockerfile (just with different arguments) and give them meaningful names. And sure, this particular example is very silly because literally everything I’m doing here at the build stage could be done just as efficiently at the run stage. But whatever.\nLet’s see what happens when I try to execute this build command. The arch-r-test image is considerably larger than arch-r-base. This one isn’t a frugal image! It takes a while, so I’m going to go have a smoke while I wait30 but the nice thing is that if you’ve done it once you don’t have to do it again. Anyway…\n\ndocker build \\\n  --tag test-queue \\\n  --build-arg user=djnavarro \\\n  --build-arg repo=queue \\\n  test-on-arch\n\n\nSending build context to Docker daemon  3.072kB\nStep 1/9 : FROM ghcr.io/djnavarro/arch-r-test:release\nrelease: Pulling from djnavarro/arch-r-test\n597018910566: Already exists \n8150bcc6bc64: Already exists \n198fc6066fb9: Pull complete \nb1600153860f: Pull complete \ned6330815f89: Pull complete \nfb2d11f79510: Pull complete \nff05f09f5a58: Pull complete \n9abaa14ad138: Pull complete \nDigest: sha256:f4605c32e18168589bd32248f5af97f8f1b57bd4de5fa6e1b54e53db13ab9514\nStatus: Downloaded newer image for ghcr.io/djnavarro/arch-r-test:release\n ---> 4f873f316861\nStep 2/9 : COPY clone-and-check.R /home/clone-and-check.R\n ---> d7c276834cf8\nStep 3/9 : ARG user\n ---> Running in efeeb43f874d\nRemoving intermediate container efeeb43f874d\n ---> d5d055328ea4\nStep 4/9 : ARG repo\n ---> Running in 75f6d1ff1502\nRemoving intermediate container 75f6d1ff1502\n ---> 7edce4d95863\nStep 5/9 : ARG cran=https://cloud.r-project.org\n ---> Running in 3f620871b0d7\nRemoving intermediate container 3f620871b0d7\n ---> 51a7ec6700ba\nStep 6/9 : ENV user=$user\n ---> Running in c7a7811e374e\nRemoving intermediate container c7a7811e374e\n ---> b8e01e708a08\nStep 7/9 : ENV repo=$repo\n ---> Running in 2f01c723898c\nRemoving intermediate container 2f01c723898c\n ---> 0939221c1a35\nStep 8/9 : ENV cran=$cran\n ---> Running in 37399a0bbe70\nRemoving intermediate container 37399a0bbe70\n ---> ccba9748fdd2\nStep 9/9 : CMD Rscript /home/clone-and-check.R\n ---> Running in 5d3eb7184e21\nRemoving intermediate container 5d3eb7184e21\n ---> 76926d5616d7\nSuccessfully built 76926d5616d7\nSuccessfully tagged test-queue:latest\n\nNotice that during the first step when downloading arch-r-test, I didn’t have to download the whole thing. Two of the layers in arch-r-test are shared with the arch-r-base image, and docker is smart enough to notice that I already have those layers in my cache. That’s what the Already exists part of the output indicates. Admittedly it doesn’t save us much in this case because its the texlive installation that causes pain, but it’s a nice feature nevertheless.\nAs a little sanity check – because, dear reader, I have been sitting here waiting very patiently while a large image downloaded over a slow connection and would like to confirm that I don’t have to do that again – let’s repeat the exercise from earlier and try building it a second time just to reassure ourselves that the cache is doing its job:\n\ndocker build \\\n  --tag test-queue \\\n  --build-arg user=djnavarro \\\n  --build-arg repo=queue \\\n  test-on-arch \n\nSending build context to Docker daemon  3.072kB\nStep 1/9 : FROM ghcr.io/djnavarro/arch-r-test:release\n ---> 4f873f316861\nStep 2/9 : COPY clone-and-check.R /home/clone-and-check.R\n ---> Using cache\n ---> d7c276834cf8\nStep 3/9 : ARG user\n ---> Using cache\n ---> d5d055328ea4\nStep 4/9 : ARG repo\n ---> Using cache\n ---> 7edce4d95863\nStep 5/9 : ARG cran=https://cloud.r-project.org\n ---> Using cache\n ---> 51a7ec6700ba\nStep 6/9 : ENV user=$user\n ---> Using cache\n ---> b8e01e708a08\nStep 7/9 : ENV repo=$repo\n ---> Using cache\n ---> 0939221c1a35\nStep 8/9 : ENV cran=$cran\n ---> Using cache\n ---> ccba9748fdd2\nStep 9/9 : CMD Rscript /home/clone-and-check.R\n ---> Using cache\n ---> 76926d5616d7\nSuccessfully built 76926d5616d7\nSuccessfully tagged test-queue:latest\nNot going to lie, I breathed a little sigh of relief. Docker used the cached layers, and that all happened instantaneously. Okay cool. I’m going to stop doing these checks from now on, but one last time let’s take a peek at the list of images I have stored locally:\n\ndocker image list\n\n\nREPOSITORY                      TAG       IMAGE ID       CREATED              SIZE\ntest-queue                      latest    76926d5616d7   About a minute ago   4.99GB\nmy-system-check                 latest    b7426ffb1484   12 minutes ago       955MB\nghcr.io/djnavarro/arch-r-test   release   4f873f316861   17 hours ago         4.99GB\nghcr.io/djnavarro/arch-r-base   release   0a9929e54a6b   17 hours ago         955MB\nhello-world                     latest    feb5d9fea6a5   15 months ago        13.3kB\n\n\n\nRun in a container\nOkay where were we? Ah yes, we’ve built our image so now it’s time to run it. Does my little queue package build cleanly and pass its unit tests on arch? Let’s find out…\n\ndocker run test-queue\n\n\nTransferred 766 of 766 objects...done!\nChecked out 34 of 34 commits... done!\n\n── R CMD build ─────────────────────────────────────────────────────────────────\n* checking for file ‘.../DESCRIPTION’ ... OK\n* preparing ‘queue’:\n* checking DESCRIPTION meta-information ... OK\n* installing the package to build vignettes\n* creating vignettes ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\n* building ‘queue_0.0.2.tar.gz’\n\n── R CMD check ─────────────────────────────────────────────────────────────────\n* using log directory ‘/tmp/Rtmp1Nld2I/file131069108/queue.Rcheck’\n* using R version 4.2.2 (2022-10-31)\n* using platform: x86_64-pc-linux-gnu (64-bit)\n* using session charset: UTF-8\n* checking for file ‘queue/DESCRIPTION’ ... OK\n* this is package ‘queue’ version ‘0.0.2’\n* package encoding: UTF-8\n* checking package namespace information ... OK\n* checking package dependencies ... OK\n* checking if this is a source package ... OK\n* checking if there is a namespace ... OK\n* checking for executable files ... OK\n* checking for hidden files and directories ... OK\n* checking for portable file names ... OK\n* checking for sufficient/correct file permissions ... OK\n* checking whether package ‘queue’ can be installed ... OK\n* checking installed package size ... OK\n* checking package directory ... OK\n* checking ‘build’ directory ... OK\n* checking DESCRIPTION meta-information ... OK\n* checking top-level files ... OK\n* checking for left-over files ... OK\n* checking index information ... OK\n* checking package subdirectories ... OK\n* checking R files for non-ASCII characters ... OK\n* checking R files for syntax errors ... OK\n* checking whether the package can be loaded ... OK\n* checking whether the package can be loaded with stated dependencies ... OK\n* checking whether the package can be unloaded cleanly ... OK\n* checking whether the namespace can be loaded with stated dependencies ... OK\n* checking whether the namespace can be unloaded cleanly ... OK\n* checking loading without being on the library search path ... OK\n* checking dependencies in R code ... NOTE\nNamespaces in Imports field not imported from:\n  ‘callr’ ‘cli’ ‘R6’ ‘tibble’\n  All declared Imports should be used.\n* checking S3 generic/method consistency ... OK\n* checking replacement functions ... OK\n* checking foreign function calls ... OK\n* checking R code for possible problems ... OK\n* checking Rd files ... OK\n* checking Rd metadata ... OK\n* checking Rd cross-references ... OK\n* checking for missing documentation entries ... OK\n* checking for code/documentation mismatches ... OK\n* checking Rd \\usage sections ... OK\n* checking Rd contents ... OK\n* checking for unstated dependencies in examples ... OK\n* checking installed files from ‘inst/doc’ ... OK\n* checking files in ‘vignettes’ ... OK\n* checking examples ... OK\n* checking for unstated dependencies in ‘tests’ ... OK\n* checking tests ...\n  Running ‘testthat.R’\n OK\n* checking for unstated dependencies in vignettes ... OK\n* checking package vignettes in ‘inst/doc’ ... OK\n* checking running R code from vignettes ...\n  ‘queue.Rmd’ using ‘UTF-8’... OK\n NONE\n* checking re-building of vignette outputs ... OK\n* checking PDF version of manual ... OK\n* DONE\n\nStatus: 1 NOTE\nSee\n  ‘/tmp/Rtmp1Nld2I/file131069108/queue.Rcheck/00check.log’\nfor details.\nSystem has not been booted with systemd as init system (PID 1). Can't operate.\nFailed to connect to bus: Host is down\nWarning: Your system is mis-configured: ‘/var/db/timezone/localtime’ is not a symlink\nWarning: ‘/var/db/timezone/localtime’ is not identical to any known timezone file\nWarning message:\nIn system(\"timedatectl\", intern = TRUE) :\n  running command 'timedatectl' had status 1\n── R CMD check results ──────────────────────────────────────── queue 0.0.2 ────\nDuration: 38.5s\n\n❯ checking dependencies in R code ... NOTE\n  Namespaces in Imports field not imported from:\n    ‘callr’ ‘cli’ ‘R6’ ‘tibble’\n    All declared Imports should be used.\n\n0 errors ✔ | 0 warnings ✔ | 1 note ✖\n\nOkay yes, this is the expected result. That note would of course get me in trouble on CRAN, but it’s what I was expecting to see: I get the same note on ubuntu. I just haven’t gotten around to fixing it yet. The only part that is different to what I see on ubuntu is this:\n\nSystem has not been booted with systemd as init system (PID 1). Can't operate.\nFailed to connect to bus: Host is down\nWarning: Your system is mis-configured: ‘/var/db/timezone/localtime’ is not a symlink\nWarning: ‘/var/db/timezone/localtime’ is not identical to any known timezone file\nWarning message:\nIn system(\"timedatectl\", intern = TRUE) :\n  running command 'timedatectl' had status 1\n\nYeah. This is interesting. I deliberately didn’t try to faff about with systemd in these images, so this is an expected warning. It’s not a problem with queue or with arch, just a consequence of how I built the images. That would have some consequences for testing a lot of packages, but I’m not trying to recreate the rocker project here so I’m not too fussed about it in this little exercise.\n\n\n\n\n\n\n\n\n\n\n\nThe colour scheme is sampled using ggthemes::canva_palettes, picking one of the ones that provides a blue/green palette.\n\n\n\n\n\nTwo images, one dockerfile\nThe advantage to passing arguments is that you can build many images from the same dockerfile, and docker will reuse the cached layers intelligently. We’ve seen this already, but here’s another example. Let’s try using the test-on-arch dockerfile to build an image that checks the praise package. Up to this point I’ve never tried testing the praise package on arch before, but (of course????) this builds immediately and without downloading anything, because everything that actually matters about this build was already done when I built the test-queue image earlier:\n\ndocker build \\\n  --tag test-praise \\\n  --build-arg user=rladies \\\n  --build-arg repo=praise \\\n  test-on-arch \n\nSending build context to Docker daemon  3.072kB\nStep 1/9 : FROM ghcr.io/djnavarro/arch-r-test:release\n ---> 4f873f316861\nStep 2/9 : COPY clone-and-check.R /home/clone-and-check.R\n ---> Using cache\n ---> d7c276834cf8\nStep 3/9 : ARG user\n ---> Using cache\n ---> d5d055328ea4\nStep 4/9 : ARG repo\n ---> Using cache\n ---> 7edce4d95863\nStep 5/9 : ARG cran=https://cloud.r-project.org\n ---> Using cache\n ---> 51a7ec6700ba\nStep 6/9 : ENV user=$user\n ---> Running in 3a9b1843d5b4\nRemoving intermediate container 3a9b1843d5b4\n ---> aa2578d71155\nStep 7/9 : ENV repo=$repo\n ---> Running in 1d15632dd6ca\nRemoving intermediate container 1d15632dd6ca\n ---> 057a61970d7c\nStep 8/9 : ENV cran=$cran\n ---> Running in e5586a32b05a\nRemoving intermediate container e5586a32b05a\n ---> 48852232e4b7\nStep 9/9 : CMD Rscript /home/clone-and-check.R\n ---> Running in 0fb9a526210c\nRemoving intermediate container 0fb9a526210c\n ---> a02feea26152\nSuccessfully built a02feea26152\nSuccessfully tagged test-praise:latest\nOnce again, we can take a look at the list of images:\n\ndocker image list\n\n\nREPOSITORY                      TAG       IMAGE ID       CREATED          SIZE\ntest-praise                     latest    a02feea26152   20 seconds ago   4.99GB\ntest-queue                      latest    76926d5616d7   4 minutes ago    4.99GB\nmy-system-check                 latest    b7426ffb1484   14 minutes ago   955MB\nghcr.io/djnavarro/arch-r-test   release   4f873f316861   17 hours ago     4.99GB\nghcr.io/djnavarro/arch-r-base   release   0a9929e54a6b   17 hours ago     955MB\nhello-world                     latest    feb5d9fea6a5   15 months ago    13.3kB\n\nAgain note the value of layer sharing. If these were all independent images we’d be looking at 17GB on disk. In fact, because arch-r-test reuses the layers from arch-r-base and all the other images are trivial additions to one of these two images, the total size of all these images is in fact “only” 5GB… i.e., the size of the arch-r-test image. And again, the only reason that one is so big is that I was really fussy about tex installations and bundled an entire texlive distribution with extra fonts and everything because I have no desire deal with tests whining about missing tex stuff.\nAnyway, let’s get back on track and run the test-praise image in a container:\n\ndocker run test-praise\n\n\nTransferred 431 of 431 objects...done!\nChecked out 26 of 26 commits... done!\n\n── R CMD build ─────────────────────────────────────────────────────────────────\n* checking for file ‘.../DESCRIPTION’ ... OK\n* preparing ‘praise’:\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\nOmitted ‘LazyData’ from DESCRIPTION\n* building ‘praise_1.0.0.tar.gz’\n\n── R CMD check ─────────────────────────────────────────────────────────────────\n* using log directory ‘/tmp/Rtmpi7Ngun/file12ad64a83/praise.Rcheck’\n* using R version 4.2.2 (2022-10-31)\n* using platform: x86_64-pc-linux-gnu (64-bit)\n* using session charset: UTF-8\n* checking for file ‘praise/DESCRIPTION’ ... OK\n* this is package ‘praise’ version ‘1.0.0’\n* checking package namespace information ... OK\n* checking package dependencies ... OK\n* checking if this is a source package ... OK\n* checking if there is a namespace ... OK\n* checking for executable files ... OK\n* checking for hidden files and directories ... OK\n* checking for portable file names ... OK\n* checking for sufficient/correct file permissions ... OK\n* checking whether package ‘praise’ can be installed ... OK\n* checking installed package size ... OK\n* checking package directory ... OK\n* checking DESCRIPTION meta-information ... OK\n* checking top-level files ... OK\n* checking for left-over files ... OK\n* checking index information ... OK\n* checking package subdirectories ... OK\n* checking R files for non-ASCII characters ... OK\n* checking R files for syntax errors ... OK\n* checking whether the package can be loaded ... OK\n* checking whether the package can be loaded with stated dependencies ... OK\n* checking whether the package can be unloaded cleanly ... OK\n* checking whether the namespace can be loaded with stated dependencies ... OK\n* checking whether the namespace can be unloaded cleanly ... OK\n* checking dependencies in R code ... OK\n* checking S3 generic/method consistency ... OK\n* checking replacement functions ... OK\n* checking foreign function calls ... OK\n* checking R code for possible problems ... OK\n* checking Rd files ... OK\n* checking Rd metadata ... OK\n* checking Rd cross-references ... OK\n* checking for missing documentation entries ... OK\n* checking for code/documentation mismatches ... OK\n* checking Rd \\usage sections ... OK\n* checking Rd contents ... OK\n* checking for unstated dependencies in examples ... OK\n* checking examples ... OK\n* checking for unstated dependencies in ‘tests’ ... OK\n* checking tests ...\n  Running ‘testthat.R’\n OK\n* checking PDF version of manual ... OK\n* DONE\n\nStatus: OK\n\nSystem has not been booted with systemd as init system (PID 1). Can't operate.\nFailed to connect to bus: Host is down\nWarning: Your system is mis-configured: ‘/var/db/timezone/localtime’ is not a symlink\nWarning: ‘/var/db/timezone/localtime’ is not identical to any known timezone file\nWarning message:\nIn system(\"timedatectl\", intern = TRUE) :\n  running command 'timedatectl' had status 1\n── R CMD check results ─────────────────────────────────────── praise 1.0.0 ────\nDuration: 25.1s\n\n0 errors ✔ | 0 warnings ✔ | 0 notes ✔\n\nOnce again we see the warning about systemd, and once again I am ignoring it. The thing that matters here, as far as I’m concerned, is that the unit tests for the praise package pass on arch.\n\n\nA small caution\nBefore we move onto the third project I want to talk about one more example using this one, as a way of cautioning anyone who might feel inclined to use it without fixing its many deficiencies. Let’s try using test-on-arch to run the unit tests for ggplot2, shall we? Unlike praise and queue, ggplot2 is a large and complicated package with substantial dependencies and a lot of unit tests. That’s going to be a problem given that test-on-arch clones the entire repository from scratch every time it’s called. Building the image is easy, because the build stage for test-on-arch doesn’t do anything except copy the script and pass a few arguments…\n\ndocker build \\\n  --tag test-ggplot2 \\\n  --build-arg user=tidyverse \\\n  --build-arg repo=ggplot2 \\\n  test-on-arch \n\nBut when we call docker run things become unpleasant for us even before we’ve had a chance to start running the unit tests, because the git clone operation is very time consuming…\n\ndocker run test-ggplot2 \n\nTransferred 15676 of 74694 objects...\n\n…uh, right. Look this is going to take a while, so maybe we should move on?\nThe main reason I wanted to point to this is to highlight that the clone step occurs at run time, and the entire clone operation is repeated every time we call it. That’s not a smart way to do this. If you really wanted to design a docker workflow for testing packages on arch, you’d want to make some smarter design choices than this! The test-on-arch project I’ve used in this blog post is a toy, nothing more.31"
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html#hosting-images",
    "href": "posts/2023-01-01_playing-with-docker/index.html#hosting-images",
    "title": "Playing with docker and the github container registry",
    "section": "Hosting images",
    "text": "Hosting images\nFor the third example, let’s look at the arch-r-base image itself. In addition to the dockerfile there are two small text files used to specify locale information. The two locale files aren’t very interesting and could easily have been included as strings in the dockerfile, but I found it neater to keep them separate. The locale-gen file specifies locales that the image understands, and locale.conf specifies configuration details. (Both are configuration files on linux). In any case, here’s the whole thing:\n\n\n\n\nbase/Dockerfile\n\nFROM archlinux:base-devel\n\nLABEL org.opencontainers.image.source \"https://github.com/djnavarro/arch-r\" \nLABEL org.opencontainers.image.authors \"Danielle Navarro <djnavarro@protonmail.com>\" \nLABEL org.opencontainers.image.description DESCRIPTION\nLABEL org.opencontainers.image.licenses \"GPL-3.0\"\n\n# set the locale\nCOPY base/locale.gen /etc/locale.gen\nCOPY base/locale.conf /etc/locale.conf\nRUN locale-gen\nENV LANG=en_US.UTF-8\nENV LC_ALL=en_US.UTF-8\n\n# install R and set default command\nRUN pacman -Syu --noconfirm r\nCMD R --no-save\n\n\n\n\n\n\nbase/locale.gen\n\nC.UTF8 UTF-8\nen_US.UTF-8 UTF-8\n\n\n\n\n\nbase/locale.conf\n\nLANG=en_US.UTF-8\nLC_ALL=en_US.UTF-8\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe names are sampled using the babynames package. I deliberately chose to ignore name frequency, sampling uniformly at random from the names in the data set. You end up with more interesting choices that way.\n\n\n\nTruly exciting stuff, I know. Thankfully only some of it is new. The FROM instruction uses the archlinux:base-devel image hosted here. The RUN instruction is used to execute commands at build time, so you can see in this example I’ve used it to create system locale information (by calling locale-gen) and to install R (using the pacman package manager used on arch linux).\nThe other new thing here is the LABEL instruction used to supply metadata about the image. This is particularly important if you’re planning to make your image public, as I have done with the arch-r-base and arch-r-test images. The labelling that I’ve supplied here follows the specifications provided by the open container initiative, or at least attempts to. I’m still new to this, but as far as I can tell this is correct? Anyway, you can see that it specifies the location of the source code, the author of the image, and the licence. That’s the main thing.\nYou are probably wondering, though, why the description just reads “DESCRIPTION” and doesn’t have an actual… you know… description. The reason for that is that I’m hosting these through the github container registry that links my github repository to the images automatically. Specifically, I’m using a github action that automates the build process and populates the description on the arch-r-base package page using the description field from the arch-r github repository. Leaving the value for that field as “DESCRIPTION” ensures that all works smoothly.\nSpeaking of which, I’m not in any way an expert on github actions – this is my first attempt at creating a workflow and I cribbed heavily from other workflows I found online – but for whatever it’s worth I figure I should share. Here’s the workflow I’m using:\n\n\n\n\n.github/workflows/build-image.yaml\n\nname: publish arch-r images\n\non:\n  push:\n    branches: ['release']\n    \nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ${{ github.repository }}\n\njobs:\n  build-and-push-image:\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        include:\n          - dockerfile: ./base/Dockerfile\n            image: ghcr.io/djnavarro/arch-r-base\n          - dockerfile: ./test/Dockerfile\n            image: ghcr.io/djnavarro/arch-r-test\n            \n    permissions:\n      contents: read\n      packages: write\n\n    steps:\n      - name: checkout repository\n        uses: actions/checkout@v2\n\n      - name: login to the container registry\n        uses: docker/login-action@f054a8b539a109f9f41c372932f1ae047eff08c9\n        with:\n          registry: ${{ env.REGISTRY }}\n          username: ${{ github.actor }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: extract metadata (tags, labels) for docker\n        id: meta\n        uses: docker/metadata-action@98669ae865ea3cffbcbaa878cf57c20bbf1c6c38\n        with:\n          images: ${{ matrix.image }}\n\n      - name: build and push docker image\n        uses: docker/build-push-action@ad44023a93711e3deb337508980b4b5e9bcdc5dc\n        with:\n          context: .\n          file: ${{ matrix.dockerfile }}\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n\n\n\nFor this workflow to run, I needed to edit the permissions associated with my github PAT to include some additional scopes. If, like me, you’ve created your PAT using the default scopes provided by usethis::create_github_token(), you’ll need a few more to run workflows that build and modify docker images if you want to work with github packages. This workflow doesn’t use all these, but the permissions typically required for to work with container images on github are these:\n\nread:packages scope to download container images and read metadata.\nwrite:packages scope to download and upload container images and read and write metadata.\ndelete:packages scope to delete container images.\n\nIn any case, this github actions workflow triggers an automatic deployment to the github container registry whenever there is a new push to the release branch of the repository. This is what creates the ghcr.io/djnavarro/arch-r-base:release and ghcr.io/djnavarro/arch-r-test:release images. I’m entirely certain that this could be done in a more sophisticated way, but it does work, and that was my main goal for this post."
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html#resources",
    "href": "posts/2023-01-01_playing-with-docker/index.html#resources",
    "title": "Playing with docker and the github container registry",
    "section": "Resources",
    "text": "Resources\nAnd that brings us to the end of the post. There’s not much else to say really. I played around with docker. Learned a few things. Had some fun. Drew some whales. Normal stuff, really. But if you’re at all keen on following up on any of the things in this post, here are some resources I relied on when writing this:\n\nThe docker reference documentation: docs.docker.com/reference\nDockerfile best practices docs.docker.com/develop/develop-images/dockerfile_best-practices\nInstructions on giving docker sudo privileges for linux users: docs.docker.com/engine/install/linux-postinstall\nThe rocker project by Carl Boettiger, Dirk Eddelbuettel, Noam Ross, and Shima Tatsuya: rocker-project.org\nSource code for the rocker repositories: github.com/rocker-org/rocker\nBlog post on docker by Colin Fay: colinfay.me/docker-r-reproducibility\nSlides on docker by Noam Ross: github.com/noamross/nyhackr-docker-talk\nDocker for beginners by Prakhar Srivastav: docker-curriculum.com\nReferencing docker images by Nigel Brown windsock.io/referencing-docker-images\nWorking with the github container registry: docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry\nInformation about open containers labels: github.com/opencontainers/image-spec/blob/main/annotations.md"
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html#postscript-making-dockerplots-in-ggplot2",
    "href": "posts/2023-01-01_playing-with-docker/index.html#postscript-making-dockerplots-in-ggplot2",
    "title": "Playing with docker and the github container registry",
    "section": "Postscript: Making “dockerplots” in ggplot2",
    "text": "Postscript: Making “dockerplots” in ggplot2\nI had a lot of fun making the whales. They’re cute, and they make me happy. The function that generates these is called sample_whales(), and you can find the source code by expanding the folded code block below. Enjoy!\n\n\nSource code for sample_whales()\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(dplyr)\n\nsample_whales <- function(seed = NULL, nrow = 4, ncol = 6) {\n\n  if(is.null(seed)) seed <- sample(1000, 1)\n  set.seed(seed)\n\n  nwhales <- nrow * ncol\n\n  # define a circle\n  circle <- tibble(\n    th = seq(0, 2*pi, length.out = 1000),\n    x = cos(th),\n    y = sin(th)\n  )\n\n  # distort a circle to create the whale body\n  whale_body <- circle |>\n    mutate(\n      y = if_else(y > 0, 0, y),\n      y = if_else(x < 0, -abs(y) ^ .6, -abs(y) ^ 1.7)\n    )\n\n  # distort a circle to create the whale tail\n  whale_tail <- circle |>\n    mutate(\n      weight = (abs(th - pi)/pi) ^ 1.3,\n      angle = pi * 1.2,\n      x = x * weight + .35 * (1 - weight),\n      x_scaled = x * .6,\n      y_scaled = y * .4,\n      x = x_scaled * cos(angle) - y_scaled * sin(angle),\n      y = x_scaled * sin(angle) + y_scaled * cos(angle),\n      x = x + 1.35,\n      y = y + 0.25\n    )\n\n  # bind the body to the tail to make a whale\n  whale <- bind_rows(whale_body, whale_tail)\n\n  # fully stacked set of boxes\n  box_stack <- expand_grid(\n    x = seq(-.7, .5, .3),\n    y = seq(.25, 1.5, .3)\n  )\n\n  # sample names using babynames package\n  names <- unique(sample(\n    x = babynames::babynames$name,\n    size = ceiling(nwhales * 1.2)\n  ))\n\n  # sample colours using a blue palette from ggthemes\n  shades <- sample(\n    x = ggthemes::canva_palettes$`Cool blues`,\n    size = nrow * ncol,\n    replace = TRUE\n  )\n\n  boxes <- list()\n  whales <- list()\n  for(i in 1:(nrow * ncol)) {\n\n    # assign the whales a name and a look\n    whales[[i]] <- whale |>\n      mutate(\n        name = names[[i]],\n        look = shades[[i]]\n      )\n\n    # assign the whales a name and colour,\n    # and randomly remove boxes off the stack\n    boxes[[i]] <- box_stack |>\n      mutate(\n        name = names[[i]],\n        look = shades[[i]]\n      ) |>\n      group_by(x) |>\n      mutate(max_height = runif(1, min = .05, max = 1.8)) |>\n      filter(y < max_height)\n  }\n\n  # collapse lists to data frames\n  boxes <- bind_rows(boxes)\n  whales <- bind_rows(whales)\n\n  # last minute tinkering... :-)\n  boxes <- boxes |> mutate(y = y - .3, x = x + .01)\n  whales <- whales |> mutate(y = y - .31)\n\n  # draw the plot\n  ggplot(mapping = aes(x, y, fill = look, colour = look)) +\n    geom_polygon(data = whales, linewidth = 2) +\n    geom_tile(\n      data = boxes,\n      width = .18,\n      height = .18,\n      linewidth = 2,\n      linejoin = \"bevel\"\n    ) +\n    facet_wrap(vars(name), nrow = nrow, ncol = ncol) +\n    coord_equal(xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5)) +\n    scale_x_continuous(labels = NULL, name = NULL) +\n    scale_y_continuous(labels = NULL, name = NULL) +\n    scale_fill_identity() +\n    scale_color_identity() +\n    theme_minimal(base_size = 14) +\n    theme(\n      axis.ticks = element_blank(),\n      panel.border = element_rect(fill = NA, colour = \"grey90\")\n    )\n}"
  },
  {
    "objectID": "posts/2021-09-07_water-colours/index.html",
    "href": "posts/2021-09-07_water-colours/index.html",
    "title": "Art, jasmines, and the water colours",
    "section": "",
    "text": "In recent weeks I’ve been posting generative art from the Water Colours series on twitter. The series has been popular, prompting requests that I sell prints, mint NFTs, or write a tutorial showing how they are made. For personal reasons I didn’t want to commercialise this series. Instead, I chose to make the pieces freely available under a CC0 public domain licence and asked people to donate to a gofundme I set up for a charitable organisation I care about (the Lou’s Place women’s refuge here in Sydney). I’m not going to discuss the personal story behind this series, but it does matter. As I’ve mentioned previously, the art I make is inherently tied to moods. It is emotional in nature. In hindsight it is easy enough to describe how the system is implemented but this perspective is misleading. Although a clean and unemotional description of the code is useful for explanatory purposes, the actual process of creating the system is deeply tied to my life, my history, and my subjective experience. Those details are inextricably bound to the system. A friend described it better than I ever could:\n\nThe computer doesn’t make this art any more than a camera makes a photograph; art is always intimate (Amy Patterson)\n\nIn this post I’ll describe the mechanistic processes involved in creating these pieces, but this is woefully inadequate as a description of the artistic process as a whole. The optical mechanics of a camera do not circumscribe the work of a skilled photographer. So it goes with generative art. The code describes the mechanics; it does not describe the art. There is a deeply personal story underneath these pieces (one that I won’t tell here), and I would no more mint an NFT from that story than I would sell a piece of my soul to a collector."
  },
  {
    "objectID": "posts/2021-09-07_water-colours/index.html#the-water-colours-repository",
    "href": "posts/2021-09-07_water-colours/index.html#the-water-colours-repository",
    "title": "Art, jasmines, and the water colours",
    "section": "The water colours repository",
    "text": "The water colours repository\n\nWhy use version control here?\nWhen I started making generative art I didn’t think much about archiving my art or keeping it organised. I liked making pretty things, and that was as far as my thought process went. I didn’t place the code under version control, and I stored everything in my Dropbox folder. There’s nothing wrong with that: some things don’t belong on GitHub. During the development phase of any art project that’s still what I do, and I’m perfectly happy with it.\nThings become a little trickier when you want to share the art. My art website is hosted on GitHub pages, and so my initial approach was to keep the art in the website repository. Huuuuge mistake. Sometimes the image files can be quite large and sometimes a series contains a large number of images. By the time I’d reached 40+ series, Hugo took a very long time to build the site (several minutes), and GitHub took even longer to deploy it (over half an hour).\nEventually I decided it made more sense to have one repository per series. Each one uses the “series-” prefix to remind me it’s an art repo. I don’t use these repositories during development: they exist solely to snapshot the release. For example, the series-water-colours repository isn’t going to be updated regularly, it’s really just an archive combined with a “docs” folder that is used to host a minimal GitHub Pages site that makes the images public. It’s convenient for my purposes because my art website doesn’t have to host any of the images: all it does is hotlink to the images that are exposed via the series repo.\nIt may seem surprising that I’ve used GitHub for this. Image files aren’t exactly well suited to version control, but it’s not like they’re going to be updated. Plus, there are a lot of advantages. I can explicitly include licencing information in the repository, I can release source code (when I want to), and I can include a readme file for anyone who wants to use it.\n\n\nThe manifest file\nOne nice feature of doing things this way is that it has encouraged me to include a manifest file. Because the image files belong to a completely different repository to the website, I need a way to automatically inspect the image repository and construct the links I need (because I’m waaaaaay too lazy to add the links by hand). That’s the primary function of the manifest. The manifest.csv file is a plain csv file with one row per image, and one column for each piece of metadata I want to retain about the images. It might seem like organisational overkill to be this precise about the art, but I’m starting to realise that if I don’t have a proper system in place I’ll forget minor details like “what the piece is called” or “when I made it”. That seems bad :-)\n\n\n\nI can use readr::read_csv() to download the manifest and do a little data wrangling to organise it into a format that is handy to me right now:\n\n\nThe data wrangling code is here\n\nmanifest\n\n# A tibble: 20 × 9\n   series      sys_id img_id short_n…¹ format long_…² date       path_…³ path_…⁴\n   <chr>       <chr>  <chr>  <chr>     <chr>  <chr>   <date>     <chr>   <chr>  \n 1 watercolour sys02  img34  teacup-o… jpg    Ocean … 2021-07-31 https:… https:…\n 2 watercolour sys02  img31  incursio… jpg    Incurs… 2021-08-14 https:… https:…\n 3 watercolour sys02  img32  percolate jpg    Percol… 2021-08-21 https:… https:…\n 4 watercolour sys02  img37  gentle-d… jpg    Gentle… 2021-08-21 https:… https:…\n 5 watercolour sys02  img41  stormy-s… jpg    Stormy… 2021-08-22 https:… https:…\n 6 watercolour sys02  img42  turmeric  jpg    Turmer… 2021-08-24 https:… https:…\n 7 watercolour sys02  img43  torn-and… jpg    Torn a… 2021-08-24 https:… https:…\n 8 watercolour sys02  img47  inferno   jpg    Sevent… 2021-08-27 https:… https:…\n 9 watercolour sys02  img48  storm-ce… jpg    Storm … 2021-08-27 https:… https:…\n10 watercolour sys02  img49  tonal-ea… jpg    Tonal … 2021-08-29 https:… https:…\n11 watercolour sys02  img50  cold-fro… jpg    Cold F… 2021-08-29 https:… https:…\n12 watercolour sys02  img51  kintsugi… jpg    Kintsu… 2021-08-29 https:… https:…\n13 watercolour sys02  img53  departure jpg    Depart… 2021-08-29 https:… https:…\n14 watercolour sys02  img54  echo      jpg    Echo    2021-08-30 https:… https:…\n15 watercolour sys02  img57  portal    jpg    Portal  2021-08-31 https:… https:…\n16 watercolour sys02  img60  salt-sto… jpg    Gods o… 2021-08-31 https:… https:…\n17 watercolour sys02  img61  amanecer… jpg    El Últ… 2021-09-01 https:… https:…\n18 watercolour sys02  img65  plume     jpg    Plume   2021-09-02 https:… https:…\n19 watercolour sys02  img67  woodland… jpg    Woodla… 2021-09-02 https:… https:…\n20 watercolour sys02  img68  below-th… jpg    Below … 2021-09-03 https:… https:…\n# … with abbreviated variable names ¹​short_name, ²​long_name, ³​path_2000,\n#   ⁴​path_500\n\n\n\n\nPreviewing the artwork\nMore to the point, the manifest data frame is nicely suited for use with the bs4cards package, so I can display some of the pieces in a neat and tidy thumbnail grid. Here are the first eight pieces from the series, arranged by date of creation:\n\nmanifest[1:8, ] %>% \n  bs4cards::cards(\n    image = path_500,\n    link = path_2000,\n    title = long_name,\n    spacing = 3,\n    width = 2\n  )  \n\n\n\n\n\n\n\n\n\nOcean in a Teacup\n\n\n\n\n\n\n\n\n\nIncursions\n\n\n\n\n\n\n\n\n\nPercolate\n\n\n\n\n\n\n\n\n\nGentle Descent\n\n\n\n\n\n\n\n\n\nStormy Seas\n\n\n\n\n\n\n\n\n\nTurmeric Against Grey Tuesday\n\n\n\n\n\n\n\n\n\nTorn and Frayed\n\n\n\n\n\n\n\n\n\nSeventh Circle\n\n\n\n\n\n\n\nEach thumbnail image links to a medium resolution (2000 x 2000 pixels) jpg version of the corresponding piece, if you’d like to see the images in a little more detail."
  },
  {
    "objectID": "posts/2021-09-07_water-colours/index.html#dependencies",
    "href": "posts/2021-09-07_water-colours/index.html#dependencies",
    "title": "Art, jasmines, and the water colours",
    "section": "Dependencies",
    "text": "Dependencies\nIn the remainder of this post I’ll walk you through the process of creating pieces “in the style of” the water colours series. If you really want to, you can take a look at the actual source, but it may not be very helpful: the code is little opaque, poorly structured, and delegates a lot of the work to the halftoner and jasmines packages, neither of which is on CRAN. To make it a little easier on you, I’ll build a new system in this post that adopts the same core ideas.\nIn this post I’ll assume you’re already familiar with data wrangling and visualisation with tidyverse tools. This is the subset of tidyverse packages that I have attached, and the code that follows relies on all these in some fashion:\n\nlibrary(magrittr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(dplyr)\n\n\n\nThe R environment is specified formally in the lockfile. It’s a story for another day, but for reproducibility purposes I have a separate renv configuration for every post\nIn addition to tidyverse and base R functions, I’ll use a few other packages as well. The magick, raster, rprojroot, fs, and ambient packages are all used in making the art. Because functions from those packages may not be as familiar to everyone, I’ll namespace the calls to them in the same way I did with bs4cards::cards() previously. Hopefully that will make it easier to see which functions belong to one of those packages."
  },
  {
    "objectID": "posts/2021-09-07_water-colours/index.html#art-from-image-processing",
    "href": "posts/2021-09-07_water-colours/index.html#art-from-image-processing",
    "title": "Art, jasmines, and the water colours",
    "section": "Art from image processing",
    "text": "Art from image processing\n\nFinding the image file\nAs in life, the place to start is knowing where you are.\nThis post is part of my blog, and I’ll need to make use of an image file called \"jasmine.jpg\" stored alongside my R markdown. First, I can use rprojroot to find out where my blog is stored. I’ll do that by searching for a \"_quarto.yml\" file:\n\nblog <- rprojroot::find_root(\"_quarto.yml\")\nblog\n\n[1] \"/home/danielle/GitHub/sites/quarto-blog\"\n\n\nI suspect that most people reading this would be more familiar with the here package that provides a simplified interface to rprojroot and will automatically detect the .Rproj or .here file associated with your project. In fact, because the here::here() function is so convenient, it’s usually my preferred method for solving this problem. Sometimes, however, the additional flexibility provided by rprojroot is very useful. Some of my projects are comprised of partially independent sub-projects, each with a distinct root directory. That happens sometimes when blogging: there are contexts in which you might want to consider “the blog” to be the project, but other contexts in which “the post” might be the project. If you’re not careful this can lead to chaos (e.g., RStudio projects nested inside other RStudio projects), and I’ve found rprojroot very helpful in avoiding ambiguity in these situations.\nHaving chosen “the blog” as the root folder, the next step in orientation is to find the post folder. Because this is a distill blog, all my posts are stored in the _posts folder, and I’ve adopted a consistent naming convention for organising the post folders. Every name begins with the post date in year-month-day format, followed by a human-readable “slug”:\n\npost <- paste(params$date, params$slug, sep = \"_\")\npost\n\n[1] \"2021-09-07_water-colours\"\n\n\nThis allows me to construct the path to the image file:\n\nfile <- fs::path(blog, \"posts\", post, \"jasmine.jpg\")\nfile\n\n/home/danielle/GitHub/sites/quarto-blog/posts/2021-09-07_water-colours/jasmine.jpg\n\n\nHere’s the image:\n\n\n\n\n\n\n\nThe photo has an emotional resonance to me: it dates back to 2011 and appeared on the cover of Learning Statistics with R. Although 10 years separate the Water Colours series from the text and the photo, the two are linked by a shared connection to events from a decade ago\n\n\nImporting the image\nOur next step is to import the image into R at a suitable resolution. The original image size is 1000x600 pixels, which is a little more than we need. Here’s a simple import_image() function that does this:\n\nimport_image <- function(path, width, height) {\n  geometry <- paste0(width, \"x\", height) # e.g., \"100x60\"\n  path %>% \n    magick::image_read() %>% \n    magick::image_scale(geometry)\n}\n\nInternally, the work is being done by the fabulous magick package that provides bindings to the ImageMagick library. In truth, it’s the ImageMagick library that is doing most the work here. R doesn’t load the complete image, it lets ImageMagick take care of that. Generally that’s a good thing for performance reasons (you don’t want to load large images into memory if you can avoid it), but in this case we’re going to work with the raw image data inside R.\nThis brings us to the next task…\n\n\nConverting the image to data\nConverting the image into a data structure we can use is a two step process. First, we create a matrix that represents the image in a format similar to the image itself. That’s the job of the construct_matrix() function below. It takes the image as input, and first coerces it to a raster object and then to a regular matrix: in the code below, the matrix is named mat, and the pixel on the i-th row and j-th column of the image is represented by the contents of mat[i, j].\n\nconstruct_matrix <- function(image) {\n  \n  # read matrix\n  mat <- image %>% \n    as.raster() %>%\n    as.matrix()\n  \n  # use the row and column names to represent co-ordinates\n  rownames(mat) <- paste0(\"y\", nrow(mat):1) # <- flip y\n  colnames(mat) <- paste0(\"x\", 1:ncol(mat))\n  \n  return(mat)\n}\n\nA little care is needed when interpreting the rows of this matrix. When we think about graphs, the values on y-axis increase as we move our eyes upwards from the bottom, so our mental model has the small numbers at the bottom and the big numbers at the top. But that’s not the only mental model in play here. When we read a matrix or a table we don’t look at it, we read it - and we read from top to bottom. A numbered list, for example, has the smallest numbers at the top, and the numbers get bigger as we read down the list. Both of those mental models are sensible, but it’s hard to switch between them.\nThe tricky part here is that the raw image is encoded in “reading format”. It’s supposed to be read like a table or a list, so the indices increase as we read down the image. The image data returned by construct_matrix() is organised this format. However, when we draw pictures with ggplot2 later on, we’re going to need to switch to a “graph format” convention with the small numbers at the bottom. That’s the reason why the code above flips the order of the row names. Our next task will be to convert this (reading-formatted) matrix into a tidy tibble, and those row and column names will become become our (graph-formatted) x- and y-coordinates, so the row names need to be labelled in reverse order.\nTo transform the image matrix into a tidy tibble, I’ve written a handy construct_tibble() function:\n\nconstruct_tibble <- function(mat) {\n  \n  # convert to tibble\n  tbl <- mat %>%\n    as.data.frame() %>%\n    rownames_to_column(\"y\") %>%\n    as_tibble() \n  \n  # reshape\n  tbl <- tbl %>%\n    pivot_longer(\n      cols = starts_with(\"x\"),\n      names_to = \"x\",\n      values_to = \"shade\"\n    ) \n  \n  # tidy\n  tbl <- tbl %>%\n    arrange(x, y) %>% \n    mutate(\n      x = x %>% str_remove_all(\"x\") %>% as.numeric(),\n      y = y %>% str_remove_all(\"y\") %>% as.numeric(),\n      id = row_number()\n    )\n  \n  return(tbl)\n}\n\nThe code has the following strucure:\n\nThe first part of this code coerces the matrix to a plain data frame, then uses rownames_to_columns() to extract the row names before coercing it to a tibble. This step is necessary because tibbles don’t have row names, and we need those row names: our end goal is to have a variable y to store those co-ordinate values.\nThe second part of the code uses pivot_longer() to capture all the other variables (currently named x1, x2, etc) and pull them down into a single column that specifies the x co-ordinate. At this stage, the tbl tibble contains three variables: an x value, a y value, and a shade that contains the hex code for a colour.\nThe last step is to tidy up the values. After pivot_longer() does its job, the x variable contains strings like \"x1\", \"x2\", etc, but we’d prefer them to be actual numbers like 1, 2, etc. The same is true for the y variable. To fix this, the last part of the code does a tiny bit of string manipulation using str_remove_all() to get rid of the unwanted prefixes, and then coerces the result to a number.\n\n\n\nThe names_prefix argument to pivot_longer() can transform x without the third step, but I prefer the verbose form. I find it easier to read and it treats x and y the same\nTaken together, the import_image(), construct_matrix(), and construct_tibble() functions provide us with everything we need to pull the data from the image file and wrangle it into a format that ggplot2 is expecting:\n\njas <- file %>% \n  import_image(width = 100, height = 60) %>% \n  construct_matrix() %>% \n  construct_tibble()\n\njas\n\n# A tibble: 6,000 × 4\n       y     x shade        id\n   <dbl> <dbl> <chr>     <int>\n 1     1     1 #838c70ff     1\n 2    10     1 #3c3123ff     2\n 3    11     1 #503d3dff     3\n 4    12     1 #363126ff     4\n 5    13     1 #443a30ff     5\n 6    14     1 #8a6860ff     6\n 7    15     1 #665859ff     7\n 8    16     1 #5a5d51ff     8\n 9    17     1 #535c4cff     9\n10    18     1 #944b61ff    10\n# … with 5,990 more rows\n\n\nA little unusually, the hex codes here are specified in RGBA format: the first two alphanumeric characters specify the hexadecimal code for the red level, the second two represent the green level (or “channel”), the third two are the blue channel, and the last two are the opacity level (the alpha channel). I’m going to ignore the alpha channel for this exercise though.\nThere’s one last thing to point out before turning to the fun art part. Notice that jas also contains an id column (added by the third part of the construct_tibble() function). It’s generally good practice to have an id column that uniquely identifies each row, and will turn out to be useful later when we need to join this data set with other data sets that we’ll generate.\n\n\nArt from data visualisation\nLet the art begin!\nThe first step is to define a helper function ggplot_themed() that provides a template that we’ll reuse in every plot. Mostly this involves preventing ggplot2 from doing things it wants to do. When we’re doing data visualisation it’s great that ggplot2 automatically provides things like “legends”, “axes”, and “scales” to map from data to visual aesthetics, but from an artistic perspective they’re just clutter. I don’t want to manually strip that out every time I make a plot, so it makes sense to have a function that gets rid of all those things:\n\nggplot_themed <- function(data) {\n  data %>% \n    ggplot(aes(x, y)) +\n    coord_equal() + \n    scale_size_identity() + \n    scale_colour_identity() + \n    scale_fill_identity() + \n    theme_void() \n}\n\nThis “template function” allows us to start with a clean slate, and it makes our subsequent coding task easier. The x and y aesthetics are already specified, ggplot2 won’t try to “interpret” our colours and sizes for us, and it won’t mess with the aspect ratio. In a sense, this function turns off the autopilot: we’re flying this thing manually…\nThere are many ways to plot the jas data in ggplot2. The least imaginative possibility is geom_tile(), which produces a pixellated version of the jasmines photo:\n\njas %>% \n  ggplot_themed() + \n  geom_tile(aes(fill = shade)) \n\n\n\n\nOf course, if you are like me you always forget to use the fill aesthetic. The muscle memory tells me to use the colour aesthetic, so I often end up drawing something where only the borders of the tiles are coloured:\n\njas %>% \n  ggplot_themed() + \n  geom_tile(aes(colour = shade)) \n\n\n\n\nIt’s surprisingly pretty, and a cute demonstration of how good the visual system is at reconstructing images from low-quality input: remarkably, the jasmines are still perceptible despite the fact that most of the plot area is black. I didn’t end up pursuing this (yet!) but I think there’s a lot of artistic potential here. It might be worth playing with at a later date. In that sense generative art is a lot like any other kind of art (or, for that matter, science). It is as much about exploration and discovery as it is about technical prowess.\nThe path I did follow is based on geom_point(). Each pixel in the original image is plotted as a circular marker in the appropriate colour. Here’s the simplest version of this idea applied to the jas data:\n\njas %>% \n  ggplot_themed() + \n  geom_point(aes(colour = shade)) \n\n\n\n\nIt’s simple, but I like it.\n\n\nExtracting the colour channels\nUp to this point we haven’t been manipulating the colours in any of the plots: the hex code in the shade variable is left intact. There’s no inherent reason we should limit ourselves to such boring visualisations. All we need to do is extract the different “colour channels” and start playing around.\nIt’s not too difficult to do this: base R provides the col2rgb() function that separates the hex code into red, green, blue channels, and represents each channel with integers between 0 and 255. It also provides the rgb2hsv() function that converts this RGB format into hue, saturation, and value format, represented as numeric values between 0 and 1.\nThis technique is illustrated by the extract_channels() helper function shown below. It looks at the shade column in the data frame, and adds six new columns, one for each channel. I’m a sucker for variable names that are all the same length (often unwisely), and I’ve named them red, grn, blu, hue, sat, and val:\n\nextract_channels <- function(tbl) {\n  rgb <- with(tbl, col2rgb(shade))\n  hsv <- rgb2hsv(rgb)\n  tbl <- tbl %>% \n    mutate(\n      red = rgb[1, ],\n      grn = rgb[2, ],\n      blu = rgb[3, ],\n      hue = hsv[1, ],\n      sat = hsv[2, ],\n      val = hsv[3, ]\n    )\n  return(tbl)\n}\n\nHere’s what that looks like applied to the jas data:\n\njas <- extract_channels(jas)\njas\n\n# A tibble: 6,000 × 10\n       y     x shade        id   red   grn   blu    hue   sat   val\n   <dbl> <dbl> <chr>     <int> <int> <int> <int>  <dbl> <dbl> <dbl>\n 1     1     1 #838c70ff     1   131   140   112 0.220  0.200 0.549\n 2    10     1 #3c3123ff     2    60    49    35 0.0933 0.417 0.235\n 3    11     1 #503d3dff     3    80    61    61 0      0.237 0.314\n 4    12     1 #363126ff     4    54    49    38 0.115  0.296 0.212\n 5    13     1 #443a30ff     5    68    58    48 0.0833 0.294 0.267\n 6    14     1 #8a6860ff     6   138   104    96 0.0317 0.304 0.541\n 7    15     1 #665859ff     7   102    88    89 0.988  0.137 0.4  \n 8    16     1 #5a5d51ff     8    90    93    81 0.208  0.129 0.365\n 9    17     1 #535c4cff     9    83    92    76 0.260  0.174 0.361\n10    18     1 #944b61ff    10   148    75    97 0.950  0.493 0.580\n# … with 5,990 more rows\n\n\nA whole new world of artistic possibilities has just emerged!\n\n\nArt from channel manipulation\nOne way to use this representation is in halftone images. If you have a printer that contains only black ink, you can approximate shades of grey by using the size of each dot to represent how dark that pixel should be:\n\nmap_size <- function(x) {\n  ambient::normalise(1-x, to = c(0, 2))\n}\n\njas %>% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(size = map_size(val)),\n    colour = \"black\", \n    show.legend = FALSE\n  )\n\n\n\n\n\n\nIn this code the ambient::normalise() function is used to rescale the input to fall within a specified range. Usually ggplot2 handles this automatically, but as I mentioned, we’ve turned off the autopilot…\nFor real world printers, this approach is very convenient because it allows us to construct any shade we like using only a few different colours of ink. In the halftone world shades of grey are merely blacks of different size, pinks are merely sizes of red (sort of), and so on.\nBut we’re not using real printers, and in any case the image above is not a very good example of a halftone format: I’m crudely mapping 1-val to the size aesthetic, and that’s not actually the right way to do this (if you want to see this done properly, look at the halftoner package). The image above is “inspired by” the halftone concept, not the real thing. I’m okay with that, and abandoning the idea of fidelity opens up new possibilities. For example, there’s nothing stopping us retaining the original hue and saturation, while using dot size to represent the intensity value. That allows us to produce “halftonesque” images like this:\n\njas %>% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(\n      colour = hsv(hue, sat, .5), \n      size = map_size(val)\n    ), \n    show.legend = FALSE\n  )\n\n\n\n\nIn this code, the hsv() function takes the hue and saturation channels from the original image, but combines them with a constant intensity value: the output is a new colour specified as a hex code that ggplot2 can display in the output. Because we have stripped out the value channel, we can reuse the halftone trick. Much like a halftone image, the image above uses the size aesthetic to represent the intensity at the corresponding pixel."
  },
  {
    "objectID": "posts/2021-09-07_water-colours/index.html#intermission",
    "href": "posts/2021-09-07_water-colours/index.html#intermission",
    "title": "Art, jasmines, and the water colours",
    "section": "Intermission",
    "text": "Intermission\nUp to this point I’ve talked about image manipulation, and I hope you can see the artistic potential created when we pair image processing tools like magick with data visualisation tools like ggplot2. What I haven’t talked about is how to choose (or generate!) the images to manipulate, and I haven’t talked about how we might introduce a probabilistic component to the process. I’m not going to say much about how to choose images. The possibilities are endless. For this post I’ve used a photo I took in my garden many years ago, but the pieces in Water Colours series have a different origin: I dripped some food colouring into a glass of water and took some photos of the dye diffusing. Small sections were cropped out of these photos and often preprocessed in some fashion by changing the hue, saturation etc. These manipulated photos were then passed into a noise generation process, and the output produced images like this:\n\n\n\n\n\n\n\n\n\n\nStorm Cell / Air Elemental\n\n\n\n\n\n\n\n\n\nTonal Earth\n\n\n\n\n\n\n\n\n\nCold Front\n\n\n\n\n\n\n\n\n\nKintsugi Dreams"
  },
  {
    "objectID": "posts/2021-09-07_water-colours/index.html#art-from-noise-generators",
    "href": "posts/2021-09-07_water-colours/index.html#art-from-noise-generators",
    "title": "Art, jasmines, and the water colours",
    "section": "Art from noise generators",
    "text": "Art from noise generators\n\nMultidimensional noise generation\nHow can we generate interesting noise patterns in R? As usual, there are many different ways you can do this, but my favourite method is to use the ambient package that provides bindings to the FastNoise C++ library. A proper description of what you can do with ambient is beyond what I can accomplish here. There are a lot of things you can do with a tool like this, and I’ve explored only a small subset of the possibilities in my art. Rather than make a long post even longer, what I’ll do is link to a lovely essay on flow fields and encourage you to play around yourself.\nTo give you a sense of what the possibilities are, I’ve written a field() function that uses the ambient package to generate noise. At its heart is ambient::gen_simplex(), a function that generates simplex noise (examples here), a useful form of multidimensional noise that has applications in computer graphics. In the code below, the simplex noise is then modified by a billow fractal that makes it “lumpier”: that’s the job of ambient::gen_billow() and ambient::fracture(). This is then modified one last time by the ambient::curl_noise() function to avoid some undesirable properties of the flow fields created by simplex noise.\nIn any case, here is the code. You’ll probably need to read through the ambient documentation to understand all the moving parts here, but for our purposes the main things to note are the arguments. The points argument takes a data frame or tibble that contains the x and y coordinates of a set of points (e.g., something like the jas data!). The frequency argument controls the overall “scale” of the noise: does it change quickly or slowly as you move across the image? The octaves argument controls the amount of fractal-ness (hush, I know that’s not a word) in the image. How many times do you apply the underlying transformation?\n\nfield <- function(points, frequency = .1, octaves = 1) {\n  ambient::curl_noise(\n    generator = ambient::fracture,\n    fractal = ambient::billow,\n    noise = ambient::gen_simplex,\n    x = points$x,\n    y = points$y,\n    frequency = frequency,\n    octaves = octaves,\n    seed = 1\n  )\n}\n\nInterpreting the output of the field() function requires a little care. The result isn’t a new set of points. Rather, it is a collection of directional vectors that tell you “how fast” the x- and y-components are flowing at each of the locations specified in the points input. If we want to compute a new set of points (which is usually true), we need something like the shift() function below. It takes a set of points as input, computes the directional vectors at each of the locations, and then moves each point by a specified amount, using the flow vectors to work out how far to move and what direction to move. The result is a new data frame with the same columns and the same number of rows:\n\nshift <- function(points, amount, ...) {\n  vectors <- field(points, ...)\n  points <- points %>%\n    mutate(\n      x = x + vectors$x * amount,\n      y = y + vectors$y * amount,\n      time = time + 1,\n      id = id\n    )\n  return(points)\n}\n\nIt’s worth noting that the shift() function assumes that points contains an id column as well as the x and y columns. This will be crucial later when we want to merge the output with the jas data. Because the positions of each point are changing, the id column will be the method we use to join the two data sets. It’s also worth noting that shift() keeps track of time for you. It assumes that the input data contains a time column, and the output data contains the same column with every value incremented by one. In other words, it keeps the id constant so we know which point is referred to by the row, but modifies its position in time and space (x and y). Neat.\n\n\nArt from the noise\nTo illustrate how this all works, I’ll start by creating a regular 50x30 grid of points:\n\npoints_time0 <- expand_grid(x = 1:50, y = 1:30) %>% \n  mutate(time = 0, id = row_number())\n\nggplot_themed(points_time0) + \n  geom_point(size = .5)\n\n\n\n\nNext, I’ll apply the shift() function three times in succession, and bind the results into a single tibble that contains the the data at each point in time:\n\npoints_time1 <- shift(points_time0, amount = 1)\npoints_time2 <- shift(points_time1, amount = 1)\npoints_time3 <- shift(points_time2, amount = 1)\n\npts <- bind_rows(\n  points_time0, \n  points_time1, \n  points_time2,\n  points_time3\n)\n\nThen I’ll quickly write a couple of boring wrapper functions that will control how the size and transparency of the markers changes as a function of time…\n\nmap_size <- function(x) {\n  ambient::normalise(x, to = c(0, 2))\n}\nmap_alpha <- function(x) {\n  ambient::normalise(-x, to = c(0, .5))\n}\n\n…and now we can create some art:\n\npts %>% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(\n      size = map_size(time), \n      alpha = map_alpha(time)\n    ),\n    show.legend = FALSE\n  )\n\n\n\n\nSo pretty!\n\n\nAccumulating art with purrr\n… but also so ugly. The code I used above is awfully inelegant: I’ve “iteratively” created a sequence of data frames by writing the same line of code several times. That’s almost never the right answer, especially when the code doesn’t know in advance how many times we want to shift() the points! To fix this I could write a loop (and contrary to folklore, there’s nothing wrong with loops in R so long as you’re careful to avoid unnecessary copying). However, I’ve become addicted to functional programming tools in the purrr package, so I’m going to use those rather than write a loop.\nTo solve my problem I’m going to use the purrr::accumulate() function, which I personally feel is an underappreciated gem in the functional programming toolkit. It does precisely the thing we want to do here: it takes one object (e.g., points) as input together with a second quantity (e.g., an amount), and uses the user-supplied function (e.g., shift()) to produce a new object that can, once again, be passed to the user-supplied function (yielding new points). It continues with this process, taking the output of the last iteration of shift() and using it as input to the next iteration, until it runs out of amount values. It is very similar to the better-known purrr::reduce() function, except that it doesn’t throw away the intermediate values. The reduce() function is only interested in the destination; accumulate() is a whole journey.\nSo let’s use it. The iterate() function below gives a convenient interface:\n\niterate <- function(pts, time, step, ...) {\n  bind_rows(accumulate(\n    .x = rep(step, time), \n    .f = shift, \n    .init = pts,\n    ...\n  ))\n}\n\nHere’s the code to recreate the pts data from the previous section:\n\npts <- points_time0 %>% \n  iterate(time = 3, step = 1)\n\nIt produces the same image, but the code is nicer!"
  },
  {
    "objectID": "posts/2021-09-07_water-colours/index.html#assembling-the-parts",
    "href": "posts/2021-09-07_water-colours/index.html#assembling-the-parts",
    "title": "Art, jasmines, and the water colours",
    "section": "Assembling the parts",
    "text": "Assembling the parts\n\nAdding noise to jasmines coordinates\nThe time has come to start assembling the pieces of the jigsaw puzzle, by applying the flow fields from the previous section to the data associated with the jasmines image. The first step in doing so is to write a small extract_points() function that will take a data frame (like jas) as input, extract the positional information (x and y) and the identifier column (id), and add a time column so that we can modify positions over time:\n\nextract_points <- function(data) {\n  data %>% \n    select(x, y, id) %>% \n    mutate(time = 0)\n}\n\nHere’s how we can use this. The code below extracts the positional information from jas and then use the iterate() function to iteratively shift those positions along the paths traced out by a flow field:\n\npts <- jas %>% \n  extract_points() %>% \n  iterate(time = 20, step = .1)\n\nThe pts tibble doesn’t contain any of the colour information from jas, but it does have the “right kind” of positional information. It’s also rather pretty in its own right:\n\nmap_size <- function(x) {\n  ambient::normalise(x^2, to = c(0, 3.5))\n}\n\npts %>% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(size = map_size(time)),\n    alpha = .01,\n    show.legend = FALSE\n  ) \n\n\n\n\n\n\nJoining the noise with jasmine colours\nWe can now take the pixels from the jasmines image and make them “flow” across the image. To do this, we’ll need to reintroduce the colour information. We can do this using full_join() from the dplyr package. I’ve written a small convenience function restore_points() that performs the join only after removing the original x and y coordinates from the jas data. The reason for this is that the pts data now contains the positional information we need, so we want the x and y values from that data set. That’s easy enough: we drop those coordinates with select() and then join the two tables using only the id column. See? I promised it would be useful!\n\nrestore_points <- function(jas, pts) {\n  jas %>% \n    select(-x, -y) %>% \n    full_join(pts, by = \"id\") %>% \n    arrange(time, id) \n}\n\nThe result is a tibble that looks like this:\n\njas <- restore_points(jas, pts)\njas\n\n# A tibble: 126,000 × 11\n   shade        id   red   grn   blu    hue   sat   val     x     y  time\n   <chr>     <int> <int> <int> <int>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 #838c70ff     1   131   140   112 0.220  0.200 0.549     1     1     0\n 2 #3c3123ff     2    60    49    35 0.0933 0.417 0.235     1    10     0\n 3 #503d3dff     3    80    61    61 0      0.237 0.314     1    11     0\n 4 #363126ff     4    54    49    38 0.115  0.296 0.212     1    12     0\n 5 #443a30ff     5    68    58    48 0.0833 0.294 0.267     1    13     0\n 6 #8a6860ff     6   138   104    96 0.0317 0.304 0.541     1    14     0\n 7 #665859ff     7   102    88    89 0.988  0.137 0.4       1    15     0\n 8 #5a5d51ff     8    90    93    81 0.208  0.129 0.365     1    16     0\n 9 #535c4cff     9    83    92    76 0.260  0.174 0.361     1    17     0\n10 #944b61ff    10   148    75    97 0.950  0.493 0.580     1    18     0\n# … with 125,990 more rows\n\n\nMore importantly though, it produces images like this:\n\nmap_size <- function(x, y) {\n  ambient::normalise((1 - x) * y^2, to = c(0, 5))\n}\n\njas %>% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(\n      colour = hsv(hue, sat, .5), \n      size = map_size(val, time)\n    ), \n    alpha = .03,\n    show.legend = FALSE\n  )\n\n\n\n\nWhen colouring the image, we’re using the same “halftonesque” trick from earlier. The colours vary only in hue and saturation. The intensity values are mapped to the size aesthetic, much like we did earlier, but this time around the size aesthetic is a function of two variables: it depends on time as well as val. The way I’ve set it up here is to have the points get larger as time increases, but there’s no reason we have to do it that way. There are endless ways in which you could combine the positional, temporal, and shading data to create interesting generative art. This is only one example.\n\n\nThe last chapter\nAt last we have the tools we need to create images in a style similar (though not identical) to those produced by the Water Colours system. We can import, reorganise, and separate the data:\n\njas <- file %>% \n  import_image(width = 200, height = 120) %>% \n  construct_matrix() %>% \n  construct_tibble() %>% \n  extract_channels()\n\nWe can define flow fields with different properties, move the pixels through the fields, and rejoin the modified positions with the colour information\n\npts <- jas %>% \n  extract_points() %>% \n  iterate(\n    time = 40, \n    step = .2, \n    octaves = 10, \n    frequency = .05\n  )\n\njas <- jas %>%\n  restore_points(pts)\n\njas\n\n# A tibble: 984,000 × 11\n   shade        id   red   grn   blu    hue    sat   val     x     y  time\n   <chr>     <int> <int> <int> <int>  <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 #9c8178ff     1   156   129   120 0.0417 0.231  0.612     1     1     0\n 2 #81b564ff     2   129   181   100 0.274  0.448  0.710     1    10     0\n 3 #8b7870ff     3   139   120   112 0.0494 0.194  0.545     1   100     0\n 4 #eedfdbff     4   238   223   219 0.0351 0.0798 0.933     1   101     0\n 5 #c29aa3ff     5   194   154   163 0.962  0.206  0.761     1   102     0\n 6 #d5e1c3ff     6   213   225   195 0.233  0.133  0.882     1   103     0\n 7 #bde8beff     7   189   232   190 0.337  0.185  0.910     1   104     0\n 8 #b3dfbcff     8   179   223   188 0.367  0.197  0.875     1   105     0\n 9 #b2dcbdff     9   178   220   189 0.377  0.191  0.863     1   106     0\n10 #b3d9bfff    10   179   217   191 0.386  0.175  0.851     1   107     0\n# … with 983,990 more rows\n\n\nWe can write customised helpers to guide how information is used:\n\nmap_size <- function(x, y) {\n  12 * (1 - x) * (max(y)^2 - y^2) / y^2\n}\n\nAnd we can render the images with ggplot2:\n\npic <- jas %>% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(\n      colour = shade, \n      size = map_size(val, time)\n    ), \n    alpha = 1,\n    stroke = 0,\n    show.legend = FALSE\n  ) \n\npic\n\nWarning: Removed 1 rows containing missing values (geom_point).\n\n\n\n\n\nThe colour bleeding over the edges here is to be expected. Some of the points created with geom_point() are quite large, and they extend some distance beyond the boundaries of the original jasmines photograph. The result doesn’t appeal to my artistic sensibilities, so I’ll adjust the scale limits in ggplot2 so that we don’t get that strange border:\n\npic +\n  scale_x_continuous(limits = c(11, 190), expand = c(0, 0)) +\n  scale_y_continuous(limits = c(7, 114), expand = c(0, 0))\n\nWarning: Removed 198656 rows containing missing values (geom_point).\n\n\n\n\n\nThe end result is something that has a qualitative similarity to the Water Colours pieces, but is also possessed of a style that is very much its own. This is as it should be. It may be true that “all art is theft” – as Picasso is often misquoted as saying – but a good artistic theft is no mere replication. It can also be growth, change, and reconstruction.\nA happy ending after all."
  },
  {
    "objectID": "posts/2021-09-07_water-colours/index.html#epilogue",
    "href": "posts/2021-09-07_water-colours/index.html#epilogue",
    "title": "Art, jasmines, and the water colours",
    "section": "Epilogue",
    "text": "Epilogue\n\nI find it so amazing when people tell me that electronic music has no soul. You can’t blame the computer. If there’s no soul in the music, it’s because nobody put it there (Björk, via Tim de Sousa)\n\n\n\n\n\n\n\n\n\n\n\nDeparture\n\n\n\n\n\n\n\n\n\nEcho\n\n\n\n\n\n\n\n\n\nPortal\n\n\n\n\n\n\n\n\n\nGods of Salt, Stone, and Storm\n\n\n\n\n\n\n\n\n\nEl Último Amanecer de Invierno\n\n\n\n\n\n\n\n\n\nPlume\n\n\n\n\n\n\n\n\n\nWoodland Spirits\n\n\n\n\n\n\n\n\n\nBelow the Horizon"
  },
  {
    "objectID": "posts/2022-12-18_crayola-crayon-colours/index.html",
    "href": "posts/2022-12-18_crayola-crayon-colours/index.html",
    "title": "Crayola crayon colours",
    "section": "",
    "text": "What am I doing? Why am I here? Oh that’s right. So I was having coffee this morning reading the nerd news on mastodon as a girl likes to do and this this very cool post about crayon colours by Kim Scheinberg caught my attention.\nThe image comes from this blog post by Stephen Von Worley – he has a follow up too. Interesting. I realise I am of course about to waste half a day on this…"
  },
  {
    "objectID": "posts/2022-12-18_crayola-crayon-colours/index.html#its-the-prologue-baby",
    "href": "posts/2022-12-18_crayola-crayon-colours/index.html#its-the-prologue-baby",
    "title": "Crayola crayon colours",
    "section": "It’s the prologue, baby",
    "text": "It’s the prologue, baby\nHaving read the blog posts by Von Worley I worked out that the source of the data is the Wikipedia list of Crayola crayon colours, and as it happens I know how to pull data from Wikipedia tables into R. Thanks to this amazing post by Isabella Velásquez, I’d learned all about using the polite package to make sure that my webscraping is appropriate and respectful, and using the rvest package to do the actual scraping. What I had assumed, is that reading the table into R was going to be a simple matter of writing some code like this…\n\nurl <- \"https://en.wikipedia.org/wiki/List_of_Crayola_crayon_colors\"\nraw <- url |>\n  polite::bow() |>\n  polite::scrape() |>\n  rvest::html_nodes(\"table.wikitable\")\n\nThis scrapes all the tables from the page, but I only want the first one. That’s the big table with the listing of Crayolas standard colours. The table itself looks a little like this:\n\nraw[[1]]\n\n{html_node}\n<table class=\"wikitable sortable\">\n[1] <caption>\\n</caption>\n[2] <tbody>\\n<tr>\\n<th width=\"10%\" class=\"sortable\">Color\\n</th>\\n<th wid ...\n\n\nI don’t want to parse the html myself, but the hope is that I can use something like the html_table() function to extract the table and return a data frame. Okay, let’s give it a go…\n\ncrayola <- raw[[1]] |> rvest::html_table()\n\nWell it hasn’t thrown an error, but when I look at the crayola data frame…\n\ncrayola\n\n# A tibble: 168 × 9\n   Color Name                    `Hexadecimal in their website depiction[b]` Years in production…¹ Notes 16-Bo…² 24-Bo…³ 32-Bo…⁴ 64-Bo…⁵\n   <lgl> <chr>                   <chr>                                       <chr>                 <chr> <chr>   <chr>   <chr>   <chr>  \n 1 NA    Red                     \"#ED0A3F\"                                   1903–present          \"\"    \"Yes\"   \"Yes\"   \"Yes\"   \"Yes\"  \n 2 NA    Maroon                  \"#C32148\"                                   1949–present          \"Kno… \"No\"    \"No\"    \"No\"    \"No\"   \n 3 NA    Scarlet                 \"#FD0E35\"                                   1998–present          \"Kno… \"No\"    \"Yes\"   \"Yes\"   \"Yes\"  \n 4 NA    Brick Red               \"#C62D42\"                                   1958–present          \"\"    \"No\"    \"No\"    \"No\"    \"Yes\"  \n 5 NA    English Vermilion       \"\"                                          1903–1935             \"Als… \"\"      \"\"      \"\"      \"\"     \n 6 NA    Madder Lake             \"\"                                          1903–1935             \"\"    \"\"      \"\"      \"\"      \"\"     \n 7 NA    Permanent Geranium Lake \"\"                                          1903–circa 1910       \"\"    \"\"      \"\"      \"\"      \"\"     \n 8 NA    Maximum Red             \"\"                                          1926–1944             \"Par… \"\"      \"\"      \"\"      \"\"     \n 9 NA    Chestnut                \"#B94E48\"                                   1903–present          \"Kno… \"No\"    \"No\"    \"Yes\"   \"Yes\"  \n10 NA    Orange-Red              \"#FF5349\"                                   1958–1990             \"\"    \"\"      \"\"      \"\"      \"\"     \n# … with 158 more rows, and abbreviated variable names ¹​`Years in production[2]`, ²​`16-Box`, ³​`24-Box`, ⁴​`32-Box`, ⁵​`64-Box`\n\n\n… I encounter a rather awkward problem. The color field, which renders on the Wikipedia page as a pretty block of colour showing what the crayon colour looks like, is empty. Sure, I do have text containing hex codes for some of the crayons, but the missing data isn’t missing at random. Old crayon colours are the ones systematically missing an official hex code. Okay, so I really would like to have some data in my color column."
  },
  {
    "objectID": "posts/2022-12-18_crayola-crayon-colours/index.html#act-i-i-think-it-works-like-the-hanky-code",
    "href": "posts/2022-12-18_crayola-crayon-colours/index.html#act-i-i-think-it-works-like-the-hanky-code",
    "title": "Crayola crayon colours",
    "section": "Act I: I think it works like the hanky code",
    "text": "Act I: I think it works like the hanky code\nLook, that’s not a big deal right? Not having any colours for a post about colours? I guess the post will be short. Eh. Let’s set that aside and focus on the important things. Those column names need a little cleaning, so I’ll do the thing I always do and break out janitor and dplyr:\n\ncrayola <- crayola |>\n  janitor::clean_names() |>\n  dplyr::rename(\n    listed = hexadecimal_in_their_website_depiction_b,\n    years = years_in_production_2\n  )\n\nMuch nicer:\n\ncrayola\n\n# A tibble: 168 × 9\n   color name                    listed    years           notes                                     x16_box x24_box x32_box x64_box\n   <lgl> <chr>                   <chr>     <chr>           <chr>                                     <chr>   <chr>   <chr>   <chr>  \n 1 NA    Red                     \"#ED0A3F\" 1903–present    \"\"                                        \"Yes\"   \"Yes\"   \"Yes\"   \"Yes\"  \n 2 NA    Maroon                  \"#C32148\" 1949–present    \"Known as \\\"Dark Red\\\", 1949–1958.[2]\"    \"No\"    \"No\"    \"No\"    \"No\"   \n 3 NA    Scarlet                 \"#FD0E35\" 1998–present    \"Known as \\\"Torch Red\\\", 1998.[2]\"        \"No\"    \"Yes\"   \"Yes\"   \"Yes\"  \n 4 NA    Brick Red               \"#C62D42\" 1958–present    \"\"                                        \"No\"    \"No\"    \"No\"    \"Yes\"  \n 5 NA    English Vermilion       \"\"        1903–1935       \"Also spelled \\\"Vermillion\\\".[2]\"         \"\"      \"\"      \"\"      \"\"     \n 6 NA    Madder Lake             \"\"        1903–1935       \"\"                                        \"\"      \"\"      \"\"      \"\"     \n 7 NA    Permanent Geranium Lake \"\"        1903–circa 1910 \"\"                                        \"\"      \"\"      \"\"      \"\"     \n 8 NA    Maximum Red             \"\"        1926–1944       \"Part of the Munsell line.[2]\"            \"\"      \"\"      \"\"      \"\"     \n 9 NA    Chestnut                \"#B94E48\" 1903–present    \"Known as \\\"Indian Red\\\" before 1999.[2]\" \"No\"    \"No\"    \"Yes\"   \"Yes\"  \n10 NA    Orange-Red              \"#FF5349\" 1958–1990       \"\"                                        \"\"      \"\"      \"\"      \"\"     \n# … with 158 more rows\n\n\nNow where was I? What’s that maxim about never using background colour to encode substantive data in a table? I seem to have run afoul of that. Let’s take a look at the elements of the html table and see if I can work out where things went wrong…\n\ncells <- raw[[1]] |> rvest::html_elements(\"td\")\ncells\n\n{xml_nodeset (1512)}\n [1] <td style=\"background: #ED0A3F; color: white\"> \\n</td>\n [2] <td>Red\\n</td>\n [3] <td align=\"center\" style=\"background:#E9E9E9\">#ED0A3F\\n</td>\n [4] <td>1903–present\\n</td>\n [5] <td>\\n</td>\n [6] <td style=\"background:#9EFF9E;vertical-align:middle;text-align:cente ...\n [7] <td style=\"background:#9EFF9E;vertical-align:middle;text-align:cente ...\n [8] <td style=\"background:#9EFF9E;vertical-align:middle;text-align:cente ...\n [9] <td style=\"background:#9EFF9E;vertical-align:middle;text-align:cente ...\n[10] <td style=\"background: #C32148; color: white\"> \\n</td>\n[11] <td>Maroon\\n</td>\n[12] <td align=\"center\" style=\"background:#E9E9E9\">#C32148\\n</td>\n[13] <td>\\n<a href=\"/wiki/1949\" title=\"1949\">1949</a>–present\\n</td>\n[14] <td>Known as \"Dark Red\", 1949–1958.<sup id=\"cite_ref-WelterColorName ...\n[15] <td style=\"background:#FFC7C7;vertical-align:middle;text-align:cente ...\n[16] <td style=\"background:#FFC7C7;vertical-align:middle;text-align:cente ...\n[17] <td style=\"background:#FFC7C7;vertical-align:middle;text-align:cente ...\n[18] <td style=\"background:#FFC7C7;vertical-align:middle;text-align:cente ...\n[19] <td style=\"background: #FD0E35; color: white\"> \\n</td>\n[20] <td>Scarlet\\n</td>\n...\n\n\nOkay yeah. You can see the problem looking at the 1st and 10th line of the output here. The piece of information we need is embedded in the css style attribute, and it’s only in the style metadata attribute. There’s no data in the actual “td” element for html_table() to capture. I’m going to have to fix that myself I guess. Sigh.\nThe first part of my process was to find the relevant subset of cells. There’s probably a better way to do it, but my approach was based on noting that (a) it’s really easy to find the cells containing the colour names (“Red”, “Maroon”, etc), and (b) the cell to the left of it is always the one that has the background colour that I’m looking for. So, my first step was to manually pull out the text in each cell. That’s easy to do with rvest thanks to the html_text() function, and just to make my life a little easier I used stringr to remove all the \\n characters at the end of each cell:\n\ncell_text <- cells |>\n  rvest::html_text() |>\n  stringr::str_remove_all(\"\\n$\")\n\nAnd here’s the text in the first 20 cells:\n\ncell_text[1:20]\n\n [1] \" \"                                   \n [2] \"Red\"                                 \n [3] \"#ED0A3F\"                             \n [4] \"1903–present\"                        \n [5] \"\"                                    \n [6] \"Yes\"                                 \n [7] \"Yes\"                                 \n [8] \"Yes\"                                 \n [9] \"Yes\"                                 \n[10] \" \"                                   \n[11] \"Maroon\"                              \n[12] \"#C32148\"                             \n[13] \"1949–present\"                        \n[14] \"Known as \\\"Dark Red\\\", 1949–1958.[2]\"\n[15] \"No\"                                  \n[16] \"No\"                                  \n[17] \"No\"                                  \n[18] \"No\"                                  \n[19] \" \"                                   \n[20] \"Scarlet\"                             \n\n\nSo when I now match this text against the colour names stored in crayola$name, I’ll detect “Red” in cell 2, “Maroon” in cell 11, and so on. If I subtract 1 from each of these values, I now have the indices of the table cells that contain the style information I need.\n\nind <- which(cell_text %in% crayola$name) - 1\n\nHere’s what those cells look like:\n\ncells[ind]\n\n{xml_nodeset (168)}\n [1] <td style=\"background: #ED0A3F; color: white\"> \\n</td>\n [2] <td style=\"background: #C32148; color: white\"> \\n</td>\n [3] <td style=\"background: #FD0E35; color: white\"> \\n</td>\n [4] <td style=\"background: #C62D42; color: white\"> \\n</td>\n [5] <td style=\"background: #CC474B; color: white\"> \\n</td>\n [6] <td style=\"background: #CC3336; color: white\"> \\n</td>\n [7] <td style=\"background: #E12C2C; color: white\"> \\n</td>\n [8] <td style=\"background: #D92121; color: white\"> \\n</td>\n [9] <td style=\"background: #B94E48; color: white\"> \\n</td>\n[10] <td style=\"background: #FF5349; color: white\"> \\n</td>\n[11] <td style=\"background: #FE4C40; color: white\"> \\n</td>\n[12] <td style=\"background: #FE6F5E; color: white\"> \\n</td>\n[13] <td style=\"background: #B33B24; color: white\"> \\n</td>\n[14] <td style=\"background: #CC553D; color: white\"> \\n</td>\n[15] <td style=\"background: #E6735C; color: white\"> \\n</td>\n[16] <td style=\"background: #FF9980; color: white\"> \\n</td>\n[17] <td style=\"background: #E58E73; color: white\"> \\n</td>\n[18] <td style=\"background: #FF7034; color: white\"> \\n</td>\n[19] <td style=\"background: #FF681F; color: white\"> \\n</td>\n[20] <td style=\"background: #FF8833; color: white\"> \\n</td>\n...\n\n\nThat’s much nicer. Now I have something with a consistent format that I can process without too much pain. The rvest package has a html_attr() function which I can use to pull out the contents of the style attribute as a string. So, after spending a few minutes trying to remember how regular expressions work, I used str_extract() to pull out the hexadecimal codes like this:\n\nbackground <- cells[ind] |>\n  rvest::html_attr(\"style\") |>\n  stringr::str_extract(\"#[0-9ABCDEF]{6}\")\n\nLet’s take a look:\n\nbackground\n\n  [1] \"#ED0A3F\" \"#C32148\" \"#FD0E35\" \"#C62D42\" \"#CC474B\" \"#CC3336\" \"#E12C2C\" \"#D92121\" \"#B94E48\" \"#FF5349\" \"#FE4C40\" \"#FE6F5E\" \"#B33B24\"\n [14] \"#CC553D\" \"#E6735C\" \"#FF9980\" \"#E58E73\" \"#FF7034\" \"#FF681F\" \"#FF8833\" \"#FFB97B\" \"#ECAC76\" \"#E77200\" \"#FFAE42\" \"#F2BA49\" \"#FBE7B2\"\n [27] \"#F2C649\" \"#F8D568\" \"#FCD667\" \"#FED85D\" \"#FBE870\" \"#F1E788\" \"#FFEB00\" \"#B5B35C\" \"#ECEBBD\" \"#FAFA37\" \"#FFFF99\" \"#FFFF9F\" \"#D9E650\"\n [40] \"#ACBF60\" \"#AFE313\" \"#BEE64B\" \"#C5E17A\" \"#5E8C31\" \"#7BA05B\" \"#9DE093\" \"#63B76C\" \"#4D8C57\" \"#01A638\" \"#6CA67C\" \"#5FA777\" \"#93DFB8\"\n [53] \"#33CC99\" \"#1AB385\" \"#29AB87\" \"#00CC99\" \"#00755E\" \"#8DD9CC\" \"#01796F\" \"#30BFBF\" \"#00CCCC\" \"#008080\" \"#8FD8D8\" \"#95E0E8\" \"#6CDAE7\"\n [66] \"#2D383A\" \"#76D7EA\" \"#7ED4E6\" \"#0095B7\" \"#009DC4\" \"#02A4D3\" \"#47ABCC\" \"#2EB4E6\" \"#339ACC\" \"#93CCEA\" \"#2887C8\" \"#003366\" \"#0066CC\"\n [79] \"#1560BD\" \"#0066FF\" \"#A9B2C3\" \"#C3CDE6\" \"#4570E6\" \"#3C69E7\" \"#7A89B8\" \"#4F69C6\" \"#8D90A1\" \"#8C90C8\" \"#7070CC\" \"#9999CC\" \"#ACACE6\"\n [92] \"#766EC8\" \"#6456B7\" \"#3F26BF\" \"#8B72BE\" \"#652DC1\" \"#6B3FA0\" \"#8359A3\" \"#8F47B3\" \"#C9A0DC\" \"#BF8FCC\" \"#803790\" \"#733380\" \"#D6AEDD\"\n[105] \"#C154C1\" \"#FC74FD\" \"#732E6C\" \"#E667CE\" \"#E29CD2\" \"#8E3179\" \"#D96CBE\" \"#D8BFD8\" \"#C8509B\" \"#BB3385\" \"#D982B5\" \"#A63A79\" \"#A50B5E\"\n[118] \"#614051\" \"#F653A6\" \"#DA3287\" \"#FF3399\" \"#FBAED2\" \"#FFB7D5\" \"#FFA6C9\" \"#F7468A\" \"#E30B5C\" \"#FDD7E4\" \"#E62E6B\" \"#DB5079\" \"#FC80A5\"\n[131] \"#F091A9\" \"#FF91A4\" \"#A55353\" \"#CA3435\" \"#FEBAAD\" \"#F7A38E\" \"#E97451\" \"#AF593E\" \"#9E5B40\" \"#87421F\" \"#926F5B\" \"#DEA681\" \"#D27D46\"\n[144] \"#664228\" \"#FA9D5A\" \"#EDC9AF\" \"#FFCBA4\" \"#805533\" \"#FDD5B1\" \"#EED9C4\" \"#665233\" \"#837050\" \"#E6BC5C\" \"#92926E\" \"#E6BE8A\" \"#C9C0BB\"\n[157] \"#DA8A67\" \"#C88A65\" \"#000000\" \"#736A62\" \"#8B8680\" \"#C8C8CD\" \"#D9D6CF\" \"#FFFFFF\" \"#F1D651\" \"#DDEBEC\" \"#D9DAD2\" \"#C0D5F0\"\n\n\nYay, those look like hex colours. Better yet, because I’ve been careful to ensure that I’ve matched everything to the correct colours and in the correct order, I can insert them into the crayola tibble where they should have been in the first place:\n\ncrayola$color <- background\ncrayola\n\n# A tibble: 168 × 9\n   color   name                    listed    years           notes                                     x16_box x24_box x32_box x64_box\n   <chr>   <chr>                   <chr>     <chr>           <chr>                                     <chr>   <chr>   <chr>   <chr>  \n 1 #ED0A3F Red                     \"#ED0A3F\" 1903–present    \"\"                                        \"Yes\"   \"Yes\"   \"Yes\"   \"Yes\"  \n 2 #C32148 Maroon                  \"#C32148\" 1949–present    \"Known as \\\"Dark Red\\\", 1949–1958.[2]\"    \"No\"    \"No\"    \"No\"    \"No\"   \n 3 #FD0E35 Scarlet                 \"#FD0E35\" 1998–present    \"Known as \\\"Torch Red\\\", 1998.[2]\"        \"No\"    \"Yes\"   \"Yes\"   \"Yes\"  \n 4 #C62D42 Brick Red               \"#C62D42\" 1958–present    \"\"                                        \"No\"    \"No\"    \"No\"    \"Yes\"  \n 5 #CC474B English Vermilion       \"\"        1903–1935       \"Also spelled \\\"Vermillion\\\".[2]\"         \"\"      \"\"      \"\"      \"\"     \n 6 #CC3336 Madder Lake             \"\"        1903–1935       \"\"                                        \"\"      \"\"      \"\"      \"\"     \n 7 #E12C2C Permanent Geranium Lake \"\"        1903–circa 1910 \"\"                                        \"\"      \"\"      \"\"      \"\"     \n 8 #D92121 Maximum Red             \"\"        1926–1944       \"Part of the Munsell line.[2]\"            \"\"      \"\"      \"\"      \"\"     \n 9 #B94E48 Chestnut                \"#B94E48\" 1903–present    \"Known as \\\"Indian Red\\\" before 1999.[2]\" \"No\"    \"No\"    \"Yes\"   \"Yes\"  \n10 #FF5349 Orange-Red              \"#FF5349\" 1958–1990       \"\"                                        \"\"      \"\"      \"\"      \"\"     \n# … with 158 more rows\n\n\nFinally!"
  },
  {
    "objectID": "posts/2022-12-18_crayola-crayon-colours/index.html#act-ii-one-hundred-years-of-tidyr",
    "href": "posts/2022-12-18_crayola-crayon-colours/index.html#act-ii-one-hundred-years-of-tidyr",
    "title": "Crayola crayon colours",
    "section": "Act II: One hundred years of tidyr",
    "text": "Act II: One hundred years of tidyr\nIf life were at all fair my data wrangling woes would now be over, but of course they are not. If I’m going to analyse the Crayola data by year it will be useful to me if the year column has nicely formatted data, and of course it does not:\n\ncrayola$years\n\n  [1] \"1903–present\"               \"1949–present\"               \"1998–present\"               \"1958–present\"              \n  [5] \"1903–1935\"                  \"1903–1935\"                  \"1903–circa 1910\"            \"1926–1944\"                 \n  [9] \"1903–present\"               \"1958–1990\"                  \"1997–present\"               \"1958–present\"              \n [13] \"1903–circa 1910\"            \"1903–1944\"                  \"1903–circa 1910\"            \"1990–present\"              \n [17] \"1926–1944\"                  \"1958–present\"               \"1930–present\"               \"1903–present\"              \n [21] \"1993–present\"               \"1926–1944, 1949–1958\"       \"2003–present\"               \"1930–present\"              \n [25] \"1926–1944\"                  \"1998–present\"               \"1903–1990\"                  \"1958–1990\"                 \n [29] \"1903–present\"               \"1990–2017[2][3][4]\"         \"1903–present\"               \"1958–present\"              \n [33] \"1926–1944\"                  \"1903–present\"               \"1958–present\"               \"1926–1944\"                 \n [37] \"1998–present\"               \"1903–1990\"                  \"1926–1944\"                  \"1926–1944\"                 \n [41] \"2003–present\"               \"1903–1935\"                  \"1930–present\"               \"1926–1944\"                 \n [45] \"1993–present\"               \"1993–present\"               \"1998–present\"               \"1926–1944\"                 \n [49] \"1903–present\"               \"1903–1939\"                  \"1949–present\"               \"1949–present\"              \n [53] \"1993–present\"               \"1998–present\"               \"1990–present\"               \"1997–present\"              \n [57] \"1993–present\"               \"1926–1944\"                  \"1903–1949, 1958–present\"    \"1926–1944\"                 \n [61] \"1993–present\"               \"1990–2003\"                  \"1958\"                       \"1949–present\"              \n [65] \"1935–present\"               \"1998–present\"               \"1958–present\"               \"1926–1944\"                 \n [69] \"1949–present\"               \"1993–present\"               \"1990–present\"               \"1926–1958\"                 \n [73] \"1903–1958\"                  \"1949–1958\"                  \"1958–present\"               \"1958–1990\"                 \n [77] \"1903–present\"               \"1958–present\"               \"1993–present\"               \"1949–present\"              \n [81] \"1958–present\"               \"1958–present\"               \"1935–1958\"                  \"2017–present[5]\"           \n [85] \"2003–present\"               \"1999–present\"               \"1998–present\"               \"1903–1958\"                 \n [89] \"1903–circa 1910\"            \"1998–present\"               \"1926–1944\"                  \"1903–circa 1910, 1930–1990\"\n [93] \"1949–present\"               \"1903–1944\"                  \"1926–1944\"                  \"1998–present\"              \n [97] \"1990–present\"               \"1930–1949, 1958–present\"    \"1949–1958\"                  \"1993–present\"              \n[101] \"1949–1958\"                  \"1997–present\"               \"1926–1944\"                  \"1993–present\"              \n[105] \"1990–present\"               \"1997–present\"               \"1903–1930\"                  \"1949–1958\"                 \n[109] \"1949–present\"               \"1958–present\"               \"1949–1958\"                  \"1949–1999\"                 \n[113] \"1958–2003\"                  \"1930–present\"               \"1926–1944\"                  \"1926–1944\"                 \n[117] \"2003–present\"               \"1998–present\"               \"1903–present\"               \"1993–present\"              \n[121] \"1990–present\"               \"1958–present\"               \"1998–present\"               \"1903–present\"              \n[125] \"1958–present\"               \"1993–present\"               \"1998–present\"               \"1935–1958\"                 \n[129] \"1998–present\"               \"1993–present\"               \"1993–present\"               \"1949–present\"              \n[133] \"1926–1944\"                  \"1949–present\"               \"1958–present\"               \"1998–present\"              \n[137] \"1903–present\"               \"1903–present\"               \"1935–1944, 1958–present\"    \"1998–present\"              \n[141] \"1998–present\"               \"1993–present\"               \"1958–present\"               \"1903–1910\"                 \n[145] \"1958–present\"               \"1998–present\"               \"1903–present\"               \"1903–1944\"                 \n[149] \"1958–present\"               \"1998–present\"               \"1903–1990\"                  \"1998–present\"              \n[153] \"1903–circa 1910\"            \"1903–1944\"                  \"1953–present\"               \"1903–present\"              \n[157] \"1903–1915, 1958–present\"    \"1998–present\"               \"1903–present\"               \"1903–1910\"                 \n[161] \"1926–present\"               \"1958–1990\"                  \"1993–present\"               \"1903–present\"              \n[165] \"2021, 2022\"                 \"2021, 2022\"                 \"2021, 2022\"                 \"2021, 2022\"                \n\n\nThere are several problems I’m going to need to solve to get this into a regular form:\n\nThe “circa” text will have to be removed\nWhite spaces will need to be removed\nFootnote text will need to be removed\nThe word “present” will have to be replaced by “2022”\nThe single year “1958” will need to be replaced by an interval “1958-1958”\nThe comma separated list “2021,2022” will need to be an interval “2021-2022”\n\nHere’s a little bit of stringr code that does that:\n\ncrayola <- crayola |>\n  dplyr::mutate(\n    years = years |>\n      stringr::str_remove_all(\" \") |>\n      stringr::str_remove_all(\"\\\\[.\\\\]\") |>\n      stringr::str_remove_all(\"circa\") |>\n      stringr::str_replace_all(\"present\", \"2022\") |>\n      stringr::str_replace_all(\"^1958$\", \"1958-1958\") |>\n      stringr::str_replace_all(\"2021,2022\", \"2021-2022\"),\n  ) \n\nThe years column now has a regular form. Each entry is either a contiguous interval like \"1903-2022\", or a comma separated list of two such intervals like \"1903–1910,1930–1990\":\n\ncrayola$years\n\n  [1] \"1903–2022\"           \"1949–2022\"           \"1998–2022\"           \"1958–2022\"           \"1903–1935\"          \n  [6] \"1903–1935\"           \"1903–1910\"           \"1926–1944\"           \"1903–2022\"           \"1958–1990\"          \n [11] \"1997–2022\"           \"1958–2022\"           \"1903–1910\"           \"1903–1944\"           \"1903–1910\"          \n [16] \"1990–2022\"           \"1926–1944\"           \"1958–2022\"           \"1930–2022\"           \"1903–2022\"          \n [21] \"1993–2022\"           \"1926–1944,1949–1958\" \"2003–2022\"           \"1930–2022\"           \"1926–1944\"          \n [26] \"1998–2022\"           \"1903–1990\"           \"1958–1990\"           \"1903–2022\"           \"1990–2017\"          \n [31] \"1903–2022\"           \"1958–2022\"           \"1926–1944\"           \"1903–2022\"           \"1958–2022\"          \n [36] \"1926–1944\"           \"1998–2022\"           \"1903–1990\"           \"1926–1944\"           \"1926–1944\"          \n [41] \"2003–2022\"           \"1903–1935\"           \"1930–2022\"           \"1926–1944\"           \"1993–2022\"          \n [46] \"1993–2022\"           \"1998–2022\"           \"1926–1944\"           \"1903–2022\"           \"1903–1939\"          \n [51] \"1949–2022\"           \"1949–2022\"           \"1993–2022\"           \"1998–2022\"           \"1990–2022\"          \n [56] \"1997–2022\"           \"1993–2022\"           \"1926–1944\"           \"1903–1949,1958–2022\" \"1926–1944\"          \n [61] \"1993–2022\"           \"1990–2003\"           \"1958-1958\"           \"1949–2022\"           \"1935–2022\"          \n [66] \"1998–2022\"           \"1958–2022\"           \"1926–1944\"           \"1949–2022\"           \"1993–2022\"          \n [71] \"1990–2022\"           \"1926–1958\"           \"1903–1958\"           \"1949–1958\"           \"1958–2022\"          \n [76] \"1958–1990\"           \"1903–2022\"           \"1958–2022\"           \"1993–2022\"           \"1949–2022\"          \n [81] \"1958–2022\"           \"1958–2022\"           \"1935–1958\"           \"2017–2022\"           \"2003–2022\"          \n [86] \"1999–2022\"           \"1998–2022\"           \"1903–1958\"           \"1903–1910\"           \"1998–2022\"          \n [91] \"1926–1944\"           \"1903–1910,1930–1990\" \"1949–2022\"           \"1903–1944\"           \"1926–1944\"          \n [96] \"1998–2022\"           \"1990–2022\"           \"1930–1949,1958–2022\" \"1949–1958\"           \"1993–2022\"          \n[101] \"1949–1958\"           \"1997–2022\"           \"1926–1944\"           \"1993–2022\"           \"1990–2022\"          \n[106] \"1997–2022\"           \"1903–1930\"           \"1949–1958\"           \"1949–2022\"           \"1958–2022\"          \n[111] \"1949–1958\"           \"1949–1999\"           \"1958–2003\"           \"1930–2022\"           \"1926–1944\"          \n[116] \"1926–1944\"           \"2003–2022\"           \"1998–2022\"           \"1903–2022\"           \"1993–2022\"          \n[121] \"1990–2022\"           \"1958–2022\"           \"1998–2022\"           \"1903–2022\"           \"1958–2022\"          \n[126] \"1993–2022\"           \"1998–2022\"           \"1935–1958\"           \"1998–2022\"           \"1993–2022\"          \n[131] \"1993–2022\"           \"1949–2022\"           \"1926–1944\"           \"1949–2022\"           \"1958–2022\"          \n[136] \"1998–2022\"           \"1903–2022\"           \"1903–2022\"           \"1935–1944,1958–2022\" \"1998–2022\"          \n[141] \"1998–2022\"           \"1993–2022\"           \"1958–2022\"           \"1903–1910\"           \"1958–2022\"          \n[146] \"1998–2022\"           \"1903–2022\"           \"1903–1944\"           \"1958–2022\"           \"1998–2022\"          \n[151] \"1903–1990\"           \"1998–2022\"           \"1903–1910\"           \"1903–1944\"           \"1953–2022\"          \n[156] \"1903–2022\"           \"1903–1915,1958–2022\" \"1998–2022\"           \"1903–2022\"           \"1903–1910\"          \n[161] \"1926–2022\"           \"1958–1990\"           \"1993–2022\"           \"1903–2022\"           \"2021-2022\"          \n[166] \"2021-2022\"           \"2021-2022\"           \"2021-2022\"          \n\n\nThat’s better because the data format is now consistent, but it’s not tidy. In the long run, what I really want is a nice tidy tibble: each row should correspond to a single observation. If “Red” was a colour in 1935, then there should be a row in my table for which name = \"Red\" and year = 1935. That’s not quite what I have here, so I have more data wrangling to do and this time tidyr will be my best friend.\nThe first thing I’m going to do is use tidyr::separate() to split the years variable into two variables, years_1 and years_2. The years_1 variable will contain the first time interval for which a particular crayon colour was in production, and the years_2 variable will contain the second interval in which it was in production. For almost all colours, years_2 will be NA. It’s only those special cases like \"1903–1910,1930–1990\" that will have values in both.\nThen, because I don’t really see a need to have two variables that both represent a period of time, I’ll use tidyr::pivot_longer() to give myself a data set in which there is one row for every continuous time interval:\n\ncrayola <- crayola |>\n  tidyr::separate(\n    col = years,\n    into = c(\"years_1\", \"years_2\"),\n    sep = \",\",\n    fill = \"right\"\n  ) |>\n  tidyr::pivot_longer(\n    cols = starts_with(\"years_\"),\n    names_prefix = \"years_\",\n    names_to = \"interval\",\n    values_to = \"years\"\n  ) |>\n  dplyr::filter(!is.na(years))\n\nTo give you a sense of what the data looks like in this form, I’ll pull out the rows corresponding to two different crayon colours, “Maroon” and “Violet-Blue”:\n\ncrayola |>\n  dplyr::filter(name == \"Maroon\" | name == \"Violet-Blue\")\n\n# A tibble: 3 × 10\n  color   name        listed  notes                                     x16_box x24_box x32_box x64_box interval years    \n  <chr>   <chr>       <chr>   <chr>                                     <chr>   <chr>   <chr>   <chr>   <chr>    <chr>    \n1 #C32148 Maroon      #C32148 \"Known as \\\"Dark Red\\\", 1949–1958.[2]\"    \"No\"    \"No\"    \"No\"    \"No\"    1        1949–2022\n2 #766EC8 Violet-Blue #766EC8 \"Known as \\\"Blue-Violet\\\", 1930–1958.[2]\" \"\"      \"\"      \"\"      \"\"      1        1903–1910\n3 #766EC8 Violet-Blue #766EC8 \"Known as \\\"Blue-Violet\\\", 1930–1958.[2]\" \"\"      \"\"      \"\"      \"\"      2        1930–1990\n\n\nThe “Maroon” crayon has been in production continuously since 1949, so there is only one row in the table for that one. The “Violet-Blue” crayon was in production from 1903 to 1910, and again from 1930 to 1990. These two production periods are each represented as a row.\nExcellent. Next, I’ll use separate() again to split the years interval into two columns, one for the year_started and another for the year_ended. Having done so, the year information is finally in a numeric format, so I can coerce it from character to integer:\n\ncrayola <- crayola |>\n  tidyr::separate(\n    col = years,\n    into = c(\"year_started\", \"year_ended\")\n  ) |>\n  dplyr::mutate(\n    interval = as.integer(interval),\n    year_started = as.integer(year_started),\n    year_ended = as.integer(year_ended)\n  )\n\n\ncrayola\n\n# A tibble: 174 × 11\n   color   name                    listed    notes                               x16_box x24_box x32_box x64_box inter…¹ year_…² year_…³\n   <chr>   <chr>                   <chr>     <chr>                               <chr>   <chr>   <chr>   <chr>     <int>   <int>   <int>\n 1 #ED0A3F Red                     \"#ED0A3F\" \"\"                                  \"Yes\"   \"Yes\"   \"Yes\"   \"Yes\"         1    1903    2022\n 2 #C32148 Maroon                  \"#C32148\" \"Known as \\\"Dark Red\\\", 1949–1958.… \"No\"    \"No\"    \"No\"    \"No\"          1    1949    2022\n 3 #FD0E35 Scarlet                 \"#FD0E35\" \"Known as \\\"Torch Red\\\", 1998.[2]\"  \"No\"    \"Yes\"   \"Yes\"   \"Yes\"         1    1998    2022\n 4 #C62D42 Brick Red               \"#C62D42\" \"\"                                  \"No\"    \"No\"    \"No\"    \"Yes\"         1    1958    2022\n 5 #CC474B English Vermilion       \"\"        \"Also spelled \\\"Vermillion\\\".[2]\"   \"\"      \"\"      \"\"      \"\"            1    1903    1935\n 6 #CC3336 Madder Lake             \"\"        \"\"                                  \"\"      \"\"      \"\"      \"\"            1    1903    1935\n 7 #E12C2C Permanent Geranium Lake \"\"        \"\"                                  \"\"      \"\"      \"\"      \"\"            1    1903    1910\n 8 #D92121 Maximum Red             \"\"        \"Part of the Munsell line.[2]\"      \"\"      \"\"      \"\"      \"\"            1    1926    1944\n 9 #B94E48 Chestnut                \"#B94E48\" \"Known as \\\"Indian Red\\\" before 19… \"No\"    \"No\"    \"Yes\"   \"Yes\"         1    1903    2022\n10 #FF5349 Orange-Red              \"#FF5349\" \"\"                                  \"\"      \"\"      \"\"      \"\"            1    1958    1990\n# … with 164 more rows, and abbreviated variable names ¹​interval, ²​year_started, ³​year_ended\n\n\nWe’re getting close. At this point the last bit of work I have to do to fix the year data is unpack it. Instead of representing the data for Maroon crayons with one row with a year_started value of 1949 and year_ended value of 2022, I want to have a single column called year, and the data should contain one row for every year in which Maroon was in production. Somewhere in the back of my head there is the thought that there must be an easy way to do this with tidyr, but my ingenuity failed me this time and I fell back on my usual solution… purrr.\nIt’s a two-step process. Step one: write a little function that expects to receive the values stored in one row of the existing data frame, and returns a new data frame in the format I want. In this case, I want a tibble that has one row for each year in range starting year_started and ending year_ended, but otherwise has the same structure as the existing data. The unpack_row() function below does that:\n\nunpack_row <- function(color, name, year_started, year_ended, ...) {\n  tibble::tibble(\n    name = name,\n    color = color,\n    year = year_started:year_ended,\n    ...\n  )\n}\n\nSo now I’ll do the unpacking with purrr::pmap_dfr(), sort the rows into a nice order using dplyr::arrange(), and add an id column to ensure that every row in the table has a unique identifier:\n\ncrayola <- crayola |>\n  purrr::pmap_dfr(unpack_row) |>\n  dplyr::arrange(year, color) |>\n  dplyr::mutate(id = dplyr::row_number())\n\nVoilà!\n\ncrayola\n\n# A tibble: 7,749 × 11\n   name                color    year listed    notes                                       x16_box x24_box x32_box x64_box inter…¹    id\n   <chr>               <chr>   <int> <chr>     <chr>                                       <chr>   <chr>   <chr>   <chr>     <int> <int>\n 1 Black               #000000  1903 \"#000000\" \"\"                                          \"Yes\"   \"Yes\"   \"Yes\"   \"Yes\"         1     1\n 2 Midnight Blue       #003366  1903 \"#003366\" \"Known as \\\"Prussian Blue\\\", 1903–1958.[2]\" \"No\"    \"No\"    \"No\"    \"No\"          1     2\n 3 Pine Green          #01796F  1903 \"#01796F\" \"Known as \\\"Dark Chrome Green\\\" (\\\"Chrome … \"No\"    \"No\"    \"No\"    \"No\"          1     3\n 4 Green               #01A638  1903 \"#01A638\" \"\"                                          \"Yes\"   \"Yes\"   \"Yes\"   \"Yes\"         1     4\n 5 Blue (I)            #2EB4E6  1903 \"\"        \"Known as \\\"Celestial Blue\\\", 1935–1949, a… \"\"      \"\"      \"\"      \"\"            1     5\n 6 Ultramarine Blue    #3F26BF  1903 \"\"        \"\"                                          \"\"      \"\"      \"\"      \"\"            1     6\n 7 Van Dyke Brown      #664228  1903 \"\"        \"Same color as \\\"Brown\\\" (1903–1910).[2]\"   \"\"      \"\"      \"\"      \"\"            1     7\n 8 Raw Umber           #665233  1903 \"#665233\" \"\"                                          \"\"      \"\"      \"\"      \"\"            1     8\n 9 Medium Chrome Green #6CA67C  1903 \"\"        \"\\\"Chrome Green, Medium\\\" on labels. Produ… \"\"      \"\"      \"\"      \"\"            1     9\n10 Celestial Blue      #7070CC  1903 \"\"        \"\"                                          \"\"      \"\"      \"\"      \"\"            1    10\n# … with 7,739 more rows, and abbreviated variable name ¹​interval"
  },
  {
    "objectID": "posts/2022-12-18_crayola-crayon-colours/index.html#act-iii-i-said-pet-i-said-luv-i-said-pet",
    "href": "posts/2022-12-18_crayola-crayon-colours/index.html#act-iii-i-said-pet-i-said-luv-i-said-pet",
    "title": "Crayola crayon colours",
    "section": "Act III: I said pet, I said LUV, I said pet…",
    "text": "Act III: I said pet, I said LUV, I said pet…\nOkay I have one last thing to do before I’m done with the data wrangling. Having hex strings associated with each crayon colour is nice and is exactly what I need for plotting, but you can’t construct a nice ordering of colours since colour space is three dimensional, more or less. I’m trying my very best to forget everything I ever learned about the psychophysics of human colour perception, but even so I’m not silly enough to try to work with raw RGB values. Instead I’ll use the colorspace package to convert extract hue/saturation/value coordinates, as well as my slightly-preferred method, CIELUV color coordinates:\n\nHSV <- colorspace::coords(as(colorspace::hex2RGB(crayola$color), \"HSV\"))\nLUV <- colorspace::coords(as(colorspace::hex2RGB(crayola$color), \"LUV\"))\n\nEach of these commands returns a matrix with three columns and the same number of rows as the crayola data frame. The first few rows of the HSV matrix look like this:\n\nHSV[1:10,]\n\n              H         S         V\n [1,]   0.00000 0.0000000 0.0000000\n [2,] 210.00000 1.0000000 0.4000000\n [3,] 175.00000 0.9917355 0.4745098\n [4,] 140.00000 0.9939759 0.6509804\n [5,] 196.30435 0.8000000 0.9019608\n [6,] 249.80392 0.8010471 0.7490196\n [7,]  25.16129 0.6078431 0.4000000\n [8,]  36.47059 0.5000000 0.4000000\n [9,] 136.55172 0.3493976 0.6509804\n[10,] 240.00000 0.4509804 0.8000000\n\n\nSo now I can store all six coordinates in the crayola dataframe, along with the LUV-space version of “hue” which I compute in the last line here:\n\ncrayola <- crayola |>\n  dplyr::mutate(\n    hue = HSV[, \"H\"],\n    sat = HSV[, \"S\"],\n    val = HSV[, \"V\"],\n    L = LUV[, \"L\"],\n    U = LUV[, \"U\"],\n    V = LUV[, \"V\"],\n    hue2 = atan2(V, U)\n  )\n\nThe Wikipedia page on CIELUV has a little more information on this, but really the thing that matters is that the hue2 column is the variable I’m going to use to arrange the crayon colours when plotting them later. And so the moment arrives that, at long last, I have the data…\n\ncrayola\n\n# A tibble: 7,749 × 18\n   name             color  year listed notes x16_box x24_box x32_box x64_box inter…¹    id   hue   sat   val     L      U       V   hue2\n   <chr>            <chr> <int> <chr>  <chr> <chr>   <chr>   <chr>   <chr>     <int> <int> <dbl> <dbl> <dbl> <dbl>  <dbl>   <dbl>  <dbl>\n 1 Black            #000…  1903 \"#000… \"\"    \"Yes\"   \"Yes\"   \"Yes\"   \"Yes\"         1     1   0   0     0       0     0      0     -3.14 \n 2 Midnight Blue    #003…  1903 \"#003… \"Kno… \"No\"    \"No\"    \"No\"    \"No\"          1     2 210   1     0.4    21.3 -11.9  -40.1   -1.86 \n 3 Pine Green       #017…  1903 \"#017… \"Kno… \"No\"    \"No\"    \"No\"    \"No\"          1     3 175   0.992 0.475  45.4 -36.1    0.638  3.12 \n 4 Green            #01A…  1903 \"#01A… \"\"    \"Yes\"   \"Yes\"   \"Yes\"   \"Yes\"         1     4 140   0.994 0.651  59.5 -54.9   61.6    2.30 \n 5 Blue (I)         #2EB…  1903 \"\"     \"Kno… \"\"      \"\"      \"\"      \"\"            1     5 196.  0.8   0.902  68.7 -44.6  -53.4   -2.27 \n 6 Ultramarine Blue #3F2…  1903 \"\"     \"\"    \"\"      \"\"      \"\"      \"\"            1     6 250.  0.801 0.749  29.9  -2.77 -96.9   -1.60 \n 7 Van Dyke Brown   #664…  1903 \"\"     \"Sam… \"\"      \"\"      \"\"      \"\"            1     7  25.2 0.608 0.4    31.5  25.9   19.9    0.655\n 8 Raw Umber        #665…  1903 \"#665… \"\"    \"\"      \"\"      \"\"      \"\"            1     8  36.5 0.5   0.4    36.2  14.7   22.3    0.987\n 9 Medium Chrome G… #6CA…  1903 \"\"     \"\\\"C… \"\"      \"\"      \"\"      \"\"            1     9 137.  0.349 0.651  63.3 -28.3   26.3    2.39 \n10 Celestial Blue   #707…  1903 \"\"     \"\"    \"\"      \"\"      \"\"      \"\"            1    10 240   0.451 0.8    51.1  -5.49 -76.1   -1.64 \n# … with 7,739 more rows, and abbreviated variable name ¹​interval"
  },
  {
    "objectID": "posts/2022-12-18_crayola-crayon-colours/index.html#the-intermission-is-late-so-what",
    "href": "posts/2022-12-18_crayola-crayon-colours/index.html#the-intermission-is-late-so-what",
    "title": "Crayola crayon colours",
    "section": "The intermission is late, so what?",
    "text": "The intermission is late, so what?\nAs a rule I make it a habit to assume that, while the internet never forgets anything at all in the general sense, it doesn’t remember anything specific for any interval longer than I’d trust a man not to have conveniently lost my phone number. Moreover, while I also make it a habit not to care greatly about the archival properties of What Went Down In The Stalls At The Duke On Friday, I’m slightly more invested in, um … crayons, I guess. Or data science practices in R. Whatever.\nMy point here is that the table on wikipedia isn’t an archival source so there’s no guarantee that anything I’ve done up to this point is reproducible unless I do a bit of extra work myself and save a copy of the data…\n\nfolder <- here::here(\"posts\", \"2022-12-18_crayola-crayon-colours\")\nreadr::write_csv(crayola, fs::path(folder, \"crayola.csv\"))\n\nAs a convenient side benefit, you can download a copy of the crayola colours data as a csv file from github should you feel so inclined. There’s even a script containing most of the code for this post too :-)"
  },
  {
    "objectID": "posts/2022-12-18_crayola-crayon-colours/index.html#act-iv-not-even-chekhov-expects-it-to-go-off",
    "href": "posts/2022-12-18_crayola-crayon-colours/index.html#act-iv-not-even-chekhov-expects-it-to-go-off",
    "title": "Crayola crayon colours",
    "section": "Act IV: Not even Chekhov expects it to go off",
    "text": "Act IV: Not even Chekhov expects it to go off\nIn any case, the final act of our little drama has arrived at last. Time to make a plot. The drums roll and the audience holds their breath in antici–\nOh let’s just get on with it and load ggplot2 already. Here’s a stacked bar chart showing the number of distinct crayon colours in the Crayola standard set every year from 1903 to 2022. It has grown over time but the growth looks linear, not exponential:\n\nlibrary(ggplot2)\nbase <- crayola |> \n  dplyr::mutate(\n    color = forcats::fct_reorder(color, hue2)\n  ) |> \n  ggplot(aes(\n    x = year,\n    group = color,\n    fill = color\n  )) +\n  scale_fill_identity() +\n  NULL\n\nbase + geom_bar(show.legend = FALSE) \n\n\n\n\nBut let’s be honest, shall we? No-one at all (least of all me) is interested in determining whether the rate of growth of Crayola crayon colours in the standard set is exponential or linear. It’s just fun. The real reason we all love the Crayola post was that the image was so terribly pretty, so let’s start making something pretty, yes?\nWe can start getting something a little closer to the original if we set position = \"fill\", and I’m going to use theme_void() because honestly it’s just prettier without the words and numbers getting in the way…\n\nbase + \n  theme_void() +\n  geom_bar(\n    position = \"fill\",\n    linetype = \"blank\",\n    width = 1,\n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n\nIt’s pretty, but it’s noticeably different from the original one. In my version, there are 39 separate colours depicted on the left hand side, whereas the version that was going around on mastodon (and appears in the original blog posts) has only 8. Out of paranoia, I decided to check the counts in my data…\n\ncrayola |> \n  dplyr::count(year) |>\n  dplyr::filter(year %in% seq(1903, 2003, by = 10))\n\n# A tibble: 11 × 2\n    year     n\n   <int> <int>\n 1  1903    40\n 2  1913    32\n 3  1923    31\n 4  1933    57\n 5  1943    57\n 6  1953    49\n 7  1963    64\n 8  1973    64\n 9  1983    64\n10  1993    80\n11  2003   108\n\n\n…which does seem consistent with what the history of Crayola crayons wikipedia article has to say on the topic too:\n\nEarly Crayola advertising mentions thirty different colors, although there is no official list; in fact thirty-eight different crayons are known from Crayola boxes of this period. The largest labeled assortment was box No. 51, titled Crayola Young Artists’ Drawing Crayons, which included twenty-eight different crayons. Other colors were found in different boxes, including the “Rubens” No. 500, a twenty-four crayon assortment.\n\nOkay, so if I haven’t made a mistake, what is going on? It turns out that although Steven Von Worley’s blog post still manages to point to the “same” Wikipedia page 12 years later, the contents of the page have changed considerably. The original post was published January 15th 2010. Conveniently the wayback machine has a snapshot of that page from only a few weeks later, on February 9th. I have a very different version of the Crayola colours data than Steven’s friend Velociraptor had.\nThere are 133 colours listed in the 2010 version, and it’s missing all the colours that had gone out of production earlier than 1990. “English Vermillion”, for example, was in production from 1903 to 1935. It appears in the 2022 version of the Wikipedia data (and so it’s represented in my plots above), but it was omitted in the 2010 version of the Wikipedia data and so doesn’t appear in the version of the image that went around on Mastodon yesterday.\nHm. So what happens to my data if I crudely simulate a censoring process a little bit like the one that applied to the 2010 version of the Wikipedia page? Let’s simply ignore all crayon colours that were out of production earlier than 1990, shall we?\n\nbase + \n  theme_void() +\n  geom_bar(\n    data =  crayola |> \n      dplyr::group_by(name) |>\n      dplyr::filter(max(year) > 1990) |>\n      dplyr::ungroup() |>\n      dplyr::mutate(color = forcats::fct_reorder(color, hue2)),\n    position = \"fill\",\n    linetype = \"blank\",\n    width = 1,\n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n\nStill not exactly equivalent, but it’s closer.\nMore to the point however, I think we all have to admit that this is really the kind of data set that desperately needs to be plotted with the aid of coord_polar(), no?\n\nbase + \n  theme_void() +\n  coord_polar(theta = \"y\") + \n  geom_bar(\n    position = \"fill\",\n    linetype = \"blank\",\n    width = 1,\n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n\nI mean… not that anyone really cares what this represents, but each concentric ring represents a year of data: 1903 is in the middle, and 2022 is on the outside. Within each annual ring there is an equal-angle segment for every crayon colour in the data for that year. Whenever the crayons changed, the rings shift a bit. The colours are arranged around the circle by hue. Well, the hue-like quantity computed from the chromaticity components of the LUV coordinates. Whatever.\nAlternatively, we could turn this into a racetrack plot, where each individual colour fills an angular section of constant size, and so only the year with the most crayon colours (2022) wraps the full way round the circle. That gives us this:\n\nbase + \n  theme_void() +\n  coord_polar(theta = \"y\") + \n  geom_bar(\n    linetype = \"blank\",\n    width = 1,\n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n\nI’m not sure it has a lot of value as a data visualisation but it is so very pretty!"
  },
  {
    "objectID": "posts/2022-01-10_setting-cran-repositories/index.html",
    "href": "posts/2022-01-10_setting-cran-repositories/index.html",
    "title": "Setting CRAN repository options",
    "section": "",
    "text": "The shiny new work laptop arrives. Pristine and beautiful in its factory-fresh state. I am in love.\nI remain in love right up the moment I remember that with new work laptop comes the peculiar torture of setting up the machine. My gorgeous little Dell XPS 13 shipped with Windows 11, and while I do quite like Windows these days, I’ve become very accustomed to working in linux, so my first task was to install Ubuntu 20.04. These days that’s a pretty easy task, and the Ubuntu installer was even thoughtful enough to give me an option to enable full disk encryption. It all went smoothly. Yay me!\nEquipped with my fabulous new operating system, my next steps were to install R and RStudio, and for the first time in my life I was smart enough to remember to install the latest version of git along with the build-essential packages that I’m pretty much guaranteed to need the moment I need to build anything from source. Yay me again!\nThen comes the horror. Installing R packages. On linux. A small part of me dies.\nI’m sure every linux-based R user shares my pain and needs no explanation, but some of the gentler souls who use Windows or Mac OS may not be aware of how tiresome package installation is on linux. The problem that linux users face is that CRAN does not maintain binaries for linux, so every time a linux user wants to install a package, it has to be built locally from the source code. This is both time consuming and frustrating, and very often you have to go hunting around to discover what other system dependencies need to be installed. So many tears have been shed over this.\nSo.\nMany.\nTears."
  },
  {
    "objectID": "posts/2022-01-10_setting-cran-repositories/index.html#rstudio-package-manager",
    "href": "posts/2022-01-10_setting-cran-repositories/index.html#rstudio-package-manager",
    "title": "Setting CRAN repository options",
    "section": "RStudio package manager",
    "text": "RStudio package manager\nRecently, however, I have become aware that a better world is possible thanks to the magic of RStudio package manager. I’d sort of known that this existed as an option, but it wasn’t until today that I realised that — in addition to the fancy commercial options — RStudio maintains a public package manager as a free service: the FAQ page is here. Anyone can configure R to install packages from the RStudio public package manager, if they want to.\n\n\n\n\n\n\n\n\nBut first a tiny bit of context… back in the distant part there was this strange, nightmarish time where I was teaching students R, but RStudio was not yet a thing. Many of the little niceties that RStudio users now take for granted didn’t yet exist. In those dark years I had to spend a lot of time explaining to students that CRAN — the comprehensive R archive network — isn’t actually a single website that contains lots of R packages. It’s more like a whole network of mirrors distributed all over the world, and you’d have to manually choose which one you wanted to install packages from. It was mildly annoying. It’s considerably simpler now, because you can use the cloud.r-project.org service that automatically directs you to an appropriate server. In fact, if you’re using RStudio you’ve probably been using this service all along.\nRStudio package manager provides a modern alternative: it works like a CRAN mirror, but it has a lot of additional functionality. It has broader coverage, for instance: it includes R packages on Bioconductor as well as packages on CRAN. For my purposes, however, the attractive property is that it hosts binaries suitable for Ubuntu and other flavours of linux.\n“But how do I try it out, Danielle?” I hear you ask.\nI’m so glad you asked, dear reader, because it’s so much easier than it sounds."
  },
  {
    "objectID": "posts/2022-01-10_setting-cran-repositories/index.html#method-1-rstudio-settings",
    "href": "posts/2022-01-10_setting-cran-repositories/index.html#method-1-rstudio-settings",
    "title": "Setting CRAN repository options",
    "section": "Method 1: RStudio settings",
    "text": "Method 1: RStudio settings\nIf you’re using RStudio, the easiest way to switch to RStudio PPM is to change your settings inside RStudio. Go to the RStudio Tools menu and select Global Options. When the popup window appears, click on Packages. You’ll see a screen that looks like this:\n\n\n\n\n\n\nIf it says “RStudio Global CDN” next to “Primary CRAN repo”, then you’re using cloud.r-project.org as your CRAN repository. To switch to RStudio PPM, click on the “change” button. It will bring up a list of CRAN mirrors, and if you want you can choose one of those. However the RStudio PPM isn’t technically a CRAN mirror, so it’s not listed there. If you want to switch to using the RStudio PPM, you have to enter the URL manually.\nSo what URL do you want? Well, it depends on whether you want to install packages from binaries or from source, and on what operating system you’re using. I’m on Ubuntu 20.04, “Focal Fossa”, and the URL that serves binaries for my operating system is:\nhttps://packagemanager.rstudio.com/all/__linux__/focal/latest\nHere’s me in the process of entering the URL:\n\n\n\n\n\n\nOkay, but what if you’re not on Ubuntu 20.04? If you’re on a different version of Ubuntu or some other operating system, you can find the link you need from the package manager setup page. The relevant part of the page should look something like this:\n\n\n\n\n\n\n\nTo get the URL you’re looking for, click on the “change” link to choose your operating system, or toggle between the binary and source options."
  },
  {
    "objectID": "posts/2022-01-10_setting-cran-repositories/index.html#method-2-edit-your-r-profile",
    "href": "posts/2022-01-10_setting-cran-repositories/index.html#method-2-edit-your-r-profile",
    "title": "Setting CRAN repository options",
    "section": "Method 2: Edit your R profile",
    "text": "Method 2: Edit your R profile\nThere are a couple of limitations to this method. The most obvious one is that it’s no help if you don’t use RStudio, and even for RStudio users it can be awkward if you don’t always use RStudio. If that’s your situation, you may want to manage your CRAN repository links by editing your R profile. To do this, open the .Rprofile file — using usethis::edit_r_profile(), for example — and add the following line:\n\noptions(repos = \"https://packagemanager.rstudio.com/all/__linux__/focal/latest\")\n\nYou’ll need to restart your R session for this change to take effect.\nIf you want to be fancy, you can list multiple URLs. If the package you want to install is not found at the first link, R will try the second link, and so on. That can be useful. For instance, this is what I use in my R profile:\n\noptions(repos = c(\n  binary = \"https://packagemanager.rstudio.com/all/__linux__/focal/latest\",\n  source = \"https://packagemanager.rstudio.com/all/latest\",\n  CRAN = \"https://cloud.r-project.org\",\n  djnavarro = \"https://djnavarro.r-universe.dev\"\n))\n\nUsing this configuration, R will look for a suitable binary version of the package on RStudio PPM. If that fails it will try to install from RStudio PPM by building the package from the source code. If that fails, it checks CRAN in the usual way. Finally, if that fails, it looks to see if the package I’m requesting is one of the packages I listed at djnavarro.r-universe.dev, my very own tiny corner of the R-universe. Obviously, you’re very unlikely to want to use my R-universe repository since it only consists of a handful of my own packages: but it’s quite handy for me since they aren’t all on CRAN!"
  },
  {
    "objectID": "posts/2022-01-10_setting-cran-repositories/index.html#epilogue",
    "href": "posts/2022-01-10_setting-cran-repositories/index.html#epilogue",
    "title": "Setting CRAN repository options",
    "section": "Epilogue",
    "text": "Epilogue\nIf you’re a Windows or Mac user, you might not be aware of how much of a game changer this is for linux users. For example, in my previous blog post I wrote about my experiences getting started using Apache Arrow. I’m a big fan of Arrow — which should come as no surprise as I’ve recently started work at Voltron Data — but if you’re installing the arrow R package on linux, it’s extremely time consuming to build all the C++ libraries from source. It was a little cumbersome, but after switching to RStudio PPM, I can install arrow on my Ubuntu machine using the exact same command I’d use on Windows…\n\ninstall.packages(\"arrow\")\n\n…and everything works. As easy on linux as it is on other operating systems! Yay! 🎉"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html",
    "title": "Everything I know about Mastodon",
    "section": "",
    "text": "Hello there fellow data science person. Have you heard rumours that a lot of folks from our community are moving to use mastodon for social networking? Are you curious, but maybe not quite sure about how to get started? Have you been thinking “twitter is a hellsite and I need to escape” a lot lately?\nIf yes, this post is for you!\nIt’s written from the time-tested pedagogical perspective of “the writer who is only one chapter ahead of her audience in the textbook”. I’ve been on mastodon for a few days, but this isn’t my first rodeo over there: I signed up for it very early on several years ago, and tried again a few years after that. This time I’m a lot more enthusiastic about it than the last two, so I’m writing a quick introductory post to help my fellow data science folks test out the waters. I sort of know what I’m doing there but not completely!\nIf you want a more detailed guide on navigating Mastodon and the fediverse, I recommend fedi.tips. There’s a lot of answers to common questions over there, from someone who actually does know what they are doing! Alternatively you can read this thread which covers a lot of the same things I’m saying here!\nOkay, let’s dive in…"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#what-is-mastodon-what-is-the-fediverse",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#what-is-mastodon-what-is-the-fediverse",
    "title": "Everything I know about Mastodon",
    "section": "What is Mastodon? What is the fediverse?",
    "text": "What is Mastodon? What is the fediverse?\nIf you’re entirely new to this, your mental model of mastodon is probably something like “mastodon is an open source twitter clone”. To a first approximation that’s right, but if you want to really feel at home there you’re going to want to refine that mental model in a few ways. Mastodon is very similar to twitter in design, but there are some important differences\nFirst off, mastodon is not a single application: it’s a distributed network of servers that all talk to each other using a shared protocol.1 If two servers talk to each other they are said to be “federated” with one another, and the network as a whole is referred to as the “fediverse”.\nThere are many different servers out there that are independently running mastodon: these are called mastodon instances. You can sign up for an account at one or more of these servers. The most popular instance is mastodon.social, but for reasons I’ll talk about in a moment this might not be the best choice for you! For example, my primary account is on fosstodon.org and my art-only account is on an instance for generative artists, genart.social.\nFortunately, it usually doesn’t matter too much which instance you pick: the servers all communicate with each other so you can follow people on different servers, talk with them, etc, and it’s entirely possible to migrate your account from one server to another (I’ll talk about that later in the post). It’s only when you get into the details that it starts to matter!"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#terminology-toots-and-boosts",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#terminology-toots-and-boosts",
    "title": "Everything I know about Mastodon",
    "section": "Terminology: Toots and boosts",
    "text": "Terminology: Toots and boosts\nPosts on twitter are called “tweets” and have a 280 character limit. Posts on mastodon are called “toots” and have a 500 character limit. If you’re thinking of making a joke about “haha it’s called tooting” well fine, but there’s a pretty good chance that everyone has already heard it. Very few of us are actually that original :-)\nSharing someone else’s post on twitter is called a “retweet”. The mastodon equivalent is called “boosting”. One deliberate design choice on mastodon is that there is no analog of “quote retweeting”: you can either boost someone else’s toot and you can post your own. You can’t share someone else’s post to your own followers with your commentary added. This is a deliberate design choice to prevent people from “dunking” on each other."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#mastodon-handles-and-tags-look-like-email-addresses",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#mastodon-handles-and-tags-look-like-email-addresses",
    "title": "Everything I know about Mastodon",
    "section": "Mastodon handles and tags look like email addresses",
    "text": "Mastodon handles and tags look like email addresses\nOn twitter, you simply have a username: I’m djnavarro there, and people would tag me into a conversation by typing @djnavarro.\nOn mastodon, you have to specify both your username and the server. It’s more like an email address. My primary handle on mastodon is djnavarro@fosstodon.org and people can tag me into a conversation by typing @djnavarro@fosstodon.org.\nIt looks a little odd when you’re used to twitter, but it gets easier.\n\n\n\nA handle. Image credit: Arawark chen. Freely available via unsplash"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#is-there-a-data-science-community-there",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#is-there-a-data-science-community-there",
    "title": "Everything I know about Mastodon",
    "section": "Is there a data science community there?",
    "text": "Is there a data science community there?\nYes! There’s definitely a data science community there. It’s much smaller than the one on twitter and things tend to move at a slightly slower pace, but there are some advantages.\nYou can find your data science friends by searching for hashtags. R folks will quickly find other R users posting with the #rstats hashtag, but you can also find #TidyTuesday and other familiar hashtags. I’ll talk about this more later, but hashtags are much more useful (and more important) on mastodon than they are on twitter. The interface for hashtags is basically the same as twitter: you can search for them in the search box (see interface section below), hashtags are clickable links, etc.\nOnce you’ve found some people, you can find more by taking a look at who they follow and who follows them. Again, the interface for that is essentially the same as twitter: click on someone’s profile, and you’ll be able to find a list of people they follow and the people who follow them. However, what you will often finds is that these lists are incomplete: generally, the follower counts are accurate, but servers only publish the list of account names for accounts on that server.2\nFinally, when you’re ready to get started you can make an introduction post: all you have to do is send out a post tagged with #introduction. With any luck that will be picked up and shared with others!"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#which-server-should-i-sign-up-on",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#which-server-should-i-sign-up-on",
    "title": "Everything I know about Mastodon",
    "section": "Which server should I sign up on?",
    "text": "Which server should I sign up on?\nThere’s a nicely server list at fediscience.org that has some commentaries. Here’s a few possibilities you might consider:\n\nmastodon.social: The largest instance. It’s general interest, so you get a reasonably diverse audience. However it’s also the focal point so any time there’s a wave of migrations from twitter it will probably be the first one to show performance hits.\nfosstodon.org: An instance with a focus on open source software. There are a lot of tech people on this one, which means you can watch the local timeline scroll by (more on that coming!) and see lots of random techy posts.\nfediscience.org: A science focused instance, including natural and social sciences.\nvis.social: Lots of cool data visualisation folks here.\ntech.lgbt: An instance for folks who work in tech, science, academia, etc who are LGBTIQ or allies.\n\nFor any server, you should look carefully at the server rules that will be posted on the “About” page. Each server has different policies that will affect moderation. Don’t sign up for vis.social if you want to post about NFTs (I’ll talk about NFTs later actually), and don’t join fosstodon.org if you want to post in languages other than English. Don’t join any of these servers if you want to post anti-trans content.3\nTake a little time to look around but don’t worry about the choice too much. You can move your account across servers without too much difficulty if you need to, and I’ll show you how later in the post."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#can-you-tell-me-about-the-web-interface",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#can-you-tell-me-about-the-web-interface",
    "title": "Everything I know about Mastodon",
    "section": "Can you tell me about the web interface?",
    "text": "Can you tell me about the web interface?\nOkay so you’ve decided on a server, signed up for an account, and are ready to get started. Let’s take a look at the interface!\nFor the purposes of this post I’ll assume you’re looking to get started by using the web interface. There are, unsurprisingly, apps you can download onto your phone (e.g., I’m using the standard mastodon app on iOS), but I’m trying not to complicate things in this post so let’s assume you’re using your laptop and are browsing through the web interface!\nMy main account is djnavarro@fosstodon.org. In my browser I’m logged in already, so when I navigate to fosstodon.org I’m automatically shown the logged in view. There are two versions you can choose between, the “standard view” and the “advanced view”.\nThe “standard view” interface looks pretty similar to what you’d expect from twitter. On the left you can write posts, in the middle there’s a column where your feed is shown (I’ve edited these screenshots to remove the actual posts, just so we can focus on interface), and on the right sidebar there’s a menu with various options you can click on:\n\nIn a lot of cases this view will work well for you, but if you want to track hashtags – more on that later because hashtags are important! – you might find it useful to switch to the “advanced view”. To switch, click on the “Preferences” option on the right hand side, which brings up a preferences screen that looks like this:\n\nClick on the “Enable advanced web interface” option, like I’ve done here, and click save changes. When you then go back to mastodon, the interface will have changed to one that looks very similar to the Tweetdeck interface that a lot of folks on Twitter use:\n\nThere are more columns. As before, the left column shows an area where you can compose posts, and on the right column a menu with options is shown. Posts will appear in the “Home” column. Mentions, favourites (similar to Twitter “likes”), and boosts (similar to Twitter retweets), will be shown in the “Notifications” column."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#how-do-i-make-a-post",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#how-do-i-make-a-post",
    "title": "Everything I know about Mastodon",
    "section": "How do I make a post?",
    "text": "How do I make a post?\nWriting a post on mastodon is similar to how you would write a post on twitter. The compose window looks like this:\n\n\n\n\n\nYou type the text you want into the box, adding hashtags, and then click the “Toot!” button (it may look different on your instance – I’ve seen the same button labelled “Post” sometimes). As you type, you’ll see the character count in the bottom right corner change to let you know how many characters you have left: you’re allowed 500 characters for a post on mastodon.\nThe nuances are important though. Those other buttons on the bottom left… those are all useful features. From left to right:\n\nThe paperclip button: clicking this will let you attach an image. When you do, there will be an option to edit the image and (especially important!) to add alt text for accessibility purposes. Mastodon has a stronger norm about alt text than twitter: always add alt text. I have a section on alt text later in this post.\nThe barchart button: this will let you add a poll. Polls on mastodon work similarly to twitter, but are a bit more flexible. You can add more options and let users select multiple options.\nThe world button: this will let you set the visibility for the post. If you click on it you will see four options: “public” means everyone can view it, “unlisted” means everyone can view it but it doesn’t get indexed by discovery features (very handy in replies and threads where you don’t want everyone to be automatically shown your comment), “followers only” means only your followers can see it, and “mentioned people only” means only the people mentioned can see it. This last one is effectively how direct messages work on mastodon, which is important to note because posts aren’t end-to-end encrypted. Do not treat your mastodon direct messages as private (see later).\nThe “CW” button: This is used to attach content warnings to your post. Use this button! It’s important. I cannot stress this enough: the content warning button is right there, and it is considered extremely poor form in the fediverse to force your followers to look at content they might not want to see. There is a whole section on this later, but remember that mastodon is not twitter – people will mute you or choose not to share your post if you don’t use content warnings appropriately. In fact, if you consistently boost posts that don’t have content warnings when they should, people may unfollow you also.\nThe “EN” button: This is used to specify the language in which the post is written. Clicking on it will show you a dropdown list you can use to select the language.\n\nTry to use these features: it makes a difference!\n\n\nPosts. Image credit: Kristina Tripkovic. Freely available via unsplash"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#why-are-hashtags-so-important",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#why-are-hashtags-so-important",
    "title": "Everything I know about Mastodon",
    "section": "Why are hashtags so important?",
    "text": "Why are hashtags so important?\nHashtags play a much more important role on mastodon than they do on twitter. There’s no analog of the twitter algorithm scanning post content to determine what to show people. If you want your post to be discovered by people who aren’t following you (which, admittedly, isn’t always the case), make sure to choose the appropriate hashtags. As on twitter #rstats is used to specify that this is a post about R, there’s a #TidyTuesday tag used for Tidy Tuesday, etc. I post my generative art using #GenerativeArt, a general purpose generative art hashtag, and also #rtistry to specify that it’s art made with R. It’s generally considered fine – good, even! – to use several hashtags on mastodon. Tagging is your way of categorising posts for others to find…"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#tracking-hashtags",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#tracking-hashtags",
    "title": "Everything I know about Mastodon",
    "section": "Tracking hashtags",
    "text": "Tracking hashtags\nSpeaking of which, perhaps you want to monitor a hashtag. Maybe you even want to follow the hashtag, so that every post that has that hashtag will appear in your mastodon feed. Good news, you can!\nFrom what I can tell, this is something where your options might be a little different on each server. For instance, on mastodon.social you can follow a hashtag directly in the standard view: when you search for a hashtag there will be a little “follow” icon that appears that you can click on (see this comment on fedi.tips). When you do that, posts with that hashtag will appear in your feed. However, not every server implements this: fosstodon.org doesn’t do that right now.\nSo let’s go with a method that seems to work everywhere I’ve looked. This post by David Hood summarises it in a single image, but I’ll go through it more slowly here…\nFirst off, you’ll need to be in “advanced view” to do this. That’s the one with lots of columns that I showed earlier in the post. You can customise this view by adding columns that correspond to the hashtags you want to follow. For example, let’s say I want to follow the #rstats hashtag. The first thing I’d do is type #rstats into the search bar (in the top left corner). The results will be shown directly below the search bar, like this:\n\nThese are clickable links. When I click on the #rstats hashtag in the results, a new column appears… containing a chronological feed that consists of posts tagged with #rstats:\n\nAgain, in real life this won’t be empty: you’ll actually see the posts! You are now tracking #rstats on mastodon, albeit temporarily.\nSuppose you want to make sure the column sticks around every time you open mastodon. We can “pin” the column in place. To do that, I click on the little “settings” icon at the top right of the #rstats column. It’s the one on the far right here:\n\n\n\n\n\nWhen you do that, you will see a small menu that gives you the option to pin! Easy.\nWe can make our #rstats column more useful. For example, there are several hashtags I want to bundle together when following R content: #TidyTuesday, #TidyModels, and #ggplot2. I don’t want a separate column for each one, I want to group them into a single feed. Click on that little settings button again. Now you’ll see a richer menu:\n\nOne of the options there is “include additional tags”. When I click on that, I can type in the other hashtags to track:\n And now we are done! I have a feed that tracks R related content on mastodon."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#why-are-content-warnings-everywhere",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#why-are-content-warnings-everywhere",
    "title": "Everything I know about Mastodon",
    "section": "Why are content warnings everywhere?",
    "text": "Why are content warnings everywhere?\nOne huge – and hugely important – difference between twitter and mastodon is that mastodon has a system that allows users to mask their posts behind content warnings. Now… if you’re coming from twitter you might be thinking “oh that doesn’t apply to me I don’t post offensive content”.\nIf that’s what you’re thinking, allow me to disabuse you of that notion quickly. Content warnings are not about “hiding offensive content”, they are about being kind to your audience. This thread by Mike McHargue is a very good summary. The whole thread is good, but I’ll quote the first part here:\n\nIf you’re part of the #twittermigration, it may seem strange the people use CWs so much here. But, CWs are really helpful. So much of our world is overwhelming, and feed presentation can bombard our nervous systems with triggers. CWs give people time and space to engage with that they have the resources to engage with. It gives them agency. I follow news and politics AND it’s helpful for my PTSD to have the chance to take a deep breath before I see a post.\n\nIf you’re posting about politics, that should be hidden behind a content warning. If you’re posting about sexual assault, definitely use a content warning. If you’re posting about trans rights, again put it behind a content warning.\nYou should use the content warning even – or perhaps especially – when you think your post is this is an important social justice issue that other people need to see, because there is a really good chance that people whose lives are affected by it will be part of the audience… and yes, some of us have PTSD.\n\n\n\n\n\n\nContent warning: trans rights, sexual assault\n\n\n\n\n\nI’ll give examples relevant to my own experience.\nI get really, really angry when people post about trans rights without a content warning. Same with sexual assault. Why? Well, because I am transgender and I am frightened about the general direction the world is headed for people like me. I am not an activist and I don’t have the kind of resilience needed to constantly hear all the worst stories in the world about attacks against people like me. It’s one of the big reasons I left twitter: twitter algorithms prioritise engagement, and I cannot help but engage with this content because I am afraid. My experience on twitter is one of emotional abuse: twitter keeps showing me my worst fears and I click on them because the threats are real. I don’t appreciate it when my friends try to support me by forcing me to see even more of that content. For that reason, if you want to be supportive of people like me, use a content warning when posting about trans rights.\nAn even more extreme example relevant to my personal experience is sexual assault. I am a rape survivor. Every time there is a highly visible discussion about sexual assault (e.g., the Brett Kavanaugh hearings in the US, the Brittney Higgins discussions in Australia), I would get bombarded with content about rape. Over and over again. Sometimes it would trigger panic attacks and rape flashbacks.\nWhen you post those things without providing me a content warning to help me make an informed choice, what you’re really telling me is that you simply don’t care if you’re forcing me to relive the experience of being raped.\n\n\n\nSo if you’re thinking about posting about these topics, the question of “should I attach a content warning?” isn’t a matter of “is this important?” it’s a matter of “could I be causing distress to people?” When you answer that question, don’t think about the typical case, think about that 1% of people who might be most severely affected and the reasons why.\nPlease, please, please… take content warnings seriously. Even if you’re “just posting about politics” or “venting some feelings”. It’s a kindness and courtesy to your audience.\nMastodon isn’t twitter."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#how-do-i-add-a-content-warning",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#how-do-i-add-a-content-warning",
    "title": "Everything I know about Mastodon",
    "section": "How do I add a content warning?",
    "text": "How do I add a content warning?\nHopefully the previous section has convinced you that you should use content warnings and err on the side of caution when deciding when to use them. Your next question is probably: how do I add a content warning?\nLuckily, it is super easy. It’s so simple that it fits into a single toot, like this post by Em on infosec.exchange. Here’s how.\n\nIn the composer box, click on the little “CW” button. This will reveal an extra title field that says “Write your warning here”.\n\n\n\n\n\nWrite a brief but informative message in that title field. This could be something very serious like “Sexual assault discussion”, but it could also be something mild like “Spoiler alert: The Good Wife season 5” or something like “Photo with direct eye contact”. Even things like “US politics” or “Australian politics” can be helpful.\nWrite your post. (Okay you could write the post first and the content warning text after. Whatever)\nWhen you post it, other users will only be shown the title field at first. If they decide they want to read, they can click on the post, and then the full text will be revealed.\nProfit! Everybody is happy.\n\nContent warnings are good for everybody.\n\n\n\nA warning. Image credit: Fleur. Freely available via unsplash"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#how-can-i-make-threads",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#how-can-i-make-threads",
    "title": "Everything I know about Mastodon",
    "section": "How can I make threads?",
    "text": "How can I make threads?\nMaking threads on mastodon is pretty similar to twitter. Just post each new toot as a reply to the previous one. Problem solved!\nThere is one subtlety to be aware of though, which is described in this thread by Quokka on scicomm.xyz. Remember earlier I mentioned that you can set the visibility of each post? The polite way to do a thread is set the first post to “public”, and then all the later ones to unlisted. The reason for that is that all public posts (including replies) will show up in various timelines. Usually, that’s not what you want. What you want is something where the first post reads “I have this important and exciting thing to to talk about: A thread…”, and only that first post shows up on people’s timelines. Then if they’re interested they can click on the first post and the rest of the thread will be revealed. That’s why people on mastodon usually set the first post to public and the later ones to unlisted."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#mastodon-favourites-vs-twitter-likes",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#mastodon-favourites-vs-twitter-likes",
    "title": "Everything I know about Mastodon",
    "section": "Mastodon favourites vs twitter likes",
    "text": "Mastodon favourites vs twitter likes\nMastodon favourites (the little star under each toot) are very similar to twitter likes (the loveheart under each tweet). They aren’t identical though. The big difference is that mastodon implementation is far, far better and not deceptive.4\nOn twitter, we’ve all become accustomed to the obnoxious fact that “likes” do two different things: they send a signal to the person that you liked what they said (i.e., what they’re supposed to do!), but whenever you do that it will trigger a “stochastic retweet”: some proportion of people who follow you will also see that tweet because you liked it. This is annoying because very often you actually enjoy a thing but don’t think it is appropriate to retweet.\nThis bothers me because it seems to me that twitter doesn’t respect your boundaries. The fact that I like something is not an act in which I give twitter permission to share that fact to other people. I think it’s abusive behaviour by twitter.\nHappily, mastodon doesn’t do anything like that. Favourites don’t trigger anything. They do exactly the thing they claim to do: they are a mechanism by which you can communicate to the other person “hey I liked this!” So you should use favourites a lot! Show people you appreciate them!\nQuite separate from that, if you think this is something your followers would appreciate seeing, then boost it too! The key thing is that on mastodon the two functions are separated cleanly… do both when both are appropriate, do one when one is appropriate. You are in control of your sharing behaviour here."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#what-are-the-local-and-federated-timelines",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#what-are-the-local-and-federated-timelines",
    "title": "Everything I know about Mastodon",
    "section": "What are the local and federated timelines?",
    "text": "What are the local and federated timelines?\nAt some point on mastodon you will find yourself discovering the local timeline and the federated timeline. There are links to these on the right hand side of the interface. The local timeline is every public-visibility post on your server, shown chronologically. This timeline has a very different feel on different servers. On fosstodon.org my local timeline has a lot of people posting about tech; on genart.social it shows a lot of generative art.\nThe federated timeline is slightly different: it shows all public posts from all users who are “known” to your instance. That is, it includes every user on your instance, but it also includes everyone that users on your instance follow – even if those users are on other servers. It’s not the same thing as “every user on mastodon” though. People on genart.social tend to follow other artists, so there is still a local “flavour” to the posts from outside the instance: they reflect people and topics that the users on your instance are interested in.\nThese timelines are useful for discovery purposes, and they’re also a reason to think carefully about the instance you’re on. It’s easier to find tech content on a tech-focused server!"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#how-do-i-move-my-account-to-a-new-server",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#how-do-i-move-my-account-to-a-new-server",
    "title": "Everything I know about Mastodon",
    "section": "How do I move my account to a new server?",
    "text": "How do I move my account to a new server?\nSo that brings me naturally to a question… what if you realise you’ve made a mistake and you want to change instances? This happens to many of us at one point or another. For instance, I initially signed up as djnavarro@mastodon.social. That wasn’t ideal for me: the server is a bit too big, it was being very slow at the time, and the content isn’t focused on things I wanted to see. So I decided I wanted to move, and become djnavarro@fosstodon.org.\nA very nice feature of mastodon is that you can “migrate” your account, so that when you switch accounts all your followers will come along for the ride! Here’s how you do it:\nStep 1: Prepare the way\n\nIt’s probably a good idea to post from your old account that you’re about to initiate a move. That way people will not be surprised when they find themselves following a new account (I didn’t do this… I should have. Oops!)\nSet up your account, with the avatar, bio, etc on the new account using the same (or similar) images and descriptions on the old account: anyone who clicks through on the new account will see that it’s you!\n\nStep 2: Export your follows from the old account\nWhen you migrate, it takes your followers across automatically. It doesn’t automatically make your new account follow everyone you were following on the old account. Luckily you don’t have to manually re-follow everyone. Instead, you export a csv file with the list of everyone you’re following at the old account, and later on you can import it as a follow list on the new one. Here’s how we export the csv at the old account:\n\nClick on the “preferences” option\nWithin preferences, select “import and export”\nOn the data export page, you’ll see a list of possible csv files you can download. Download the ones you want, especially the “follows” csv.\n\nStep 3: Set up alias on the new account\nMastodon requires both accounts to authorise the move in some fashion, to prevent anyone from trying to steal other people’s accounts. First, your new account needs to signal that yes, it does wish to be an “alias” for your old account. From the new account – djnavarro@fosstodon.org for me – we need to set that up:\n\nClick on the “preferences” options\nWithin preferences, select “account”\nOn the account settings page, scroll down to the bottom to the section called “moving from a different account” and click on the link “create an account alias”\nOn the account aliases page, specify the handle of your old account – e.g., djnavarro@mastodon.social in my case – and click “create alias”\n\nYou’re done: the alias is set up. You may have to wait a while for this to propagate to the old account. When I moved I had to wait overnight because mastodon.social was running very slowly due to the massive spike of new users from twitter. Hopefully it won’t be that long for most people now.\nStep 4: Initiate the move from the old account\nWith the new account now signalling that it is ready to be an alias for the old one, we can authorise the move from the old account. On the old account (i.e., djnavarro@mastodon.social for me) do the following:\n\nClick on the “preferences” options\nWithin preferences, select “account”\nOn the account settings page, scroll down to the bottom to the section called “moving to a different account” and click on the link “configure it here”\nOn the moving accounts page, type the handle of the new account – in my case djnavarro@fosstodon.org – and enter the password for your old account to confirm. Click “move followers”.\n\nThis will initiate the move. All your followers at the old account will automatically unfollow the old account and then follow the new one. It’ll take a little while and it might happen in bursts.\nStep 5: Import your follows at the new account\nThe last step (optionally) is to have your new account re-follow everyone from that you were following at the old account. We can do that using the csv that you downloaded in step 2. So, again from your new account:\n\nClick on the “preferences” options\nWithin preferences, select “import and export”\nOn the menu on the left, click the “import” submenu\nOn the import screen, select the import type (e.g., “following list”), click on “browse” to select the csv file you exported earlier, and then click “upload”.\n\nYour new account will now automatically follow all the accounts your old account followed.\nWith any luck, you are now successfully moved into your new account!\n\n\n\nMoving. Image credit: Michal Balog. Freely available via unsplash"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#etiquette-on-alt-text",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#etiquette-on-alt-text",
    "title": "Everything I know about Mastodon",
    "section": "Etiquette on alt-text?",
    "text": "Etiquette on alt-text?\nOn twitter, you’ve probably found that there’s some pressure and expectation to supply alt-text for your images. The norm is much stronger on mastodon: people will expect that images have alt-text, and that the alt-text be informative. Here’s a walkthrough. First I might start writing a post, and after clicking on the paperclip icon to attach an image, I have a screen that looks like this:\n\n\n\n\n\nAs usual I’d write the content of my post in the composer box, but I would also click on the “edit” link in the top-right hand corner of my image. That brings up the image editing screen that looks like this:\n\nThere are two things I usually do with this. On the right hand side I can drag and drop the focus circle around to help improve the image preview that gets shown to users. More importantly, on the left hand side I can write my alt-text. For some images it’s easy to come up with a good description, for others it is hard. For something like this one I’d usually aim to write a short paragraph that captures this information:\n\nthis is generative art made with R\nthe title of the piece is “Gods of Salt, Stone, and Storm”\nthe palette is blue/green with a hint of white against a very dark background\nthe image is comprised of swirling patterns throughout\nthe overall impression is something akin to dark storm clouds overhead or maybe unsettled seas\n\nIt’s not a perfect description, but it does capture what I think is important about the artwork."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#etiquette-on-nfts",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#etiquette-on-nfts",
    "title": "Everything I know about Mastodon",
    "section": "Etiquette on NFTs?",
    "text": "Etiquette on NFTs?\nA lot of artists on twitter, especially generative artists, like to post NFTs. It’s understandable: for generative artists, it’s the one reliable income stream they have for their art. However, you need to be very, very careful. NFTs are not well liked on the fediverse, and a lot of servers have outright bans on any form of NFT posting. For instance, you cannot post about NFTs at all on vis.social or mastodon.art. It is written into the server rules, so you should not sign up on those servers if that’s something you’re interested in. However, even on servers that do permit NFTs, there is often a strong suggestion that you should be polite and respect the preferences that folks outside the instance will have. For example, the generative art instance I’m on genart.social does not impose an outright ban on NFTs but it is discouraged, and in the rare instance that you do post NFT content, it must be behind a content warning.\nPersonally I’ve stopped even trying to make money from my art, so it doesn’t affect me: I’ve given up. I’m only bothering to mention it here because I don’t want to see generative art folks run afoul of the local norms."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#etiquette-on-cross-posting-from-twitter",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#etiquette-on-cross-posting-from-twitter",
    "title": "Everything I know about Mastodon",
    "section": "Etiquette on cross-posting from twitter?",
    "text": "Etiquette on cross-posting from twitter?\nCross-posting from twitter is another one where you have to be careful. There are tools that will let you automatically repost from one to the other, but it’s worth thinking about this from a social perspective rather than a technical one. What will people on mastodon start thinking when your mastodon feed is just a long series of posts where you’re responding to something on twitter, or retweeting something on twitter? What will they conclude when they try to reply to you and you don’t respond because you were on twitter, not mastodon? Probably what will happen is people will realise you’re not actually on mastodon at all and unfollow you. I’ve done this a few times already. I’m trying to leave twitter for a reason, and it irritates me when people who are ostensibly (but not really) on mastodon keep trying to direct me to content on there."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#etiquette-on-bots-and-automated-accounts",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#etiquette-on-bots-and-automated-accounts",
    "title": "Everything I know about Mastodon",
    "section": "Etiquette on bots and automated accounts?",
    "text": "Etiquette on bots and automated accounts?\nBots are allowed on mastodon, but you should check the local server rules and you should make certain that the bot is marked as an automated account in the account preferences."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#direct-messages-and-privacy",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#direct-messages-and-privacy",
    "title": "Everything I know about Mastodon",
    "section": "Direct messages and privacy",
    "text": "Direct messages and privacy\nAs a final point, a note on direct messages. Direct messages on mastodon are just regular posts whose visibility is set to include only those people tagged in that post. That’s all. This is important to recognise because – at present – posts are not transmitted with end-to-end encryption: they are “private” only in the sense that a postcard is private or an unencrypted email is private. They won’t be broadcast to anyone else, but they aren’t secured while in transit.\nYou should never send any sensitive information via mastodon."
  },
  {
    "objectID": "posts/2022-12-31_btw-i-use-arch/index.html",
    "href": "posts/2022-12-31_btw-i-use-arch/index.html",
    "title": "btw I use Arch now",
    "section": "",
    "text": "I installed Arch Linux on an aging Dell XPS 13 today.\nDid I plan to do this when I woke up this morning? No, it was an impulsive decision. Was it a sensible way to spend the last Friday of 2022? Also no. Was it the kind of thing that will pay off in the long run through a deeper technical understanding of… something? Again, the answer is no.\nAh, but will it impress people and make them think I am cool?\nOh honey.\nNo.1\nBut look…\nSo pretty. So pointless. So fun."
  },
  {
    "objectID": "posts/2022-12-31_btw-i-use-arch/index.html#installing-arch",
    "href": "posts/2022-12-31_btw-i-use-arch/index.html#installing-arch",
    "title": "btw I use Arch now",
    "section": "Installing Arch",
    "text": "Installing Arch\nI have absolutely no intention of writing a “how to install Arch” guide, for many excellent reasons. Top of that list is that I am a complete amateur when it comes to Arch and I have no idea whatsoever what I’m doing. The only reason any of this happened is that I happened to have a spare laptop2 and couldn’t think of anything better to do with it. My point is I have no business whatsoever in trying to guide anyone else through the installation process. What I am going to do, however, is jot down my notes to myself on the process. I may need them again…\n\nThe installation guide (https://wiki.archlinux.org/title/installation_guide) is good but dense, and I had to read a lot of the documents it links to in order to make progress. It took me three tries to get a working installation, because I missed some subtle thing (e.g., the first time I didn’t have the bootable USB booting in UEFI mode, because obviously I would have thought of that at the beginning… sigh).\nEverything became easier once I started feeling comfortable using pacman.\nThe network configuration step gave me some grief. Eventually I got it to work when I installed NetworkManager and enabled NetworkManager.service, which in turn only made sense to me after I’d read about start/enable and realised that “starting” a service sets it running in the background now, and “enabling” it means it will start automatically on startup.\nToward the end of the installation it tells you to install a bootloader. I thought I wouldn’t need to since I already had grub on my machine from the previous Ubuntu install but in the end it didn’t work without going through the GRUB install process\nSetting up users was an odd exercise in reminding me that I’d forgotten what life was like before sudo. During the initial installation I set up a root user, but no regular users, so my first step was to give myself a … um, me. This article on adding users on Arch Linux was helpful. Partly because it’s a nice walkthrough, but also because it clued me into the fact that Arch doesn’t come with sudo, so I had to install that. It also highlighted things I’d never thought about with sudo before, mostly around which users have sudo privileges. Anyway, the article walked me through the process so now I am danielle on my Arch box (insert the usual Australian joke about getting a root). I added myself to the wheel group so that I can escalate to admin privileges using sudo, and I’m done.\nAs much as the thought of returning to my childhood and running a computer without any desktop environment at all amused me… no. No we are going to have a desktop. I did think about other possible desktop environments besides old faithful, but in the end decided that I actually quite like the look and feel of default GNOME (even without the various tweaks that distros usually overlay on top of it) so I installed that. I did, however, make one concession to nostalgia. I decided not to have the machine automatically boot into GNOME. Instead I followed the instructions so I could start it with startx as soon as I log in, or choose to stay in the shell.\nInstalling a decent browser (firefox, obviously…) was easy, but not surprisingly the font situation for the web was a bit tricky. Arch doesn’t come with an extensive font library so the browser would often rely on fallback fonts for pages that don’t bundle the fonts, making a lot of pages look a bit unpleasant. Fixing that took a bit of digging. The best description on fonts I found was this gist. I have a suspicion that it’s the noto-fonts package that does a lot of the work in fixing the issues\nFixing the touchpad scroll direction and speed was awkward too. Initially the settings panel in GNOME didn’t acknowledge that I even had a touchpad, which was annoying. So I started trying to edit the xorg.conf settings and… actually I don’t think that fixed anything but weirdly after editing /etc/X11/xorg.conf the touchpad settings magically showed up in the GNOME settings panel and then I could edit them. Yeah. I have no idea whatsoever whether this had anything to do with me, or if the gods at large were messing with me or what. So um… yeah, future Danielle, best of luck!\n\nMoving on…"
  },
  {
    "objectID": "posts/2022-12-31_btw-i-use-arch/index.html#setting-up-r-on-arch",
    "href": "posts/2022-12-31_btw-i-use-arch/index.html#setting-up-r-on-arch",
    "title": "btw I use Arch now",
    "section": "Setting up R on Arch",
    "text": "Setting up R on Arch\nAny time I get a new machine set up, my first discretionary task is to make sure that I can do data science work with it (that’s a depressingly revealing statement about me). Once upon a time that really meant getting R set up, but I’m a bit more polyglot nowadays (gasp!). Nevertheless, R is my first love so I always start there…\nSome of the set up tasks are easy, or at least as easy as anything gets on linux. For example, installing R is “easy”, and installing git is “easy”. Similarly, if you use VS code as your IDE, that too is “easy”. These are all easy tasks because they’re officially supported. You can install all three with one command:\nsudo pacman -S r git code\nThat installs the most recent R release, the current git release, and the current VS code release. It’s considerably easier than installing on a Ubuntu LTS release, especially for R, to be honest. On Ubuntu there’s a lot of messing about trying to get the latest versions. The Arch repositories are up to date, which does simplify matters considerably.\nThings become a little trickier when you have to venture outside the official repositories. For example, suppose I want to use RStudio as my IDE. There isn’t an Arch package for RStudio, and – to the surprise of nobody – Posit doesn’t release one either. However, I’m hardly the first person to want to use RStudio on Arch, so it is equally unsurprising that the Arch community has stepped in to help fill the gap.\nIn other words, I’ve reached the point where I have to start installing from the Arch user repository (AUR). This is a community resource, so you have to be a bit more careful in checking that the packages you install from here are good ones, but that’s no different to investigating an R package before installing from CRAN or GitHub. It’s also really important to read through the AUR guidelines before you start trying to use it, because it talks about the tools you’ll need to install first and has a nice walkthrough of the process. So I did that, installed everything from the base-devel group and got started…3\nThere are a few different user submitted packages for RStudio on the AUR. The one I decided to use was rstudio-desktop-bin, largely because it’s a binary4 and because other Arch users seem to like it. It’s a multi-step process. First I had to get a copy of the package files. The easiest way is with git:\ngit clone https://aur.archlinux.org/rstudio-desktop-bin.git\nThe rstudio-desktop-bin folder that I just downloaded contains a PKGBUILD file… opening it in a text editor reveals that it’s basically a recipe for building a package. It doesn’t actually contain any of the components you need, and in fact for RStudio what you’ll notice when you take a peek at the inside is that it’s essentially a wrapper telling Arch how to use the .deb binary that Posit releases for Ubuntu/Debian systems.\nTo build the package I navigate to this folder and call makepkg:\ncd rstudio-desktop-bin\nmakepkg -s\nThe -s flag attempts to synchronise: it will download and install any dependencies, as long as those are official dependencies (I think?). It doesn’t install dependencies when those are also packages on the AUR. Those you just have to do manually.\nAnyway once makepkg does its job, you end up with a whole lot of new files in that folder. The one that we care most about is the one with the .pkg.tar.zst file extension. That’s the one that pacman can install:\nsudo pacman -U rstudio-desktop-bin-2022.12.0.353-1-x86_64.pkg.tar.zst\nThis will install RStudio and… it almost works. When I attempt to open RStudio I get a message in the Rstudio window complaining about a missing library. Digging into the comments on the rstudio-desktop-bin revealed the problem. One of the system dependencies for RStudio is missing from the PKGBUILD file: you have to install openssl-1.1 to make it work. This did the trick for me:\nsudo pacman -s openssl-1.1\nAnd that’s it. Just like that I have RStudio running on Arch…\n… and just like that I felt an immediate need to also get quarto running on Arch so that I could write this blog post on my new Arch box. Quarto is also available on the AUR, and I installed it using the quarto-cli package, following essentially the same process described above. There were no missing dependencies, and everything seems to work fine.\nI’m suspicious. Things aren’t supposed to work first time. I fully expect my laptop to catch fire simply because I am doing something foolish."
  },
  {
    "objectID": "posts/2022-12-31_btw-i-use-arch/index.html#was-it-worth-it",
    "href": "posts/2022-12-31_btw-i-use-arch/index.html#was-it-worth-it",
    "title": "btw I use Arch now",
    "section": "Was it worth it?",
    "text": "Was it worth it?\nProbably not. But I don’t care. I had fun. Sometimes we do things because it’s useful to do them. Sometimes we do things because we like doing them. It’s important to know the difference. This was fun."
  },
  {
    "objectID": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html",
    "href": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html",
    "title": "Arrays and tables in Arrow",
    "section": "",
    "text": "My catholic taste in the devil\nAll gilded and golden, yes, I’m your girl\nHell, if it glitters, I’m going\n  – Heaven is Here, Florence + The Machine\nIf you’ve made the life choice to become a developer advocate with a focus on Apache Arrow, you’re probably not unfamiliar with masochism.\nDon’t believe me? Let’s consider my past choices:\nI don’t regret any of these choices, particularly the fact that they have helped keep me gainfully employed, but there’s no denying the fact that a lot of blood and tears have been spilled in the endeavour.\nIn any case, what I am trying to convey to you, dear reader, is that – setting aside the superficial trappings of whips and chains and the various other devices that propelled E. L. James to great fortune – I am intimately acquainted with pain. It is important to me that you understand this, and that when I mention the pain I encountered when trying to learn how the arrow R package works, I am not using the term lightly.\nSo let talk about my latest pain point, shall we?"
  },
  {
    "objectID": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#data-objects",
    "href": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#data-objects",
    "title": "Arrays and tables in Arrow",
    "section": "Data objects",
    "text": "Data objects\nHere’s the thing that has been giving me grief. Suppose you are an R user who is new to this whole Apache Arrow business. You’ve installed the arrow package, and you’re now reading the Get Started page in the hopes that you too will be able to, well, get started. When you visit this page, one of the very first things you encounter is a table listing a variety of data structures used by Arrow. Specifically, the table tells you that Arrow has classes for zero-dimensional data (scalars), one-dimensional data (arrays and other vector-like data), and two-dimensional data (tabular or data frame-like data). It shows you that…\n…actually, you know what? Instead of describing it, let’s take a look at the actual table. Here’s what it tells you about the hierarchy of data structures in arrow:\n\n\n\n\n\n\n\n\n\n\nDim\nClass\nDescription\nHow to create an instance\n\n\n\n\n0\nScalar\nsingle value and its DataType\nScalar$create(value, type)\n\n\n1\nArray\nvector of values and its DataType\nArray$create(vector, type)\n\n\n1\nChunkedArray\nvectors of values and their DataType\nChunkedArray$create(..., type) or alias chunked_array(..., type)\n\n\n2\nRecordBatch\nlist of Arrays with a Schema\nRecordBatch$create(...) or alias record_batch(...)\n\n\n2\nTable\nlist of ChunkedArray with a Schema\nTable$create(...), alias arrow_table(...), or arrow::read_*(file, as_data_frame = FALSE)\n\n\n2\nDataset\nlist of Tables with the same Schema\nDataset$create(sources, schema) or alias open_dataset(sources, schema)\n\n\n\n\nNow, perhaps there are some devilishly clever R users who can look at this table and immediately decode all its mysteries. But I will be honest with you, and confess that I am not one of these people. When I first started learning Arrow, I had no idea what any of this meant. This whole table was completely intimidating. I looked at it and thoughts roughly along the following lines went through my head:\n\nOh… f**k me. I’m completely out of my depth, I am too stupid to understand any of this. I should quit now and find a new job before everyone realises I’m a total fraud. They made a terrible mistake hiring me and… blah blah blah\n\nThe self-pity went on for a while and the names I called myself became quite inappropriate for a family restaurant, but I’ll be kind and spare you the tiresome details.\nEventually I remembered that this is my impostor syndrome talking and that I am in fact quite good at learning technical concepts. The problem I’m encountering here is that this table isn’t self-explanatory, and isn’t accompanied by the explanatory scaffolding that helps new users orient themselves. That’s a documentation issue, not a user problem. At a later point someone1 might need to add a few explanatory paragraphs and probably a vignette to ensure that new Arrow users don’t get confused at this point, but for now let’s see if we can’t unpack it here?\nLooking at this table, a new user might have some very reasonable questions. What exactly is a ChunkedArray and how is it different from an Array? Why are these necessary as distinct concepts? While we are at it, what is a RecordBatch, a Table and a Dataset, and what makes them different from one another? Unless someone takes the time to explain it all to you, it does look like Arrow is unnecessarily complicated, doesn’t it? These are core concepts in Arrow, but new users don’t know what they are yet!\nIn short, the time has come to tell the story behind this table. With that in mind, I’ll go through this table row by row and talk about what each line actually means.2\nAdventure!\nRomance!\nDrama!\nJoy!\nI am absolutely not going to deliver any of those things, but hopefully this will be useful.\n\nlibrary(arrow)\nlibrary(spotifyr)\nlibrary(dplyr)\nlibrary(tibble)\noptions(scipen = 20)"
  },
  {
    "objectID": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#scalars",
    "href": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#scalars",
    "title": "Arrays and tables in Arrow",
    "section": "Scalars",
    "text": "Scalars\nLet’s start with scalars. A scalar object is simply a single value, that can be of any type. It might be an integer, a string, a timestamp, or any of the different data types that Arrow supports. I won’t talk about the different types in this post because I already wrote an extremely long post on that topic. For the current purposes, what matters is that a scalar is one value. It is “zero dimensional”. All higher order data structures are built on top of scalars, so they are in some sense fundamental, but there is not much I need to say about them for this post. For the record though, you can create a scalar using Scalar$create():\n\nScalar$create(\"hi\")\n\nScalar\nhi\n\n\nOh the excitement. I can barely contain myself."
  },
  {
    "objectID": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#arrays",
    "href": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#arrays",
    "title": "Arrays and tables in Arrow",
    "section": "Arrays",
    "text": "Arrays\n\nAll the gods have been domesticated\nAnd heaven is now overrated\n  – Cassandra, Florence + The Machine\n\nOkay, so scalars aren’t very interesting for the purposes of this post. Let’s turn our attention to arrays next. An array is roughly analogous to a vector in R, and the arrow package is written in a way that allows you to interact with Arrow arrays in ways that feel familiar to R users.\nI can create an array using Array$create():\n\narr <- Array$create(c(\"hello\", \"cruel\", \"world\"))\narr\n\nArray\n<string>\n[\n  \"hello\",\n  \"cruel\",\n  \"world\"\n]\n\n\nI can create a subset of an array using square brackets:\n\narr[2:3]\n\nArray\n<string>\n[\n  \"cruel\",\n  \"world\"\n]\n\n\nNotice that I used 2:3 here to extract the 2nd and 3rd elements of the array. Unlike R, Arrow uses zero-based indexing, so if I were writing this using “Arrow native” code, the relevant subset would be 1:2. However, as a general principle, arrow tries to make Arrow data structures behave like R native objects. The design principle here is that you should be able to use your usual R code to manipulate Arrow data objects, without needing to think too much about the Arrow implementation.\nI’m stressing the principle now because later in this post I am going to violate it!\nBecause one of the other things arrow does is expose a low-level interface to Arrow. You don’t need to use this (and in general you don’t really need to), but it’s there if you want it, and because this post explores the R/Arrow interface I am going to use it sometimes. When that happens, you’ll start to see zero-based indexing appear! I promise I will signpost this every time it happens so you aren’t caught unawares.\n\n\nStructure of arrays\nAnyway, as I was saying, an array in Arrow is analogous to a vector in R: it is a sequence of values with known length, all of which have the same type. When you’re using the arrow package on a day-to-day basis, you really don’t need to know much more than that. But if you want to understand data objects in Arrow properly, it helps to do a slightly deeper dive. All the low level details are described on the Arrow specification page, but the full specification is a little overwhelming when you’re first starting out. I’ll start by introducing two key concepts:\n\nThe data in an array are stored in one or more buffers. A buffer is a sequential virtual address space (i.e., block of memory) with a given length. As long as you have a pointer specifting the memory address for the buffer (i.e., where it starts), you can reach any byte in the buffer using an “offset” value that tells you the location of that byte relative to the start of the buffer.\nThe physical layout of an array is a term used to describe how data in an array is laid out in memory, without taking into account of how that information is interpreted. For example, a 32-bit signed integer and 32-bit floating point number have the same layout: they are both 32 bits, represented as 4 contiguous bytes in memory. The meaning is different, but the layout is the same. However, unlike simple scalars, an array can have a relatively complex layout, storing data and metadata in a structured arrangement.\n\n\n\nLayouts and buffers\nLet’s unpack some of these ideas using a simple array of integer values:\n\narr <- Array$create(c(1L, NA, 2L, 4L, 8L))\narr\n\nArray\n<int32>\n[\n  1,\n  null,\n  2,\n  4,\n  8\n]\n\n\nWhat precisely is this thing? Well that’s a mess of different questions. In one sense, the answer is straightforward. It’s an Arrow array, and the values contained within the array are all stored as signed 32 bit integers:\n\narr$type\n\nInt32\nint32\n\n\nBut that’s not a very satisfying answer at some level. What does this thing look like in memory? How is the information structured? In other words, what is the physical layout of this object?\nThe Arrow documentation page helps us answer that. Our array contains two pieces of metadata, namely the length of the array (i.e. 5) and a count of the number of null values (i.e., 1), both of which are stored as 64-bit integers. The arrow package makes it easy to extract these values, because the Array object has fields and methods that will return them:\n\narr$length()\n\n[1] 5\n\narr$null_count\n\n[1] 1\n\n\nOkay, that seems reasonable. What about the data itself? Where is that stored? In Arrow, these are stored within buffers, a contiguous block of memory assigned to the array. The number of buffers associated with an array depends on the exact type of data being stored. For an integer array such as arr, there are two buffers, a validity bitmap buffer and a data value buffer. So we have a data structure that could be depicted like this:\n\n\n\n\n\nIn this figure I’ve shown the array as a grey rectangle subdivided into two parts, one for the metadata and the other for the buffers. Underneath I’ve unpacked this it a little, showing the contents of the two buffers in the area enclosed in a dotted line. At the lowest level of the figure, you can see the contents of specific bytes. Notice that the numbering of the bytes starts at zero: I’m referring to Arrow data structures here, and Arrow is zero-indexed. Later in the post I’ll talk about how you can access the raw content of these buffers, but for now let’s talk about what each of these buffers contains.\n\n\n\nThe validity bitmap buffer\nThe validity bitmap is binary-valued, and contains a 1 whenever the corresponding slot in the array contains a valid, non-null value. Setting aside some very tiresome technicalities we can imagine that the validity bitmap is a buffer that contains the following five bits:\n10111\nExcept… this isn’t really true, for three reasons. First, memory is allocated in byte-size units, so we have to pad it out to the full 8 bits. That gives us the bitmap 10111000. Second, that’s still a little inaccurate because – assuming you read left to right – you’re looking it with the “most significant bit” first (i.e., big endian format), and the bits are actually organised with the least significant bit first (i.e., little endian format) so the bits in this byte should be shown in the reverse order, 00011101. Third, this is still misleading because I’ve not padded it enough. For reasons that make a lot of sense if you start diving into the Arrow specifications at a low level, you have to imagine another 503 trailing zeros.3 So that the nice and neat 10111 I’ve shown above actually looks like this in memory:\n\n\n\n\n\nByte 0 (validity bitmap)\nBytes 1-63\n\n\n\n\n00011101\n0 (padding)\n\n\n\n\n\nI probably wouldn’t have gone into quite this much detail, except for the fact that you can find this exact example when reading about physical layouts in the Arrow documentation, and I think it’s helpful to have a clear point of contact between this post and the documentation.\nAnyway, I realise I’m being boring. So let’s move on.\n\n\n\nThe data value buffer\nOkay, now let’s have a look at the value buffer. It’s essentially the same logic. Again notice that its padded out to a length of 64 bytes to preserve natural alignment, but for our purposes those details don’t matter too much. Here’s the diagram showing the physical layout, again lifted straight from the Arrow specification page:\n\n\n\n\n\n\n\n\n\n\n\n\n\nBytes 0-3\nBytes 4-7\nBytes 8-11\nBytes 12-15\nBytes 16-19\nBytes 20-63\n\n\n\n\n1\nunspecified\n2\n4\n8\nunspecified\n\n\n\n\n\nEach integer occupies 4 bytes, as required by the int32 data type. (If you want to know more about how Arrow represents integers, it’s discussed in the data types post).\n\n\n\n\n\n\nPeeking inside arrays\nI mentioned earlier that arrow exposes some “low level” tools that allow you to interact with Arrow data objects in more of a bare bones fashion than a data analyst normally would. For example, you wouldn’t normally have a need to extract the raw bytes that comprise the buffers in an array. There’s no “high level” interface that lets you do this. But if you really want to see what’s going on under the hood you absolutely can, and arrow lets you do this. To show you how it works, I’ll uses a small data set containing the track listing for the new Florence + The Machine album, Dance Fever:\n\ndance_fever <- read_csv_arrow(\"dance_fever_tracks.csv\")\ndance_fever\n\n# A tibble: 14 × 3\n   track_number title             duration\n          <int> <chr>                <int>\n 1            1 King                   280\n 2            2 Free                   234\n 3            3 Choreomania            213\n 4            4 Back in Town           236\n 5            5 Girls Against God      280\n 6            6 Dream Girl Evil        227\n 7            7 Prayer Factory          73\n 8            8 Cassandra              258\n 9            9 Heaven Is Here         111\n10           10 Daffodil               214\n11           11 My Love                231\n12           12 Restraint               48\n13           13 The Bomb               165\n14           14 Morning Elvis          262\n\n\nI’ll start by taking the duration variable and creating an Arrow array from it:\n\nduration <- Array$create(dance_fever$duration)\nduration\n\nArray\n<int32>\n[\n  280,\n  234,\n  213,\n  236,\n  280,\n  227,\n  73,\n  258,\n  111,\n  214,\n  231,\n  48,\n  165,\n  262\n]\n\n\nAs a reminder, here’s a crude schematic diagram showing how that object is laid out. It has some metadata that you’ve already learned how to extract (e.g., using duration$null_count), and it has two data buffers that I talked about at tedious length but haven’t actually shown you yet:\n\n\n\n\n\nTo take a more detailed look at the data stored in the duration object, we can call its data() method to return an ArrayData object. Admittedly, the results are not immediately very exciting:\n\nduration$data()\n\nArrayData\n\n\nThis output is a little underwhelming because at the moment the print method for an ArrayData object doesn’t do anything except print the class name. Boring! However, because an ArrayData object is stored as an R6 object, all the information is tucked away in an environment. We can find out the names of objects contained in that environment easily enough:\n\nnames(duration$data())\n\n [1] \".__enclos_env__\" \"buffers\"         \"offset\"          \"null_count\"     \n [5] \"length\"          \"type\"            \".:xp:.\"          \"clone\"          \n [9] \"print\"           \"set_pointer\"     \"pointer\"         \"initialize\"     \n\n\nHm. There’s a buffers variable in there. I wonder what that is…\n\nduration$data()$buffers\n\n[[1]]\nNULL\n\n[[2]]\nBuffer\n\n\nOh look, there are two buffers here! What’s the chance that the first one is the validity bitmap and the second one is the data buffer? (Answer: 100% chance). It turns out that this is another situation where a Buffer object belongs to an R6 class with a boring print method. I could bore you by going through the same process I did last time, but I’d rather not waste your time. It turns out that Buffer objects have a data() method of their own. When we call the data() method returns the bytes stored in the relevant buffer as a raw vector. At long last, we can pull out the raw bytes:\n\ndata_buffer <- duration$data()$buffers[[2]]$data()\ndata_buffer\n\n [1] 18 01 00 00 ea 00 00 00 d5 00 00 00 ec 00 00 00 18 01 00 00 e3 00 00 00 49\n[26] 00 00 00 02 01 00 00 6f 00 00 00 d6 00 00 00 e7 00 00 00 30 00 00 00 a5 00\n[51] 00 00 06 01 00 00\n\n\nShockingly, I personally cannot read binary, but as it turns out the readBin() function from base R is perfectly well equipped to do that. Let’s see what happens when we interpret these 56 bytes as a sequence of 14 integers:4\n\nreadBin(data_buffer, what = \"integer\", n = 14)\n\n [1] 280 234 213 236 280 227  73 258 111 214 231  48 165 262\n\n\nThose are the values stored in the duration array. Yay!\n\n\n\nPrettier ArrayData\nA little digression before we move on to talking about chunked arrays. Later in this post I’ll occasionally want to show you the internal structure of an array, just so you can see that the buffers and metadata have the values you’d expect them to. The information I need for this is stored in the ArrayData object returned by a command like duration$data() but as we saw in the last section there’s no convenient way to display these objects. To make this a little simpler, I wrote my own array_layout() function that shows you the metadata and buffer contents associated with an Arrow array – the source code is here) — that doesn’t work for all array types, but can handle the ones I’m using in this post. When applied to the duration array it produces this output:\n\nduration |>\n  array_layout()\n\n\n── Metadata \n• length : 14\n• null count : 0\n\n── Buffers \n• validity : null\n• data : 280 234 213 236 280 227 73 258 111 214 231 48 165 262\n\n\nThe output here is divided into two sections, structured to mirror how the Arrow columnar specification is described on the website (and also to mirrot the diagrams in the post). There is one section showing the metadata variables stored: array length, and a count of the number of null values. Underneath that we have a section listing all the buffers associated with an array. For an integer array like duration there are two buffers, the validity bitmap buffer and the data values buffer.\nThe array_layout() function also works for string arrays and produces similar output. However, character data in Arrow are stored using three buffers rather than two. As before the first buffer stores the validity bitmap. The second buffer is a vector of offsets specifying the locations for each of the substrings. The third buffer contains the character data itself. Here’s an example of that:\n\ndance_fever$title |>\n  Array$create() |>\n  array_layout()\n\n\n── Metadata \n• length : 14\n• null count : 0\n\n── Buffers \n• validity : null\n• offset : 0 4 8 19 31 48 63 77 86 100 108 115 124 132 145\n• data : KingFreeChoreomaniaBack in TownGirls Against GodDream Girl EvilPrayer\nFactoryCassandraHeaven Is HereDaffodilMy LoveRestraintThe BombMorning Elvis\n\n\nIf you want more information about how character data are stored in Arrow and how the offset buffer and data buffer are used to define the array as a whole, I wrote about it in tiresome detail in my data types post. For the purposes of this post, it’s enough to understand that string arrays are organised using these three buffers."
  },
  {
    "objectID": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#chunked-arrays",
    "href": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#chunked-arrays",
    "title": "Arrays and tables in Arrow",
    "section": "Chunked arrays",
    "text": "Chunked arrays\n\nI need my golden crown of sorrow, my bloody sword to swing\nI need my empty halls to echo with grand self-mythology\n’Cause I am no mother, I am no bride\nI am king\n  – King, Florence + The Machine\n\nThe next entry in the table of data objects refers to “chunked arrays”. In most respects a chunked array behaves just like a regular array. It is a one-dimensional data structure. It requires every value stored to be of the same type: all integers, or all strings, or whatever. From the perspective of a data scientist who just wants to analyse the data, an array and a chunked array are essentially identical. Under the hood, however, they are quite different – and the reason for this is fundamentally a data engineering issue. In this section I’ll unpack this.\nRecall at the beginning I emphasised that an array is an immutable object. Once an array has been created by Arrow, the values it stores cannot be altered. The decision to make arrays immutable reduces the need to create copies: many other objects can all safely refer to the array (via pointers) without making copies of the data, safe in the knowledge that it is impossible5 for anyone else to change the data values. For large data sets that’s a huge advantage: you really don’t want to be making copies of data if you can avoid doing so. Immutable arrays are good.\nWell, mostly good.\nThere are some limitations to immutable arrays, and one of the big ones is prompted by the very simple question: what happens when a new batch of data arrives? An array is immutable, so you can’t add the new information to an existing array. The only thing you can do if you don’t want to disturb or copy your existing array is create a new array that contains the new data. Doing that preserves the immutability of arrays and doesn’t lead to any unnecessary copying – which keeps us happy(ish) from a data engineering perspective – but now we have a new problem: the data are now split across two arrays. Each array contains only one “chunk” of the data. We need some way of “pretending” that these two arrays are in fact a single array-like object.\nThis is the problem that chunked arrays solve. A chunked array is a wrapper around a list of arrays, and allows you to index their contents “as if” they were a single array. Physically, the data are still stored in separate places – each array is one chunk, and these chunks don’t have to be adjacent to each other in memory – but the chunked array provides us will a layer of abstraction that allows us to pretend that they are all one thing.\n\n\nList-like aspects\nHere’s an example. I’ll take some lyrics to King by Florence + The Machine, and use the chunked_array() function from arrow to store them as a chunked array that is comprised of three smaller arrays:\n\nking <- chunked_array(\n  c(\"I\", \"am\", \"no\", \"mother\"), # chunk 0\n  c(\"I\", \"am\", NA, \"bride\"),    # chunk 1\n  c(\"I\", \"am\", \"king\")          # chunk 2\n)\n\nAn alternative way to do the same thing would be to use the create() method of the R6 object ChunkedArray. In fact, the chunked_array() function is just a slightly nicer wrapper around the same functionality that ChunkedArray$create() provides. But that’s a bit of a digression. Let’s take a look at the object I just created:\n\nking\n\nChunkedArray\n[\n  [\n    \"I\",\n    \"am\",\n    \"no\",\n    \"mother\"\n  ],\n  [\n    \"I\",\n    \"am\",\n    null,\n    \"bride\"\n  ],\n  [\n    \"I\",\n    \"am\",\n    \"king\"\n  ]\n]\n\n\nThe double bracketing in this output is intended to highlight the “list-like” nature of chunked arrays. There are three separate arrays that I have created here, wrapped in a handly little container object that is secretly a list of arrays, but allows that list to behave just like a regular one-dimensional data structure. Schematically, this is what I’ve just created:\n\n\n\n\n\nAs this figure illustrates, there really are three arrays here. I can pull them out individually by referring to their position in the array list by using the chunk() method that all chunked array objects possess. This is another one of those situations where I’m using a low-level feature, and the zero-based indexing in Arrow reappears. To extract the second chunk, here’s what I do:\n\nking$chunk(1)\n\nArray\n<string>\n[\n  \"I\",\n  \"am\",\n  null,\n  \"bride\"\n]\n\n\nNotice from the output that this chunk is a vanilla Array object, and I can take a peek at the underlying metadata and buffers associated with that object by using the array_layout() function I wrote earlier. Here’s what that chunk looks like:\n\nking$chunk(1) |> \n  array_layout()\n\n\n── Metadata \n• length : 4\n• null count : 1\n\n── Buffers \n• validity : 1 1 0 1\n• offset : 0 1 3 3 8\n• data : Iambride\n\n\nHopefully by now this all looks quite familiar to you! The Array object here has length 4, contains 1 missing value (referred to as null values in Arrow), and because it is a string array, it contains three buffers: a validity bitmap, an offset buffer, and a data buffer.\n\n\n\nVector-like aspects\nIn the previous section I highlighted the fact that internally a chunked array is “just” a list of arrays and showed you how you can interact with a chunked array in a “list-like” way. Most of the time though, when you’re working with a chunked array as a data analyst you aren’t really interested in its list-like properties, what you actually care about is the abstraction layer that provides it with vector-like properties. Specifically, what you actually care about is the fact that a chunked array is a one-dimensional object with a single indexing scheme. Let’s go back to the king data to illustrate this. Suppose I want to extract a subset of the elements. Specifically I want to grab the 3rd through 6th elements. These slots actually belong to different arrays, and it would be a pain to extract the 3rd and 4th slots from the first array, and the 1st and 2nd slots from the second array. No data analyst wants that headache. Fortunately, I don’t have to:\n\nking[3:6]\n\nChunkedArray\n[\n  [\n    \"no\",\n    \"mother\"\n  ],\n  [\n    \"I\",\n    \"am\"\n  ]\n]\n\n\nAs an R user you are probably breathing a sigh of relief to see the return of one-based indexing! Again I should stress that this is the norm: as a general rule, the arrow package tries to mimic R conventions whenever you are “just trying to do normal R things”. If you’re trying to manipulate and analyse data, the intention is that your regular dplyr functions should work the same way they always did, and the same goes for subsetting data. In R, the first element of a vector is element 1, not element 0, and that convention is preserved here. The only time you’re going to see arrow adopt zero-based indexing is when you are interacting with Arrow at a low level.\nAnother thing to highlight about chunked arrays is that the “chunking” is not considered semantically meaningful. It is an internal implementation detail only: you should never treat the chunk as a meaningful unit! Writing the data to disk, for example, often results in the data being organised into different chunks. Two arrays that have the same values in different chunking arrangements are deemed equivalent. For example, here’s the same four values as king[3:6] all grouped into a single chunk:\n\nno_mother <- chunked_array(c(\"no\", \"mother\", \"I\", \"am\"))\nno_mother\n\nChunkedArray\n[\n  [\n    \"no\",\n    \"mother\",\n    \"I\",\n    \"am\"\n  ]\n]\n\n\nWhen I test for equality using ==, you can see that the results are shown element-wise. All four elements are the same, so the result is a (chunked) array of four true values:\n\nno_mother == king[3:6]\n\nChunkedArray\n[\n  [\n    true,\n    true,\n    true,\n    true\n  ]\n]\n\n\nThe intention, ultimately, is that users should be able to interact with chunked arrays as if they were ordinary one-dimensional data structures without ever having to think much about their list-like nature. Chunked arrays exist as an abstraction to help bridge the gap between the needs of the data engineer and the needs of the data scientist. So except in those special cases where you have to think carefully about the engineering aspect, a data analyst should be able to treat them just like regular vectors!"
  },
  {
    "objectID": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#record-batches",
    "href": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#record-batches",
    "title": "Arrays and tables in Arrow",
    "section": "Record batches",
    "text": "Record batches\nNow that we have a good understanding of zero dimensional data objects (scalars), and one dimensional data objects (arrays and chunked arrays), the time has come to make the jump into the second dimension and talk about tabular data structures in arrow. In the data objects table I presented at the start of the post there are three of these listed: record batches, tables, and datasets. Record batches are the simplest of the three so I’m going to start there, but in everyday data analysis you’re not likely to be using them very much: in practice tables and datasets are the things you’re likely to care about most. Even so, from a learning standpoint it really helps to have a good understanding of record batches because the other concepts are built on top of them.\nA record batch is table-like data structure comprised of a sequence of arrays. The arrays can be of different types but they must all be the same length. Each array is referred to as one of the “fields” or “columns” of the record batch. This probably sounds terribly familiar to you as an R user, because – apart from a few differences in terminology – what I’ve just described to you is essentially the same kind of thing as a data frame. The parallels between record batches and data frames run deeper too:\n\nIn R, the columns in a data frame must be named.6 Record batches have the same property: each field must have a (UTF8-encoded) name, and these names form part of the metadata for the record batch.\nA data frame in R is secretly just a list of vectors, and like any other list it does not really “contain” those vectors: rather it consists of a set of pointers that link to those vector objects. There’s a good discussion of list references in chapter 2 of Advanced R. Record batches in Arrow are much the same. When stored in memory, the record batch does not include physical storage for the values stored in each field, it simply contains pointers to the relevant array objects. It does, however, contain its own validity bitmap.\n\nTo illustrate, let’s return to our dance_fever data set. Here it is as a data frame (well, tibble technically, but whatever):\n\ndance_fever\n\n# A tibble: 14 × 3\n   track_number title             duration\n          <int> <chr>                <int>\n 1            1 King                   280\n 2            2 Free                   234\n 3            3 Choreomania            213\n 4            4 Back in Town           236\n 5            5 Girls Against God      280\n 6            6 Dream Girl Evil        227\n 7            7 Prayer Factory          73\n 8            8 Cassandra              258\n 9            9 Heaven Is Here         111\n10           10 Daffodil               214\n11           11 My Love                231\n12           12 Restraint               48\n13           13 The Bomb               165\n14           14 Morning Elvis          262\n\n\nThe arrow package provides two different ways to create a record batch. I can either use RecordBatch$create() or I can use the record_batch() function. The latter is simpler, so I’ll do that. The record_batch() function is pretty flexible, and can accept inputs in several formats. I can pass it a data frame, one or more named vectors, an input stream, or even a raw vector containing appropriate binary data. But I don’t need all that fancy complexity here so I’ll just give it a data frame:\n\ndf <- record_batch(dance_fever)\ndf\n\nRecordBatch\n14 rows x 3 columns\n$track_number <int32>\n$title <string>\n$duration <int32>\n\n\nThe output is amusingly terse. It doesn’t give a preview of the data, but it kindly confirms that this is a record batch containing 14 rows and 3 columns. It also tells me the column names and the type of data stored in each column. The arrow package supplies a $ method for record batch objects, and it behaves the same way you’d expect for a data frame. If I want to look at a particular column in my record batch, I can refer to it by name like so:\n\ndf$title\n\nArray\n<string>\n[\n  \"King\",\n  \"Free\",\n  \"Choreomania\",\n  \"Back in Town\",\n  \"Girls Against God\",\n  \"Dream Girl Evil\",\n  \"Prayer Factory\",\n  \"Cassandra\",\n  \"Heaven Is Here\",\n  \"Daffodil\",\n  \"My Love\",\n  \"Restraint\",\n  \"The Bomb\",\n  \"Morning Elvis\"\n]\n\n\nAt an abstract level the df object behaves like a two dimensional structure with rows and columns, but in terms of how it is represented in memory it is fundamentally a list of arrays, and so schematically I’ve drawn it like this:\n\n\n\n\n\nIn some respects it’s structurally similar to a chunked array, insofar record batches and chunked arrays are both lists of arrays, but in other ways they are quite different. The arrays indexed in a record batch can be different types, but they must be the same length: this is required to ensure that at a high level we can treat a record batch like a two dimensional table. In contrast, the arrays indexed by a chunked array can be different lengths, but must all be the same type: this is required to ensure that at a high level we can treat a chunked array like a one dimensional vector.\nReturning to the practical details, it’s worth noting that in addition to the $ operator that refers to columns by name, you can use double brackets [[ to refer to columns by position. Just like we saw with chunked array, these positions follow the R convention of using 1 to refer to the first element. The df$title array is the 2nd column in our record batch so I can extract it with this:\n\ndf[[2]]\n\nArray\n<string>\n[\n  \"King\",\n  \"Free\",\n  \"Choreomania\",\n  \"Back in Town\",\n  \"Girls Against God\",\n  \"Dream Girl Evil\",\n  \"Prayer Factory\",\n  \"Cassandra\",\n  \"Heaven Is Here\",\n  \"Daffodil\",\n  \"My Love\",\n  \"Restraint\",\n  \"The Bomb\",\n  \"Morning Elvis\"\n]\n\n\nFinally there is a [ method that allows you to extract subsets of a record batch in the same way you would for a data frame. The command df[1:10, 1:2] extracts the first 10 rows and the first 2 columns:\n\ndf[1:10, 1:2]\n\nRecordBatch\n10 rows x 2 columns\n$track_number <int32>\n$title <string>\n\n\nIf you are wondering what df[1:2] returns, try it out for yourself. Hopefully you will not be surprised!\n\n\nSerialisation\nBefore I move on to talk about Arrow tables, I want to make a small digression. At the beginning of this post I mentioned that my very first attempt to write about Arrow ended up becoming a post about data serialisation in R that had nothing to do with Arrow. That didn’t happen entirely by accident, and I’ll try to explain some of that now.\nWhen we talk about data serialisation, what we’re talking about is taking a data structure stored in memory and organising it into a format that is suitable for writing to disk (serialising to file format) or transmitting over some other communication channel (serialising to a stream). From the beginning, data structures in Arrow were designed together with file formats and streaming formats, with the intention that – to the extent that it is practical and doesn’t mess with other important design considerations – the thing you send across the communication channel (or write to disk) has the same structure as the thing you need to store in memory. That way, when the data arrive at the other end, you don’t have to do a lot of work reorganising the data.\nThat… makes sense, right?\nIf I want to tell you to meet me under the bridge, the message I should send you should be “meet me under the bridge”. It makes no sense whatsoever for me to say “meet me [the title of that really annoying Red Hot Chilli Peppers song]” and expect you to decode it. There is no point in me expending effort deliberately obscuring what I’m trying to say, and then forcing you to expend effort trying to interpret my message.\nYet, surprisingly, that’s what happens a lot of the time when we send data across communication channels. For example, suppose you and I are both R users. We both work with data frames. Because a data frame is fundamentally a list of variables (each of which is a vector), we use data that are organised column-wise: the first thing in a data frame is column 1, then column 2, then column 3, and so on. Okay, cool. So now let’s say you want to send me a data set, and what you do is send me a CSV file. A CSV file is written row by row: the first thing in a CSV file is row 1, then row 2, then row 3. It is a row-wise data structure. In order for you to send data to me, what has to happen is you take your column-wise data frame, invert it so that it is now a row-wise structure, write that to a CSV and then send it to me. At the other end, I have to invert the whole process, transforming the row-wise structure into a column-wise organisation that I can now load into memory as a data frame.7\nUm… that doesn’t make sense.\nThis particular problem arises quite a lot, largely because serialisation formats and in-memory data structures aren’t always designed in tandem. To get around this, Arrow specifies the Interprocess Communication (IPC) serialisation format that is designed specifically to ensure that Arrow data objects can be transmitted (and saved) efficiently. Because data sets are typically organised into tabular structures, the primitive unit for communication is the record batch. I’m not going to dive into the very low level details of how IPC messages are structured, but the key thing for our purposes is that IPC is designed to ensure that the structure of the serialised record batch is essentially identical to the physical layout of an in-memory record batch.\nI’ll give a very simple example. Let’s take the first few rows of the dance_fever data set and convert them into a small record batch:\n\ndf_batch_0 <- record_batch(dance_fever[1:4,])\ndf_batch_0\n\nRecordBatch\n4 rows x 3 columns\n$track_number <int32>\n$title <string>\n$duration <int32>\n\n\nSuppose I want to share this. Currently this is an object in memory that consists of three arrays (which are contiguous in memory) but as we’ve seen earlier, these arrays are themselves comprised of multiple buffers. What the IPC format does is collect the relevant metadata into a “data header”,8 and then lay out the buffers one after the other. Glossing a few minor details9, this is essentially what the IPC message would look like for this record batch:\n\n\n\n\n\nAt this point you’re probably thinking okay that’s nice Danielle, but how do I do this? There are three functions you can use:\n\nTo send the data directly to an output stream use write_ipc_stream().\nTo write data in IPC format to a static file on disk it is referred to as a “feather” formatted file,10 and you use write_feather().\nTo construct the same sequence of bytes but return them in R as raw vectors, you can use write_to_raw().\n\nIn order to show you what the byte stream actually looks like, I’ll use the write_to_raw() function:\n\ndf_ipc_0 <- write_to_raw(df_batch_0)\ndf_ipc_0\n\n  [1] ff ff ff ff f0 00 00 00 10 00 00 00 00 00 0a 00 0c 00 06 00 05 00 08 00 0a\n [26] 00 00 00 00 01 04 00 0c 00 00 00 08 00 08 00 00 00 04 00 08 00 00 00 04 00\n [51] 00 00 03 00 00 00 7c 00 00 00 3c 00 00 00 04 00 00 00 a0 ff ff ff 00 00 01\n [76] 02 10 00 00 00 1c 00 00 00 04 00 00 00 00 00 00 00 08 00 00 00 64 75 72 61\n[101] 74 69 6f 6e 00 00 00 00 8c ff ff ff 00 00 00 01 20 00 00 00 d4 ff ff ff 00\n[126] 00 01 05 10 00 00 00 1c 00 00 00 04 00 00 00 00 00 00 00 05 00 00 00 74 69\n[151] 74 6c 65 00 00 00 04 00 04 00 04 00 00 00 10 00 14 00 08 00 06 00 07 00 0c\n[176] 00 00 00 10 00 10 00 00 00 00 00 01 02 10 00 00 00 28 00 00 00 04 00 00 00\n[201] 00 00 00 00 0c 00 00 00 74 72 61 63 6b 5f 6e 75 6d 62 65 72 00 00 00 00 08\n[226] 00 0c 00 08 00 07 00 08 00 00 00 00 00 00 01 20 00 00 00 00 00 00 00 ff ff\n[251] ff ff f8 00 00 00 14 00 00 00 00 00 00 00 0c 00 16 00 06 00 05 00 08 00 0c\n[276] 00 0c 00 00 00 00 03 04 00 18 00 00 00 58 00 00 00 00 00 00 00 00 00 0a 00\n[301] 18 00 0c 00 04 00 08 00 0a 00 00 00 8c 00 00 00 10 00 00 00 04 00 00 00 00\n[326] 00 00 00 00 00 00 00 07 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n[351] 00 00 00 00 00 00 00 00 00 00 10 00 00 00 00 00 00 00 10 00 00 00 00 00 00\n[376] 00 00 00 00 00 00 00 00 00 10 00 00 00 00 00 00 00 14 00 00 00 00 00 00 00\n[401] 28 00 00 00 00 00 00 00 1f 00 00 00 00 00 00 00 48 00 00 00 00 00 00 00 00\n[426] 00 00 00 00 00 00 00 48 00 00 00 00 00 00 00 10 00 00 00 00 00 00 00 00 00\n[451] 00 00 03 00 00 00 04 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 04 00 00\n[476] 00 00 00 00 00 00 00 00 00 00 00 00 00 04 00 00 00 00 00 00 00 00 00 00 00\n[501] 00 00 00 00 01 00 00 00 02 00 00 00 03 00 00 00 04 00 00 00 00 00 00 00 04\n[526] 00 00 00 08 00 00 00 13 00 00 00 1f 00 00 00 00 00 00 00 4b 69 6e 67 46 72\n[551] 65 65 43 68 6f 72 65 6f 6d 61 6e 69 61 42 61 63 6b 20 69 6e 20 54 6f 77 6e\n[576] 00 18 01 00 00 ea 00 00 00 d5 00 00 00 ec 00 00 00 ff ff ff ff 00 00 00 00\n\n\nTo reassure you that this byte stream does indeed contain the relevant information, I’ll use the read_ipc_stream() function to decode it. By default this function returns data to R natively as a tibble, which is fine for my purposes:\n\nread_ipc_stream(df_ipc_0)\n\n# A tibble: 4 × 3\n  track_number title        duration\n         <int> <chr>           <int>\n1            1 King              280\n2            2 Free              234\n3            3 Choreomania       213\n4            4 Back in Town      236\n\n\nWe can use the same logic to write data to disk. As mentioned above, when writing data in IPC format to file, the result is called a “feather” file. So okay, let’s take the rest of the dance_fever data, and write it to a feather file:\n\ndance_fever[5:14,] |>\n  record_batch() |>\n  write_feather(\"df_ipc_1.feather\")\n\nNow we can read this feather file from disk:\n\nread_feather(\"df_ipc_1.feather\")\n\n# A tibble: 10 × 3\n   track_number title             duration\n          <int> <chr>                <int>\n 1            5 Girls Against God      280\n 2            6 Dream Girl Evil        227\n 3            7 Prayer Factory          73\n 4            8 Cassandra              258\n 5            9 Heaven Is Here         111\n 6           10 Daffodil               214\n 7           11 My Love                231\n 8           12 Restraint               48\n 9           13 The Bomb               165\n10           14 Morning Elvis          262\n\n\nYay! It’s always nice when things do what you expect them to do.\nBefore moving on, there’s one last thing I should mention. The feather file format is a handy thing to know about, and can be very convenient in some instances, but it’s not really optimised to be the best “big data file format”. It’s intended to be the file format analog of IPC messages, and those in turn are designed for optimal streaming of Arrow data. Because of this, in practice you will probably not find yourself using the feather format all that much. Instead, you’re more likely to use something like Apache Parquet, which is explicitly designed for this purpose. Arrow and parquet play nicely with one another, and arrow supports reading and parquet files using the read_parquet() and write_parquet() functions. However, parquet is a topic for a future post, so that’s all I’ll say about this today!"
  },
  {
    "objectID": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#tables",
    "href": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#tables",
    "title": "Arrays and tables in Arrow",
    "section": "Tables",
    "text": "Tables\n\nTell me where to put my love\nDo I wait for time to do what it does?\nI don’t know where to put my love\n  – My Love, Florence + The Machine\n\nEarlier when I introduced the concept of chunked arrays, I explained that Arrow needs these structures because arrays are immutable objects, and Arrow is designed to avoid copying data whenever possible: when a new block of data arrive, it is stored as its own array without disturbing the existing ones, and we use the chunked array as a wrapper that lets us pretend that all these chunks are laid out end to end in a single vector. The previous section shows you exactly how that can happen. If I have a data set that arrives sequentially as a sequence of record batches, I have this problem for every column in the data set! Quite by accident11 that’s what happened in the last section – the dance_fever data set has been serialised in two parts. In that example it happened because I wanted to show you what an IPC stream looked like (creating one record batch for that) as well as what a feather file looks like (creating another record batch), but in real life it’s more likely to happen every time you receive an update on an ongoing data collection process (e.g., today’s data arrive).\nTo deal with this situation, we need a tabular data structure that is similar to a record batch with one exception: instead of storing each column as an array, we now want to store it as a chunked array. This is what the Table class in arrow does. Schematically, here’s what the data structure for a table looks like:\n\n\n\n\n\nTables have a huge advantage over record batches: they can be concatenated. You can’t append one record batch to another because arrays are immutable: you can’t append one array to the end of another array. You would have to create a new array with all new data – and do this for every column in the data – which is a thing we really don’t want to do. But because tables are built from chunked arrays, concatenation is easy: all you have to do is update the chunked arrays so that they include pointers to the newly-arrived arrays as well as the previously-existing arrays.\nBecause tables are so much more flexible than record batches, functions in arrow tend to return tables rather than record batches. Unless you do what I did in the previous section and deliberately call record_batch() you’re not likely to encounter them as the output of normal data analysis code.12 For instance, in the previous section I serialised two record batches, one to a file and one to a raw vector. Let’s look at what happens when I try to deserialise (a.k.a. “read”) them. First the IPC stream:\n\ndf_table_0 <- read_ipc_stream(\n  file = df_ipc_0, \n  as_data_frame = FALSE\n)\ndf_table_0\n\nTable\n4 rows x 3 columns\n$track_number <int32>\n$title <string>\n$duration <int32>\n\n\nThat’s the same data as before, but it’s a table not a record batch. Each column is a chunked array, not an array. The same happens when I read from the feather file:\n\ndf_table_1 <- read_feather(\n  file = \"df_ipc_1.feather\", \n  as_data_frame = FALSE\n)\ndf_table_1\n\nTable\n10 rows x 3 columns\n$track_number <int32>\n$title <string>\n$duration <int32>\n\n\nAgain, this is a table. In general, you won’t get a record batch in arrow unless you explicitly ask for one. Tables are the default tabular data structure, which is usually what you want anyway.\nOkay, so now I have the two fragments of my data set represented as tables. The difference between the table version and the record batch version is that the columns are all represented as chunked arrays. Each array from the original record batch is now one chunk in the corresponding chunked array in the table:\n\ndf_batch_0$title\n\nArray\n<string>\n[\n  \"King\",\n  \"Free\",\n  \"Choreomania\",\n  \"Back in Town\"\n]\n\ndf_table_0$title\n\nChunkedArray\n[\n  [\n    \"King\",\n    \"Free\",\n    \"Choreomania\",\n    \"Back in Town\"\n  ]\n]\n\n\nIt’s the same underlying data (and indeed the same immutable array is referenced by both), just enclosed by a new, flexible chunked array wrapper. However, it is this wrapper that allows us to concatenate tables:\n\ndf <- concat_tables(\n  df_table_0,\n  df_table_1\n)\ndf\n\nTable\n14 rows x 3 columns\n$track_number <int32>\n$title <string>\n$duration <int32>\n\n\nThis is successful (yay!) and the result will behave exactly like a two dimensional object with $, [[, and [ operators that behave as you expect them to13 (yay!), but if you look closely you can still see the “seams” showing where the tables were appended:\n\ndf$title\n\nChunkedArray\n[\n  [\n    \"King\",\n    \"Free\",\n    \"Choreomania\",\n    \"Back in Town\"\n  ],\n  [\n    \"Girls Against God\",\n    \"Dream Girl Evil\",\n    \"Prayer Factory\",\n    \"Cassandra\",\n    \"Heaven Is Here\",\n    \"Daffodil\",\n    \"My Love\",\n    \"Restraint\",\n    \"The Bomb\",\n    \"Morning Elvis\"\n  ]\n]\n\n\nWhen tables are concatenated the chunking is preserved. That’s because those are the original arrays, still located at the same spot in memory. That’s efficient from a memory use perspective, but again, don’t forget that the chunking is not semantically meaningful, and there is no guaranteed that a write-to-file operation (e.g., to parquet format) will preserve those chunks."
  },
  {
    "objectID": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#datasets",
    "href": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#datasets",
    "title": "Arrays and tables in Arrow",
    "section": "Datasets",
    "text": "Datasets\n\nWhat kind of man loves like this?\nTo let me dangle at a cruel angle\nOh, my feet don’t touch the floor\nSometimes you’re half in and then you’re half out\nBuy you never close the door\n  – What Kind Of Man, Florence + The Machine\n\nSo what about datasets? They’re the last item on that table, and you might be wondering where they fall in all this. I’m not going to dive into the details on datasets in this post, because they’re a whole separate topic and they deserve their own blog post. However, it’s a little unsatisfying to write all this and not say anything about them, so I’ll give a very quick overview here.\nUp to this point I’ve talked about tabular data sets that are contained entirely in memory. When such data are written to disk, they are typically written to a single file. For larger-than-memory data sets, a different strategy is needed. Only a subset of the data can be stored in memory at any point in time, and as a consequence it becomes convenient to write the data to disk by partitioning it into many smaller files. This functionality is supported in Arrow via Datasets.\nI’ll give a simple example here, using a small data set. Let’s suppose I’ve downloaded the entire Florence + The Machine discography using the spotifyr package:\n\nflorence <- get_discography(\"florence + the machine\")\nflorence\n\n\n\n# A tibble: 414 × 41\n# Groups:   album_name [18]\n   artist_name   artis…¹ album…² album…³ album…⁴ album…⁵ album…⁶ album…⁷ dance…⁸\n   <chr>         <chr>   <chr>   <chr>   <list>  <chr>     <dbl> <chr>     <dbl>\n 1 Florence + T… 1moxjb… 0uGwPm… album   <df>    2022-0…    2022 day       0.73 \n 2 Florence + T… 1moxjb… 0uGwPm… album   <df>    2022-0…    2022 day       0.467\n 3 Florence + T… 1moxjb… 0uGwPm… album   <df>    2022-0…    2022 day       0.635\n 4 Florence + T… 1moxjb… 0uGwPm… album   <df>    2022-0…    2022 day       0.346\n 5 Florence + T… 1moxjb… 0uGwPm… album   <df>    2022-0…    2022 day       0.599\n 6 Florence + T… 1moxjb… 0uGwPm… album   <df>    2022-0…    2022 day       0.565\n 7 Florence + T… 1moxjb… 0uGwPm… album   <df>    2022-0…    2022 day       0.472\n 8 Florence + T… 1moxjb… 0uGwPm… album   <df>    2022-0…    2022 day       0.571\n 9 Florence + T… 1moxjb… 0uGwPm… album   <df>    2022-0…    2022 day       0.542\n10 Florence + T… 1moxjb… 0uGwPm… album   <df>    2022-0…    2022 day       0.643\n# … with 404 more rows, 32 more variables: energy <dbl>, key <int>,\n#   loudness <dbl>, mode <int>, speechiness <dbl>, acousticness <dbl>,\n#   instrumentalness <dbl>, liveness <dbl>, valence <dbl>, tempo <dbl>,\n#   track_id <chr>, analysis_url <chr>, time_signature <int>, artists <list>,\n#   available_markets <list>, disc_number <int>, duration_ms <int>,\n#   explicit <lgl>, track_href <chr>, is_local <lgl>, track_name <chr>,\n#   track_preview_url <lgl>, track_number <int>, type <chr>, track_uri <chr>, …\n\n\nThe florence data frame is of course quite small, and I have no real need to use Arrow Datasets: it’s small enough that I can store it natively in R as a tibble! But it will suffice to illustrate concepts that come in handy when working with large datasets.\nLet’s suppose I want to partition this in into many data files, using the album release year as the basis for the partitioning. To do this I’ll use the write_dataset() function, specifying partitioning = \"album_release_year\" to ensure that files are created after splitting the data set by release year. By default, the write_dataset() function writes individual data files in the parquet format, which is in general a very good default choice for large tabular data sets. However, because I have not talked about Apache Parquet in this post, I’ll make a different choice and write the data files in the feather format that we’ve seen earlier in this post. I can do that by setting format = \"feather\". Finally, I’ll set path = \"spotify_florence\" to ensure that all the files are stored in a folder by that name. That gives this command:\n\nflorence |> \n  select(where(~!is.list(.))) |>  # drop list columns\n  as_arrow_table() |>             # convert to an arrow table\n  write_dataset(                  # write to multi-file storage\n    path = \"spotify_florence\",\n    format = \"feather\",\n    partitioning = \"album_release_year\"\n  )\n\nThe result is that the following files are written to disk:\n\nlist.files(\"spotify_florence\", recursive = TRUE)\n\n[1] \"album_release_year=2009/part-0.feather\"\n[2] \"album_release_year=2010/part-0.feather\"\n[3] \"album_release_year=2011/part-0.feather\"\n[4] \"album_release_year=2012/part-0.feather\"\n[5] \"album_release_year=2015/part-0.feather\"\n[6] \"album_release_year=2018/part-0.feather\"\n[7] \"album_release_year=2022/part-0.feather\"\n\n\nThese file names are written in “Hive partitioning” format. It looks a little weird the first time you encounter it, because = is a character most coders instinctively avoid including in file names because it has such a strong meaning in programming contexts. However, when files are named in Hive partitioning format, the intended interpretation is exactly the one you implicitly expect as a coder: it’s a field_name=value statement, so you will often encounter files with names like\n/year=2019/month=2/data.parquet\nFor more information see the help documentation for the hive_partitioning() function in the arrow package.\nIn any case, the key thing is that I’ve now written the data to disk in a fashion that splits it across multiple files. For the Florence + The Machine discography data this is is really not needed because the entire spotify_florence folder occupies a mere 320kB on my hard drive. However, elsewhere on my laptop I have a copy of the infamous New York City Taxi data set, and that one occupies a rather more awkward 69GB of storage. For that one, it really does matter that I have it written to disk in a sensible format!\nHaving a data set stored in a distributed multi-file format is nice, but it’s only useful if I can open it and work with it as if it were the same as a regular tabular data set. The open_dataset() function allows me to do exactly this. Here’s what happens when I open the file:\n\nflorence_dataset <- open_dataset(\"spotify_florence\", format = \"feather\")\nflorence_dataset\n\nFileSystemDataset with 7 Feather files\nartist_name: string\nartist_id: string\nalbum_id: string\nalbum_type: string\nalbum_release_date: string\nalbum_release_date_precision: string\ndanceability: double\nenergy: double\nkey: int32\nloudness: double\nmode: int32\nspeechiness: double\nacousticness: double\ninstrumentalness: double\nliveness: double\nvalence: double\ntempo: double\ntrack_id: string\nanalysis_url: string\ntime_signature: int32\ndisc_number: int32\nduration_ms: int32\nexplicit: bool\ntrack_href: string\nis_local: bool\ntrack_name: string\ntrack_preview_url: bool\ntrack_number: int32\ntype: string\ntrack_uri: string\nexternal_urls.spotify: string\nalbum_name: string\nkey_name: string\nmode_name: string\nkey_mode: string\ntrack_n: double\nalbum_release_year: int32\n\nSee $metadata for additional Schema metadata\n\n\nOkay yes, the output makes clear that I have loaded something and it has registered the existence of the 7 constituent files that comprise the dataset as a whole. But can I work with it? One of the big selling points to the arrow package is that it supplies a dplyr backend that lets me work with Tables as if they were R data frames, using familiar syntax. Can I do the same thing with Datasets?\n\ndanceability <- florence_dataset |> \n  select(album_name, track_name, danceability) |>\n  distinct() |>\n  arrange(desc(danceability)) |> \n  head(n = 10) |> \n  compute()\n\ndanceability\n\nTable\n10 rows x 3 columns\n$album_name <string>\n$track_name <string>\n$danceability <double>\n\nSee $metadata for additional Schema metadata\n\n\nYes. Yes I can. Because I called compute() at the end of this pipeline rather than collect(), the results have been returned to me as a Table rather than a data frame. I did that so that I can show that the danceability output is no different to the Table objects we’ve seen earlier, constructed from ChunkedArray objects:\n\ndanceability$track_name\n\nChunkedArray\n[\n  [\n    \"Heaven Is Here\",\n    \"King\",\n    \"King\",\n    \"Hunger\",\n    \"My Love - Acoustic\",\n    \"Ghosts - Demo\",\n    \"What The Water Gave Me - Demo\",\n    \"What The Water Gave Me - Demo\",\n    \"South London Forever\",\n    \"What The Water Gave Me - Demo\"\n  ]\n]\n\n\nIf I want to I can convert this to a tibble, and discover that “Dance Fever” does indeed contain the most danceable Florence + The Machine tracks, at least according to Spotify:\n\nas.data.frame(danceability)\n\n# A tibble: 10 × 3\n   album_name                            track_name                    danceab…¹\n   <chr>                                 <chr>                             <dbl>\n 1 Dance Fever                           Heaven Is Here                    0.852\n 2 Dance Fever                           King                              0.731\n 3 Dance Fever (Deluxe)                  King                              0.73 \n 4 High As Hope                          Hunger                            0.729\n 5 Dance Fever (Deluxe)                  My Love - Acoustic                0.719\n 6 Lungs (Deluxe Version)                Ghosts - Demo                     0.681\n 7 Ceremonials                           What The Water Gave Me - Demo     0.68 \n 8 Ceremonials (Deluxe Edition)          What The Water Gave Me - Demo     0.68 \n 9 High As Hope                          South London Forever              0.679\n10 Ceremonials (Original Deluxe Version) What The Water Gave Me - Demo     0.678\n# … with abbreviated variable name ¹​danceability\n\n\n\nI am certain you are as reassured by this as I am."
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "",
    "text": "In the last post on this blog I showed how Apache Arrow makes it possible to hand over data sets from R to Python (and vice versa) without making wasteful copies of the data.\nThe solution I outlined there was to use the reticulate package to conduct the handover, and rely on Arrow tools both sides to manage the data. In one sense it’s a perfectly good solution to the problem… but it’s a solution tailor made for R users who need access to Python. When viewed from the perspective of a Python user who needs access to R, it’s a little awkward to have an R package (reticulate) governing the handover.1 Perhaps we can find a more Pythonic way to approach this?\nA solution to our problem is provided by the rpy2 library that provides an interface to R from Python, and the rpy2-arrow extension that allows it to support Arrow objects. Let’s take a look, shall we?"
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#setting-up-the-python-environment",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#setting-up-the-python-environment",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "Setting up the Python environment",
    "text": "Setting up the Python environment\nFor the purposes of this post I’ll create a fresh conda environment that I’ll call “continuation”, partly because this post is a continuation of the previous one and partly because the data set I’ll use later is taken from a database of serialised fiction called To Be Continued….\nI was able install most packages I need through conda-forge, but for rpy2 and rpy2-arrow I was only able to do so from pypi so I had to use pip for that. So the code for setting up my Python environment, executed at the terminal, was as follows:\nconda create -n continuation\nconda install -n continuation pip pyarrow pandas jupyter\nconda activate continuation\npip install rpy2 rpy2-arrow\nAs long as I render this post with the “continuation” environment active everything works smoothly.3"
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#introducing-rpy2",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#introducing-rpy2",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "Introducing rpy2",
    "text": "Introducing rpy2\nThe purpose of the rpy2 library is to allow users to call R from Python, typically with the goal of allowing access to statistical packages distributed through CRAN. I’m currently using version 3.5.4, and while this blog post won’t even come close to documenting the full power of the library, the rpy2 documentation is quite extensive. To give you a bit of a flavour of it, let’s import the library:\n\nimport rpy2\nrpy2.__version__\n\n'3.5.4'\n\n\nThis does not in itself give us access to R. That doesn’t happen until we explicitly import either the robjects module (a high level interface to R) or import the rinterface model (a low level interface) and call rinterface.initr(). This post won’t cover rinterface at all; we can accomplish everything we need to using only the high level interface provided by robjects. So let’s import the module and, in doing so, start R running as a child process:\n\nimport rpy2.robjects as robjects\n\n\f\n\n\nR version 4.2.1 (2022-06-23) 🌈\n\n\nYou’ll notice that this prints a little startup message. If you’re following along at home you’ll probably see something different on your own machine: most likely you’ll see the standard R startup message here. It’s shorter in this output because I modified my .Rprofile to make R less chatty on start up.4\nAnyway, our next step is to load some packages. In native R code we’d use the library() function for this, but rpy2 provides a more Pythonic approach. Importing the packages submodule gives us access to importr(), which is allows us to load packages. The code below illustrates how you can expose the base R package and the utils R package (both of which come bundled with any minimal R installation) to Python:\n\nimport rpy2.robjects.packages as pkgs\n\nbase = pkgs.importr(\"base\")\nutils = pkgs.importr(\"utils\")\n\nOnce we have access to utils we can call the R function install.packages() to install additional packages from CRAN. However, at this point we need to talk a little about how names are translated by rpy2. As every Python user would immediately notice, install.packages() is not a valid function name in Python: the dot is a special character and not permitted within the name of a function. In contrast, although not generally recommended in R except in special circumstances,5 function names containing dots are syntactically valid in R and there are functions that use them. So how do we resolve this?\nIn most cases, the solution is straightforward: rpy2 will automatically convert dots in R to underscores in Python, and so in this instance the function name becomes install_packages(). For example, if I want to install the fortunes package using rpy2, I would use the following command:6\nutils.install_packages(\"fortunes\")\nThere are some subtleties around function name translation, however. I won’t talk about them in this post, other to mention that the documentation discusses this in the section on calling functions.\nIn any case, now that I have successfully installed the fortunes package I can import it, allowing me to call the fortune() function:\n\nftns = pkgs.importr(\"fortunes\")\nftn7 = ftns.fortune(7)\nprint(ftn7)\n\n\nWhat we have is nice, but we need something very different.\n   -- Robert Gentleman\n      Statistical Computing 2003, Reisensburg (June 2003)\n\n\n\n\nI’m rather fond of this quote, and it seems very appropriate to the spirit of what polyglot data science is all about. Whatever language or tools we’re working in, we’ve usually chosen them for good reason. But there is no tool that works all the time, nor any language that is ideal for every situation. Sometimes we need something very different, and when we do it is very helpful if our tools able to talk fluently to each other.\nWe’re now at the point that we can tackle the problem of transferring data from Python to R, but in order to do that we’ll need some data…\n\n\n\n\n\nThis was the header illustration to a story entitled “The Trail of the Serpent” by M. E. Braddon. It was published in the Molong Express and Western District Advertiser on 4 August 1906. The moment I saw it I knew I had to include it here. I can hardly omit a serpent reference in a Python post, now can I? That would be grossly irresponsible of me as a tech blogger. Trove article 139469044"
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#about-the-data",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#about-the-data",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "About the data",
    "text": "About the data\nI’ve given you so many teasers about the data set for this post that it almost feels a shame to spoil it by revealing the data, but all good things must come to an end I suppose. The data I’m using are taken from the To Be Continued… database of fiction published in Australian newspapers during the 19th and early 20th century. Originally collected using the incredibly cool Trove resource run by the National Library of Australia, the To Be Continued… data are released under a CC-BY-4.0 licence and maintained by Katherine Bode and Carol Hetherington. I’m not using the full data set here, only the metadata. In the complete database you can find full text of published pieces, and in the Trove links you can find the digitised resources from which they were sourced, but I don’t need that level of detail here. All I need is an interesting data table that I can pass around between languages. For that, the metadata alone will suffice!\nTo give you a sense of what the data set (that is, the restricted version I’m using here) looks like, let’s fire up pandas and take a peek at the structure of the table. It’s stored as a CSV file, so I’ll call read_csv() to import the data:\n\nimport pandas\n\nfiction = pandas.read_csv(\"fiction.csv\", low_memory = False)\nfiction.head()\n\n\n\n\n\n  \n    \n      \n      Trove ID\n      Common Title\n      Publication Title\n      Start Date\n      End Date\n      Additional Info\n      Length\n      Curated Dataset\n      Identified Sources\n      Publication Source\n      ...\n      Other Names\n      Publication Author\n      Gender\n      Nationality\n      Nationality Details\n      Author Details\n      Inscribed Gender\n      Inscribed Nationality\n      Signature\n      Name Category\n    \n  \n  \n    \n      0\n      1\n      The Mystery of Edwin Drood\n      The Mystery of Edwin Drood\n      1871-03-04\n      1871-06-03\n      NaN\n      0.0\n      Y\n      LCVF\n      NaN\n      ...\n      NaN\n      Dickens, Charles\n      Male\n      British\n      NaN\n      LCVF\n      Male\n      British\n      NaN\n      Attributed\n    \n    \n      1\n      2\n      The Mystery of Edwin Drood\n      The Mystery of Edwin Drood\n      1871-03-07\n      1871-05-16\n      NaN\n      0.0\n      Y\n      LCVF\n      NaN\n      ...\n      NaN\n      Dickens, Charles\n      Male\n      British\n      NaN\n      LCVF\n      Male\n      British\n      NaN\n      Attributed\n    \n    \n      2\n      3\n      Sporting Recollections in Various Countries\n      Sporting Recollections in Various Countries\n      1847-06-16\n      1847-07-07\n      NaN\n      0.0\n      Y\n      WPEDIA\n      Sunday Times\n      ...\n      NaN\n      Viardot, M. Louis\n      Male\n      French\n      NaN\n      WPEDIA\n      Male\n      British\n      NaN\n      Attributed\n    \n    \n      3\n      4\n      Brownie's Triumph\n      The Jewels\n      1880-05-08\n      1880-08-14\n      NaN\n      0.0\n      Y\n      TJW\n      NaN\n      ...\n      Sarah Elizabeth Forbush Downs; Downs, Mrs Geor...\n      Unattributed\n      Female\n      American\n      NaN\n      WPEDIA\n      Uninscribed\n      British\n      NaN\n      Unattributed\n    \n    \n      4\n      5\n      The Forsaken Bride\n      Abandoned\n      1880-08-21\n      1880-12-18\n      Fiction. From English, American and Other Peri...\n      0.0\n      Y\n      TJW\n      NaN\n      ...\n      Sarah Elizabeth Forbush Downs; Downs, Mrs Geor...\n      Unattributed\n      Female\n      American\n      NaN\n      WPEDIA\n      Uninscribed\n      British\n      NaN\n      Unattributed\n    \n  \n\n5 rows × 28 columns\n\n\n\nOkay, that’s helpful. We can see what all the columns are and what kind of data they contain. I’m still pretty new to data science workflows in Python, but it’s not too difficult to do a little bit of data wrangling with Pandas. For instance, we can take a look at the distribution of nationalities among published authors. The table shown below counts the number of distinct publications (Trove IDs) and authors for each nationality represented in the data:\n\nfiction[[\"Nationality\", \"Trove ID\", \"Publication Author\"]]. \\\n  groupby(\"Nationality\"). \\\n  nunique()\n\n\n\n\n\n  \n    \n      \n      Trove ID\n      Publication Author\n    \n    \n      Nationality\n      \n      \n    \n  \n  \n    \n      American\n      3399\n      618\n    \n    \n      Australian\n      4295\n      757\n    \n    \n      Australian/British\n      95\n      12\n    \n    \n      Austrian\n      3\n      2\n    \n    \n      British\n      10182\n      1351\n    \n    \n      British/American\n      2\n      2\n    \n    \n      Canadian\n      185\n      29\n    \n    \n      Dutch\n      1\n      1\n    \n    \n      English\n      2\n      2\n    \n    \n      French\n      187\n      64\n    \n    \n      German\n      39\n      15\n    \n    \n      Hungarian\n      2\n      1\n    \n    \n      Irish\n      63\n      33\n    \n    \n      Italian\n      12\n      1\n    \n    \n      Japanese\n      1\n      1\n    \n    \n      Multiple\n      3\n      2\n    \n    \n      New Zealand\n      67\n      23\n    \n    \n      Polish\n      1\n      1\n    \n    \n      Russian\n      18\n      13\n    \n    \n      Scottish\n      2\n      2\n    \n    \n      South African\n      14\n      5\n    \n    \n      Swedish\n      1\n      1\n    \n    \n      Swiss\n      2\n      1\n    \n    \n      United States\n      2\n      2\n    \n    \n      Unknown\n      13133\n      2692\n    \n    \n      Unknown, not Australian\n      882\n      88\n    \n  \n\n\n\n\nIt would not come as any surprise, at least not to anyone with a sense of Australian history, that there were far more British authors than Australian authors published in Australian newspapers during that period. I was mildly surprised to see so many American authors represented though, and I have nothing but love for the lone Italian who published 12 pieces.\nNow that we have a sense of the data, let’s add Arrow to the mix!\n\n\n\n\n\nAn illustration from “The Lass That Loved a Miner” by J. Monk Foster. Published in Australian Town and Country Journal, 14 April 1894. The story features such fabulous quotes as “Presently the two dark figures slid slowly, noiselessly, along the floor towards the scattered gold dust and he canisters filled with similar precious stuff. Inch by inch, foot by foot the two thieves crept like snakes nearer and nearer to the to the treasure they coveted”. Admit it, you’re hooked already, right? Trove article 71212612"
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#pandas-to-arrow-tables",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#pandas-to-arrow-tables",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "Pandas to Arrow Tables",
    "text": "Pandas to Arrow Tables\nTo give ourselves access to Apache Arrow from Python we’ll use the PyArrow library. Our immediate goal is to convert the fiction data from a Pandas DataFrame to an Arrow Table. To that end, pyarrow supplies a Table object with a from_pandas() method that we can call:\n\nimport pyarrow\n\nfiction2 = pyarrow.Table.from_pandas(fiction)\nfiction2\n\npyarrow.Table\nTrove ID: int64\nCommon Title: string\nPublication Title: string\nStart Date: string\nEnd Date: string\nAdditional Info: string\nLength: double\nCurated Dataset: string\nIdentified Sources: string\nPublication Source: string\nNewspaper ID: int64\nNewspaper: string\nNewspaper Common Title: string\nNewspaper Location: string\nNewspaper Type: string\nColony/State: string\nAuthor ID: int64\nAuthor: string\nOther Names: string\nPublication Author: string\nGender: string\nNationality: string\nNationality Details: string\nAuthor Details: string\nInscribed Gender: string\nInscribed Nationality: string\nSignature: string\nName Category : string\n----\nTrove ID: [[1,2,3,4,5,...,35491,35492,35493,35494,35495]]\nCommon Title: [[\"The Mystery of Edwin Drood\",\"The Mystery of Edwin Drood\",\"Sporting Recollections in Various Countries\",\"Brownie's Triumph\",\"The Forsaken Bride\",...,\"The Heart of Maureen\",\"His Lawful Wife\",\"Love's Reward\",\"Only a Flirt\",\"The Doctor's Protegee\"]]\nPublication Title: [[\"The Mystery of Edwin Drood\",\"The Mystery of Edwin Drood\",\"Sporting Recollections in Various Countries\",\"The Jewels\",\"Abandoned\",...,\"The Heart of Maureen\",\"His Lawful Wife\",\"Love's Reward\",\"Only a Flirt\",\"The Doctor's Protegee\"]]\nStart Date: [[\"1871-03-04\",\"1871-03-07\",\"1847-06-16\",\"1880-05-08\",\"1880-08-21\",...,\"1914-01-06\",\"1912-10-26\",\"1911-02-04\",\"1916-05-06\",\"1911-11-25\"]]\nEnd Date: [[\"1871-06-03\",\"1871-05-16\",\"1847-07-07\",\"1880-08-14\",\"1880-12-18\",...,\"1914-01-06\",\"1912-10-26\",\"1911-02-04\",\"1916-05-06\",\"1911-11-25\"]]\nAdditional Info: [[null,null,null,null,\"Fiction. From English, American and Other Periodicals\",...,\"Published by special arrangement. All rights reserved.\",\"Published by special arrangement. All rights reserved.\",\"Published by special arrangement. All rights reserved.\",\"All  Rights Reserved\",\"Published by special arrangement. All rights reserved.\"]]\nLength: [[0,0,0,0,0,...,0,0,0,0,0]]\nCurated Dataset: [[\"Y\",\"Y\",\"Y\",\"Y\",\"Y\",...,\"N\",\"N\",\"N\",\"N\",\"N\"]]\nIdentified Sources: [[\"LCVF\",\"LCVF\",\"WPEDIA\",\"TJW\",\"TJW\",...,null,null,null,null,null]]\nPublication Source: [[null,null,\"Sunday Times\",null,null,...,null,null,null,null,null]]\n...\n\n\n\nThe fiction2 object contains the same data as fiction but it is structured as an Arrow Table, and the data is stored in memory allocated by Arrow. Python itself only stores some metadata and the C++ pointer that refers to the Arrow Table. This isn’t exciting, but it will be important (and powerful!) later in a moment we transfer the data to R.\nSpeaking of which, we have arrived at the point where we get to do the fun part… seamlessly handing the reins back and forth between Python and R without needing to copy the Arrow Table itself."
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#passing-tables-from-python-to-r",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#passing-tables-from-python-to-r",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "Passing Tables from Python to R",
    "text": "Passing Tables from Python to R\nTo pass Arrow objects between Python and R, rpy2 needs a little help because it doesn’t know how to handle Arrow data structures. That’s where the rpy2-arrow module comes in. As the documentation states:\n\nThe package allows the sharing of Apache Arrow data structures (Array, ChunkedArray, Field, RecordBatch, RecordBatchReader, Table, Schema) between Python and R within the same process. The underlying C/C++ pointer is shared, meaning potentially large gain in performance compared to regular arrays or data frames shared between Python and R through the conversion rules included in rpy2.\n\nI won’t attempt to give a full tutorial on rpy2-arrow in this post. Instead, I’ll just show you how to use it to solve the problem at hand. Our first step is to import the conversion tools from rpy_arrow:\n\nimport rpy2_arrow.pyarrow_rarrow as pyra\n\nHaving done that, the pyarrow_table_to_r_table() function allows us to pass an Arrow Table from Python to R:\n\nfiction3 = pyra.pyarrow_table_to_r_table(fiction2)\nfiction3\n\n<rpy2.rinterface_lib.sexp.SexpEnvironment object at 0x7f71bfb8a6c0> [RTYPES.ENVSXP]\n\n\nThe printed output isn’t the prettiest thing in the world, but nevertheless it does represent the object of interest. On the Python side we have fiction2, a data structure that points to an Arrow Table and enables various compute operations supplied through pyarrow. On the R side we have now created fiction3, a data structure that points to the same Arrow Table and enables compute operations supplied by the R arrow package. In the same way that fiction2 only stores a small amount of metadata in Python, fiction3 stores a small amount of metadata in R. Only this metadata has been copied from Python to R: the data itself remains untouched in Arrow.\n\n\n\n\n\nHeader illustration to “Where flowers are Rare” by Val Jameson. Published in The Sydney Mail, 8 December 1909. I honestly have no logical reason for including this one. But I was listening to Kylie Minogue at the time I was browsing the database and the title made me think of Where the Wild Roses Grow, and anyway both the song and the story have death in them. So then I simply had to include the image because… it’s Kylie. Obviously. Sheesh. Trove article 165736425"
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#accessing-the-table-from-the-r-side",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#accessing-the-table-from-the-r-side",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "Accessing the Table from the R side",
    "text": "Accessing the Table from the R side\nWe’re almost done, but the tour isn’t really complete until we’ve stepped out of Python entirely, manipulated the object on the R side, and then passed something back to Python. So let’s do that next.\nIn order to pull off that trick within this quarto document – which is running jupyter under the hood – we’ll need to employ a little notebook magic, again relying on rpy2 to supply all the sparkly bits. To help us out in this situation, the rpy2 library supplies an interface for interactive work that we can invoke in a notebook context like this:\n\n%load_ext rpy2.ipython\n\nNow that we’ve included this line, all I have to do is preface each cell with %%R and the subsequent “Python” code will be passed to R and interpreted there.7 To start with I’ll load the dplyr and arrow packages, using the suppressMessages() function to prevent them being chatty:\n\n%%R\n\nsuppressMessages({\n  library(dplyr)\n  library(arrow)\n})\n\nHaving loaded the relevant packages, I’ll use the dplyr/arrow toolkit to do a little data wrangling on the fiction3 Table. I’m not doing anything fancy, just a little cross-tabulation counting the joint distribution of genders and nationalities represented in the data using the count() function, and using arrange() to sort the results:\n\n%%R -i fiction3\n\ngender <- fiction3 |> \n  count(Gender, Nationality) |>\n  arrange(desc(n)) |>\n  compute()\n  \ngender\n\nTable\n\n\n\n\n\n63 rows x 3 columns\n$Gender <string>\n$Nationality <string>\n$n <int64>\n\nSee $metadata for additional Schema metadata\n\n\n\n\n\nThe output isn’t very informative, but don’t worry, by the end of the post there will be a gender reveal I promise.8 Besides, the actual values of gender aren’t important right now. In truth, the part that we’re most interested in here is the first line of code. By using %%R -i fiction3 to specify the cell magic, we’re able to access the fiction3 object from R within this cell and perform the required computations.\nOh, and also we now have a new gender object in our R session that we probably want to pull back into Python!"
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#the-journey-home-a-tale-of-four-genders",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#the-journey-home-a-tale-of-four-genders",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "The journey home: A tale of four genders",
    "text": "The journey home: A tale of four genders\nOkay. So we now have an object in the embedded R session that we might wish to access from the Python session and convert to a Python object. First we’ll pass the Arrow Table from R to Python and then convert to a Pandas DataFrame. Here’s how that process works. If you recall from earlier in the post, we imported robjects to start the embedded R session. When we did so, we also exposed robjects.r, which provides access to all objects within that R session. To create a Python object gender2 that refers to the R data structure we created in the last section, here’s what we do:\n\ngender2 = robjects.r('gender')\ngender2\n\n<rpy2.robjects.environments.Environment object at 0x7f71b6784bc0> [RTYPES.ENVSXP]\nR classes: ('Table', 'ArrowTabular', 'ArrowObject', 'R6')\nn items: 36\n\n\nImportantly, notice that this is the same object. The gender2 variable still refers to the Arrow Table in R: it’s not a pyarrow table. If we want to convert it to a data structure that pyarrow understands, we can again use the rpy-arrow conversion tools. In this case, we can use the rarrow_to_py_table() function:\n\ngender3 = pyra.rarrow_to_py_table(gender2)\ngender3\n\npyarrow.Table\nGender: string\nNationality: string\nn: int64\n----\nGender: [[\"Unknown\",\"Male\",\"Female\",\"Male\",\"Female\",...,\"Both\",\"Female\",\"Female\",\"Female\",null]]\nNationality: [[\"Unknown\",\"British\",\"British\",\"Australian\",\"Australian\",...,\"Australian/British\",\"British/American\",\"South African\",\"Polish\",\"Australian\"]]\nn: [[12832,6420,3346,2537,1687,...,1,1,1,1,1]]\n\n\nJust like that, we’ve handed over the Arrow Table from R back to Python. Again, it helps to remember that gender2 is an R object and gender3 is a Python object, but both of them point to the same underlying Arrow Table.\nIn any case, now that we have gender3 on the Python side, we can use the to_pandas() method from pyarrow.Table to convert it to a pandas data frame:\n\ngender4 = pyarrow.Table.to_pandas(gender3)\ngender4\n\n\n\n\n\n  \n    \n      \n      Gender\n      Nationality\n      n\n    \n  \n  \n    \n      0\n      Unknown\n      Unknown\n      12832\n    \n    \n      1\n      Male\n      British\n      6420\n    \n    \n      2\n      Female\n      British\n      3346\n    \n    \n      3\n      Male\n      Australian\n      2537\n    \n    \n      4\n      Female\n      Australian\n      1687\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      58\n      Both\n      Australian/British\n      1\n    \n    \n      59\n      Female\n      British/American\n      1\n    \n    \n      60\n      Female\n      South African\n      1\n    \n    \n      61\n      Female\n      Polish\n      1\n    \n    \n      62\n      None\n      Australian\n      1\n    \n  \n\n63 rows × 3 columns\n\n\n\nAnd with that our transition home is complete!"
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#summary",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#summary",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "Summary",
    "text": "Summary\nThis post has wandered over a few topics, which is perhaps to be expected given the nature of polyglot data science. To make it all work smoothly I needed to think a little about how my Python and R environments are set up: the little asides I buried in footnotes mention the frictions I encountered in getting rpy2 to work smoothly for me, for instance. As someone who primarily uses R it took me a little while to work out how to get quarto to switch cleanly from a knitr engine to a jupyter engine. The R and Python libraries implementing Apache Arrow make it look seamless when we handover data from one language to another – and in some ways they actually do make it seamless in spite of the many little frictions that exist with Arrow, no less than any other powerful and rapidly-growing tool – but a lot of work has gone into making that transition smooth. Whether you’re an R focused developer using reticulate or a Python focused developer who prefers rpy2, the toolkit is there. I’m obviously biased in this because so much of my work revolves around Arrow these days, but at some level I’m still actually shocked that it (and other polyglot tools) works as well as it does. Plus, I’m having a surprising amount of fun teaching myself “Pythonic” ways of thinking and coding, so that’s kind of cool too.\nHopefully this post will help a few other folks get started in this area!\n\n\n\n\n\nHeader illustration to “The Black Motor Car” by J. B. Harris Burland. Published in – just to bring us full circle – The Arrow, 25 November 1905. I cannot properly do justice to this work of art so I will merely quote: “Again he took her in his arms, and this time she did not try to free herself from his embrace. But she looked up at him with pleading eyes. He bent down his face and kissed her tenderly on the forehead. His whole nature cried out for the touch of her lips, but he was man enough to subdue the passion that burnt within him.” Trove article 103450814"
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#acknowledgements",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#acknowledgements",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nIn writing this post I am heavily indebted to Isabella Velásquez, whose fabulous post on calling R from Python with rpy2 helped me immensely. The documentation on integrating PyArrow with R was extremely helpful too! Thank you to Kae Suarez for reviewing this post."
  },
  {
    "objectID": "posts/2021-07-08_generative-art-in-r/index.html",
    "href": "posts/2021-07-08_generative-art-in-r/index.html",
    "title": "Generative art in R",
    "section": "",
    "text": "A little while ago I was invited by Sara Mortara to contribute art as part of an exhibit to be presented at the 2021 useR! conference, along with several artists who I admire greatly. I could hardly say no to that, now could I? So I sent some pieces that I’m fond of, most of which are posted somewhere on my art website. I realised later though that I was going to have to talk a little about my art too, and Sara suggested an informal Q&A during the timeslot allocated to the exhibit. Naturally, I agreed since that meant I didn’t have to prepare anything formal, and like all artists I am extremely lazy. Later though, it occurred to me that it actually wouldn’t be terrible if I wrote a blog post to accompany my contribution to the exhibit, loosely based on the questions Sara suggested. And so here we are…\nWhen did you start using R for art? Do you remember your first piece?\nI started making art in R some time in late 2019. I’d discovered some of the art that Thomas Lin Pedersen had been making – at the time he was posting pieces from his Genesis series – and at the same time I found the ambient package that he was using to create the pieces. Thomas famously does not post source code for his art, and being stubborn and curious I wanted to work out how he was doing it, so I started playing with ambient to see if I could reverse engineer his system. My very first piece was Constellations, shown below. It’s certainly not the prettiest thing I’ve created, and there are a lot of things I’d like to change about it now, but it’s nice to have your early work lying around to see how you’ve changed since then:\nIf you follow the link above and look at Thomas’ Genesis pieces you can tell that it’s not even remotely close to the mark, but I did eventually get the hang of it and managed to produce a few pieces like Rainbow Prisms which are closer to the kind of work he was producing:\nIt’s still not quite the same as Thomas’ in style, but by the time I’d worked out how to produce these I decided it was time to change my approach and branch out a bit. I love Thomas’ work of course, but I didn’t want my art to be just a low quality imitation of his! And besides, by that point I’d started discovering a whole lot of other people making generative art in R, such as Will Chase, Antonio Sánchez Chinchón, Marcus Volz, and (somewhat later) Ijeamaka Anyene. Each has their own style and – following the famous advice that art is theft – have shamelessly taken ideas and inspiration from each at different times.\nSome of those early pieces are still around, as part of the Rosemary gallery.\nWere you an artist before making generative art in R?\nNot really. I always wanted to do more artistic and creative things, but the only thing I’d ever done that required any kind of mix of aesthetic sensibility and craftwork was gardening. I used to have a lovely garden in Adelaide with a mix of Mediterranean and Australian native plants, and I had the same kind of enthusiasm for gardening then as I do for art now. Maybe one day I’ll garden again but there’s no space for that in my Sydney apartment!\nCan you talk about your creative process? Do you begin from code or from the outcome you are looking for? Do you start with the color palette in mind, or is it an iterative process?\nI’m honestly not sure I have a consistent process? I spend a lot of time browsing artwork by other people on twitter and instagram, and from time to time I read posts about the techniques that they use. Whenever I do this I end up thinking a bit about how I might use this technique or wondering what methods other artists use to create their work, but I don’t usually act on that information until I think of something I want to do with it. That kind of technical or stylistic information is like background knowledge that lies dormant until I need it.\nMost of the time the starting point for my art is an emotion. I might be angry or lonely or tired, or just in need of something to occupy my mind and distract me from something else. When I start implementing a new system it’s often (though not always) a modification of a previous one. In principle this modification process could go in any direction, but my aesthetic sensibilities depend a lot on my state of mind, and that imposes a bias. I tweak the code one way, and see what it produces. If I like it, I keep the change, if I don’t I reject it. It’s a lot like a Metropolis-Hastings sampler that way, but my mood strongly shapes the accept/reject decision, so the same starting point can lead to different outcomes. As a concrete example, the Pollen, Bursts and Embers series are all based on the same underlying engine, the fractal flame algorithm created by Scott Draves, but my emotional state was very different at the time I coded each version. For example, the Pollen Cloud piece I contributed to the useR exhibit is soft and gentle largely because I was feeling peaceful and relaxed at the time:\nBy way of contrast, the Geometry in a Hurricane piece from Bursts is layered in jagged textures with a chaotic energy because I was angry at the time I was coding:\nThe Soft Ember piece below (also included in the exhibit) has a different feel again. There’s more energy to it than the pollen pieces, but it’s not as chaotic as the bursts series. Again, that’s very much a reflection of my mood. I wasn’t angry when I coded this system, but I wasn’t relaxed either. At the time, something exciting had happened in my life that I wasn’t quite able to do anything about, but I was indulging in the anticipation of a new thing, and some of that emotion ended up showing through in the pieces that I made at the time:\nTo bring all this back to the question, it’s very much an iterative process. The driver behind the process is usually an emotion, and the colour choices, the shapes, and the code are all adapted on the fly to meet with how I’m feeling.\nWhat is your inspiration?\nTo the extent that my art is driven by emotion, the inspiration for it tends to be tied to sources of strong emotion in my life. Sometimes that emotion comes from the sources of love and joy: family, intimate partners, and so on. The Heartbleed series is one of those. The background texture to these images is generated by simulating a simple Turing machine known as a turmite and the swirly hearts in the foreground are generated using the toolkit provided by the ambient package. This system is very much motivated from emotional responses to the loved ones in my life. One of the pieces in the exhibit is from this series:\nOther times the emotional motivation comes from sources of pain - sometimes things that were physically painful, sometimes that were psychologically painful. The Orchid Thorn piece I included in the exhibit is one of those, linked to an intense physically painful experience.\nThe Bitterness piece below, which I haven’t done much with other than post to my instagram, is strongly tied to the psychological stresses associated with my gender transition. Yes, there’s a softness to the piece, but there’s also a sandpaper-like texture there that makes me think of abrasion. The colour shifts make me think about transitions, but the roughness at some of the boundaries reminds me that change is often painful.\nOne odd property of the art, at least from my point of view, is that looking at a given piece recalls to mind the events and emotions that inspired the work, and to some extent that recollection becomes a way of re-experiencing the events. Sometimes that’s a good thing. Not always though.\nWhat is your advice for people who wants to create art in R?\nI think I’d suggest three things. Find artists you like, read about their processes. Sometimes they’ll show source code or link to algorithms like I’ve done in a few places in this piece, and it can be really valuable to try to retrace their steps. There’s nothing wrong with learning technique by initially copying other artists and then developing your own style as you go.\nThe second thing I’d suggest, for R folks specifically, is to take advantage of the skills you already have. Most of us have skills in simulation, data wrangling, and data visualisation, and those skills can be repurposed for artistic work quite easily. A lot of my pieces are created using that specific combination. I’ll often define a stochastic process and sample data from it using tools in base R, use dplyr to transform and manipulate it, then use ggplot2 to map the data structure onto a visualisation. One of the nice things about dplyr and ggplot2 being compositional grammars is the fact that you can “reuse” their parts for different purposes. I get a lot of artistic mileage out of geom_point() and geom_polygon(), and quite frankly purrr is an absolute godsend when the generative process you’re working with is iterative in nature.\nThe other thing would be try not to put pressure on yourself to be good at it immediately. I wasn’t, and I don’t think anyone else was either. Earlier I showed the Constellations piece and referred to it as the first piece I created. In a way that’s true, because it was the first time I reached a level that I felt comfortable showing to other people. But I made a lot of junk before that, and I made a lot of junk after that. I make some good art now (or so people tell me) precisely because I made a lot of bad art before. Even now, though, I can’t tell which systems will end up good and which will end up bad. It’s a bit of a lottery, and I’m trying my best not to worry too much about how the lottery works. I like to have fun playing with visual tools, and sometimes the tinkering takes me interesting places.\nAnything to add about your pieces in the exhibit?\nNot a lot. Several of the pieces I’ve contributed are already linked above, but I might just say a little about the other pieces and how they were made. The Silhouette in Teal piece uses the flametree generative art package to create the tree shown in silhouette in the foreground, and a simple random walk to generate the texture in the background:\nIt has also been surprisingly popular on my Society6 store, which you can visit if you want some of my art on random objects. I am not sure why, but I have sold a lot more shower curtains and yoga mats than I would have expected to sell in my lifetime.\nLeviathan emerged from my first attempt to create simulated watercolours in R using this guide written by Tyler Hobbs. I was in a dark mood at the time and the ominous mood to the piece seems quite fitting to me.\nThe Floral Effect piece is an odd one. It’s part of the Viewports series that I created by applying Thomas Lin Pedersen’s ggfx package over the top of the output of the same system I used to create the Native Flora series, which in turn is an application of the flametree system I mentioned earlier. I quite like it when these systems build on top of one another.\nThe last piece I included, Fire and Ice, is a little different from the others in that it’s not a “pure” generative system. It works by reading an image file into R, using Chris Marcum’s halftoner package to convert it to a halftone image, and then manipulate that image using the tools provided in the ambient package. The end result is something that still resembles the original image but has more of a painted feel:"
  },
  {
    "objectID": "posts/2021-07-08_generative-art-in-r/index.html#last-updated",
    "href": "posts/2021-07-08_generative-art-in-r/index.html#last-updated",
    "title": "Generative art in R",
    "section": "Last updated",
    "text": "Last updated\n\n2022-08-23 13:12:01 AEST"
  },
  {
    "objectID": "posts/2021-07-08_generative-art-in-r/index.html#details",
    "href": "posts/2021-07-08_generative-art-in-r/index.html#details",
    "title": "Generative art in R",
    "section": "Details",
    "text": "Details\n\nsource code, R environment"
  },
  {
    "objectID": "posts/2022-03-17_using-aws-s3-in-r/index.html",
    "href": "posts/2022-03-17_using-aws-s3-in-r/index.html",
    "title": "Using Amazon S3 with R",
    "section": "",
    "text": "I have a shameful confession to make, one that may shock and surprise you. Although I am an R user, data scientist, and developer of many years experience, I’ve never used Amazon Web Services.\nIt’s hard to believe, I know, but I’ve never spun up a virtual machine on “Amazon EC2” (…whatever that is), I don’t know what “AWS Lambda” is, and the only thing I know about “Amazon S3” is that fancy data science people use it to store data. Or something along those lines. Honestly, I really haven’t been paying attention. Every time people start talking about it my eyes glaze over and my impostor syndrome arrives to berate me. A true data scientist is born knowing how to spin up EC2 instances, and if baby doesn’t post her drawings on S3 then she’s already falling behind, etc etc. It’s terribly stressful.\nMy dark and terrible personal tragedy notwithstanding,1 I suspect my situation is not entirely uncommon. Back in my academic days, I knew very few people who used Amazon Web Services (a.k.a. AWS) for much of anything. It wasn’t needed, so it wasn’t knowledge that people acquired. Now that I’m working in an industry setting I’m finding that it’s so widely used that it’s almost assumed knowledge. Everyone knows this stuff, so there’s not a lot said about why you might care, or how to get started using these tools if you decide that you do care.\nToday I decided to do something about this, starting by teaching myself how to use Amazon’s Simple Storage Service (a.k.a S3). With the help of the aws.s3 package authored by Thomas Leeper and currently maintained by Simon Urbanek, it’s surprisingly easy to do.\nIn this post I’ll walk you through the process."
  },
  {
    "objectID": "posts/2022-03-17_using-aws-s3-in-r/index.html#what-is-s3-and-why-do-i-care",
    "href": "posts/2022-03-17_using-aws-s3-in-r/index.html#what-is-s3-and-why-do-i-care",
    "title": "Using Amazon S3 with R",
    "section": "What is S3 and why do I care?",
    "text": "What is S3 and why do I care?\nLet’s get started. As with everything else in life, the place to start is asking yourself if you even care. I mean, if we don’t care what S3 is or what it does, why even bother? Just let your eyes glaze over as the nerd keeps talking and wonder if there’s anything good on TV…\nStill here? Cool.\nFrom the user perspective, Amazon’s “Simple Storage Service” isn’t particularly complicated. It’s just a remote storage system that you can dump files into, kind of like a programmable Dropbox. Each file (and its accompanying metadata) is stored as an object, and collections of objects are grouped together into a bucket. If you want to store files on S3, all you need to do is open an account, create a new bucket, and upload your files. It’s exactly that boring, and the only reason anyone cares (as far as I know) is that Amazon designed this to work at scale and it’s fairly easy to write scripts that allow you to control the whole thing programmatically. Which is actually a pretty handy service, now that I think about it!"
  },
  {
    "objectID": "posts/2022-03-17_using-aws-s3-in-r/index.html#downloading-public-data-from-s3",
    "href": "posts/2022-03-17_using-aws-s3-in-r/index.html#downloading-public-data-from-s3",
    "title": "Using Amazon S3 with R",
    "section": "Downloading public data from S3",
    "text": "Downloading public data from S3\nThe first thing to understand about S3 is that there’s many different ways of using it. Very often, you’re not interested in storing your own data on S3. You might just want to download data that someone else has stored, and if that data has been made publicly accessible, then you don’t even need an Amazon Web Services (AWS) account at all. You can download to your hearts content. For a data scientist it’s a fun way to start, because you get to dive straight into playing with data insted of fiddling about with accounts and credentials and all those dull things.\nSo let’s find a public data set to play with. While browsing through the registry of open data sets listed on the S3 website I came across the National Herbarium of NSW data set. As described on the website:\n\nThe National Herbarium of New South Wales is one of the most significant scientific, cultural and historical botanical resources in the Southern hemisphere. The 1.43 million preserved plant specimens have been captured as high-resolution images and the biodiversity metadata associated with each of the images captured in digital form. Botanical specimens date from year 1770 to today, and form voucher collections that document the distribution and diversity of the world’s flora through time, particularly that of NSW, Austalia and the Pacific. The data is used in biodiversity assessment, systematic botanical research, ecosystem conservation and policy development. The data is used by scientists, students and the public.\n\nAs an example, here’s one of the images stored in the data set, of a plant specimen collected quite close to where I currently live, albeit quite a long time ago:\n\n\n\n\n\n\n\nSo yeah, I love this data set already and I want to play with it. But how do I do that? I’ve never done anything with S3 before and it’s all a bit new to me. Well, on the right hand side of the listing page for the National Herbarium data, there’s a section that contains the following metadata:\nDescription\nHerbarium Collection Image files\n\nResource type\nS3 Bucket\n\nAmazon Resource Name (ARN)\narn:aws:s3:::herbariumnsw-pds\n\nAWS Region\nap-southeast-2\nUsing this information, I can get started. I know what the data is (an S3 bucket), I know where the data is (in the \"ap-southeast-2\" region), and on top of that I know the name of the data (\"herbariumnsw-pds\"). This should be enough for me to find what I’m looking for!\n\nFinding the bucket\nOkay, so let’s see if we can find this bucket using R code. The aws.s3 package contains a handy function called bucket_exists(), which returns TRUE when it finds an S3 bucket at the specified location (and using whatever credentials you currently have available), and FALSE when it does not. That seems relatively easy. We know the name of our bucket, specified more precisely as \"s3://herbariumnsw-pds/\", and we can verify that it exists. And of course when we do this it turns out that there…\n\nbucket_exists(\"s3://herbariumnsw-pds/\")\n\n\n\n[1] FALSE\n\n\n…isn’t? Wait, what????\nI’ve made a very common mistake here, and forgotten to specify the region. S3 is very picky about regions and you need to tell it explicitly which one to use. The National Herbarium is an Australian institution and the data are stored in Amazon’s Sydney data center. In Amazon parlance, that’s the \"ap-southeast-2\" region, but unless you’ve done something to set a different default (more on that later), everything you do will probably default to the \"us-east-1\" region. To override this default, we can explicitly specify the region that bucket_exists() should look in. So now let’s try that again:\n\nbucket_exists(\n  bucket = \"s3://herbariumnsw-pds/\", \n  region = \"ap-southeast-2\"\n)\n\n\n\n[1] TRUE\n\n\nMuch better!\n\n\nOkay, okay, I lied…\nOne more thing. If you’ve been following along at home and trying out these commands, you’ve probably noticed that the output you’re getting is a little more verbose than simply returning TRUE or FALSE. The actual output comes with a lot of additional metadata, stored as attributes. I didn’t really want to clutter the output by showing all that stuff, so the examples above secretly removed the attributes before printing the results. What you’ll actually see is something like this:\n\nbucket_exists(\n  bucket = \"s3://herbariumnsw-pds/\", \n  region = \"ap-southeast-2\"\n)\n\n[1] TRUE\nattr(,\"x-amz-id-2\")\n[1] \"cr7uaPSaKx4B5BJNRCDIU+Cpns0menZBjxjT5OltIViGFUlStJxSqI5rT1lZfN3ASVz+p4XMalE=\"\nattr(,\"x-amz-request-id\")\n[1] \"AE47G6MWJA7NRMYJ\"\nattr(,\"date\")\n[1] \"Fri, 22 Apr 2022 09:04:11 GMT\"\nattr(,\"x-amz-bucket-region\")\n[1] \"ap-southeast-2\"\nattr(,\"x-amz-access-point-alias\")\n[1] \"false\"\nattr(,\"content-type\")\n[1] \"application/xml\"\nattr(,\"server\")\n[1] \"AmazonS3\"\n\n\nIf you stare at this long enough this metadata all starts to make sense, especially after you’ve been playing around with S3 for a while. There’s a timestamp, there’s some information about which region the data came from, and so on. Nothing particularly special or interesting here, so let’s move on to something more fun.\n\n\nListing bucket contents\nAt this point in the journey we’ve located the bucket, but we have no idea what it contains. To get a list of the bucket contents, the get_bucket_df() function from aws.s3 is our friend. The National Herbarium data set contains a lot of objects, so I’ll be “frugal” and restrict myself to merely downloading max = 20000 records:\n\nherbarium_files <- get_bucket_df(\n  bucket = \"s3://herbariumnsw-pds/\", \n  region = \"ap-southeast-2\", \n  max = 20000\n) %>% \n  as_tibble()\n\nNow that we’ve downloaded a list of the bucket contents, let’s have a look and see what we’ve got:\n\nherbarium_files\n\n# A tibble: 20,000 × 8\n   Key                        LastM…¹ ETag  Size  Owner…² Owner…³ Stora…⁴ Bucket\n   <chr>                      <chr>   <chr> <chr> <chr>   <chr>   <chr>   <chr> \n 1 ReadMe.txt                 2020-0… \"\\\"5… 2729  97c09b… herbar… STANDA… herba…\n 2 dwca-nsw_avh-v1.0.zip      2019-1… \"\\\"2… 8231… 97c09b… herbar… STANDA… herba…\n 3 herbariumnsw-pds/PublicDa… 2021-0… \"\\\"3… 33    <NA>    <NA>    STANDA… herba…\n 4 herbariumnsw-pds/PublicDa… 2021-0… \"\\\"5… 433   <NA>    <NA>    STANDA… herba…\n 5 herbariumnsw-pds/PublicDa… 2021-0… \"\\\"5… 33    <NA>    <NA>    STANDA… herba…\n 6 herbariumnsw-pds/PublicDa… 2021-0… \"\\\"f… 433   <NA>    <NA>    STANDA… herba…\n 7 herbariumnsw-pds/PublicDa… 2021-0… \"\\\"b… 33    <NA>    <NA>    STANDA… herba…\n 8 herbariumnsw-pds/PublicDa… 2021-0… \"\\\"4… 433   <NA>    <NA>    STANDA… herba…\n 9 herbariumnsw-pds/PublicDa… 2021-0… \"\\\"f… 33    <NA>    <NA>    STANDA… herba…\n10 herbariumnsw-pds/PublicDa… 2021-0… \"\\\"6… 433   <NA>    <NA>    STANDA… herba…\n# … with 19,990 more rows, and abbreviated variable names ¹​LastModified,\n#   ²​Owner_ID, ³​Owner_DisplayName, ⁴​StorageClass\n\n\nWonderful! The very first object in the bucket happens to be a file called ReadMe.txt. Perhaps I should download this marvelous object and perhaps even read it?\n\n\nDownloading files\nOkay then. We are now at the step where we want to download a specific object from the bucket, and save it locally as a file. To do this we use the save_object() function. As before, we specify the bucket and the region, but we’ll also need to specify which object should be downloaded, and the file path to which it should be saved. Here’s how that works for the Read Me file:\n\nsave_object(\n  object = \"ReadMe.txt\",\n  bucket = \"s3://herbariumnsw-pds/\", \n  region = \"ap-southeast-2\",\n  file = \"herbarium/ReadMe.txt\"\n)\n\n[1] \"herbarium/ReadMe.txt\"\n\n\nOnce again this works and so off I go, reading the Read Me in search of further clues.\nAs you might hope, the Read Me file does in fact tell us something about how the National Herbarium dataset is organised. In particular, one line in the Read Me informs me that there’s a file storing all the metadata, encoded as a zipped csv file:\n\nA zipped csv containing the biocollections metadata for the images is available as a DarwinCore Archive at: https://herbariumnsw-pds.s3-ap-southeast-2.amazonaws.com/dwca-nsw_avh-v1.0.zip\n\nThis sounds like a good place to start, doesn’t it? Once again, I’ll use save_object() and try to download the metadata file dwca-nsw_avh-v1.0.zip:\n\nsave_object(\n  object = \"dwca-nsw_avh-v1.0.zip\",\n  bucket = \"s3://herbariumnsw-pds/\", \n  region = \"ap-southeast-2\",\n  file = \"herbarium/dwca-nsw_avh-v1.0.zip\"\n) \n\n[1] \"herbarium/dwca-nsw_avh-v1.0.zip\"\n\n\nSuccess!\nI now have a copy of the 79MB zip file on my laptop, and after decompressing the file it turns out I have a 402MB file called occurrence.txt that contains the metadata. As it turns out, the metadata aren’t stored in comma-separated value format, they’re stored in tab-separated value format. Still, that’s fine: the read_tsv() function from the readr package can handle it:\n\nherbarium <- read_tsv(\"herbarium/dwca-nsw_avh-v1.0/occurrence.txt\")\n\nWarning: One or more parsing issues, see `problems()` for details\n\n\nRows: 725507 Columns: 74\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (38): id, type, institutionCode, collectionCode, basisOfRecord, occurre...\ndbl   (7): minimumElevationInMeters, maximumElevationInMeters, minimumDepthI...\nlgl  (28): lifeStage, associatedSequences, associatedTaxa, previousIdentific...\ndttm  (1): modified\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nherbarium\n\n# A tibble: 725,507 × 74\n   id     type  modified            institutionCode collectionCode basisOfRecord\n   <chr>  <chr> <dttm>              <chr>           <chr>          <chr>        \n 1 NSW:N… Phys… 2013-11-28 11:56:00 NSW             NSW            PreservedSpe…\n 2 NSW:N… Phys… 2012-08-09 15:47:00 NSW             NSW            PreservedSpe…\n 3 NSW:N… Phys… 2015-03-13 15:51:00 NSW             NSW            PreservedSpe…\n 4 NSW:N… Phys… 2018-07-24 10:06:00 NSW             NSW            PreservedSpe…\n 5 NSW:N… Phys… 2015-03-13 15:51:00 NSW             NSW            PreservedSpe…\n 6 NSW:N… Phys… 2013-07-24 15:16:00 NSW             NSW            PreservedSpe…\n 7 NSW:N… Phys… 2015-03-13 15:51:00 NSW             NSW            PreservedSpe…\n 8 NSW:N… Phys… 2010-12-01 14:25:00 NSW             NSW            PreservedSpe…\n 9 NSW:N… Phys… 2018-01-24 16:49:00 NSW             NSW            PreservedSpe…\n10 NSW:N… Phys… 2018-07-24 10:05:00 NSW             NSW            PreservedSpe…\n# … with 725,497 more rows, and 68 more variables: occurrenceID <chr>,\n#   catalogNumber <chr>, occurrenceRemarks <chr>, recordNumber <chr>,\n#   recordedBy <chr>, lifeStage <lgl>, reproductiveCondition <chr>,\n#   establishmentMeans <chr>, occurrenceStatus <chr>, preparations <chr>,\n#   associatedSequences <lgl>, associatedTaxa <lgl>,\n#   previousIdentifications <lgl>, eventDate <chr>, verbatimEventDate <chr>,\n#   habitat <chr>, eventRemarks <lgl>, continent <lgl>, waterBody <lgl>, …\n\n\nThere’s quite a lot of interesting information stored in the 74 columns of the herbarium data, but I won’t dive very deep into it in this post. I will mention, however, that if you find yourself following along at home you’ll likely discover that there is a small proportion of the 725507 rows that cause problems for read_tsv(), likely because they contain additional tab characters that mess up the parsing slightly. In real life I’d want to look into this, but this is a blog post. Nothing here is real and nobody is watching, right?\n\n\nWrangling the data\nNow that I have some data, I can do a little poking around to see what’s in it. Exploring a new data set is always fun, but this isn’t really a post about data wrangling, so I’ll keep this brief. A quick look suggests that (unsurprisingly) there are a lot of records corresponding to samples collected in Australia, and a disproportionate number of those come from New South Wales:\n\nherbarium %>% \n  filter(country == \"Australia\") %>% \n  count(stateProvince)\n\n# A tibble: 10 × 2\n   stateProvince                     n\n   <chr>                         <int>\n 1 Australian Capital Territory     52\n 2 External Territories           1549\n 3 New South Wales              394439\n 4 Northern Territory            28922\n 5 Queensland                    89016\n 6 South Australia               20206\n 7 Tasmania                      23994\n 8 Victoria                      40984\n 9 Western Australia             80447\n10 <NA>                           4287\n\n\nThat’s nice, but doesn’t immediately suggest a fun example for me to continue this post. On a whim, I decide to name search my neighbourhood. I live in Newtown (in Sydney), so I’m going to find the subset of images in the National Herbarium data whose locality matches the string \"Newtown\":\n\nnewtowners <- herbarium %>% \n  filter(\n    country == \"Australia\", \n    locality %>% str_detect(\"Newtown\")\n  )\n\nYeah, no. This is misleading.\nFrom a data science point of view I’m being extremely sloppy here. If my intention had been to find only plants from my neighbourhood, I would also be wise to filter by recorded longitude and latitude where available, and I would certainly want to exclude cases listed as coming from another Australian state. “Newtown” is not an uncommon name, and – to the surprise of nobody – it turns out that there are several different locations called “Newtown” in different parts of Australia. Fortunately for me, I really don’t care! All I wanted was a query that would return around 20-30 results, so this is fine for my purposes.\nNow that we’ve got a subset of records, let’s pull out the catalog numbers:\n\nnewtowners %>% \n  pull(catalogNumber)\n\n [1] \"NSW 395530\"  \"NSW 461895\"  \"NSW 650052\"  \"NSW 1055313\" \"NSW 1056305\"\n [6] \"NSW 29246\"   \"NSW 36860\"   \"NSW 39618\"   \"NSW 687458\"  \"NSW 121207\" \n[11] \"NSW 214616\"  \"NSW 306564\"  \"NSW 307215\"  \"NSW 389387\"  \"NSW 395529\" \n[16] \"NSW 402973\"  \"NSW 403188\"  \"NSW 404127\"  \"NSW 421494\"  \"NSW 446243\" \n[21] \"NSW 570557\"  \"NSW 702035\"  \"NSW 676197\"  \"NSW 776212\"  \"NSW 777249\" \n[26] \"NSW 739455\"  \"NSW 751830\" \n\n\nThe Read Me file had something useful to say about these numbers. Specifically, the catalog numbers are used as the basis of the file naming convention for images stored in the bucket:\n\nImage data are organized by NSW specimen barcode number. For example, the file for Dodonaea lobulata recorded on 1968-09-07 = NSW 041500 can be accessed via the URI https://herbariumnsw-pds.s3-ap-southeast-2.amazonaws.com/images/NSW041500.jp2\n\nHm. I wonder if I can write code to extract these images?\n\n\n\n\n\n\n\n\n\nScripting the download\nOkay, now I want to pull the images for these records. First, I’m going to construct the paths. I am not going to download the jp2 files because they’re about 100MB each. Multiplying that number by the number of records gives… well, it gives a big enough number that I think I’ve worked out why the National Herbarium dataset is on S3 and not on a laptop in a damp basement somewhere!\nIn any case, for a lot of the records there’s a jpg file that is considerably smaller in size, so I’m going to try to download those. Based on the barcodes I’ve got, these are the files I’m expecting to find:\n\nobjects <- newtowners %>%\n  pull(catalogNumber) %>% \n  str_remove_all(\" \") %>% \n  str_c(\"images/\", ., \".jpg\")\n\nobjects\n\n [1] \"images/NSW395530.jpg\"  \"images/NSW461895.jpg\"  \"images/NSW650052.jpg\" \n [4] \"images/NSW1055313.jpg\" \"images/NSW1056305.jpg\" \"images/NSW29246.jpg\"  \n [7] \"images/NSW36860.jpg\"   \"images/NSW39618.jpg\"   \"images/NSW687458.jpg\" \n[10] \"images/NSW121207.jpg\"  \"images/NSW214616.jpg\"  \"images/NSW306564.jpg\" \n[13] \"images/NSW307215.jpg\"  \"images/NSW389387.jpg\"  \"images/NSW395529.jpg\" \n[16] \"images/NSW402973.jpg\"  \"images/NSW403188.jpg\"  \"images/NSW404127.jpg\" \n[19] \"images/NSW421494.jpg\"  \"images/NSW446243.jpg\"  \"images/NSW570557.jpg\" \n[22] \"images/NSW702035.jpg\"  \"images/NSW676197.jpg\"  \"images/NSW776212.jpg\" \n[25] \"images/NSW777249.jpg\"  \"images/NSW739455.jpg\"  \"images/NSW751830.jpg\" \n\n\nThis all seems pretty reasonable, but there’s a nuance here that is worth pointing out. When you look at the output above, it’s tempting to think that \"images\" must be a subfolder within the S3 bucket. That intuition isn’t correct: each S3 bucket is a flat datastore. It doesn’t contain any subfolders: the \"/\" is treated as part of the object name, nothing more. It can be convenient to name objects this way, though, because it makes it a little easier to organise them into subfolders later on if you want to move them onto a more traditional hierarchical file system.\nAnyway…\nSince I’m going to try downloading objects that may or may not actually exist (i.e., I’m not certain if all these records actually have jpg files), I’m going to start out by writing a helper function save_herbarium_image() that does three things:\n\nFirst, it uses the object_exists() function to check if an object with that name exists in this bucket. The object_exists() function works similarly to the bucket_exists() function I used earlier: the only difference is that I also specify the object name.\nSecond, if the object exists, it downloads the file and stores it locally, in the \"herbarium\" subfolder in the folder that contains this blog post.\nThird, it returns information to the user. If the object exists and was successfully downloaded, it returns a character string specifying the location of the saved file. If the object doesn’t exist, it returns NA.\n\nHere’s the code:\n\nsave_herbarium_image <- function(file) {\n  \n  # if object doesn't exist in bucket, return NA\n  ok <- object_exists(\n    object = file,\n    bucket = \"s3://herbariumnsw-pds/\", \n    region = \"ap-southeast-2\"\n  )\n  if(!ok) return(NA_character_)\n  \n  # if object exists, save it and return file path\n  save_object(\n      object = file,\n      bucket = \"s3://herbariumnsw-pds/\", \n      region = \"ap-southeast-2\",\n      file = paste0(\"herbarium/\", file)\n  )\n}\n\nAnd here it is applied to the first file:\n\nobjects[1] %>% \n  save_herbarium_image()\n\n[1] \"herbarium/images/NSW395530.jpg\"\n\n\nThat seemed to work well when applied to a single file, so I’ll use the functional programming tools from purrr to vectorise the operation. More precisely, I’ll use map_chr() to iterate over all of the objects, applying the save_herbarium_image() function to each one, and collecting the return values from all these function calls into a character vector:\n\nobjects %>% \n  map_chr(save_herbarium_image)\n\nClient error: (404) Not Found\nClient error: (404) Not Found\nClient error: (404) Not Found\nClient error: (404) Not Found\n\n\n [1] \"herbarium/images/NSW395530.jpg\"  \"herbarium/images/NSW461895.jpg\" \n [3] \"herbarium/images/NSW650052.jpg\"  \"herbarium/images/NSW1055313.jpg\"\n [5] NA                                \"herbarium/images/NSW29246.jpg\"  \n [7] \"herbarium/images/NSW36860.jpg\"   \"herbarium/images/NSW39618.jpg\"  \n [9] \"herbarium/images/NSW687458.jpg\"  \"herbarium/images/NSW121207.jpg\" \n[11] \"herbarium/images/NSW214616.jpg\"  \"herbarium/images/NSW306564.jpg\" \n[13] \"herbarium/images/NSW307215.jpg\"  \"herbarium/images/NSW389387.jpg\" \n[15] NA                                \"herbarium/images/NSW402973.jpg\" \n[17] \"herbarium/images/NSW403188.jpg\"  \"herbarium/images/NSW404127.jpg\" \n[19] NA                                \"herbarium/images/NSW446243.jpg\" \n[21] \"herbarium/images/NSW570557.jpg\"  \"herbarium/images/NSW702035.jpg\" \n[23] \"herbarium/images/NSW676197.jpg\"  \"herbarium/images/NSW776212.jpg\" \n[25] \"herbarium/images/NSW777249.jpg\"  \"herbarium/images/NSW739455.jpg\" \n[27] NA                               \n\n\nDid it work? Well, kind of. Notice there are some missing values in the output. In those cases the object doesn’t exist in this bucket, and when that happens the save_herbarium_image() function bails and doesn’t try to download anything. But in most cases images it worked.\n\n\nA minor irritant appears!\nAt this point, I’d like to start displaying the images. It’s nice to have pretty pictures in a blog post, don’t you think? Like, maybe what I could do is include some of those images in this post. One problem though is that the files stored in the National Herbarium dataset are high resolution images and as consequence even the jpg files are usually about 7MB each. That’s a bit excessive, so I think what I’ll do is write a little helper function that reads in each image, resizes it to something smaller, and then saves that smaller file.\nIf I want to do this within R, the magick package is my friend. It’s extremely well suited to this kind of image manipulation task. This post isn’t about the magick package, so I’m not going to explain this part of the code,2 but suffice it to say that this helper function solves the problem:\n\nshrink_herbarium_image <- function(file) {\n  on.exit(gc())\n  img_from <- file.path(\"herbarium\", \"images\", file)\n  img_to <- file.path(\"herbarium\", \"tiny_images\", file)\n  image_read(img_from) %>% \n    image_resize(geometry_size_pixels(width = 1000)) %>% \n    image_write(img_to)\n}\n\nNow that I have this function, I can iterate over every image stored in my local images folder, shrink it, and save the small version to the tiny_images folder:\n\nlist.files(\"herbarium/images\") %>% \n  map_chr(shrink_herbarium_image)\n\n [1] \"herbarium/tiny_images/NSW1055313.jpg\"\n [2] \"herbarium/tiny_images/NSW121207.jpg\" \n [3] \"herbarium/tiny_images/NSW214616.jpg\" \n [4] \"herbarium/tiny_images/NSW29246.jpg\"  \n [5] \"herbarium/tiny_images/NSW306564.jpg\" \n [6] \"herbarium/tiny_images/NSW307215.jpg\" \n [7] \"herbarium/tiny_images/NSW36860.jpg\"  \n [8] \"herbarium/tiny_images/NSW389387.jpg\" \n [9] \"herbarium/tiny_images/NSW395530.jpg\" \n[10] \"herbarium/tiny_images/NSW39618.jpg\"  \n[11] \"herbarium/tiny_images/NSW402973.jpg\" \n[12] \"herbarium/tiny_images/NSW403188.jpg\" \n[13] \"herbarium/tiny_images/NSW404127.jpg\" \n[14] \"herbarium/tiny_images/NSW446243.jpg\" \n[15] \"herbarium/tiny_images/NSW461895.jpg\" \n[16] \"herbarium/tiny_images/NSW570557.jpg\" \n[17] \"herbarium/tiny_images/NSW650052.jpg\" \n[18] \"herbarium/tiny_images/NSW676197.jpg\" \n[19] \"herbarium/tiny_images/NSW687458.jpg\" \n[20] \"herbarium/tiny_images/NSW702035.jpg\" \n[21] \"herbarium/tiny_images/NSW739455.jpg\" \n[22] \"herbarium/tiny_images/NSW776212.jpg\" \n[23] \"herbarium/tiny_images/NSW777249.jpg\" \n\n\nThe output here is a character vector containing names for the created files. That’s nice as a way of checking that everything worked, but I want pretty pictures! So here’s the contents of the tiny_images folder, but shown as the actual images rather than file names:3\n\n\n\nusing 'image-only' layout\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgress!"
  },
  {
    "objectID": "posts/2022-03-17_using-aws-s3-in-r/index.html#accounts-and-credentials",
    "href": "posts/2022-03-17_using-aws-s3-in-r/index.html#accounts-and-credentials",
    "title": "Using Amazon S3 with R",
    "section": "Accounts and credentials",
    "text": "Accounts and credentials\nAt this point it is starting to dawn on me that it would be kind of neat to create my own S3 bucket and store the tiny images there. I could make the tiny images public and then display them in this post. The National Herbarium data is released under a Creative Commons By-Attribution licence, so I’m allowed to use the images that way as long as I properly acknowledge the source… which I think is fairly well covered in this post already!\nThe task I’m going to set for myself later in this post is to do exactly that, and use tools from the aws.s3 package to do everything in R. However, I can’t do any of that unless I have an AWS account of my very own. The time has come for me to do that.\n\nCreating the account\nSigning up for the account turns out to be pretty easy. All I had to do was visit https://aws.amazon.com/s3/ and click on the “Create an AWS Account” button shown in the image below:\n\n\n\n\n\n\n\nThis then led me through a pretty standard sign up process. I had to provide an email address for the “root user” (i.e., me!), specify a password, and so on. I didn’t sign up for anything that cost money. The free tier allows you 5GB of storage for 12 months, which is fairly convenient for “playing around” purposes, and that’s all I’m intending to do here.\n\n\nCreating credentials\nThe next step is to create an access key, so that R can interact with S3 using my credentials. At this point a little care is needed. It is possible to create access credentials for the root user, but that’s not a good idea. The root user has access to every AWS service, not just S3, and it’s a bad idea to give R access to any credentials that have those permissions. What I’ll do here is create an an “IAM user” – where “IAM” stands for “Identity and Access Management” – that only has access to my S3 storage, and the credentials I supply to R will be associated with that user. Here’s how I did that. First, I went over to the IAM console here:\nhttps://us-east-1.console.aws.amazon.com/iamv2/home#/users\nOn this screen there’s an “add users” button that I dutifully click…\n\n\n\n\n\n\nFrom here it’s mostly a matter of following prompts. The screenshot below shows me part way through the creation process. The IAM user has its own username, and it will be allowed programmatic access using an access key:\n\n\n\n\n\n\nWhen I get to the next screen it asks me to set the permissions associated with this user. I click on “attach existing policies directly”, and then type “S3” into the search box. It comes up with a list of permission policies associated with S3 and I select the one I want:\n\n\n\n\n\n\nThe third screen is boring. It asks for tags. I don’t give it any. I move onto the fourth screen, which turns out to be a review screen:\n\n\n\n\n\n\nHaving decided I am happy with these settings, I click on the “next” button that isn’t actually shown in these screenshots (it’s at the bottom of the page) and it takes me to a final screen that gives me the access key ID and the secret access key:\n\n\n\n\n\nThese are the two pieces of information I need to let R access to my S3 storage.\n\n\nStoring your AWS credentials in R\nThere are several ways of storing these credentials in R. The easiest is to add the credentials to your .Renviron file, which you can conveniently open with the edit_r_environ() function from the usethis package. To get access to the account, the following lines need to be added to your .Renviron file:\nAWS_ACCESS_KEY_ID=<your access key id>\nAWS_SECRET_ACCESS_KEY=<your secret key>\nHowever, if you’re going to be using the same AWS region all the time (e.g., you’re in Sydney so you tend to use \"ap-southeast-2\" rather than \"us-east-1\"), you might as well add a third line that sets your default region. That way, you won’t need to bother manually specifying the region argument every time you want to interact with S3: the aws.s3 package will use your default. So for me, the relevant lines ended up looking like this:\nAWS_ACCESS_KEY_ID=<my access key id>\nAWS_SECRET_ACCESS_KEY=<my secret key>\nAWS_DEFAULT_REGION=ap-southeast-2\nAfter restarting R, these new settings will apply."
  },
  {
    "objectID": "posts/2022-03-17_using-aws-s3-in-r/index.html#manipulating-your-s3-storage-from-r",
    "href": "posts/2022-03-17_using-aws-s3-in-r/index.html#manipulating-your-s3-storage-from-r",
    "title": "Using Amazon S3 with R",
    "section": "Manipulating your S3 storage from R",
    "text": "Manipulating your S3 storage from R\nNow that I have an AWS account and credentials, I can start using the aws.s3 package for more than just downloading files. I can create my own buckets, put objects into those buckets, control the access settings for those objects, and a good deal more besides. So let’s give that a go, shall we?\n\nCreating a new bucket\nThe function to create a new bucket is called put_bucket() and now that my credentials are set up it’s almost comically easy to use. If I want to create a bucket called \"tiny-herbs\", this is what I do:\n\nput_bucket(\"tiny-herbs\")\n\n\n\n[1] TRUE\n\n\nThat seems too easy? I am skeptical. I’m convinced that something must have gone wrong, so my first impulse is to use bucket_exists() to verify that it worked. Okay, so… does this new bucket exist?\n\nbucket_exists(\"s3://tiny-herbs/\") \n\n[1] TRUE\nattr(,\"x-amz-id-2\")\n[1] \"4B8L2PauxADAbOSyFra5ra/OHwObxniV89yWTPe44PJ0TRjIOdsNQdHl1El1t0/39aSQOyJS1FE=\"\nattr(,\"x-amz-request-id\")\n[1] \"GH35D6V4G9W945BY\"\nattr(,\"date\")\n[1] \"Tue, 23 Aug 2022 03:05:15 GMT\"\nattr(,\"x-amz-bucket-region\")\n[1] \"ap-southeast-2\"\nattr(,\"x-amz-access-point-alias\")\n[1] \"false\"\nattr(,\"content-type\")\n[1] \"application/xml\"\nattr(,\"server\")\n[1] \"AmazonS3\"\n\n\nIt does, and notice that both put_bucket() and bucket_exists() have respected my default region setting. When I called put_bucket(), the aws.s3 package supplied the region from my default and so the bucket was created in Sydney (i.e., “ap-southeast-2”), and it did the same again when I used bucket_exists() to look for the buckets.\nSo what’s in the bucket? Just like I did with the National Herbarium bucket, I can use the get_bucket_df() function to inspect the contents of my bucket:\n\nget_bucket_df(\"s3://tiny-herbs/\") %>% \n  as_tibble()\n\n\n\n# A tibble: 0 × 8\n# … with 8 variables: Key <chr>, LastModified <chr>, ETag <chr>, Size <chr>,\n#   Owner_ID <chr>, Owner_DisplayName <chr>, StorageClass <chr>, Bucket <chr>\n\n\nHm. Well, yes. Of course it’s empty: I haven’t put any objects in it yet. Maybe I should do that? It does seem like a good idea!\nBut first…\n\n\nManaging access control\nOne thing though… is this private or public? This is governed by the Access Control List (ACL) settings. By default, S3 buckets are set to private. You can read and write to them, but no-one else has any access at all. Let’s soften that slightly, and allow anyone to read from the “tiny-herbs” bucket. I could have done that from the beginning by setting acl = \"public-read\" when I called put_bucket(). However, because I “forgot” to do that earlier, I’ll change it now using put_acl()\n\nput_acl(\n  bucket = \"s3://tiny-herbs/\",\n  acl = \"public-read\"\n)\n\n[1] TRUE\n\n\nNow everyone has read access to the bucket.4\n\n\nAdding objects to your bucket\nTo put an object inside my new bucket, the function I need is put_object(). When calling it, I need to specify the local path to the file that I want to upload, the name that the object will be assigned when it is added to the bucket, and of course the bucket itself. This time around, I’ll also explicitly set acl = \"public-read\" to ensure that – while only I have write access – everyone has read access and can download the object if they want to. Because I’m going to call this repeatedly, I’ll wrap all this in a helper function called put_tiny_image():\n\nput_tiny_image <- function(file) {\n  put_object(\n    file = file.path(\"herbarium\", \"tiny_images\", file),\n    object = file, \n    bucket = \"s3://tiny-herbs/\",\n    acl = \"public-read\"\n  )\n}\n\nTo see this in action, let’s create a vector that lists the names of all the tiny images, and then apply the put_tiny_image() function to the first one:\n\ntiny_images <- list.files(\"herbarium/tiny_images\")\ntiny_images[1] %>% \n  put_tiny_image()\n\n[1] TRUE\n\n\nOkay that seems to work, so once again I’ll use purrr to iterate over all the tiny_images, uploading them one by one into my newly-created bucket:\n\ntiny_images %>% \n  map_lgl(put_tiny_image)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nThat looks pretty good! I’m seeing nothing but TRUE values in the output so it looks like I’ve successfully uploaded all the tiny images. Now that I’ve done this, I can try calling get_bucket_df() again to inspect the current contents of the bucket:\n\nget_bucket_df(\"s3://tiny-herbs/\") %>% \n  as_tibble()\n\n# A tibble: 23 × 8\n   Key            LastModified        ETag  Size  Owner…¹ Owner…² Stora…³ Bucket\n   <chr>          <chr>               <chr> <chr> <chr>   <chr>   <chr>   <chr> \n 1 NSW1055313.jpg 2022-08-23T03:05:1… \"\\\"c… 1494… b8b231… djnava… STANDA… tiny-…\n 2 NSW121207.jpg  2022-04-22T09:08:0… \"\\\"6… 1562… b8b231… djnava… STANDA… tiny-…\n 3 NSW214616.jpg  2022-04-22T09:08:0… \"\\\"f… 2041… b8b231… djnava… STANDA… tiny-…\n 4 NSW29246.jpg   2022-04-22T09:08:0… \"\\\"e… 1043… b8b231… djnava… STANDA… tiny-…\n 5 NSW306564.jpg  2022-04-22T09:08:0… \"\\\"d… 1819… b8b231… djnava… STANDA… tiny-…\n 6 NSW307215.jpg  2022-04-22T09:08:0… \"\\\"e… 1684… b8b231… djnava… STANDA… tiny-…\n 7 NSW36860.jpg   2022-04-22T09:08:1… \"\\\"f… 1962… b8b231… djnava… STANDA… tiny-…\n 8 NSW389387.jpg  2022-04-22T09:08:1… \"\\\"d… 1240… b8b231… djnava… STANDA… tiny-…\n 9 NSW395530.jpg  2022-04-22T09:08:1… \"\\\"3… 1435… b8b231… djnava… STANDA… tiny-…\n10 NSW39618.jpg   2022-04-22T09:08:1… \"\\\"a… 1028… b8b231… djnava… STANDA… tiny-…\n# … with 13 more rows, and abbreviated variable names ¹​Owner_ID,\n#   ²​Owner_DisplayName, ³​StorageClass\n\n\nYay! It’s done!\n\n\nURLs for objects in public buckets\nOne last thing. Because the \"tiny-herbs\" bucket is public, the objects it contains each have their own URL. To make my life a little easier, I wrote a helper function that constructs these URL:\n\ntiny_herb_url <- function(object, \n                          bucket = \"tiny-herbs\",\n                          region = \"ap-southeast-2\") {\n  paste0(\n    \"https://\", bucket, \".\", \"s3-\", \n    region, \".amazonaws.com\", \"/\", object\n  )\n}\n\nFor example, here’s one of the URLs associated with the \"tiny-herbs\" bucket:\n\ntiny_herb_url(\"NSW121207.jpg\")\n\n[1] \"https://tiny-herbs.s3-ap-southeast-2.amazonaws.com/NSW121207.jpg\"\n\n\nThe images I’ve been showing throughout this post aren’t the original ones from the National Herbarium data set. Rather, they’re the smaller files I stored in the \"tiny-herbs\" bucket, and the code I’ve been using to display the images throughout the post looks like this:\n\ntiny_herb_url(\"NSW121207.jpg\") %>% \n  knitr::include_graphics()"
  },
  {
    "objectID": "posts/2022-03-17_using-aws-s3-in-r/index.html#wrapping-up",
    "href": "posts/2022-03-17_using-aws-s3-in-r/index.html#wrapping-up",
    "title": "Using Amazon S3 with R",
    "section": "Wrapping up",
    "text": "Wrapping up\nAt the end of all this you might have all kinds of questions. Questions like, “Danielle, what’s wrong with you?” and “Danielle, is this even your job? Aren’t you supposed to be working on Apache Arrow?” While I could write an entire novel trying to answer the first one, I think I’ll skip over it and move straight onto the second on, because that’s more interesting and doesn’t require anyone to get a therapist.\nAlthough this isn’t a post about Apache Arrow – and so is not directly related to the work I do every day – of the reasons I found myself looking into S3 in the first place is that Arrow is a tool designed to let data scientists work with very large data sets, and S3 is a tool designed to make it easy to store very large data sets. These two things go well together, so much so that the arrow R package has its own support for S3 data storage, and many of the data sets that new arrow users encounter are stored on S3. From an educational perspective (sorry – I used to be an academic and I can’t help myself) it’s really difficult for people when they need to learn lots of things at the same time. Trying to learn how Arrow works is really hard when you’re still confused about S3. When I started learning Arrow I didn’t know anything about S3, and it was extremely frustrating to have to learn Arrow concepts with all this confusing S3 stuff floating around.\nHence… this post. My main goal here was to talk about S3 as a topic in its own right, and how tools like aws.s3 allow R users to write code that interacts with S3 data storage. But it’s very handy background knowledge to have if you’re planning to use arrow later on.\nOn top of all that, the aws.s3 package has a lot more functionality that I haven’t talked about here. You can use it to copy objects from one bucket to another, and delete objects and buckets that you control. You can use it to add tagging metadata, you can use it to configure your S3 bucket as a website (yes, even with all that painful cross-origin resource sharing configuration stuff), and a good deal more besides. It’s a really nice package and I’m glad I took the time to learn it!"
  },
  {
    "objectID": "posts/2022-12-22_queue/index.html",
    "href": "posts/2022-12-22_queue/index.html",
    "title": "Queue",
    "section": "",
    "text": "Okay. So I wrote a simple package for multi-threaded tasks queues in R this week. It wasn’t intentional, I swear. I was just trying to teach myself how to use the callr package,1 and making sure I had a solid grasp of encapsulated object-oriented programming with R6. Things got a little out of hand. Sorry.\nAnd let’s be very clear about something at the outset. If you want to do parallel computing in R correctly, you go look at futureverse.org. The future package by Henrik Bengtsson provides a fabulous way to execute R code asynchronously and in parallel. And there are many excellent packages built on top of that, so there’s a whole lovely ecosystem there just waiting for you.2 Relatedly, if the reason you’re thinking about parallel computing is that you’ve found yourself with a burning need to analyse terabytes of data with R then babe it might be time to start learning some R workflows using Spark, Arrow, Kubernetes. It may be time to learn about some of those other eldritch words of power that have figured rather more prominently in my life than one might expect for a simple country girl.3\nMy little queue package is a personal project. I happen to like it, but you should not be looking at it as an alternative to serious tools.\nThat’s been said now. Good. We can put aside all pretension."
  },
  {
    "objectID": "posts/2022-12-22_queue/index.html#what-does-it-do",
    "href": "posts/2022-12-22_queue/index.html#what-does-it-do",
    "title": "Queue",
    "section": "What does it do?",
    "text": "What does it do?\nLet’s say I have a generative art function called donut(), based loosely on a teaching example from my art from code workshop. The donut() function takes an input seed, creates a piece of generative art using ggplot2, and writes the output to an image file. This process takes several seconds to complete on my laptop:\n\nlibrary(tictoc)\ntic()\ndonut(seed = 100)\ntoc()\n\n5.277 sec elapsed\n\n\nHere’s the piece, by the way:\n\nThat’s nice and I do like this piece, but generative art is an iterative process and I like to make many pieces at once to help me get a feel for the statistical properties of the system. Waiting five or six seconds for one piece to render is one thing: waiting 8-10 minutes for 100 pieces to render is quite another. So it’s helpful if I can do this in parallel.\n\nlibrary(queue)\n\nHere’s how I might do that using queue. I designed the package using R6 classes – more on that later – so we’ll be working in the “encapsulated” object oriented programming style that is more common in other programming languages. The first step is to initialise a Queue object, specifying the number of workers we want to use. I’ll use six:\n\nqueue <- Queue$new(workers = 6)\n\nWhen I do this, the queue package starts six R sessions for us, and all my computations will be done in those R sessions. Under the hood, all the hard work of managing the R sessions is being done by the wonderful callr package by Gábor Csárdi4 – the only thing that queue does is provide a layer of abstraction and automation to the whole process.\nNext, I’ll add some tasks to the queue. Queue objects have an add() method that take a function and a list of arguments, so I can do this to push a task to the queue:\n\nqueue$add(donut, args = list(seed = 100))\n\nWhen the queue executes, it will be in a “first in, first out” order,5 so this task will be the first one to be assigned to a worker. Though of course that’s no guarantee that it will be the first one to finish!\nAnyway, let’s load up several more tasks. There’s some weird aversion out there to using loops in R, but this isn’t one of those situations where we need to worry about unnecessary copying, so I’m going to use a loop:\n\nfor(s in 101:108) queue$add(donut, list(seed = s))\n\nSo now we have nine tasks loaded onto a queue with six workers. To start it running I call the run() method for the queue. By default, all you’d see while the queue is running is a spinner with a progress message telling you how many tasks have completed so far, how many are currently running, and how many are still waiting. But I’ll ask it to be a bit more chatty. I’ll call it setting message = \"verbose\" so that we can see a log showing the order in which the tasks completed and time each task took to complete, in addition to the total time elapsed on my system while the queue was running:\n\nout <- queue$run(message = \"verbose\")\n\n→ Done: task_5 finished in 3.18 secs\n\n\n→ Done: task_2 finished in 5.78 secs\n\n\n→ Done: task_6 finished in 5.78 secs\n\n\n→ Done: task_4 finished in 7.34 secs\n\n\n→ Done: task_3 finished in 8.09 secs\n\n\n→ Done: task_1 finished in 9.46 secs\n\n\n→ Done: task_7 finished in 7.76 secs\n\n\n→ Done: task_9 finished in 6.09 secs\n\n\n→ Done: task_8 finished in 6.92 secs\n\n\n✔ Queue complete: 9 tasks done in 12.7 secs\n\n\nHere are the nine pieces that popped off the queue in 13 seconds:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo it’s a three-step process: (1) create the queue, (2) load up the tasks, (3) execute the tasks. In practice I would probably simplify the code to this:\n\nqueue <- Queue$new(workers = 6)\nfor(s in 100:108) queue$add(donut, list(seed = s))\nout <- queue$run()\n\nTrue, I could simplify it further. For example, if I know that I’m always calling the same function and always passing the same the same arguments – just with different values – this could be wrapped up in purrr style syntax, but honestly I’m not sure why I would bother doing that when furrr already exists? I’m not planning to reinvent the wheel, especially not when Davis Vaughn already offers a fully-operational mass-transit system free of charge."
  },
  {
    "objectID": "posts/2022-12-22_queue/index.html#what-does-it-store",
    "href": "posts/2022-12-22_queue/index.html#what-does-it-store",
    "title": "Queue",
    "section": "What does it store?",
    "text": "What does it store?\nThe output object out stores quite a lot of information about the tasks, the results, and the events that occurred during task execution, but most of it isn’t immediately interesting to us (especially when things actually work!) So let’s keep things simple for the moment and just look at the first five columns:\n\nout[, 1:5]\n\n  task_id worker_id state        result       runtime\n1  task_1    577202  done donut_100.png 9.457713 secs\n2  task_2    577226  done donut_101.png 5.782191 secs\n3  task_3    577239  done donut_102.png 8.087054 secs\n4  task_4    577251  done donut_103.png 7.336802 secs\n5  task_5    577263  done donut_104.png 3.183247 secs\n6  task_6    577275  done donut_105.png 5.780861 secs\n7  task_7    577263  done donut_106.png 7.763061 secs\n8  task_8    577226  done donut_107.png 6.921016 secs\n9  task_9    577275  done donut_108.png 6.093996 secs\n\n\nThe columns are pretty self-explanatory I think?\n\ntask_id is a unique identifier for the task itself\nworker_id is a unique identifier for the worker that completed the task (it’s also the process id for the R session)\nstate summarises the current state of the task (they’re all \"done\" because the queue is finished)\nresult is a list column containing the output from each task\nruntime is a difftime column telling you how long each task took to finish\n\nAs for the the full output… well… here it is…\n\nout\n\n  task_id worker_id state        result       runtime                                                                          fun args\n1  task_1    577202  done donut_100.png 9.457713 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  100\n2  task_2    577226  done donut_101.png 5.782191 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  101\n3  task_3    577239  done donut_102.png 8.087054 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  102\n4  task_4    577251  done donut_103.png 7.336802 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  103\n5  task_5    577263  done donut_104.png 3.183247 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  104\n6  task_6    577275  done donut_105.png 5.780861 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  105\n7  task_7    577263  done donut_106.png 7.763061 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  106\n8  task_8    577226  done donut_107.png 6.921016 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  107\n9  task_9    577275  done donut_108.png 6.093996 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  108\n              created              queued            assigned             started            finished code\n1 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:28:03  200\n2 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:28:00  200\n3 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:28:02  200\n4 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:28:01  200\n5 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:57  200\n6 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:28:00  200\n7 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:57 2022-12-23 12:27:57 2022-12-23 12:28:05  200\n8 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:28:00 2022-12-23 12:28:00 2022-12-23 12:28:06  200\n9 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:28:00 2022-12-23 12:28:00 2022-12-23 12:28:06  200\n                             message stdout stderr error\n1 done callr-rs-result-8ce3e320ef539                NULL\n2 done callr-rs-result-8ce3e6e032153                NULL\n3 done callr-rs-result-8ce3e5d32c7ba                NULL\n4  done callr-rs-result-8ce3e72346af                NULL\n5 done callr-rs-result-8ce3e4193129d                NULL\n6 done callr-rs-result-8ce3e42c3653c                NULL\n7 done callr-rs-result-8ce3e6ecedbfa                NULL\n8 done callr-rs-result-8ce3e41e0f9e6                NULL\n9 done callr-rs-result-8ce3e5a3c4630                NULL\n\n\nOkay so there’s a bit more to unpack here. Let’s take a look…\n\nThe fun and args columns contain the functions and arguments that were originally used to specify the task\nThe created, queued, assigned, started, and finished columns contain POSIXct timestamps indicating when the task was created, added to a queue, assigned to a worker, started running on a worker, and returned from the worker\ncode is a numeric code returned by the callr R session: of particular note 200 means it returned successfully, 500 means the session exited cleanly, and 501 means the session crashed\nmessage is a message returned by callr\nstdout and stderr are the contents of the output and error streams from the worker session while the task was running\nerror currently is NULL because I haven’t implemented that bit yet lol."
  },
  {
    "objectID": "posts/2022-12-22_queue/index.html#surviving-a-crash",
    "href": "posts/2022-12-22_queue/index.html#surviving-a-crash",
    "title": "Queue",
    "section": "Surviving a crash",
    "text": "Surviving a crash\nI’m going to be honest. Sometimes6 I write bad code when I am exploring a new generative art system. Code that crashes the R session unpredictably. So it would be nice if the queue had a little bit of robustness for that. To be honest, the queue package isn’t very sophisticated in detecting sessions that have crashed,7 but it does have some ability to recover when a task crashes its thread. Let’s keep this simple. I’ll define a perfectly safe function that waits for a moment and then returns, and another function that always crashes the R session as soon as it is called:\n\nwait <- function(x) {\n  Sys.sleep(x)\n  x\n}\ncrash <- function(x) .Call(\"abort\")\n\nNow let’s define a queue that has only two workers, but has no less than three tasks that are guaranteed to crash the worker the moment the tasks are started:\n\nqueue <- Queue$new(workers = 2)\nqueue$add(wait, list(x = .1))\nqueue$add(crash)\nqueue$add(crash)\nqueue$add(crash)\nqueue$add(wait, list(x = .1))\n\nThe queue allocates task in a first-in first-out order, so the three “crash tasks” are guaranteed to be allocated before the final “wait task”. Let’s take a look at what happens when the queue runs:\n\nqueue$run()\n\n✔ Queue complete: 5 tasks done in 3.34 secs\n\n\n# A tibble: 5 × 17\n  task_id worker_id state result    runtime        fun    args            \n  <chr>       <int> <chr> <list>    <drtn>         <list> <list>          \n1 task_1     624437 done  <dbl [1]> 0.1263957 secs <fn>   <named list [1]>\n2 task_2     624445 done  <NULL>    1.1492345 secs <fn>   <list [0]>      \n3 task_3     624437 done  <NULL>    2.0157561 secs <fn>   <list [0]>      \n4 task_4     624467 done  <NULL>    1.9129426 secs <fn>   <list [0]>      \n5 task_5     624484 done  <dbl [1]> 0.1606448 secs <fn>   <named list [1]>\n# … with 10 more variables: created <dttm>, queued <dttm>, assigned <dttm>,\n#   started <dttm>, finished <dttm>, code <dbl>, message <chr>,\n#   stdout <list>, stderr <list>, error <list>\n\n\nIt’s a little slower than we’d hope, but it does finish both valid tasks and returns nothing for the tasks that crashed their R sessions. What has happened in the background is that the queue runs a simple check to see if any of the R sessions have crashed, and attempts to replace them with a new worker whenever it detects that this has happened. It’s not in any sense optimised, but it does sort of work."
  },
  {
    "objectID": "posts/2022-12-22_queue/index.html#design",
    "href": "posts/2022-12-22_queue/index.html#design",
    "title": "Queue",
    "section": "Design",
    "text": "Design\nAlthough my confidence in my ability to have a career in tech is at an all-time low, I have to admit that the work I’ve done over the last year has made me a better programmer. I didn’t much effort into writing queue, but the code feels cleaner and more modular than the code I was writing a year ago. Good practices have become habits, I suppose. That’s a nice feeling. I automatically write proper unit tests as I go, knowing that those tests will save me when I need to make changes later. I document properly as I go, knowing that I won’t remember a bloody thing about how my own code works six hours later – never mind six months. And, maybe most importantly of all, my code now seems to have this habit of organising itself into small, manageable abstractions. I have no idea when that happened, because I wasn’t actually part of a software engineering team. I was just the girl who wrote some docs and few little blog posts.8\nHere’s what I mean. If you take a look at the source code for the Queue object, it’s actually not very long: the file is mostly devoted to the documentation, and the object doesn’t have very many methods. Honestly, we’ve already seen most of them:\n\nnew() creates a new queue\nadd() adds a task to a queue\nrun() sets the queue running\n\nIf everything works smoothly you don’t need anything else, so why burden the user with extra details? Sure, there’s a little complexity to these methods which is of course documented on the relevant pkgdown page because I’m not a jerk, but this isn’t a complicated package…\n…when it’s working.\nOf course, when things start to break, you start to care a lot more about the internals. Fair enough. There are two important data structures within the Queue:\n\nInternally, a Queue manages a WorkerPool comprised of one or more Worker objects. As you’d expect given the names, these provide abstractions for managing the R sessions. A Worker object provides a wrapper around a callr R session, and tools that automate the interaction between that session and a task.\nThe Queue also holds a TaskList comprised of one or more Task objects. Again, as you might expect from the names, these are the storage classes. A Task object is a container that holds a function, its arguments, any results it might have returned, and any logged information about the process of its execution.\n\nIn some situations it can be awfully handy to have access to these constituent data structures, particularly because those objects expose additional tools that I deliberately chose not to make available at the Queue level. From the Queue itself what you can do is return the objects:\n\nworkers <- queue$get_workers()\ntasks <- queue$get_tasks()\n\nThese objects are R6 classes: they have reference semantics so anything I do with workers and tasks will have corresponding effects on queue. For this blog post I don’t intend to dive into details of what I did when designing the WorkerPool and TaskList classes – especially because queue is only at version 0.0.2 and I don’t yet know what I’m going to do with this cute little package – but I’ll give one example.\nLet’s take the workers. By default, a Queue cleans up after itself and closes any R sessions that it started. The WorkerPool object associated with a Queue has a get_pool_state() method that I can use to check the state of the workers, and some other methods to modify the workers if I so choose. Let’s have a go. I ask workers to report on the status of the R sessions, this is what I get:\n\nworkers$get_pool_state()\n\n    624484     624496 \n\"finished\" \"finished\" \n\n\nYes, as expected the workers have stopped. But I can replace them with live R sessions by calling the refill_pool() method:\n\nworkers$refill_pool()\n\n624512 624524 \n\"idle\" \"idle\" \n\n\nAnd I can shut them down again by calling shutdown_pool():\n\nworkers$shutdown_pool()\n\n    624512     624524 \n\"finished\" \"finished\" \n\n\nAlong similar lines the TaskList object has some methods that let me manipulate the data storage associated with my Queue. Normally I don’t need to. Sometimes I do. It’s handy to have those tools lying around. At the moment the toolkit feels a little light, but the nice thing about writing your own package is that I can always add more if I need them :-)"
  },
  {
    "objectID": "posts/2022-12-22_queue/index.html#epilogue",
    "href": "posts/2022-12-22_queue/index.html#epilogue",
    "title": "Queue",
    "section": "Epilogue",
    "text": "Epilogue\n\nqueue <- Queue$new(workers = 6)\nqueue$add(wait, list(x = 1.3), id = toupper(\"multithreading\"))\nqueue$add(wait, list(x = 0.1), id = toupper(\"it is\"))\nqueue$add(wait, list(x = 0.7), id = toupper(\"acknowledged\"))\nqueue$add(wait, list(x = 1.0), id = toupper(\"post\"))\nqueue$add(wait, list(x = 0.5), id = toupper(\"universally\"))\nqueue$add(wait, list(x = 0.1), id = toupper(\"a truth\"))\nqueue$add(wait, list(x = 1.2), id = toupper(\"must be\"))\nqueue$add(wait, list(x = 0.9), id = toupper(\"about\"))\nqueue$add(wait, list(x = 1.6), id = toupper(\"trick\"))\nqueue$add(wait, list(x = 0.1), id = toupper(\"that a\"))\nqueue$add(wait, list(x = 0.5), id = toupper(\"in want of\"))\nqueue$add(wait, list(x = 1.0), id = toupper(\"an async\"))\nout <- queue$run(message = \"verbose\")\n\n→ Done: IT IS finished in 0.172 secs\n\n\n→ Done: A TRUTH finished in 0.169 secs\n\n\n→ Done: UNIVERSALLY finished in 0.536 secs\n\n\n→ Done: ACKNOWLEDGED finished in 0.776 secs\n\n\n→ Done: THAT A finished in 0.173 secs\n\n\n→ Done: POST finished in 1.07 secs\n\n\n→ Done: ABOUT finished in 0.957 secs\n\n\n→ Done: MULTITHREADING finished in 1.37 secs\n\n\n→ Done: MUST BE finished in 1.25 secs\n\n\n→ Done: IN WANT OF finished in 0.582 secs\n\n\n→ Done: AN ASYNC finished in 1.06 secs\n\n\n→ Done: TRICK finished in 1.66 secs\n\n\n✔ Queue complete: 12 tasks done in 2.21 secs"
  },
  {
    "objectID": "posts/2022-08-23_visualising-a-billion-rows/index.html",
    "href": "posts/2022-08-23_visualising-a-billion-rows/index.html",
    "title": "How to visualise a billion rows of data in R with Apache Arrow",
    "section": "",
    "text": "It’s been a couple of months since I published anything on this blog. In my defence, I’ve been busy: I spent the month of June developing a workshop and website on larger than memory workflows in R with Apache Arrow for the useR! conference, and I spent July doing the same thing for my art from code workshop at rstudio::conf. But I am back to blogging now and I’m going to ease myself into it with a post that mixes some ideas from both of those workshops: how to use Arrow to assist in visualising large data sets. Specifically, I’m going to construct a map showing the geographic distribution of pickup locations for a billion or so taxi rides in New York.1"
  },
  {
    "objectID": "posts/2022-08-23_visualising-a-billion-rows/index.html#the-nyc-taxi-data",
    "href": "posts/2022-08-23_visualising-a-billion-rows/index.html#the-nyc-taxi-data",
    "title": "How to visualise a billion rows of data in R with Apache Arrow",
    "section": "The NYC taxi data",
    "text": "The NYC taxi data\nAt this point in my life I have used the “NYC Taxi Data” for so many illustrative examples I feel like I don’t need to explain it: doesn’t “everyone” know about this data by now? Yeah, no dice sweetie. That’s a terrible intuition. Most people don’t know the data, and those that do can just skip to the next section! :-)\nHere’s a quick summary of the data set. In its full form, the data set takes the form of one very large table with about 1.7 billion rows and 24 columns. Each row corresponds to a single taxi ride sometime between 2009 and 2022. There’s a complete data dictionary for the NYC taxi data on the useR workshop site, but the columns that will be relevant for us are as follows:\n\npickup_longitude (double): Longitude data for the pickup location\npickup_latitude (double): Latitude data for the pickup location\ndropoff_longitude (double): Longitude data for the dropoff location\ndropoff_latitude (double): Latitude data for the dropoff location\n\nOn my laptop I have a copy of both the full data set, located at \"~/Datasets/nyc-taxi\" on my machine, and a much smaller “tiny” data set that contains 1 out of every 1000 records from the original, located at \"~/Datasets/nyc-taxi-tiny/\". This tiny version has a mere 1.7 million rows of data, and as such is small enough that it will fit in memory. Instructions for downloading both data sets are available at the same location as the data dictionary."
  },
  {
    "objectID": "posts/2022-08-23_visualising-a-billion-rows/index.html#loading-the-data",
    "href": "posts/2022-08-23_visualising-a-billion-rows/index.html#loading-the-data",
    "title": "How to visualise a billion rows of data in R with Apache Arrow",
    "section": "Loading the data",
    "text": "Loading the data\nSince I have local copies of the data, I’ll use the open_dataset() function from the {arrow} package to connect to both versions of the NYC taxi data:2\n\nnyc_taxi <- open_dataset(\"~/Datasets/nyc-taxi/\")\nnyc_taxi_tiny <- open_dataset(\"~/Datasets/nyc-taxi-tiny/\")\n\nStarting with Arrow 9.0.0 it’s been possible to use the {dplyr} glimpse() function to take a look at the data sets, so let’s do that:\n\nglimpse(nyc_taxi)\n\nFileSystemDataset with 158 Parquet files\n1,672,590,319 rows x 24 columns\n$ vendor_name             <string> \"VTS\", \"VTS\", \"VTS\", \"DDS\", \"DDS\", \"DDS\", \"DD…\n$ pickup_datetime  <timestamp[ms]> 2009-01-04 13:52:00, 2009-01-04 14:31:00, 200…\n$ dropoff_datetime <timestamp[ms]> 2009-01-04 14:02:00, 2009-01-04 14:38:00, 200…\n$ passenger_count          <int64> 1, 3, 5, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, …\n$ trip_distance           <double> 2.63, 4.55, 10.35, 5.00, 0.40, 1.20, 0.40, 1.…\n$ pickup_longitude        <double> -73.99196, -73.98210, -74.00259, -73.97427, -…\n$ pickup_latitude         <double> 40.72157, 40.73629, 40.73975, 40.79095, 40.71…\n$ rate_code               <string> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ store_and_fwd           <string> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ dropoff_longitude       <double> -73.99380, -73.95585, -73.86998, -73.99656, -…\n$ dropoff_latitude        <double> 40.69592, 40.76803, 40.77023, 40.73185, 40.72…\n$ payment_type            <string> \"Cash\", \"Credit card\", \"Credit card\", \"Credit…\n$ fare_amount             <double> 8.9, 12.1, 23.7, 14.9, 3.7, 6.1, 5.7, 6.1, 8.…\n$ extra                   <double> 0.5, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, …\n$ mta_tax                 <double> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ tip_amount              <double> 0.00, 2.00, 4.74, 3.05, 0.00, 0.00, 1.00, 0.0…\n$ tolls_amount            <double> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_amount            <double> 9.40, 14.60, 28.44, 18.45, 3.70, 6.60, 6.70, …\n$ improvement_surcharge   <double> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ congestion_surcharge    <double> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ pickup_location_id       <int64> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ dropoff_location_id      <int64> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ year                     <int32> 2009, 2009, 2009, 2009, 2009, 2009, 2009, 200…\n$ month                    <int32> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\n\nIf you’ve used glimpse() before this output will look very familiar. Each line in the output show the name of one column in the data, followed by the first few entries in that column.3 However, when you look at the size of the data set, you might begin to suspect that some magic is going on. Behind the scenes there are 1.7 billion rows of data in one huge table, and this is just too big to load into memory. Fortunately, the {arrow} package allows us to work with it anyway!"
  },
  {
    "objectID": "posts/2022-08-23_visualising-a-billion-rows/index.html#plotting-a-million-rows",
    "href": "posts/2022-08-23_visualising-a-billion-rows/index.html#plotting-a-million-rows",
    "title": "How to visualise a billion rows of data in R with Apache Arrow",
    "section": "Plotting a million rows",
    "text": "Plotting a million rows\nOkay, let’s start with a data visualisation problem that wouldn’t be too difficult to manage on a small data set. I want to draw an image that plots the pickup location for every taxi ride in the data set. Here’s how I might go about that. First, I’ll do a minimal amount of data wrangling in {arrow}. Specifically, I’ll use the {dplyr} select() and filter() functions to limit the amount of data I have to collect() into R:\n\ntic()\nnyc_pickups <- nyc_taxi_tiny |>\n  select(pickup_longitude, pickup_latitude) |>\n  filter(\n    !is.na(pickup_longitude),\n    !is.na(pickup_latitude)\n  ) |>\n  collect()\ntoc()\n\n0.16 sec elapsed\n\n\nAt this point I have a regular R data frame, nyc_pickups, that contains only the data I need: the pickup locations for all those taxi rides (in the tiny taxi data set) that actually contain longitude and latitude data. Let’s use glimpse() again:\n\nglimpse(nyc_pickups)\n\nRows: 1,249,107\nColumns: 2\n$ pickup_longitude <dbl> -73.95557, -73.97467, -73.78190, -73.97872, -73.97400…\n$ pickup_latitude  <dbl> 40.76416, 40.76222, 40.64478, 40.75371, 40.77901, 0.0…\n\n\nCompared to the full NYC taxi data, this is a relatively small data set. Drawing a scatter plot from 1.2 million observations isn’t a trivial task, to be sure, but it is achievable. In fact the {ggplot2} package handles this task surprisingly well:\n\nx0 <- -74.05 # minimum longitude to plot\ny0 <- 40.6   # minimum latitude to plot\nspan <- 0.3  # size of the lat/long window to plot\n\ntic()\npic <- ggplot(nyc_pickups) +\n  geom_point(\n    aes(pickup_longitude, pickup_latitude), \n    size = .2, \n    stroke = 0, \n    colour = \"#800020\"\n  ) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_void() +\n  coord_equal(\n    xlim = x0 + c(0, span), \n    ylim = y0 + c(0, span)\n  )\npic\n\n\n\ntoc()\n\n3.365 sec elapsed\n\n\nIt’s not lightning fast or anything, but it’s still pretty quick!\nAs neat as this visualisation is there are limitations.4 In some parts of the plot – notably midtown in Manhattan – the data are so dense that you can’t make out any fine detail. In other parts – Brooklyn and Queens, for instance – there are so few data points that you can’t see much at all:\n\n\n\n\n\nHow do we improve this image?"
  },
  {
    "objectID": "posts/2022-08-23_visualising-a-billion-rows/index.html#scaling-to-a-billion-rows",
    "href": "posts/2022-08-23_visualising-a-billion-rows/index.html#scaling-to-a-billion-rows",
    "title": "How to visualise a billion rows of data in R with Apache Arrow",
    "section": "Scaling to a billion rows",
    "text": "Scaling to a billion rows\nTo make a better version of this plot, we’re going to have to do two things at once:\n\nUse a lot more data. If we use the full NYC taxi data set, the visualisation will be a lot more detailed in areas where it is currently too sparse.\nShow gradation at each location. In the dense areas there are too many points plotted atop one another. Instead of overplotting, we’ll use shading to represent the number of pickups at each location.\n\nHow do we do this? Let’s say I want to create a 4000 x 4000 pixel image, and I want the “intensity” at each pixel to represent the number of pickups that fall in the geographic region spanned by that pixel. There are a total of 16 million pixels, so our task is to assign each of observation one of those those 16 million bins, and then count the number of observations in each bin. We’ll have to rely on Arrow to do all the heavy lifting here. This binning cannot be done natively in R: the data set is just too big. Even after filtering out missing and out-of-bounds data points, there are still 1.2 billion rows, and R can’t do that without assistance.\nHere’s what the solution looks like:\n\ntic()\npixels <- 4000\npickup <- nyc_taxi |>\n  filter(\n    !is.na(pickup_longitude),\n    !is.na(pickup_latitude),\n    pickup_longitude > x0,\n    pickup_longitude < x0 + span,\n    pickup_latitude > y0,\n    pickup_latitude < y0 + span\n  ) |>\n  mutate(\n    unit_scaled_x = (pickup_longitude - x0) / span,\n    unit_scaled_y = (pickup_latitude - y0) / span,\n    x = as.integer(round(pixels * unit_scaled_x)), \n    y = as.integer(round(pixels * unit_scaled_y))\n  ) |>\n  count(x, y, name = \"pickup\") |>\n  collect()\ntoc()\n\n31.101 sec elapsed\n\n\nMy laptop solves this binning problem in about 30 seconds. As before, I’ll use glimpse() to take a peek at the results:\n\nglimpse(pickup)\n\nRows: 4,677,864\nColumns: 3\n$ x      <int> 1058, 1024, 1162, 3525, 865, 794, 856, 705, 647, 762, 802, 1207…\n$ y      <int> 2189, 2040, 2265, 552, 1983, 1646, 2018, 1590, 1723, 2010, 1645…\n$ pickup <int> 6514, 5030, 3818, 67, 2408, 2415, 932, 3607, 2664, 1024, 2207, …\n\n\nThis is a data frame where x and y specify the pixel, and and a pickup counts the number of pickups associated with that pixel. Note that the pixels aren’t arranged in a meaningful order, and only those pixels with at least one pickup (a little under 30% of all pixels) are included in data.\nWe can visualise this in a number of ways. One possibility is to create a scatter plot, using the pickup value to specify the shading of each plot marker:\n\ntic()\nggplot(pickup) +\n  geom_point(\n    aes(x, y, colour = log10(pickup)), \n    size = .01, \n    stroke = 0, \n    show.legend = FALSE\n  ) +\n  scale_colour_gradient(low = \"white\", high = \"#800020\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_void() +\n  coord_equal()\n\n\n\ntoc()\n\n12.159 sec elapsed\n\n\nAs you can see, {ggplot2} has no problems drawing a scatter plot from a few million observations, and it’s an improvement on our first attempt. However, we can do better. Instead of trying to draw a scatter plot of all the points listed in the pickup data frame, let’s use it to populate a bitmap. We’ll create a 4000x4000 matrix, and fill in the cells with the pickup counts at the corresponding pixel.\nThe computation is a two part process. First, we use expand_grid() to initialise a “grid like” tibble containing all combination of x and y values, and use left_join() to populate a column containing the pickup counts:\n\ntic()\ngrid <- expand_grid(x = 1:pixels, y = 1:pixels) |>\n  left_join(pickup, by = c(\"x\", \"y\")) |>\n  mutate(pickup = replace_na(pickup,  0))\ntoc()\n\n8.228 sec elapsed\n\n\nNote that the elements of grid are complete (all 16 million pixels are there), and meaningfully ordered. We can check this by calling glimpse() again:\n\nglimpse(grid)\n\nRows: 16,000,000\nColumns: 3\n$ x      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ y      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, …\n$ pickup <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\nBecause the elements of grid$pickup are arranged in this fashion, it is easy to construct the required 4000x4000 matrix:\n\ntic()\npickup_grid <- matrix(\n  data = grid$pickup,\n  nrow = pixels,\n  ncol = pixels\n)\ntoc()\n\n0.02 sec elapsed\n\n\nThis is our bitmap. It’s a matrix whose values correspond to the pixel intensities to be plotted. Just so you can see what it looks like, here’s a tiny 10x10 pixel section from that matrix:\n\npickup_grid[2000:2009, 2000:2009]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]   11    2    2    4    6    3    7   15   54    96\n [2,]    5    3    3    1    5   27   47   55   74   100\n [3,]    5    6    7   38   39   48   60   99   95    75\n [4,]   16   37   51   45   35   61   64   67   51    18\n [5,]   67   50   97  141   55   24   26   26   40    29\n [6,]   65  133   56   18   11   10  659    6    4     9\n [7,]   35   78   13    3   82  105   68    2    2     4\n [8,]    7    7    4    3    7   25    4    2    2     3\n [9,]    8   10    3    3   17    5   98    2    4     3\n[10,]    8    6    8    2   19    6    1    2    3    23\n\n\nNow that the data are in an image-like format, all we have to do is write the image file. We don’t even need {ggplot2}: we can use image() to draw the bitmap directly. Here’s a little helper function I wrote to do this:\n\nrender_image <- function(mat, cols = c(\"white\", \"#800020\")) {\n  op <- par(mar = c(0, 0, 0, 0))\n  shades <- colorRampPalette(cols)\n  image(\n    z = log10(t(mat + 1)),\n    axes = FALSE,\n    asp = 1,\n    col = shades(256),\n    useRaster = TRUE\n  )\n  par(op)\n}\n\nHere’s what happens when I call it:\n\ntic()\nrender_image(pickup_grid)\n\n\n\ntoc()\n\n2.149 sec elapsed\n\n\nThis method is slightly faster the previous version, but the real advantage isn’t speed – it’s clarity. There’s less blurring in the denser parts of the plot (midtown Manhattan), and there’s also more clarity in the sparser areas (e.g., the Brooklyn streets are sharper).\nWe can push it slightly further by tweaking the colour palette. Plotting the logarithm of the number of pickups ensures that all the streets are visible (not just the extremely common ones), but it does have the downside that it’s hard to tell the difference between moderately popular pickup locations and extremely popular ones. A well-chosen diverging palette helps rectify this a little:\n\nrender_image(pickup_grid, cols = c(\"#002222\", \"white\", \"#800020\"))\n\n\n\n\nAt long last we have a visualisation that shows all the billion rows of data, crisply delineates all the streets on which taxi pickups are at least moderately frequent, and does a reasonable job of highlighting those locations where taxi pickups are extremely common. Yay! 🎉"
  },
  {
    "objectID": "posts/2022-08-23_visualising-a-billion-rows/index.html#lessons-learned",
    "href": "posts/2022-08-23_visualising-a-billion-rows/index.html#lessons-learned",
    "title": "How to visualise a billion rows of data in R with Apache Arrow",
    "section": "Lessons learned?",
    "text": "Lessons learned?\nTo wrap this post up, I think it’s useful to reflect on the process I went through in constructing this image. In one sense, the process I’ve gone through here isn’t actually much different to what we do when creating any other data visualisation in R. For example, if you’re working in {tidyverse}, a typical work flow is to use {dplyr} to wrangle the data into an appropriate format and then use {ggplot2} to plot the data. What I’ve done here isn’t that different: okay yes, my {dplyr} code only works because it’s backed by the {arrow} engine, and in the end I decided to use base graphics rather than {ggplot2} to draw the final image, but I don’t think those differences constitute a major departure from my usual approach.\nThat being said, I think there are two key principles I’ve taken away from this. When trying to visualise very large data sets in R, the things I’m going to try to keep in mind are:\n\nPush as much of the computational work onto {arrow} as possible. The {arrow} package is designed specifically to handle these kinds of data manipulation problems, and things go much more smoothly when I don’t make {ggplot2} do the computational heavy lifting.\nThink carefully about the data representation. The reason why the final plot drawn with image() is nicer than the earlier ones drawn with {ggplot2} has nothing at all to do with the “base R vs tidyverse” issue. Instead, it’s because the data structure I created (i.e., pickup_grid) is the exact bitmap that needed to be rendered, and that’s exactly what image() is good for."
  },
  {
    "objectID": "posts/2022-08-23_visualising-a-billion-rows/index.html#acknowledgments",
    "href": "posts/2022-08-23_visualising-a-billion-rows/index.html#acknowledgments",
    "title": "How to visualise a billion rows of data in R with Apache Arrow",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThank you to Kae Suarez, Keith Britt, and François Michonneau for reviewing this post."
  },
  {
    "objectID": "posts/2021-04-05_welcome/index.html",
    "href": "posts/2021-04-05_welcome/index.html",
    "title": "Welcome to the jungle",
    "section": "",
    "text": "I’ve decided the time has come to restart my blog. I’ve tried blogging many times before with mixed success, and this time around I’d like to avoid the mistakes of the past. I’ve set up this blog with a few principles in mind:\nAt this stage I’m not entirely certain how I’ll use the blog. There are a lot of possibilities, and I have some thoughts on which ones I’d like to explore. A self-contained blog such as this seems nicely suited to teaching materials. An obvious example would be to write blog posts to accompany the data science slides and youtube videos I’ve been making. The lack of written material to go with those talks has bothered me for some time. Another possibility might be to write tutorials on generative art. I use my art website to post the art itself, but the site functions as a gallery rather than a classroom. I get a lot of people asking questions about how I make my art, and this blog might be a good place to provide answers. Those aren’t the only possibilities, of course, but they are appealing ones.\nNot sure how this will go, but fingers crossed!"
  },
  {
    "objectID": "posts/2021-04-05_welcome/index.html#last-updated",
    "href": "posts/2021-04-05_welcome/index.html#last-updated",
    "title": "Welcome to the jungle",
    "section": "Last updated",
    "text": "Last updated\n\n2022-08-23 13:12:44 AEST"
  },
  {
    "objectID": "posts/2021-04-05_welcome/index.html#details",
    "href": "posts/2021-04-05_welcome/index.html#details",
    "title": "Welcome to the jungle",
    "section": "Details",
    "text": "Details\n\nsource code, R environment"
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html",
    "title": "Generative art with grid",
    "section": "",
    "text": "As I approach four months of unemployment I’m finding I need projects to work on purely for the sake of my mental health. One project that has helped a lot is working on the ggplot2 book (which I coauthor with Hadley Wickham and Thomas Lin Pedersen). At the moment I’m working on the book chapters that discuss the ggplot2 extension system: it’s been quite a lot of fun. One really nice thing about working on those chapters is that I’ve ended up learning a lot about the grid graphics system upon which ggplot2 is built.1\nAt this point we’re really not sure how much grid to incorporate into the book, but as a fun side-project I decided to adapt some of it and use it as the basis for a post on generative art."
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#grobs",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#grobs",
    "title": "Generative art with grid",
    "section": "Grobs",
    "text": "Grobs\nTo understand how grid works, the first thing we need to talk about are grobs. Grobs (graphic objects) are the atomic representations of graphical elements in grid, and include types like points, lines, circles, rectangles, and text. The grid package provides functions like pointsGrob(), linesGrob(), circleGrob(), rectGrob(), and textGrob() that create graphical objects without drawing anything to the graphics device.2 These functions are vectorised, allowing a single point grob to represent multiple points, for instance:\n\nlibrary(grid)\n\nset.seed(1)\nn <- 8\nx <- runif(n, min = .3, max = .7) # x coordinate\ny <- runif(n, min = .3, max = .7) # y coordinate\nr <- runif(n, min = 0, max = .15) # radius\n\ncircles <- circleGrob(x = x, y = y, r = r)\n\nNotice that this does not create any output. Much like a ggplot2 plot object, this grob is a declarative description of a set of circles. To trigger a drawing operation we first call grid.newpage() to clear the current graphics device, and then grid.draw() to perform a draw operation:\n\ngrid.newpage()\ngrid.draw(circles)\n\n\n\n\nIn addition to providing geometric primitives, grid also allows you to construct composite objects that combine multiple grobs using grobTree(). Here’s an illustration:\n\nsquares <- rectGrob(x = x, y = y, width = r * 2.5, height =  r * 2.5)\ncomposite <- grobTree(squares, circles)\ngrid.newpage()\ngrid.draw(composite)\n\n\n\n\nIt is also possible to define your own grob classes. You can define a new primitive grob class using grob() or a new composite class using gTree(), and specify special behaviour for your new class. We’ll see an example of this in a moment."
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#viewports",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#viewports",
    "title": "Generative art with grid",
    "section": "Viewports",
    "text": "Viewports\nThe second key concept in grid is the idea of a viewport. A viewport is a rectangular plotting region that supplies its own coordinate system for grobs that are drawn within it, and can also provide a tabular grid in which other viewports an be nested. An individual grob can have its own viewport or, if none is provided, it will inherit one. In the example below I’ll use viewport() to define three different viewports, one with default parameters, and two more that are rotated around the midpoint by 15 and 30 degrees respectively:\n\nvp_default <- viewport()\nvp_rotate1 <- viewport(angle = 15)\nvp_rotate2 <- viewport(angle = 30)\n\nThis time around, when we create our composite grobs, we’ll explicitly assign them to specific viewports by setting the vp argument:\n\ncomposite_default <- grobTree(squares, circles, vp = vp_default)\ncomposite_rotate1 <- grobTree(squares, circles, vp = vp_rotate1)\ncomposite_rotate2 <- grobTree(squares, circles, vp = vp_rotate2)\n\nWhen we plot these two grobs, we can see the effect of the viewport: although composite_default and composite_rotated are comprised of the same two primitive grobs (i.e., circles and squares), they belong to different viewports so they look different when the plot is drawn:\n\ngrid.newpage()\ngrid.draw(composite_default)\ngrid.draw(composite_rotate1)\ngrid.draw(composite_rotate2)"
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#graphical-parameters",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#graphical-parameters",
    "title": "Generative art with grid",
    "section": "Graphical parameters",
    "text": "Graphical parameters\nThe next concept we need to understand is the idea of graphical parameters. When we defined the circles and labels grobs, we only specified some of its properties. For example, we said nothing about colour or transparency, and so these properties are all set to their default values. The gpar() function in grid allows you to specify graphical parameters as distinct objects:\n\ngpA <- gpar(fill = \"grey30\", col = \"white\", lwd = 20)\ngpB <- gpar(fill = \"white\", col = \"grey30\", lwd = 20)\n\nThe gpA and gpB objects provide lists of graphical settings that can now be applied to any grob we like using the gp argument:\n\nset.seed(1)\nn <- 5\ncircles <- circleGrob(\n  x = runif(n, min = .2, max = .8),\n  y = runif(n, min = .2, max = .9),\n  r = runif(n, min = 0, max = .15)\n)\n\ngrob1 <- grobTree(circles, vp = vp_default, gp = gpA)\ngrob2 <- grobTree(circles, vp = vp_rotate1, gp = gpB)\ngrob3 <- grobTree(circles, vp = vp_rotate2, gp = gpA)\n\nWhen we plot these two grobs, they inherit the settings provided by the graphical parameters as well as the viewports to which they are assigned:\n\ngrid.newpage()\ngrid.draw(grob1)\ngrid.draw(grob2)\ngrid.draw(grob3)"
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#units",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#units",
    "title": "Generative art with grid",
    "section": "Units",
    "text": "Units\nThe last core concept that we need to discuss is the grid unit system. The grid package allows you to specify the positions (e.g. x and y) and dimensions (e.g. length and width) of grobs and viewports using a flexible language. In the grid unit system there are three qualitatively different styles of unit:\n\nAbsolute units (e.g. centimeters, inches, and points refer to physical sizes).\nRelative units (e.g. npc which scales the viewport size between 0 and 1).\nUnits based on other grobs (e.g. grobwidth).\n\nThe unit() function is the main function we use when specifying units: unit(1, \"cm\") refers to a length of 1 centimeter, whereas unit(0.5, \"npc\") refers to a length half the size of the relevant viewport. The unit system supports arithmetic operations that are only resolved at draw time, which makes it possible to combine different types of units: unit(0.5, \"npc\") + unit(1, \"cm\") defines a point one centimeter to the right of the center of the current viewport."
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#building-grob-classes",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#building-grob-classes",
    "title": "Generative art with grid",
    "section": "Building grob classes",
    "text": "Building grob classes\nNow that we have a basic understanding of grid, let’s attempt to create our own “transforming” grob class: objects that are circles if they are smaller than some threshold (1cm by default), but transform into squares whenever they are larger than the threshold.3 This is not the most useful kind of graphical object, but it’s useful for illustrating the flexibility of the grid system. The first step is to write our own constructor function using grob() or gTree(), depending on whether we are creating a primitive or composite object. We begin by creating a “thin” constructor function:\n\ntransGrob <- function(x, \n                      y, \n                      size,\n                      threshold = 1,\n                      default.units = \"npc\", \n                      name = NULL, \n                      gp = gpar(), \n                      vp = NULL) {\n  \n  # Ensure that input arguments are units\n  if (!is.unit(x)) x <- unit(x, default.units)\n  if (!is.unit(y)) y <- unit(y, default.units)\n  if (!is.unit(size)) size <- unit(size, default.units)\n  \n  # Construct the grob class as a gTree\n  gTree(\n    x = x, \n    y = y, \n    size = size, \n    threshold = threshold,\n    name = name, \n    gp = gp, \n    vp = vp, \n    cl = \"trans\"\n  )\n}\n\nThe transGrob() function doesn’t do very much on its own. All it does is ensure that the x, y, and size arguments are grid units, and sets the class name to be “trans”. To define the behaviour of our grob, we need to specify methods for one or both of the generic functions makeContext() and makeContent():\n\nmakeContext() is called when the parent grob is rendered and allows you to control the viewport of the grob. We won’t need to use that for our surprise grob.\nmakeContent() is called every time the drawing region is resized and allows you to customise the look of the grob based on the size or other aspect.\n\nBecause these generic functions use the S3 object oriented programming system, we can define our method simply by appending the class name to the end of the function name. That is, the makeContent() method for our surprise grob is defined by creating a function called makeContent.trans() that takes a grob as input and returns a modified grob as output:\n\nmakeContent.trans <- function(x) {\n  x_pos <- x$x\n  y_pos <- x$y\n  size <- convertWidth(x$size, unitTo = \"cm\", valueOnly = TRUE)\n  threshold <- x$threshold\n  \n  # Work out which shapes are circles, and which are not\n  circles <- size < threshold\n  \n  # Create a circle grob for the small ones\n  if (any(circles)) {\n    circle_grob <- circleGrob(\n      x = x_pos[circles], \n      y = y_pos[circles], \n      r = unit(size[circles] / 2, \"cm\")\n    )\n  } else {\n    circle_grob <- nullGrob()\n  }\n  \n  # Create a rect grob for the large ones\n  if (any(!circles)) {\n    square_grob <- rectGrob(\n      x = x_pos[!circles], \n      y = y_pos[!circles], \n      width = unit(size[!circles], \"cm\"),\n      height = unit(size[!circles], \"cm\")\n    )\n  } else {\n    square_grob <- nullGrob()\n  }\n  \n  # Add the circle and rect grob as children of our input grob\n  setChildren(x, gList(square_grob, circle_grob))\n}\n\nSome of the functions we’ve called here are new, but they all reuse the core concepts that we discussed earlier. Specifically:\n\nconvertWidth() is used to convert grid units from one type to another.\nnullGrob() creates a blank grob.\ngList() creates a list of grobs.\nsetChildren() specifies the grobs that belong to a gTree composite grob.\n\nThe effect of this function is to ensure that every time the grob is rendered the absolute size of each shape is recalculated. All shapes smaller than the threshold become circles, and all shapes larger than the threshold become squares. To see how this plays out, lets call our new function:\n\nset.seed(1)\nn <- 20\ntrans <- transGrob(\n  x = runif(n, min = .2, max = .8),\n  y = runif(n, min = .2, max = .8),\n  size = runif(n, min = 0, max = .15),\n  threshold = 2\n)\n\nThe trans grob contains shapes whose locations and sizes have been specified relative to the size of the viewport. At this point in time we have no idea which of these shapes will be circles and which will be squares, because that depends on the size of the viewport in which the trans grob is to be drawn. Here’s what we end up with for this quarto post that defines the figure size to be 8x8 inches:\n\ngrid.newpage()\ngrid.draw(trans)\n\n\n\n\nThe exact same code, but now I’ve made the plot size smaller and as a consequence all the shapes have turned into circles:\n\ngrid.newpage()\ngrid.draw(trans)\n\n\n\n\n\n\n\n\nIf you run this code interactively and resize the plotting window you’ll see that the objects change shape based on the size of the plotting window. It’s not the most useful application of grid, but it is fun to play with."
  },
  {
    "objectID": "posts/2022-09-09_reticulated-arrow/index.html",
    "href": "posts/2022-09-09_reticulated-arrow/index.html",
    "title": "Passing Arrow data between R and Python with reticulate",
    "section": "",
    "text": "As the 21st century gears up for its quarter-life crisis, the trend in data science is toward multi-language tools. I use quarto to write this blog, a document preparation system that supports code evaluation in R, Python, Julia, and more. My work revolves around Apache Arrow, a toolbox for data analysis and interchange with implementations in multiple languages. You get the idea. In one sense this new development is fantastic – your language of choice is much more likely to be supported in the future than it ever was in the past. In another sense it is daunting – it sometimes feels like we need to learn all the things in order to get by in this brave new world. Meanwhile we all have our actual jobs to do and we don’t have the time. In the immortal words of Bob Katter commenting on same sex marriage legislation in Australia,\nI mean, he makes a good point? Or at least, it’s a good point about data science: I’m not convinced it was a stellar contribution to the discussion of LGBT rights in the antipodes.1 There’s a lot going on in the data science world, none of us can keep pace with all of it, and we’re all trying our best not to be eaten by crocodiles."
  },
  {
    "objectID": "posts/2022-09-09_reticulated-arrow/index.html#data-interchange-in-a-polyglot-world",
    "href": "posts/2022-09-09_reticulated-arrow/index.html#data-interchange-in-a-polyglot-world",
    "title": "Passing Arrow data between R and Python with reticulate",
    "section": "Data interchange in a polyglot world",
    "text": "Data interchange in a polyglot world\nIn the spirit of saving you from at least one reptilian threat, this post is a primer on how to efficiently pass control of a large data set between R and Python without making any wasteful copies of the data.\nThe idea to write this post emerged from a recent discussion on Twitter started by Cass Wilkinson Saldaña about passing control of a data set from R to Python, and a comment in that discussion by Jon Keane mentioning that with the assistance of Apache Arrow this handover can be made very smooth, and incredibly efficient too. Unfortunately, to be able to do this you need to know the trick, and as they regretfully mentioned in the thread, the trick isn’t well documented yet.\nIn time the documentation will of course improve, but in the here-and-now it seems like a good idea to explain how the magic trick works…\n\n\nThe reticulate trick\nThe “trick” is simple: if your data are stored as an Arrow Table, and you use the reticulate package to pass it from R to Python (or vice versa), only the metadata changes hands. Because an Arrow Table has the same structure in-memory when accessed from Python as it does in R, the data set itself does not need to be touched at all. The only thing that needs to happen is the language on the receiving end needs to be told where the data are stored. Or, to put it another way, we just pass a pointer across. This all happens invisibly, so if you know how to use reticulate,2 you already know almost everything you need to know and can skip straight to the section on passing Arrow objects. If you’re like Danielle-From-Last-Month and have absolutely no idea how reticulate works, read on…\n\n\n\nManaging the Python environment from R\nIf reticulate is not already on your system, you can install it from CRAN with install.packages(\"reticulate\"). Once installed, you can load it in the usual fashion:\n\n\n\n[R code]\n\nlibrary(reticulate)\n\n\nWhat happens next depends a little on whether you already have a Python set up. If you don’t have a preferred Python configuration on your machine and would like to let reticulate manage everything for you, then you can do something like this:\n\n\n\n[R code]\n\ninstall_python()\ninstall_miniconda()\n\n\nThis will set you up with a default Python build, managed by a copy of Miniconda that it installs in an OS-specific location that you can discover by calling miniconda_path().\nThe previous approach is a perfectly sensible way to use reticulate, but in the end I took a slightly different path. If you’re like me and already have Python and Miniconda configured on your local machine, you probably don’t want reticulate potentially installing new versions and possibly making a mess of things.3 You probably want to use your existing set up and ensure that reticulate knows where to find everything. If that’s the case, what you want to do is edit your .Renviron file4 and set the RETICULATE_MINICONDA_PATH variable. Add a line like this one,\n\n\n\n[within .Renviron]\n\nRETICULATE_MINICONDA_PATH=/home/danielle/miniconda3/\n\n\nwhere you should specify the path to your Miniconda installation, not mine 😁\nRegardless of which method you’ve followed, you can use conda_list() to display a summary of all your Python environments.5 Somehow, despite the fact that I went to the effort of setting everything up, I haven’t used Python much on this machine, so my list of environments is short:\n\n\n\n[R code]\n\nconda_list()\n\n\n\n\n          name                                                 python\n1         base                   /home/danielle/miniconda3/bin/python\n2 continuation /home/danielle/miniconda3/envs/continuation/bin/python\n3 r-reticulate /home/danielle/miniconda3/envs/r-reticulate/bin/python\n\n\nFor the purposes of this post I’ll create a new environment that – in honour of Bob Katter and the reptilian terror in the north – I will call “reptilia”. To keep things neat I’ll install6 the pandas and pyarrow packages that this post will be using at the same time:\n\n\n\n[R code]\n\nconda_create(\n  envname = \"reptilia\",\n  packages = c(\"pandas\", \"pyarrow\")\n)\n\n\nWhen I list my conda environments I see that the reptilia environment exists:\n\n\n\n[R code]\n\nconda_list()\n\n\n          name                                                 python\n1         base                   /home/danielle/miniconda3/bin/python\n2 continuation /home/danielle/miniconda3/envs/continuation/bin/python\n3 r-reticulate /home/danielle/miniconda3/envs/r-reticulate/bin/python\n4     reptilia     /home/danielle/miniconda3/envs/reptilia/bin/python\n\n\nTo ensure that reticulate uses the reptilia environment throughout this post,7 I call the use_miniconda() function and specify the environment name:\n\n\n\n[R code]\n\nuse_miniconda(\"reptilia\")\n\n\nOur set up is now complete!\n\n\n\n\n\nA tree frog photographed near Cairns, because some reptiles are cute and adorable – even in Queensland. Original image freely available courtesy of David Clode via Unsplash.\n\n\n\n\n\n\nUsing reticulate to call Python from R\nNow that my environment is set up I’m ready to use Python. When calling Python code from within R, some code translation is necessary due to the differences in syntax across languages. As a simple example, let’s say I have my regular Python session open and I want to check my Python version and executable. To do this I’d import the sys library:\n\n\n\n[python code]\n\nimport sys\nprint(sys.version)\nprint(sys.executable)\n\n\n3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:15:10) \n[GCC 10.3.0]\n/home/danielle/miniconda3/envs/reptilia/bin/python\n\n\nTo execute these commands from R, the code needs some minor changes. The import() function replaces the import keyword, and $ replaces . as the accessor:\n\n\n\n[R code]\n\nsys <- import(\"sys\")\nsys$version\nsys$executable\n\n\n[1] \"3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:15:10) \\n[GCC 10.3.0]\"\n[1] \"/home/danielle/miniconda3/envs/reptilia/bin/python\"\n\n\nThe code looks more R-like, but Python is doing the work.8\n\n\n\nCopying data frames between languages\nOkay, now that we understand the basics of reticulate, it’s time to tackle the problem of transferring data sets between R and Python. For now, let’s leave Arrow out of this. All we’re going to do is take an ordinary R data frame and transfer it to Python.\nFirst, let’s load some data into R. Sticking to the reptilian theme we’ve got going here, the data are taken from The Reptile Database (accessed August 31 2022), an open and freely available catalog of reptile species and their scientific classifications.9\n\n\n\n[R code]\n\ntaxa <- read_csv2(\"taxa.csv\")\ntaxa\n\n\n# A tibble: 14,930 × 10\n   taxon_id family subfa…¹ genus subge…² speci…³ autho…⁴ infra…⁵ infra…⁶ infra…⁷\n   <chr>    <chr>  <chr>   <chr> <lgl>   <chr>   <chr>   <chr>   <chr>   <chr>  \n 1 Ablepha… Scinc… Eugong… Able… NA      alaicus ELPATJ… <NA>    <NA>    <NA>   \n 2 Ablepha… Scinc… Eugong… Able… NA      alaicus ELPATJ… subsp.  alaicus ELPATJ…\n 3 Ablepha… Scinc… Eugong… Able… NA      alaicus ELPATJ… subsp.  kucenk… NIKOLS…\n 4 Ablepha… Scinc… Eugong… Able… NA      alaicus ELPATJ… subsp.  yakovl… (EREMC…\n 5 Ablepha… Scinc… Eugong… Able… NA      anatol… SCHMID… <NA>    <NA>    <NA>   \n 6 Ablepha… Scinc… Eugong… Able… NA      bivitt… (MENET… <NA>    <NA>    <NA>   \n 7 Ablepha… Scinc… Eugong… Able… NA      budaki  GÖCMEN… <NA>    <NA>    <NA>   \n 8 Ablepha… Scinc… Eugong… Able… NA      cherno… DAREVS… <NA>    <NA>    <NA>   \n 9 Ablepha… Scinc… Eugong… Able… NA      cherno… DAREVS… subsp.  cherno… DAREVS…\n10 Ablepha… Scinc… Eugong… Able… NA      cherno… DAREVS… subsp.  eiselti SCHMID…\n# … with 14,920 more rows, and abbreviated variable names ¹​subfamily,\n#   ²​subgenus, ³​specific_epithet, ⁴​authority, ⁵​infraspecific_marker,\n#   ⁶​infraspecific_epithet, ⁷​infraspecific_authority\n\n\nCurrently this object is stored in-memory as an R data frame and we want to move it to Python. However, because Python data structures are different from R data structures, what this actually requires us to do is make a copy of the whole data set inside Python, using a Python-native data structure (in this case a Pandas DataFrame). Thankfully, reticulate does this seamlessly with the r_to_py() function:\n\n\n\n[R code]\n\npy_taxa <- r_to_py(taxa)\npy_taxa\n\n\n                            taxon_id  ...    infraspecific_authority\n0                 Ablepharus_alaicus  ...                         NA\n1         Ablepharus_alaicus_alaicus  ...          ELPATJEVSKY, 1901\n2        Ablepharus_alaicus_kucenkoi  ...             NIKOLSKY, 1902\n3      Ablepharus_alaicus_yakovlevae  ...         (EREMCHENKO, 1983)\n4              Ablepharus_anatolicus  ...                         NA\n...                              ...  ...                        ...\n14925           Zygaspis_quadrifrons  ...                         NA\n14926               Zygaspis_vandami  ...                         NA\n14927     Zygaspis_vandami_arenicola  ...  BROADLEY & BROADLEY, 1997\n14928       Zygaspis_vandami_vandami  ...         (FITZSIMONS, 1930)\n14929              Zygaspis_violacea  ...                         NA\n\n[14930 rows x 10 columns]\n\n\nWithin the Python session, an object called r has been created: the Pandas DataFrame object is stored as r.py_taxa, and we can manipulate it using Python code in whatever fashion we normally might.\nIt helps to see a concrete example. To keep things simple, let’s pop over to our Python session and give ourselves a simple data wrangling task. Our goal is to count the number of entries in the data set for each reptilian family using Pandas syntax:\n\n\n\n[python code]\n\ncounts = r. \\\n  py_taxa[[\"family\", \"taxon_id\"]]. \\\n  groupby(\"family\"). \\\n  agg(len)\n  \ncounts\n\n\n                 taxon_id\nfamily                   \nAcrochordidae           3\nAgamidae              677\nAlligatoridae          16\nAlopoglossidae         32\nAmphisbaenidae        206\n...                   ...\nXenodermidae           30\nXenopeltidae            2\nXenophidiidae           2\nXenosauridae           15\nXenotyphlopidae         1\n\n[93 rows x 1 columns]\n\n\nNaturally I could have done this in R using dplyr functions, but that’s not the point of the post. What matters for our purposes is that counts is a Pandas DataFrame that now exists in the Python session, which we would like to pull back into our R session.\nThis turns out to be easier than I was expecting. The reticulate package exposes an object named py to the user, and any objects I created in my Python session can be accessed that way:\n\n\n\n[R code]\n\npy$counts\n\n\n                   taxon_id\nAcrochordidae             3\nAgamidae                677\nAlligatoridae            16\nAlopoglossidae           32\nAmphisbaenidae          206\nAnguidae                113\nAniliidae                 3\nAnomalepididae           23\nAnomochilidae             3\n...\n\n\nWhat’s especially neat is that the data structure has been automatically translated for us: the counts object in Python is a Pandas DataFrame, but when accessed from R it is automatically translated into a native R data structure: py$counts is a regular data frame:\n\n\n\n[R code]\n\nclass(py$counts)\n\n\n[1] \"data.frame\"\n\n\n\n\n\n\n\nA chameleon. I suppose there is some logic for this image, at least insofar as reticulate allows R to mimic Python and as for arrow Arrow – while it does a lot of the work in the next section — it blends seamlessly into the background. Like a chameleon. Get it? I’m so clever. Original image freely available courtesy of David Clode via Unsplash."
  },
  {
    "objectID": "posts/2022-09-09_reticulated-arrow/index.html#data-interchange-with-arrow-in-the-polyglot-world",
    "href": "posts/2022-09-09_reticulated-arrow/index.html#data-interchange-with-arrow-in-the-polyglot-world",
    "title": "Passing Arrow data between R and Python with reticulate",
    "section": "Data interchange with Arrow in the polyglot world",
    "text": "Data interchange with Arrow in the polyglot world\nSo far we have not touched Arrow, and you might be wondering if it’s even necessary to do so given that reticulate seems so smooth and seamless. Appearances can be a little deceiving however. The example from the last section only looks smooth and seamless because the data set is small. As I’ll show later in the post, cracks in the facade start to appear when you have to pass large data sets across languages. This happens for the very simple reason that a Pandas DataFrame is a different thing to an R data frame. It’s not possible for the two languages to share a single copy of the same data object because they don’t agree on what constitutes “a data object”. The only way we can do the handover is to make a copy of the data set and convert it to a format more suitable to the destination language. When the data set is small, this is not a problem. But as your data set grows, this becomes ever more burdensome. These copy-and-convert operations are not cheap.\nWouldn’t it be nice if R and Python could both agree to represent the data as, oh let’s say…. an Arrow Table? On the R side we could interact with it using the arrow R package, and on the Python side we could interact with it using the pyarrow module. But regardless of which language we’re using, the thing in memory would be exactly the same… handing over the data set from one language to the other would no longer require any copying. A little metadata would change hands, and that’s all.\nThat sounds much nicer.\n\n\nSetting up arrow\nI’m not going to talk much about setting up arrow for R in this post, because I’ve written about it before! In addition to the installation instructions on the arrow documentation there’s a getting started with arrow post on this blog. But in any case, it’s usually pretty straightfoward: you can install the arrow R package from CRAN in the usual way using install.packages(\"arrow\") and then load it in the usual fashion:\n\n\n\n[R code]\n\nlibrary(arrow)\n\n\nOn the Python side, I’ve already installed pyarrow earlier when setting up the “reptilia” environment. But had I not done so, I could redress this now using conda_install() with a command such as this:\n\n\n\n[R code]\n\nconda_install(\n  packages = \"pyarrow\", \n  envname = \"reptilia\"\n)\n\n\nFrom there we’re good to go. On the R side, let’s start by reading the reptiles data directly from file into an Arrow Table:\n\n\n\n[R code]\n\ntaxa_arrow <- read_delim_arrow(\n  file = \"taxa.csv\", \n  delim = \";\", \n  as_data_frame = FALSE\n)\ntaxa_arrow\n\n\nTable\n14930 rows x 10 columns\n$taxon_id <string>\n$family <string>\n$subfamily <string>\n$genus <string>\n$subgenus <null>\n$specific_epithet <string>\n$authority <string>\n$infraspecific_marker <string>\n$infraspecific_epithet <string>\n$infraspecific_authority <string>\n\n\nNext let’s import pyarrow on the Python side and check the version:10\n\n\n\n[python code]\n\nimport pyarrow as pa\npa.__version__\n\n\n'8.0.0'\n\n\nEverything looks good here too!\n\n\n\nHandover to Python\nAfter all that set up, it’s almost comically easy to do the transfer itself. It’s literally the same as last time: we call r_to_py(). The taxa_arrow variable refers to an Arrow Table on the R side, so now all I have to do is use r_to_py() to create py_taxa_arrow, a variable that refers to the same Arrow Table from the Python side:\n\n\n\n[R code]\n\npy_taxa_arrow <- r_to_py(taxa_arrow)\n\n\nSince we’re in Python now, let’s just switch languages and take a peek, shall we? Just like last time, objects created by reticulate are accessible on the Python side via the r object, so we access this object in Python with r.py_taxa_arrow:\n\n\n\n[python code]\n\nr.py_taxa_arrow\n\n\npyarrow.Table\ntaxon_id: string\nfamily: string\nsubfamily: string\ngenus: string\nsubgenus: null\nspecific_epithet: string\nauthority: string\ninfraspecific_marker: string\ninfraspecific_epithet: string\ninfraspecific_authority: string\n----\ntaxon_id: [[\"Ablepharus_alaicus\",\"Ablepharus_alaicus_alaicus\",\"Ablepharus_alaicus_kucenkoi\",\"Ablepharus_alaicus_yakovlevae\",\"Ablepharus_anatolicus\",...,\"Plestiodon_egregius_onocrepis\",\"Plestiodon_egregius_similis\",\"Plestiodon_elegans\",\"Plestiodon_fasciatus\",\"Plestiodon_finitimus\"],[\"Plestiodon_gilberti\",\"Plestiodon_gilberti_cancellosus\",\"Plestiodon_gilberti_gilberti\",\"Plestiodon_gilberti_placerensis\",\"Plestiodon_gilberti_rubricaudatus\",...,\"Zygaspis_quadrifrons\",\"Zygaspis_vandami\",\"Zygaspis_vandami_arenicola\",\"Zygaspis_vandami_vandami\",\"Zygaspis_violacea\"]]\nfamily: [[\"Scincidae\",\"Scincidae\",\"Scincidae\",\"Scincidae\",\"Scincidae\",...,\"Scincidae\",\"Scincidae\",\"Scincidae\",\"Scincidae\",\"Scincidae\"],[\"Scincidae\",\"Scincidae\",\"Scincidae\",\"Scincidae\",\"Scincidae\",...,\"Amphisbaenidae\",\"Amphisbaenidae\",\"Amphisbaenidae\",\"Amphisbaenidae\",\"Amphisbaenidae\"]]\nsubfamily: [[\"Eugongylinae\",\"Eugongylinae\",\"Eugongylinae\",\"Eugongylinae\",\"Eugongylinae\",...,\"Scincinae\",\"Scincinae\",\"Scincinae\",\"Scincinae\",\"Scincinae\"],[\"Scincinae\",\"Scincinae\",\"Scincinae\",\"Scincinae\",\"Scincinae\",...,null,null,null,null,null]]\ngenus: [[\"Ablepharus\",\"Ablepharus\",\"Ablepharus\",\"Ablepharus\",\"Ablepharus\",...,\"Plestiodon\",\"Plestiodon\",\"Plestiodon\",\"Plestiodon\",\"Plestiodon\"],[\"Plestiodon\",\"Plestiodon\",\"Plestiodon\",\"Plestiodon\",\"Plestiodon\",...,\"Zygaspis\",\"Zygaspis\",\"Zygaspis\",\"Zygaspis\",\"Zygaspis\"]]\nsubgenus: [11142 nulls,3788 nulls]\nspecific_epithet: [[\"alaicus\",\"alaicus\",\"alaicus\",\"alaicus\",\"anatolicus\",...,\"egregius\",\"egregius\",\"elegans\",\"fasciatus\",\"finitimus\"],[\"gilberti\",\"gilberti\",\"gilberti\",\"gilberti\",\"gilberti\",...,\"quadrifrons\",\"vandami\",\"vandami\",\"vandami\",\"violacea\"]]\nauthority: [[\"ELPATJEVSKY, 1901\",\"ELPATJEVSKY, 1901\",\"ELPATJEVSKY, 1901\",\"ELPATJEVSKY, 1901\",\"SCHMIDTLER, 1997\",...,\"BAIRD, 1858\",\"BAIRD, 1858\",\"(BOULENGER, 1887)\",\"(LINNAEUS, 1758)\",\"OKAMOTO & HIKIDA, 2012\"],[\"(VAN DENBURGH, 1896)\",\"(VAN DENBURGH, 1896)\",\"(VAN DENBURGH, 1896)\",\"(VAN DENBURGH, 1896)\",\"(VAN DENBURGH, 1896)\",...,\"(PETERS, 1862)\",\"(FITZSIMONS, 1930)\",\"(FITZSIMONS, 1930)\",\"(FITZSIMONS, 1930)\",\"(PETERS, 1854)\"]]\ninfraspecific_marker: [[null,\"subsp.\",\"subsp.\",\"subsp.\",null,...,\"subsp.\",\"subsp.\",null,null,null],[null,\"subsp.\",\"subsp.\",\"subsp.\",\"subsp.\",...,null,null,\"subsp.\",\"subsp.\",null]]\ninfraspecific_epithet: [[null,\"alaicus\",\"kucenkoi\",\"yakovlevae\",null,...,\"onocrepis\",\"similis\",null,null,null],[null,\"cancellosus\",\"gilberti\",\"placerensis\",\"rubricaudatus\",...,null,null,\"arenicola\",\"vandami\",null]]\ninfraspecific_authority: [[null,\"ELPATJEVSKY, 1901\",\"NIKOLSKY, 1902\",\"(EREMCHENKO, 1983)\",null,...,\"(COPE, 1871)\",\"(MCCONKEY, 1957)\",null,null,null],[null,\"(RODGERS & FITCH, 1947)\",\"(VAN DENBURGH, 1896)\",\"(RODGERS, 1944)\",\"(TAYLOR, 1936)\",...,null,null,\"BROADLEY & BROADLEY, 1997\",\"(FITZSIMONS, 1930)\",null]]\n\n\nThe output is formatted slightly differently because the Python pyarrow library is now doing the work. You can see from the first line that this is a pyarrow Table, but nevertheless when you look at the rest of the output it’s pretty clear that this is the same table.\nEasy!\n\n\n\nHandover to R\nRight then, what’s next? Just like last time, let’s do a little bit of data wrangling on the Python side. In the code below I’m using pyarrow to do the same thing I did with Pandas earlier: counting the number of entries for each reptile family.\n\n\n\n[python code]\n\ncounts_arrow = r.py_taxa_arrow. \\\n  group_by(\"family\"). \\\n  aggregate([(\"taxon_id\", \"count\")]). \\\n  sort_by([(\"family\", \"ascending\")])\n  \ncounts_arrow\n\n\npyarrow.Table\ntaxon_id_count: int64\nfamily: string\n----\ntaxon_id_count: [[3,677,16,32,206,...,2,2,15,1,5]]\nfamily: [[\"Acrochordidae\",\"Agamidae\",\"Alligatoridae\",\"Alopoglossidae\",\"Amphisbaenidae\",...,\"Xenopeltidae\",\"Xenophidiidae\",\"Xenosauridae\",\"Xenotyphlopidae\",null]]\n\n\nFlipping back to R, the counts_arrow object is accessible via the py object. Let’s take a look:\n\n\n\n[R code]\n\npy$counts_arrow\n\n\nTable\n93 rows x 2 columns\n$taxon_id_count <int64>\n$family <string>\n\n\nThe output is formatted a little differently because now it’s the R arrow package tasked with printing the output, but it is the same Table.\nMission accomplished!\nBut… was it all worthwhile?\n\n\n\n\n\nA baby crocodile, just so that Bob Katter doesn’t feel like I completely forgot about his worries. It doesn’t look like it’s about to tear anyone to pieces but what would I know? I’m not an expert on such matters. Original image freely available courtesy of David Clode via Unsplash."
  },
  {
    "objectID": "posts/2022-09-09_reticulated-arrow/index.html#does-arrow-really-make-a-big-difference",
    "href": "posts/2022-09-09_reticulated-arrow/index.html#does-arrow-really-make-a-big-difference",
    "title": "Passing Arrow data between R and Python with reticulate",
    "section": "Does Arrow really make a big difference?",
    "text": "Does Arrow really make a big difference?\nAt the end of all this, you might want to know if using Arrow makes much of a difference. As much as I love learning new things for the sheer joy of learning new things, I prefer to learn useful things when I can! So let’s do a little comparison. First, I’ll define a handover_time() function that takes two arguments. The first argument n specifies the number of rows in the to-be-transferred data set. The second argument arrow is a logical value: setting arrow = FALSE means that an R data frame will be passed to Python as a Panda DataFrame, wheras arrow = TRUE means that an Arrow Table in R will be passed to Python and remain an Arrow Table. The actual data set is constructed by randomly sampling n rows from the taxa data set (with replacement):\n\n\n\n[R code]\n\nhandover_time <- function(n, arrow = FALSE) {\n  data_in_r <- slice_sample(taxa, n = n, replace = TRUE)\n  if(arrow) {\n    data_in_r <- arrow_table(data_in_r)\n  }\n  tic()\n  data_in_python <- r_to_py(data_in_r)\n  t <- toc(quiet = TRUE)\n  return(t$toc - t$tic)\n}\n\n\nNow that I’ve defined the test function, let’s see what happens. I’ll vary the number of rows from 10000 to 1000000 for both the native data frame version and the Arrow Table version, and store the result as times:\n\n\n\n[R code]\n\ntimes <- tibble(\n  n = seq(10000, 1000000, length.out = 100),\n  data_frame = map_dbl(n, handover_time),\n  arrow_table = map_dbl(n, handover_time, arrow = TRUE),\n)\n\n\nNow let’s plot the data:\n\n\n\n[R code]\n\ntimes |> \n  pivot_longer(\n    cols = c(\"data_frame\", \"arrow_table\"), \n    names_to = \"type\", \n    values_to = \"time\"\n  ) |> \n  mutate(\n    type = type |> \n      factor(\n        levels = c(\"data_frame\", \"arrow_table\"),\n        labels = c(\"Data Frames\", \"Arrow Tables\")\n      )\n  ) |>\n  ggplot(aes(n, time)) + \n  geom_point() + \n  facet_wrap(~type) + \n  theme_bw() + \n  labs(\n    x = \"Number of Rows\",\n    y = \"Handover Time (Seconds)\", \n    title = \"How long does it take to pass data from R to Python?\"\n  )\n\n\n\n\n\nOkay yeah. I’ll be the first to admit that this isn’t a very sophisticated way to do benchmarking, but when the difference is this stark you really don’t have to be sophisticated. Without Arrow, the only way to hand data from R to Python is to copy and convert the data, and that’s time consuming. The time cost gets worse the larger your data set becomes. With Arrow, the problem goes away because you’re not copying the data at all. The time cost is tiny and it stays tiny even as the data set gets bigger.\nSeems handy to me?"
  },
  {
    "objectID": "posts/2022-09-09_reticulated-arrow/index.html#acknowledgments",
    "href": "posts/2022-09-09_reticulated-arrow/index.html#acknowledgments",
    "title": "Passing Arrow data between R and Python with reticulate",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThank you to Marlene Mhangami and Fernanda Foertter for reviewing this post."
  },
  {
    "objectID": "posts/2022-09-04_sudo-askpass/index.html",
    "href": "posts/2022-09-04_sudo-askpass/index.html",
    "title": "Sudo ask me a password",
    "section": "",
    "text": "One peculiar feature of earning one’s keep in society by writing a data science blog is that it provides the opportunity to be unabashedly weird. Personality is important. Other developers will read your strange content – no matter how weird and unprofessional it is – because professional corporate style is very nearly as dull as academic writing, and it is a relief to learn a new thing from an actual human being who write with a certain level of human character.\nEven if she is an irredeemably catty bitch.\nAll of which is by way of preamble, and a way to acknowlege that when the topic is package dependencies in R, the queer double entendres kind of write themselves.1 And so without further ado, today’s unhinged rambling…"
  },
  {
    "objectID": "posts/2022-09-04_sudo-askpass/index.html#managing-package-dependencies-in-r",
    "href": "posts/2022-09-04_sudo-askpass/index.html#managing-package-dependencies-in-r",
    "title": "Sudo ask me a password",
    "section": "Managing package dependencies in R",
    "text": "Managing package dependencies in R\nOkay so you’ve done your due diligence. You’ve read his pkgdown site, checked out his repo, and you still like him. You really, really want to install his package. You’ve inspected the documentation and it is a very impressive package. I mean, I get it babe.\nI. have. been. there.\nThere’s a thing with packages though. When you’re installing them into wherever you like to put packages (the precise location doesn’t matter for this post2), what you see isn’t necessarily all you get.\nI’ll give a concrete example. For reasons unknown even to me I woke up this morning and decided today was the day I’d explore the rig installation manager for R that lets you manage multiple R installations on the one machine. It’s very nice, and possibly the topic for a future post. However, one side consequence to adopting rig is that I ended up with adorably fresh copies of R that had no installed packages and needed to be properly set up. In the process, I started thinking a little about the tools I use to install packages. When I first started using R my go to method was to use the install.packages() function supplied by the utils package: after all, it comes bundled with R, which makes it an easy place to start. As I matured as an R user I found myself switching to the remotes package because it provides a coherent set of functions for installing packages from CRAN, Bioconductor, GitHub, and more. I’m a huge fan of remotes, but for reasons I’ll explain in a moment I’m starting to prefer the pak package developed by Gábor Csárdi and Jim Hester. When using pak, the function you use to install packages is called pkg_install(). I’ll walk you through it. Here’s what happens when I try to install the quarto R package without properly configuring my setup. First I call the function:\n\npak::pkg_install(\"quarto\")\n\nWhen I hit enter, pak starts doing its job, resolving the R dependencies and then asking if I want to continue:\n✓ Loading metadata database ... done\n                                                                            \n→ Will install 2 packages.\n→ Will update 1 package.\n→ Will download 3 packages with unknown size.\n+ packrat         0.8.1  [bld][dl]\n+ quarto    1.1 → 1.2    [bld][dl]\n+ rsconnect       0.8.27 [bld][dl]\n? Do you want to continue (Y/n) \nI really like this approach. The interface is very clear about precisely what is happening, and pak doesn’t download any more packages than is absolutely necessary to give you what you asked for (unless you specify upgrade = TRUE in the install command).\nI agree to continue, so off pak goes, fetching the appropriate R packages:\nℹ Getting 3 pkgs with unknown sizes\n✓ Got quarto 1.2 (source) (67.58 kB)                                             \n✓ Got rsconnect 0.8.27 (source) (685.57 kB)                                      \n✓ Got packrat 0.8.1 (source) (681.50 kB)                                         \n✓ Downloaded 3 packages (1.43 MB)in 6.7s\nSo far, so good. The output is informative and succinct at the same time. It appeals to my aesthetic sensibilities. But then pak – which is very diligent about managing all dependencies including system dependencies – attempts to install the external libraries upon which quarto depends,3 and this happens:\nℹ Installing system requirements\nℹ Executing `sudo sh -c apt-get install -y make`\nError: System command 'sudo' failed, exit status: 1, stdout + stderr:\nE> sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\nAh."
  },
  {
    "objectID": "posts/2022-09-04_sudo-askpass/index.html#what-went-wrong-here",
    "href": "posts/2022-09-04_sudo-askpass/index.html#what-went-wrong-here",
    "title": "Sudo ask me a password",
    "section": "What went wrong here?",
    "text": "What went wrong here?\nAn important thing to understand here is that neither pak nor sudo are to blame for the installation failure.4 On the pak side, it’s a good thing that it tries to uncover and install system dependencies: the package isn’t going to work if you don’t have those dependencies installed, and it can be a nightmare trying to track them all down when the package manager doesn’t help identify them for you.5 On the sudo side, it is extremely reasonable to expect the user to authenticate before enabling superuser privileges. Speaking for myself, I have a very strong expectation that I will be explicitly asked for my consent before packages are installed on my system.6\nThe breakdown happens because pak has invoked sudo outside of the terminal context. If you haven’t configured sudo to handle this situation, there’s no opportunity for the user to authenticate, and sudo throws an error.\nHow can we resolve this?"
  },
  {
    "objectID": "posts/2022-09-04_sudo-askpass/index.html#configuring-sudo",
    "href": "posts/2022-09-04_sudo-askpass/index.html#configuring-sudo",
    "title": "Sudo ask me a password",
    "section": "Configuring sudo",
    "text": "Configuring sudo\nA little bit of digging revealed that sudo is a lot more configurable than I had preciously realised, and you can deal with this issue in a few different ways. One possibility would be to enable passwordless sudo, in which case the system dependencies would be installed without requiring a password at all. That would certainly minimise the amount of hassle at my end, but it’s also a hell of a security risk. Even if I personally felt willing to take that risk with my own property, this is a work laptop and I think a little risk-aversion is a good idea in this case.7\nFortunately, the error message itself contains some hints that there is an alternative fix that doesn’t require you to weaken your security settings (or not by very much, at any rate). Specifically, the error message refers to an “askpass helper”: a program, usually with a GUI, that sudo will invoke whenever it needs authentication from the user but is not running in a terminal. However, in order to get sudo to invoke one of these helpers, you have to explicitly configure it within the sudo configuration file, sudo.conf. This configuration file is located at /etc/sudo.conf is discussed pretty thoroughly in the sudo help documentation. Here’s the relevant part of the output when I type man sudo.conf:8\n\n askpass   The fully qualified path to a helper program used to read the\n           user's password when no terminal is available.  This may be\n           the case when sudo is executed from a graphical (as opposed to\n           text-based) application.  The program specified by askpass\n           should display the argument passed to it as the prompt and\n           write the user's password to the standard output.  The value\n           of askpass may be overridden by the SUDO_ASKPASS environment\n           variable.\n\nOkay, so I need to do two things. I need to edit sudo.conf to configure sudo to use the askpass helper, and I also need the askpass helper itself. So where do I find one of these askpass helper programs? The one I chose to go with is ssh-askpass, which I installed on my system using the following:\nsudo apt-get install ssh-askpass ssh-askpass-gnome\nNotice that I installed both ssh-askpass and ssh-askpass-gnome. The latter isn’t the askpass helper itself, and isn’t intended to be invoked separately from ssh-askpass. Rather it’s there because ssh-askpass on its own uses X11 to do the graphical user interface part and it’s not very pretty on my Ubuntu installation. By installing ssh-askpass-gnome as well, the dialog box that comes up when ssh-askpass is invoked is much nicer.\nAt the terminal, I can invoke ssh-askpass manually if I want to. It doesn’t do much by itself: all it does is create the dialog box and return the text input by the user.\nssh-askpass\nIn practice you don’t actually call this directly. Instead, you configure sudo to that whenever it needs authentication but doesn’t have access to a terminal. In order to accomplish this, here’s the lines I added to my sudo.conf file:9\n# specify ssh-askpass as my helper\nPath askpass /usr/bin/ssh-askpass\nSo I did this and then10 tried to install quarto using pkg_install(). This time around sudo no longer errored when pak tried to install system dependencies. Instead it brought up the askpass dialog box:\n\nWhen I typed in my password, pak and sudo were able to play nicely together and the installation worked just fine. Well, mostly.."
  },
  {
    "objectID": "posts/2022-09-04_sudo-askpass/index.html#can-we-relax-just-a-little-sweetie",
    "href": "posts/2022-09-04_sudo-askpass/index.html#can-we-relax-just-a-little-sweetie",
    "title": "Sudo ask me a password",
    "section": "Can we relax just a little sweetie?",
    "text": "Can we relax just a little sweetie?\nThe only problem is that quarto installation requires five system dependencies to be installed, and as the output below shows, pak starts a new shell process every single time…\nℹ Executing `sudo sh -c apt-get install -y make`\nℹ Executing `sudo sh -c apt-get install -y libcurl4-openssl-dev`\nℹ Executing `sudo sh -c apt-get install -y libicu-dev`\nℹ Executing `sudo sh -c apt-get install -y libssl-dev`\nℹ Executing `sudo sh -c apt-get install -y pandoc`\n…and as a consequence of this I had to enter my password five times.\nThat’s mildly irritating, and I was not expecting it. My original assumption would be that entering the password the first time would invoke the sudo password cache: that is, after entering my password once, the elevated permissions11 would persist for about 15 minutes. That’s what happens by default at the terminal, and I had presumed the same would apply when the call to sudo occurs within an R session. However, that’s not quite accurate. This little gem in man sudo explains the relevant security policy issue:\n     Security policies may support credential caching to allow the\n     user to run sudo again for a period of time without requiring\n     authentication.  By default, the sudoers policy caches creden‐\n     tials on a per-terminal basis for 15 minutes.  See the\n     timestamp_type and timestamp_timeout options in sudoers(5) for\n     more information.  By running sudo with the -v option, a user\n     can update the cached credentials without running a command.\n\nThe reason why the “15 minutes” rule doesn’t apply here is that the credentials are cached on a “per-terminal” basis. Each sudo sh command invoked by pak effectively runs a new instance of the shell and the password caching doesn’t transfer. Gr."
  },
  {
    "objectID": "posts/2022-09-04_sudo-askpass/index.html#editing-the-sudoers-file",
    "href": "posts/2022-09-04_sudo-askpass/index.html#editing-the-sudoers-file",
    "title": "Sudo ask me a password",
    "section": "Editing the sudoers file",
    "text": "Editing the sudoers file\nAs a general rule I can understand why sudo is conservative and does not permit the credential cache to transfer across processes. Who knows what other processes are running and what they might be doing? But in this instance I’m willing to slightly relax the security policy to ensure that the pak/sudo combination doesn’t drive me crazy by relentlessly asking for permission on every little thing.\nThe security policies in sudo are managed by plugins12 configured using the “sudoers” file(s) located at /etc/sudoers. I’ll talk about this file momentarily, but first here’s the relevant extract from man sudoers that tells us what setting we need to modify:\n     timestamp_type    sudoers uses per-user time stamp files for\n                       credential caching.  The timestamp_type op‐\n                       tion can be used to specify the type of time\n                       stamp record used.  It has the following\n                       possible values:\n\n                       global  A single time stamp record is used\n                               for all of a user's login sessions,\n                               regardless of the terminal or parent\n                               process ID.  An additional record is\n                               used to serialize password prompts\n                               when sudo is used multiple times in\n                               a pipeline, but this does not affect\n                               authentication.\n\n                       ppid    A single time stamp record is used\n                               for all processes with the same par‐\n                               ent process ID (usually the shell).\n                               Commands run from the same shell (or\n                               other common parent process) will\n                               not require a password for\n                               timestamp_timeout minutes (15 by\n                               default).  Commands run via sudo\n                               with a different parent process ID,\n                               for example from a shell script,\n                               will be authenticated separately.\n\n                       tty     One time stamp record is used for\n                               each terminal, which means that a\n                               user's login sessions are authenti‐\n                               cated separately.  If no terminal is\n                               present, the behavior is the same as\n                               ppid.  Commands run from the same\n                               terminal will not require a password\n                               for timestamp_timeout minutes (15 by\n                               default).\n\n                       kernel  The time stamp is stored in the ker‐\n                               nel as an attribute of the terminal\n                               device.  If no terminal is present,\n                               the behavior is the same as ppid.\n                               Negative timestamp_timeout values\n                               are not supported and positive val‐\n                               ues are limited to a maximum of 60\n                               minutes.  This is currently only\n                               supported on OpenBSD.\n\n                       The default value is tty.\n\n                       This setting is only supported by version\n                       1.8.21 or higher.\n\nThis documentation makes clear where the problem lies. When pak invokes sudo, a new process is spawned and unless the value of timestamp_type is set to global, the sudo credential cache doesn’t get shared across processes.\nIt’s possible to modify this setting, and I’ll show you how to do that below, but first I strongly recommend that you read this article on how to edit the sudoers file carefully. For realsies, my dears, read it. Editing policies for sudo needs to be done with a lot of care. You don’t want to mess it up and lose the ability to invoke sudo because it’s been incorrectly configured. So please, please read the linked page.\nYou read it, right?\nGood.\nAfter reading through the linked article, I made the decision that instead of editing the main sudoers file, I would instead add a small file to the /etc/sudoers.d/ directory. By default, files in this folder are automatically included when the sudoers plugin is loaded, so it’s a convenient place to add your customisations rather than editing the main file. I created one that exists solely to manage the timestamp settings for my primary user:\nsudo visudo -f /etc/sudoers.d/timestamp_type\nNotice that I’ve used visudo, and not some other editor. If you read the linked article you know why I did that, and why it is astonishingly important to do it this way in order to practice safe sudo13 policy editing. If you didn’t read the linked article… well, you would be extremely ill-advised to try the next step without actually reading it.\nOkay, that feels like enough warning. Let’s look at what I included in my new /etc/sudoers.d/timestamp_type file:\n# specify the timeout type (usual default=tty)\nDefaults:danielle timestamp_type=global\n\n# specify the timeout interval (usual default=15)\nDefaults:danielle timestamp_timeout=2\nI’ve done two things. First, in order to allow the sudo password cache to work everywhere regardless of which process invokes it, I set timestamp_type=global. Second, because this makes me a tiny bit nervous (it’s a very mild softening of security policies), I shortened the cache expiry time from 15 minutes to 2 minutes by setting timestamp_timeout=2. In practice, I very rarely do anything requiring superuser privileges that requires more than two minutes, and it seems best to let those privileges expire quickly."
  },
  {
    "objectID": "posts/2022-09-04_sudo-askpass/index.html#youre-a-star",
    "href": "posts/2022-09-04_sudo-askpass/index.html#youre-a-star",
    "title": "Sudo ask me a password",
    "section": "You’re a star",
    "text": "You’re a star\nAt this point I have a set up that lets me use pak without needing to weaken my security policies (well, not by much) and won’t cause me to lose my mind by typing in my password over and over until I beg for the sweet release14 of death. Was it all worth it?\nWell, let me just say this: out of curiosity I decided to try installing the stars package, which necessarily entails installing a lot of geospatial dependencies. Back when I first tried installing these tools on linux a couple of years ago it was a nightmare. I had to track down the dependencies myself and manually install them, which was pretty daunting at the time because I was very new to the whole business.15 Here’s what happened when I tried it with pak after configuring sudo to ask for my password only the once:\n\npak::pkg_install(\"stars\")\n\n→ Will install 1 package.\n→ Will download 1 package with unknown size.\n+ stars   0.5-6 [bld][dl]\nℹ Getting 1 pkg with unknown size\n✓ Got stars 0.5-6 (source) (3.42 MB)                                  \n✓ Downloaded 1 package (3.42 MB)in 4.2s                               \nℹ Installing system requirements\nℹ Executing `sudo sh -c apt-get install -y libgdal-dev`\nℹ Executing `sudo sh -c apt-get install -y gdal-bin`\nℹ Executing `sudo sh -c apt-get install -y libgeos-dev`\nℹ Executing `sudo sh -c apt-get install -y libssl-dev`\nℹ Executing `sudo sh -c apt-get install -y libproj-dev`\nℹ Executing `sudo sh -c apt-get install -y libudunits2-dev`\nℹ Building stars 0.5-6\n✓ Built stars 0.5-6 (1.4s)                                       \n✓ Installed stars 0.5-6  (98ms)                                    \n✓ 1 pkg + 16 deps: kept 12, added 1, dld 1 (3.42 MB) [20.7s]    \nOne line of code. One line of code, that worked the first time. One line of code that worked the first time and installed everything quickly. It’s a true Christmas miracle."
  },
  {
    "objectID": "posts/2023-01-14_p5js/index.html",
    "href": "posts/2023-01-14_p5js/index.html",
    "title": "Fun and games with p5.js and observable.js in quarto",
    "section": "",
    "text": "Be sweet to me, baby  I wanna believe in you  I wanna believe  Be sweet  Be sweet to me, baby  I wanna believe in you  I wanna believe in something    – Japanese Breakfast\nOkay, so… I write this blog using quarto, and quarto has native support for observable.js … and observable.js supports third-party javascript libraries such as p5.js executing in code cells… so, like… I can use p5.js to create generative art, inside the browser, inside a blog post? Right?\nApparently the answer to that is yes.\nThere is but one tiny problem. I don’t know anything about observable.js or p5.js. I supposed I’d best remedy that."
  },
  {
    "objectID": "posts/2023-01-14_p5js/index.html#enabling-p5js",
    "href": "posts/2023-01-14_p5js/index.html#enabling-p5js",
    "title": "Fun and games with p5.js and observable.js in quarto",
    "section": "Enabling p5js",
    "text": "Enabling p5js\nThe first step in the process is enabling p5.js, which is not one of the core libraries in observable, and is not immediately available. To use a third-party library that exists as an NPM modules we can import it using require().\n\nP5 = require(\"p5\")\n\n\n\n\n\n\nJust like that, thanks to the joy of the jsDelivr CDN, p5.js is now available to me in this post.\nWell, sort of. As you can see from the output,1 the P5 object is a function that takes three inputs. To do anything useful with it, I’ll use a trick I learned from this fabulous notebook by Tom MacWright to run p5.js in “instance mode”. Normally, p5.js works by defining a lot of global objects. That works fine if you’re only doing one “sketch” on a single page, but it’s not so clean if you want to write modular code where a single page (like this one) could contain multiple p5.js sketches.\nTo run p5.js in instance mode, and in a way that plays nicely with observable.js and quarto, I’ll define createSketch as a generator function:\n\nfunction* createSketch(sketch) {\n  const element = DOM.element('div');\n  yield element;\n  const instance = new P5(sketch, element, true);\n  try {\n    while (true) {\n      yield element;\n    }\n  } finally {\n    instance.remove();\n  }\n}\n\n\n\n\n\n\nUsing this approach, each instantiation of P5 is attached to a div element that created when createSketch is called. If you want to know more about how this approach works, it’s probably best to go to the original source that I adapted it from, because Tom has commented it and explained it nicely: observablehq.com/@tmcw/p5"
  },
  {
    "objectID": "posts/2023-01-14_p5js/index.html#donut-1",
    "href": "posts/2023-01-14_p5js/index.html#donut-1",
    "title": "Fun and games with p5.js and observable.js in quarto",
    "section": "Donut 1",
    "text": "Donut 1\nIn keeping with the tradition I’ve set up in the last few blog posts, all the examples are donut themed.2 When calling createSketch I’ll pass an anonymous function that takes a single argument s, the document element to which all the p5 functions are attached. I’ll use the arrow notation, so my code is going to look something like this:\n\ncreateSketch(s => {\n  // add some p5.js code \n})\n\n\n\n\n\n\nThe idea in p5.js is all the work is done by two functions. The setup function includes code that is called only once, and if you want to draw static images you can do everything at the setup stage. In contrast the draw function is called repeatedly, so you can use that to add dynamic elements.\nHere’s an example of a static sketch that draws a single donut shape using two circles:\n\ncreateSketch(s => {\n    s.setup = function() {\n      s.createCanvas(500, 500);\n      s.background(\"black\");\n      s.fill(\"red\").circle(250, 250, 100);\n      s.fill(\"black\").circle(250, 250, 30);\n    };\n  }\n)\n\n\n\n\n\n\nIn this example:\n\ncreateCanvas creates the drawing area in which the sketch will be rendered. Arguments are the width and height in pixels\nbackground sets the background colour. The colour specification is flexible: it can be a recognised colour name, a hex string, or a numeric RGB specification\nfill sets the fill colour\ncircle draws a circle: the first two arguments specify the origin of the circle, and the third argument specifies the diameter\n\nI’ve used method chaining here to remind me that the first fill and the first circle go together: writing s.fill(\"red\").circle(250, 250, 100) on a single line helps me group code together conceptually. It’s mostly for my own convenience though."
  },
  {
    "objectID": "posts/2023-01-14_p5js/index.html#donut-2",
    "href": "posts/2023-01-14_p5js/index.html#donut-2",
    "title": "Fun and games with p5.js and observable.js in quarto",
    "section": "Donut 2",
    "text": "Donut 2\nOkay Danielle, that’s nice but it’s not that nice. Can we do something a little more interesting? Maybe with some dynamics? Well okay, Other Danielle, since you asked so sweetly, here’s an example with a moving circle that changes colour and traces out a donut shape:\n\ncreateSketch(s => {\n  \n    s.setup = function() {\n      s.createCanvas(500, 500);\n      s.background(0);\n      s.noStroke();\n    };\n    \n    s.draw = function() {\n      s.translate(\n        100 * s.cos(s.millis() * .001 * s.PI),\n        100 * s.sin(s.millis() * .001 * s.PI),\n      );\n      if (s.random(0, 1) < .1) {\n        s.fill(s.random(0, 255));\n      }\n      s.circle(250, 250, 100);\n    };\n    \n  }\n)\n\n\n\n\n\n\nThis example makes use of some geometry functions included in p5.j (sin, cos, translate), a random number generator (random), and timer that returns the number of milliseconds since the sketch started (millis). These are all documented in the p5.js reference."
  },
  {
    "objectID": "posts/2023-01-14_p5js/index.html#donut-3",
    "href": "posts/2023-01-14_p5js/index.html#donut-3",
    "title": "Fun and games with p5.js and observable.js in quarto",
    "section": "Donut 3",
    "text": "Donut 3\nFor the third example we’ll introduce some fonts, adapting an example from observablehq.com/@tmcw/p5. First, I’ll add some CSS to import the Courgette font:\n\n@import url(https://fonts.googleapis.com/css?family=Courgette);\n\n\nNow we can use that font in a p5.js scrolling window:\n\ncreateSketch(s => {\n  \n    s.setup = function() {\n      s.createCanvas(746, 300);\n      s.textFont('Courgette');\n      s.textStyle(s.BOLD);\n      s.textAlign(s.CENTER, s.CENTER)\n    };\n    \n    s.draw = function() {\n      s.translate(\n        s.millis() * (-0.1) % (s.width + 1000), \n        s.height / 2\n      );\n      s.background('#222222');\n      s.fill('#DC3F74').textSize(100);\n      s.text('Donuts: A Hole World', s.width + 500, 0);\n    };\n    \n  }\n)\n\n\n\n\n\n\nCould life be any more thrilling than this?"
  },
  {
    "objectID": "posts/2023-01-14_p5js/index.html#donut-4",
    "href": "posts/2023-01-14_p5js/index.html#donut-4",
    "title": "Fun and games with p5.js and observable.js in quarto",
    "section": "Donut 4",
    "text": "Donut 4\nWell, maybe it can. We could make it a little more interesting by using webGL to move our donut plots into the… THIRD DIMENSION! (Gasp!)\n\ncreateSketch(s => {\n\n  s.setup = function() {\n    s.createCanvas(746, 746, s.WEBGL);\n    s.noStroke();\n  }\n\n  s.draw = function() {\n\n    s.background(0);\n\n    let locX = s.mouseX - s.height / 2;\n    let locY = s.mouseY - s.width / 2;  \n    \n    s.ambientLight(60, 60, 60);\n    s.pointLight(190, 80, 190, locX, locY, 100);\n    s.pointLight(80, 80, 190, 0, 0, 100);\n  \n    s.specularMaterial(255);\n    s.rotateX(s.frameCount * 0.01);\n    s.rotateY(s.frameCount * 0.01);\n    s.torus(150, 80, 64, 64);\n  }\n\n})\n\n\n\n\n\n\nIf you move the mouse over the donut3 you’ll see that the light source moves with it."
  },
  {
    "objectID": "posts/2023-01-14_p5js/index.html#donut-5",
    "href": "posts/2023-01-14_p5js/index.html#donut-5",
    "title": "Fun and games with p5.js and observable.js in quarto",
    "section": "Donut 5",
    "text": "Donut 5\nFor the final example, I’ll do a tiny bit of object-oriented programming. Inspired by a generative art course by Bernat Ferragut (ga-course.surge.sh) that I was skimming yesterday, I’ll define a Dot class that creates a particle that moves around on the canvas and has the ability to bounce off circular boundaries:\n\nclass Dot {\n  constructor(sketch, x, y, colour, size) {\n    this.s = sketch;\n    this.x = x | 0;\n    this.y = y | 0;\n    this.colour = colour;\n    this.size = size;\n    this.velX = this.s.random(-2, 2);\n    this.velY = this.s.random(-2, 2);\n  }\n\n  on() {\n    this.s.noStroke();\n    this.s.fill(this.colour);\n    this.s.circle(this.x, this.y, this.size);\n  }\n\n  move() {\n    this.x += this.velX;\n    this.y += this.velY;\n  }\n  \n  bounce(radius, inside) {\n    let x = this.x - this.s.width/2;\n    let y = this.y - this.s.height/2;\n    if (\n      inside && x*x + y*y > radius * radius ||\n      !inside && x*x + y*y < radius * radius\n    ) {\n    \n      // https://math.stackexchange.com/a/611836\n      let nx = x / this.s.sqrt(x*x + y*y);\n      let ny = y / this.s.sqrt(x*x + y*y);\n      let vx = this.velX;\n      let vy = this.velY;\n      this.velX = (ny*ny - nx*nx)*vx - 2*nx*ny*vy;\n      this.velY = (nx*nx - ny*ny)*vy - 2*nx*ny*vx;\n    \n    }\n  }\n  \n}\n\n\n\n\n\n\nNaturally, I will use this to draw a donut:\n\ncreateSketch(s => {\n\n  let n = 100;\n  let dot;\n  let dotList = [];\n  let palette = [\n    s.color(\"#6B1B00\"),\n    s.color(\"#AE8B70\"),\n    s.color(\"#F9FEFB\"),\n    s.color(\"#56382D\") \n  ];\n\n  s.setup = function() {\n    s.createCanvas(746, 746);\n    for(let i = 0; i < n; i++) {\n      let angle = s.random(0, s.TWO_PI);\n      let radius = s.width * s.random(.12, .33);\n      dotList.push(dot = new Dot(\n        s,\n        s.width/2 + s.cos(angle) * radius,\n        s.height/2 + s.sin(angle) * radius,\n        s.random(palette),\n        s.random(1, 5)\n      ));\n    }\n  };\n    \n  s.draw = function() {\n    dotList.map(dot => {\n      dot.on();\n      dot.move();\n      dot.bounce(s.width * .35, true);\n      dot.bounce(s.width * .1, false);\n    });\n  };\n})\n\n\n\n\n\n\nMmmm…. donuts."
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html",
    "href": "posts/2021-04-18_pretty-little-clis/index.html",
    "title": "Pretty little CLIs",
    "section": "",
    "text": "Anytime you write R code whose output needs to be understood by a human being, it is an act of kindness to spend a little time making sure that the output shown to the human being properly communicates with that human. As a consequence of this, you often find yourself needing to write information to the R console, just to cater to those precious human sensibilities. Perhaps the simplest way to do this is to use the cat() function. It’s a simple tool and it gets the job done in most cases.\nFor example, consider the use case for the antagonist character “A” from Pretty Little Liars, whose stalking and threats were delivered mostly via text message. Had she used R to craft her threatening text messages, she could have written code like this:\nEquipped with a function that specifies her threat, complete with a dramatic pause for effect, she’s ready to go. When her unwitting victim does something to trigger the send_cat_threat() function, a two part message is displayed on the console. The first part shows up immediately\nand after a two second delay, her call sign is revealed\nIt’s not too difficult to imagine what this message might look like at the R console, but where’s the fun in that? Thanks to the asciicast package (Csárdi et al. 2019), there’s no need to leave anything to the imagination, and we can see the malevolent message in screencast form:\nUsing cat() to craft messages works perfectly well for simple text communication, but sometimes you want something that looks a little fancier. After all, if the big picture plan here is to impersonate a dead teenager and terrorise her friends - and for some reason you’ve chosen R to do so - you might as well put a little effort into the details, right?"
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#meet-the-cli-package",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#meet-the-cli-package",
    "title": "Pretty little CLIs",
    "section": "Meet the cli package",
    "text": "Meet the cli package\nOne thing I love about the R community is that if you search long enough you’ll find that someone else has already written a package that solves the problem you’re facing. If your problem is “how to craft nicely formatted messages” then you’ll be delighted to learn that many wonderful things become possible if you have the cli package (Csárdi 2021a) as your talented assistant. To craft a beautiful command line interface (CLI) of her very own, the first thing A will need to do is load the package:\n\nlibrary(cli)\n\nOnce this is done, it is a very trivial task for A to write the same threatening text message using cli_text()…\n\nsend_cli_threat <- function() {\n  cli_text(\"Dead girls walking.\"); wait()\n  cli_text(\"--A.\")\n}\nsend_cli_threat()\n\n\n\n\n\n\n…which is nice and all, but it doesn’t make much of a case for using cli. Stalking and threatening is busy work, and I’d imagine that A would want a more compelling justification before deciding to switch her evil workflow. However - much like A herself - the R console has many dark secrets, and fancier tricks than this are possible once you know how to expose them using cli."
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#using-the-status-bar",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#using-the-status-bar",
    "title": "Pretty little CLIs",
    "section": "Using the status bar",
    "text": "Using the status bar\nOne piece of magic that I have wondered about for a long time is how fancy progress bars work: often when you’re doing something that takes a long time, you’ll see an ASCII progress bar rendered in text on the screen, which suddenly vanishes once the process is complete. How exactly does this work? Normally you can’t “unprint” a message from the console, so how is it possible for the progress bar to update without leaving an ugly trail of earlier messages behind it?\nWhile teaching myself cli, I found the answer. The most recent line of text generated at the terminal is speciall. It’s called the status bar: the state of the status bar can be manipulated, and the cli package provides a neat toolkit for doing so. So let’s say I were trying to convince A to switch to the cli tools. Right now, she’s writing a function that will send a four-part message, using cli_text() because I’ve at least convinced her to try the new tools:\n\nmessage_scroll <- function() {\n  cli_text(\"You found my bracelet.\"); wait()\n  cli_text(\"Now come find me.\"); wait()\n  cli_text(\"Good luck bitches.\"); wait()\n  cli_text(\"-A\"); wait()\n}\nmessage_scroll()\n\nWhen her victim triggers this message the lines will appear on screen, one after the other with an appropriate dramatic pause separating them. The victim might see something that looks like this:\n\n\n\n\n\nThe problem – when viewed from an evil point of view – is that this message stays on screen after delivery.1 The victim has time to think about it, take a screenshot to show her friends, that kind of thing. Wouldn’t the gaslighting be so much more effective if she were to send the message piece by piece, each part disappearing as the next one appears, only to have the whole thing vanish without a trace and leaving the victim wondering if she imagined the whole thing? This is where the status bar comes in handy. Here’s how it would work:\n\nmessage_inline <- function() {\n  id <- cli_status(\"\")\n  cli_status_update(id, \"You found my bracelet.\"); wait()\n  cli_status_update(id, \"Now come find me.\"); wait()\n  cli_status_update(id, \"Good luck bitches.\"); wait()\n  cli_status_update(id, \"-A\"); wait()\n  cli_status_clear(id)\n}\n\nThe first line in this function uses cli_status() to create a blank message on the status bar, and returns an identifier that refers to the status bar. The next four lines all use cli_status_update() to overwrite the current state of the status bar, and then pause dramatically for two seconds. In a final act of malice, the last line in the function clears the status bar using cli_status_clear(), leaving nothing except a blank space behind. So what the victim sees is something more like this:\n\nmessage_inline()\n\n\n\n\n\n\n\n\nThis message was sent to Aria in episode 10 of season one. I’m sure it is deeply important to everyone that I mention this."
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#creating-spinners",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#creating-spinners",
    "title": "Pretty little CLIs",
    "section": "Creating spinners",
    "text": "Creating spinners\nThe ability to control the status bar opens up a world of new possibilities. Progress bars are one such possibility, but the progress package (Csárdi and FitzJohn 2019) already does this nicely, and in any case I suspect that A might be more intrigued by the possibility of spinners, since they just spin and spin and give the victim no clue about when the process is going to end. Much more appealing when the developer doesn’t know (or doesn’t want to reveal) when the wait will end. The cli package has a nice makes_spinner function that serves this purpose. Here’s an example:\n\nspinny <- make_spinner(\n  which = \"dots2\",\n  template = \"{spin} It's not over until I say it is.\"\n)\n\nThe which argument is used to choose how the spinner would look, and the template argument is used to define how the “spinny bit” is placed relative to the rest of the text. The spinny object includes functions to update the state of the spinner (in this case spinny$spin() would be that function), and a function to clear the spinner from the status bar. So here’s how A might define a function that uses a spinner to keep the victim in suspense…\n\ntheatrics <- function(which) {\n  \n  # define the spinner\n  spinny <- make_spinner(\n    which = which,\n    template = \"{spin} It's not over until I say it is.\"\n  )\n  \n  # update the spinner 100 times\n  for(i in 1:100) {\n    spinny$spin()\n    wait(.05)\n  }\n  \n  # clear the spinner from the status bar\n  spinny$finish()\n  \n  # send the final part of the message\n  cli_alert_success(\"Sleep tight while you still can, bitches. -A\")\n}\n\nHere’s what happens:\n\ntheatrics(\"dots2\")\n\n\n\n\n\n\n\n\nThis message was sent to all four of the liars in the final episode of season one. I don’t think A used a spinner though, which feels like a missed opportunity to me\nSetting which = \"dots2\" is only one possibility. There are quite a lot of different spinner types that come bundled with the cli package, and I’d imagine A would want to look around to see which one suits her needs. Personally, I’m a fan of hearts:\n\ntheatrics(\"hearts\")\n\n\n\n\n\n\nTo see the full list use the list_spinners() function:\n\nlist_spinners()\n\n [1] \"dots\"                \"dots2\"               \"dots3\"              \n [4] \"dots4\"               \"dots5\"               \"dots6\"              \n [7] \"dots7\"               \"dots8\"               \"dots9\"              \n[10] \"dots10\"              \"dots11\"              \"dots12\"             \n[13] \"line\"                \"line2\"               \"pipe\"               \n[16] \"simpleDots\"          \"simpleDotsScrolling\" \"star\"               \n[19] \"star2\"               \"flip\"                \"hamburger\"          \n[22] \"growVertical\"        \"growHorizontal\"      \"balloon\"            \n[25] \"balloon2\"            \"noise\"               \"bounce\"             \n[28] \"boxBounce\"           \"boxBounce2\"          \"triangle\"           \n[31] \"arc\"                 \"circle\"              \"squareCorners\"      \n[34] \"circleQuarters\"      \"circleHalves\"        \"squish\"             \n[37] \"toggle\"              \"toggle2\"             \"toggle3\"            \n[40] \"toggle4\"             \"toggle5\"             \"toggle6\"            \n[43] \"toggle7\"             \"toggle8\"             \"toggle9\"            \n[46] \"toggle10\"            \"toggle11\"            \"toggle12\"           \n[49] \"toggle13\"            \"arrow\"               \"arrow2\"             \n[52] \"arrow3\"              \"bouncingBar\"         \"bouncingBall\"       \n[55] \"smiley\"              \"monkey\"              \"hearts\"             \n[58] \"clock\"               \"earth\"               \"moon\"               \n[61] \"runner\"              \"pong\"                \"shark\"              \n[64] \"dqpb\""
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#showing-cli-messages-in-r-markdown",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#showing-cli-messages-in-r-markdown",
    "title": "Pretty little CLIs",
    "section": "Showing cli messages in R markdown",
    "text": "Showing cli messages in R markdown\nThroughout this post I’ve relied on asciicast to display screencasts of the R console as animated SVG files, rather than what I might normally do and rely on regular R markdown code chunks to do the work. There’s a reason for this: the R console is a terminal, and its behaviour doesn’t always translate nicely to HTML. Part of the magic of the rmarkdown package (Xie, Allaire, and Grolemund 2018) is that most of the time it is able to capture terminal output and translate it seamlessly into HTML, and we mere mortal users never notice how clever this is. However, when dealing with cli output, we run into cases where this breaks down and the law of leaky abstractions comes into play: text generated at the R console does not follow the same rules as text inserted into an HTML document, and R Markdown sometimes needs a little help when transforming one to the other.\nAn important thing to remember about cli is that the text it produces is a message, so its visibility in R Markdown depends on the chunk option for messages. As long as the message option is set to TRUE, R Markdown will include them as part of the output.2 In the simplest case, R Markdown works nicely, so as long as all A wants to do is send an unformatted threat within an R Markdown document, then this works:\n\ncli_text(\"I'm still here bitches, and I know everything. -A\")\n\nI'm still here bitches, and I know everything. -A\n\n\nHowever, the moment A tries to use any fancy formatting, things will go haywire for her. For example, suppose she wanted to send the message above as a simple “alert” message using cli_alert(), which uses fancy symbols and colours in the output. It is at this point that the cracks in the R Markdown pipeline start to leak. In this case, the leak would result in the document failing to knit and an error message complaining about\nPCDATA invalid Char value\nIntuitively she might guess that somewhere in the R Markdown pipeline, an invalid or malformed character has been created.3 The reason this happens is that the colours and symbols used by cli, and supported in the R console, rely on ANSI escape codes, but those escape codes aren’t recognised in HTML and – apparently – they can wreak havoc when R markdown writes those characters into the HTML document. ANSI colours in R are usually generated with the help of the crayon package (Csárdi 2021b), and per the issue #24 thread that I encounter on a semi-regular basis, it can be tricky to manage the process of translating these to HTML via R Markdown.\nSolving this issue requires A to jump through a few hoops. It’s annoying I know, but no-one ever said that running an unhinged stalking campaign via text messages was easy, right? Her first task is to make sure that the R Markdown document turns on crayon support:\n\noptions(crayon.enabled = TRUE)\n\nThis isn’t the whole solution, however, because while that tells R Markdown to stop ignoring all the ANSI stuff, it doesn’t necessarily allow it to render ANSI sequences properly. To fix this she needs to specify the knit hooks that explicitly tell R Markdown what to do. She can do this with the help of the fansi package (Gaslam 2021), which contains an obscurely-named function sgr_to_html() that translates a subset of the ANSI control sequences to HTML, and strips out all the others. Using this, she can write an ansi_aware_handler() function that will take an input string x and return HTML output appropriate for the R Markdown context:\n\nansi_aware_handler <- function(x, options) {\n  paste0(\n    \"<pre class=\\\"r-output\\\"><code>\",\n    fansi::sgr_to_html(x = x, warn = FALSE, term.cap = \"256\"),\n    \"</code></pre>\"\n  )\n}\n\nFrom there, it’s relatively easy. All she needs to do is tell knitr (Xie 2021) to use this function whenever it needs to handle output. Just for good measure she might do the same for messages, errors, and warnings:\n\nknitr::knit_hooks$set(\n  output = ansi_aware_handler, \n  message = ansi_aware_handler, \n  warning = ansi_aware_handler,\n  error = ansi_aware_handler\n)\n\nAt long last she is done.4 Her campaign of bullying and cruelty can continue:\n\ncli_alert(\"I'm still here bitches, and I know everything. -A\")\n→ I'm still here bitches, and I know everything. -A\n\n\n\n\nThis message was sent in the pilot episode. Yes, the quotes I’ve used are all from season one: I’ve just started a rewatch of the show, so the early episodes are quite fresh in my memory!"
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#writing-longer-messages",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#writing-longer-messages",
    "title": "Pretty little CLIs",
    "section": "Writing longer messages",
    "text": "Writing longer messages\nUp to this point the threatening messages that A has been sending have been short, only one line long. In several cases the messages have been cleverly constructed so that the same line (the status bar) is used to display multiple pieces of text, but ultimately it’s still one line messaging. A needs to take a little care when she wants to branch out. Conceptually, a message should correspond to “one semantically meaningful bundle of information” that might be split over several lines. However, as far as R is concerned, each call to cli_text() creates a distinct message. To see how this might cause A some grief, here’s the letter that she sent to Aria’s mother announcing the infidelity of Aria’s father:\n\nsend_cruel_letter_piecewise <- function() {\n  cli_text('Your husband, Byron, is involved with another woman')\n  cli_text('and when I say involved I mean in a \"romantic\" way.')\n  cli_text('This is not something recent. It started before your')\n  cli_text('family went away to Iceland and from the look of')\n  cli_text('things, it may be starting up again now that you\\'re')\n  cli_text('back. I know this is hard to hear, but it is the')\n  cli_text('truth. If you don\\'t believe this about your husband,')\n  cli_text('ask your daughter. She knows all about it.')\n  cli_text('Sincerely,')\n  cli_text('A')\n}\n\nsend_cruel_letter_piecewise()\nYour husband, Byron, is involved with another woman\n\nand when I say involved I mean in a \"romantic\" way.\n\nThis is not something recent. It started before your\n\nfamily went away to Iceland and from the look of\n\nthings, it may be starting up again now that you're\n\nback. I know this is hard to hear, but it is the\n\ntruth. If you don't believe this about your husband,\n\nask your daughter. She knows all about it.\n\nSincerely,\n\nA\n\n\nThis is not an ideal implementation. What A wants to send is one message spanning 10 lines not 10 separate one-line messages, but it’s the latter that she has actually implemented here. This is where the cli() function is handy: to takes an expression as input and collects all the constituent parts into a single message. This version of the function now sends a single message:\n\nsend_cruel_letter_singly <- function() {\n  cli({\n    cli_text('Your husband, Byron, is involved with another woman')\n    cli_text('and when I say involved I mean in a \"romantic\" way.')\n    cli_text('This is not something recent. It started before your')\n    cli_text('family went away to Iceland and from the look of')\n    cli_text('things, it may be starting up again now that you\\'re')\n    cli_text('back. I know this is hard to hear, but it is the')\n    cli_text('truth. If you don\\'t believe this about your husband,')\n    cli_text('ask your daughter. She knows all about it.')\n    cli_text('Sincerely,')\n    cli_text('A')\n  })\n}\n\nsend_cruel_letter_singly()\nYour husband, Byron, is involved with another woman\nand when I say involved I mean in a \"romantic\" way.\nThis is not something recent. It started before your\nfamily went away to Iceland and from the look of\nthings, it may be starting up again now that you're\nback. I know this is hard to hear, but it is the\ntruth. If you don't believe this about your husband,\nask your daughter. She knows all about it.\nSincerely,\nA\n\n\n\n\nThe letter was sent to Ella in episode four season one. Even on a rewatch I’m finding it impossible to imagine Holly Marie Combs as anyone other than Piper from Charmed and I keep expecting “Ella” to stop time and, idk, shave off her husbands eyebrows or something?\nMuch nicer. As every would-be tormenter knows, it’s important to pay attention to the details."
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#creating-structured-messages",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#creating-structured-messages",
    "title": "Pretty little CLIs",
    "section": "Creating structured messages",
    "text": "Creating structured messages\nWriting long messages when sending a threatening letter is a simple enough thing, but at some point A will likely find herself wanting to add some structure to these missives. Lists are nice. Stalkers like keeping lists, I hear. With that in mind, a nice property of cli is that it allows you to separate style from structure using an HTML-like syntax. Top level headings are specified using cli_h1(), and second level headings are produced by cli_h2(). Unordered lists are produced using cli_ul() and ordered lists by cli_ol(). This make it easy to write structured messages to the R console:\n\ncli({\n  cli_h1(\"Characters\")\n  cli_h2(\"The Liars\")\n  cli_ul(c(\n    \"Alison DiLaurentis\",\n    \"Spencer Hastings\",\n    \"Aria Montgomery\",\n    \"Hanna Marin\",\n    \"Emily Fields\"\n  ))\n  cli_h2(\"The A-Team\")\n  cli_ul(c(\n    \"Mona Vanderwaal\",\n    \"Lucas Gottesman\",\n    \"Melissa Hastings\"\n  ))\n})\n\n── Characters ──────────────────────────────────────────────────────────────────\n\n── The Liars ──\n\n• Alison DiLaurentis\n• Spencer Hastings\n• Aria Montgomery\n• Hanna Marin\n• Emily Fields\n\n── The A-Team ──\n\n• Mona Vanderwaal\n• Lucas Gottesman\n• Melissa Hastings\n\n\nBetter yet, the cli package has a whole swathe of other utilities that follow this same HTML-like naming scheme, making it possible to send elaborate and disturbing messages in so many different ways."
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#epilogue",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#epilogue",
    "title": "Pretty little CLIs",
    "section": "Epilogue",
    "text": "Epilogue\nThere is a lot more to the cli package that I haven’t talked about in this post. I’ve not talked about how to modify the themes, how to create custom cli “apps” that use different themes or send output to different connections. I’ve not talked about how to use conditional logic within a cli call, displaying different messages depending on whether a process succeeds or fails. Those will have to remain secret for now, because this post is quite long enough already and quite frankly I’m still learning myself. Besides, these powers would no doubt would be put to terrible purposes in an R-themed Pretty Little Liars spinoff show, and I’m not entirely sure that all secrets need sharing…\n\ncli(\n  cli_blockquote(\n    quote = \"Friends share secrets, that's what keeps us close\",\n    citation = \"Alison\"\n  )\n)\n\n    “Friends share secrets, that's what keeps us close”\n    — Alison"
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#last-updated",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#last-updated",
    "title": "Pretty little CLIs",
    "section": "Last updated",
    "text": "Last updated\n\n2022-08-23 13:12:32 AEST"
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#details",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#details",
    "title": "Pretty little CLIs",
    "section": "Details",
    "text": "Details\n\nsource code, R environment"
  },
  {
    "objectID": "posts/2021-08-08_git-credential-helpers/index.html",
    "href": "posts/2021-08-08_git-credential-helpers/index.html",
    "title": "Managing GitHub credentials from R, difficulty level linux",
    "section": "",
    "text": "There are days when I regret switching to linux as an R user. It’s not that I’m particularly enamoured of Apple or Microsoft, and I do enjoy the freedom to tinker that linux systems provide, but without the same resourcing that underpins Windows or Mac OS, I do spent a disproportionate amount my time trying to make my long-suffering Ubuntu laptop do something that would “just work” if I’d gone with one of the more traditional options. But such is life, and besides, there’s a case to be made that the time I spend on these things is not wasted: usually, I end up learning something useful.\nThis is one of those stories."
  },
  {
    "objectID": "posts/2021-08-08_git-credential-helpers/index.html#the-story-is-quite-short",
    "href": "posts/2021-08-08_git-credential-helpers/index.html#the-story-is-quite-short",
    "title": "Managing GitHub credentials from R, difficulty level linux",
    "section": "The story is quite short…",
    "text": "The story is quite short…\n\nUsing GitHub credentials with R\nFor some years now I have been using git repositories for version control, with some ambivalence to my feelings. I absolutely love version control, and I think GitHub is a fabulous tool, but git itself gives me headaches. It feels counterintuitive and untidy, and I am resistant to learning new git tricks because of that. However, now that GitHub is moving to end password authentication for git operations, I find myself needing to do precisely that. Sigh.\nLike many R users, whenever I encounter a git problem my first impulse is to see whether Happy Git and GitHub for the useR (Bryan 2018) can help me out, and true to form, it can. Having decided that I will revert to being an https girl, renouncing my flirtation with ssh, I’ve found the chapter on caching https credentials extremely useful. The usethis article on git credentials is also worth the read.\nThe problem can be broken into three parts:\n\nHow do I set up an authentication token on my GitHub account?\nHow do I configure my git installation to use the authentication token?\nHow do I ensure that R detects these credentials?\n\nThanks to the fabulous work of the tidyverse team, it’s possible for R users to solve the problem in a fairly painless way. The solution has been documented repeatedly, but for the sake of completeness I’ll repeat the advice here.\n\n\nSetting up the credentials\nThe first thing you’ll need to do is set up a GitHub token. You can do this on the GitHub website, but for an R user it’s probably easiest to use the usethis package (Wickham and Bryan 2021):\n\nusethis::create_github_token()\n\nThis will open GitHub in a browser window, take you to the “create a new token page”, and pre-populate all the fields with sensible default values. After accepting these values, the token is created and you’ll be given a PAT, a “personal authentication token”. It’ll look something like this…\nghp_dgdfasdklfjsdklfjsadfDKFJASDLKFJ3453\n…and you should immediately save this in a secure password manager, like 1password, lastpass, etc, because GitHub will only show it to you this one time. You did save it to your password manager, right? Right? I mean, you might need it again. You really might. Yes, you. All right then. I’ll trust you’ve taken sensible precautions now, so let’s keep going. The next step in the process is to configure your git installation to use your token. This is, once again, quite easy to do with gitcreds (Csárdi 2020):\n\ngitcreds::gitcreds_set()\n\nWhen you call this function interactively, R will ask for your PAT. Paste it into the console, hit enter, and you are done. Your git installation is now configured to use the token. Yay! Let’s move onto the third step, which is to ensure that R will recognise and use these credentials. As it turns out, step three doesn’t require you to do anything, because it happens automatically! Functions like usethis::pr_push() recognise your credentials as soon as gitcreds sets them up, and everything works perfectly…\n\n\n Quinn. (Figure from giphy.com)"
  },
  {
    "objectID": "posts/2021-08-08_git-credential-helpers/index.html#unless-youre-on-linux",
    "href": "posts/2021-08-08_git-credential-helpers/index.html#unless-youre-on-linux",
    "title": "Managing GitHub credentials from R, difficulty level linux",
    "section": "… unless you’re on linux",
    "text": "… unless you’re on linux\nIf you’re on linux, you might find yourself in the same boat I was. The credentials you just set up work flawlessly for about 15 minutes, at which time R complains that it cannot find any credentials and you spend the next 15 minutes crying melodramatically.\nWhen this happened to me I assumed the problem was my R environment. I tried updating gitcreds, usethis, and every other R package I could think of that might possibly be involved in communicating with git. Nothing worked. The reason nothing worked is that the problem wasn’t with R at all… it was git, and in hindsight I realise that the problem is specific to git on linux. All those beautiful people with their fancy Windows and Mac machines won’t run into the problem I encountered. They won’t spend an entire Saturday trying to teach themselves git credential management. They will never know my pain. Curse them and their superior purchasing decisions.\n\n\n Daria. (Figure from giphy.com)\nJust kidding. I love my quirky little Ubuntu box and I have a lot of fun learning how to fix her up every time she sets herself on fire.\n\nWhere did I leave my config?\nOkay, I’m going to need to make changes to my git configuration. Although git makes it possible to store configuration locally, at the repository level, I rarely need this flexibility. The relevant information is stored in the global configuration file: on my machine, this is located at /home/danielle/.gitconfig. I can use git config to list these configuration settings, like this\n\ngit config --global --list\n\nand at the start of this exercise the output would have looked like this:\nuser.name=Danielle Navarro\nuser.email=d.navarro@unsw.edu.au\nI’m not sure why this is, but I always feel slightly more reassured when I’m able to inspect the configuration file itself. Opening my .gitconfig file shows the same information, but the formatting is slightly different in the raw file:\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\nTo solve the git credential problem, we’re going to need to edit this configuration information. Depending on which solution you go with, you might need to install new software too.\n\n\nDon’t forget to update git\nBefore starting, it’s a good idea to make sure you have the latest version of git: older versions may not have the tools you need. As it happens, I had already updated git to the most recent version (2.32.0 at the time of writing), but in case anyone ends up relying on this post, here’s how you do it:\nsudo add-apt-repository ppa:git-core/ppa\nsudo apt update\nsudo apt install git"
  },
  {
    "objectID": "posts/2021-08-08_git-credential-helpers/index.html#three-solutions",
    "href": "posts/2021-08-08_git-credential-helpers/index.html#three-solutions",
    "title": "Managing GitHub credentials from R, difficulty level linux",
    "section": "Three solutions",
    "text": "Three solutions\n\n1. Set a long timeout for the git cache\nRecent versions of git are released with a credential cache that retains your credentials in memory temporarily. The information is never written to disk, and it expires after a time. You can tell git to use this cache as your “credential helper” by typing the following command at the terminal:\n\ngit config --global credential.helper cache\n\nAfter doing this, my .gitconfig file now looks like this:\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\n[credential]\n    helper = cache\nUnfortunately this isn’t an ideal solution, because the cache expires after 900 seconds (15 minutes). As soon as the cache expires, git loses track of your GitHub credentials and so does R. So you have to set the credentials again by calling gitcreds::gitcreds_set() and entering the PAT again. That’s annoying, but you did store the PAT in a password manager right? You were smart. You definitely aren’t going to be foolish like me, forget to store your PAT every time, and end up needing to create a new GitHub token every 15 minutes.\nA simple solution to this problem is to ask git to store information in the cache for just a teeny tiny little bit longer. Instead of having the cache expire after the default 900 seconds, maybe set it to expire after 10 million seconds. That way, you’ll only have to refresh the cache using gitcreds::gitcreds_set() once every four months instead of four times an hour. Implementing this solution requires only one line of code at the terminal:\n\ngit config --global credential.helper 'cache --timeout=10000000'\n\nAfter typing this, my .gitconfig file looks like this:\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\n[credential]\n    helper = cache --timeout=10000000\nIn some ways this is a bit of a hack. If cache expiry normally happens every 15 minutes, there’s something a little odd about dragging it out and making it hang around for 16 weeks. That being said, I’ve done many stranger things than this in my life. It may not be the most elegant way to solve the problem, but it works.\n\n\n Trent. (Figure from giphy.com)\n\n\n2. Use libsecret credential manager\nIt puzzled me slightly that this problem only exists for linux computers, so I did a little more reading on how git manages credentials. It turns out you don’t have to rely on the in-memory cache: you can tell git to use some other program to supply the credentials. This is what all those swanky Mac and Windows people have been doing all along. On Macs, for example, git defaults to using the OS X keychain to store credentials safely on disk. It’s possible to do the same thing on linux using libsecret (source on gitlab) and thankfully it’s not much harder to set this up than to use the “long cache” trick described in the previous section.\nThe first step is ensuring libsecret is installed on your machine. It probably is (or at least, it was on my Ubuntu 20.04 box), but in case it isn’t here’s the command you need\n\nsudo apt install libsecret-1-0 libsecret-1-dev\n\nIt helps to realise that libsecret isn’t an application designed to work with git (i.e., it’s not the credential manager), nor is it the keyring where the passwords are stored. Rather, it’s a library that communicates with the keyring: I found this post useful for making sense of it. So if we want to use libsecret to access the keyring, we’re going to need a git credential manager that knows how to talk to libsecret. As it turns out, git comes with one already, you just have to build it using make:\n\ncd /usr/share/doc/git/contrib/credential/libsecret\nsudo make\n\nThis will build the git-credential-libsecret application for you and now all you have to do is tell git to use this as the “credential helper” application that supplies the GitHub credentials:\n\ngit config --global credential.helper \\\n  /usr/share/doc/git/contrib/credential/libsecret/git-credential-libsecret\n\nAfter typing that, my .gitconfig file looks like this…\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\n[credential]\n    helper = /usr/share/doc/git/contrib/credential/libsecret/git-credential-libsecret\n… and I’m all set and ready to go.\nOne thing I found handy during this step is to check that R was reading the correct configuration information. It’s possible to do this with gitcreds:\n\ngitcreds::gitcreds_list_helpers()\n\n\n\n[1] \"/usr/share/doc/git/contrib/credential/libsecret/git-credential-libsecret\"\n\n\nIn any case, if all the applications are talking to each other properly, the next time you call gitcreds::gitcreds_set() they’ll all send the message along: R will pass your PAT to git, git will pass it to git-credential-libsecret, git-credential-libsecret will pass it to libsecret, and the PAT will end up in your linux keychain. Whenever you need to authenticate and push some commits up to GitHub from R, it should find the credentials using the same communication channel. Everything should work swimmingly.\n\n\n Quinn et al. (Figure from giphy.com)\n\n\n3. Use GCM core\nAs far as I can tell, the libsecret credential manager is a perfectly good solution to the problem, but in the end I made a different choice: I decided to go with “git credential manager core”, or GCM Core. It’s developed by Microsoft and, perhaps unsurprisingly, it is what GitHub currently recommends. It’s slightly more painful to set up, and the installation instructions are different depending on what flavour of linux you’re running. Because I’m on Ubuntu 20.04, I downloaded the .deb file associated with the most recent release of GCM core, and then installed the application using the dpkg command:\n\nsudo dpkg -i <path-to-deb-file>\n\nThis will build GCM core on your system, and once that’s done you can ask it to take care of the git configuration for you:\n\ngit-credential-manager-core configure\n\nThis will edit the .gitconfig file, so for me it now looks like this:\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\n[credential]\n    helper = \n    helper = /usr/bin/git-credential-manager-core\n[credential \"https://dev.azure.com\"]\n    useHttpPath = true\nIn a happier world you would be done at this point, but we don’t live in a happy world. We live in a sick sad world that has global pandemics and pineapple on pizzas. So there’s still one job left to do.\nMuch like the libsecret credential manager I built in the previous section, GCM core is “just” a git credential manager: it communicates with git, but it isn’t a password manager or a keyring, and it doesn’t store the PAT itself. Instead, it offers you several different options for how the PAT is to be stored. If you click through and take a look at the list, the first suggested option is to connect to a secret service API. As far as I can tell “secret service” isn’t an application, it’s a specification, and in practice it’s just a fancy way of referring to a linux keychain. Just as the libsecret credential manager needs some way of communicating with the keychain (i.e., the libsecret library itself), GCM core needs an intermediary. In fact, it turns out GCM core also uses libsecret to talk to the keychain. So that’s the option I went with. The terminal command to set this up is this:\n\ngit config --global credential.credentialStore secretservice\n\nAfter running the command, my .gitconfig file looks like this:\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\n[credential]\n    helper = \n    helper = /usr/bin/git-credential-manager-core\n    credentialStore = secretservice\n[credential \"https://dev.azure.com\"]\n    useHttpPath = true\n\n\n Jane. (Figure from giphy.com)\nAs before, I can check that R is reading the correct configuration information…\n\ngitcreds::gitcreds_list_helpers()\n\n[1] \"/usr/local/share/gcm-core/git-credential-manager-core\"\n\n\n…and now I’m ready to go. My problems are solved. The sun is shining, the birds are singing, and git is working properly from R again. All is well in heaven and earth. Oh the sheer excitement of it all. I hope I can contain my boundless enthusiasm and joy.\n\n\n\n Daria. (Figure from giphy.com)"
  },
  {
    "objectID": "posts/2023-03-13_shattered-landscapes/index.html",
    "href": "posts/2023-03-13_shattered-landscapes/index.html",
    "title": "Shattered landscapes",
    "section": "",
    "text": "Update: I made a little shiny app based on this post: djnavarro.shinyapps.io/shattered-landscapes (source code). It doesn’t implement the complete system because rayshader causes out of memory problems for my free tier shinyapps account, but in other respects it’s the same thing.\nIn the last few weeks I’ve been tinkering with a generative art system I ended up calling Broken Lands. It creates maps of bizarre and impossible landscapes in R, using the ambient package to generate the topography, and rayshader to render shadows cast by a hypothetical light source. It creates images like these:\nTo my eye, at least, these images are both beautiful and tragic. I cannot help but interpret them as coastal landscapes in an alien geography of some kind, a land that has suffered some cataclysm like the Doom of Valyria or the Fall of Istar. The contours feel too contorted to be the result of any terrestrial process, and – again, by my interpretation – there’s a tension between the smoothness of the individual contours and the jagged, chaotic structure of the landscape overall.\nBut what would I know? I wrote the code that makes the system work, but I don’t have a monopoly of interpretation of the images. Death of the author and all that. Barthes would call me the “scriptor” rather than the author, I suppose, which honestly feels about right for generative art. So yeah. The pieces are what they are, quite separate from the artist and from the process by which the system was constructed.\nThat said, if you’re familiar with the R ecosystem you can probably take an educated guess about how I made these pieces. As mentioned at the start, the spatial noise patterns are created using the ambient package and the shadows and three-dimensional look are provided by rayshader. I wrote about both of these packages in my workshop on generative art in R workshop (specifically: ambient art, rayshader art), and those tutorials provide a lot of clues about how these pieces are made. But there are also some details I haven’t talked about before, and in any case it’s always fun to write about art."
  },
  {
    "objectID": "posts/2023-03-13_shattered-landscapes/index.html#starting-simple",
    "href": "posts/2023-03-13_shattered-landscapes/index.html#starting-simple",
    "title": "Shattered landscapes",
    "section": "Starting simple",
    "text": "Starting simple\nI’ll start by building a simple system that doesn’t go very far beyond what I covered in the Art From Code workshop. It’s built using three functions. There’s a new_grid() function used to define a grid of x and y coordinates, a generate_simplex() function used to create spatial noise patterns on such a grid, and a render() function used to create an image. First, the new_grid() function:\n\n\n\n\nnew_grid <- function(n = 1000) {\n  ambient::long_grid(\n    x = seq(0, 1, length.out = n),\n    y = seq(0, 1, length.out = n)\n  )\n}\n\nnew_grid()\n\n# A tibble: 1,000,000 × 2\n       x       y\n   <dbl>   <dbl>\n 1     0 0      \n 2     0 0.00100\n 3     0 0.00200\n 4     0 0.00300\n 5     0 0.00400\n 6     0 0.00501\n 7     0 0.00601\n 8     0 0.00701\n 9     0 0.00801\n10     0 0.00901\n# … with 999,990 more rows\n\n\nThe output appears to be a tibble that contains x and y coordinates.2 This defines the spatial locations that we’ll use to create the image, but we’ll need to assign colours to each of those locations.\n\n\nPainting a canvas with spatial noise\nIn order to do this, we’ll write a function called generate_simplex() that generates interesting patterns of spatial noise:\n\ngenerate_simplex <- function(x, y, seed = NULL) {\n  if(!is.null(seed)) {\n    set.seed(seed)\n  }\n  ambient::fracture(\n    noise = ambient::gen_simplex,\n    fractal = ambient::billow,\n    octaves = 10,\n    freq_init = .02,\n    frequency = ~ . * 2,\n    gain_init = 1,\n    gain = ~ . * .8,\n    x = x,\n    y = y\n  )\n}\n\nThe particular choices I’ve made here came about from trial and error. I played around with a lot of different settings when creating generative art in this style, and these were things I liked. I’m not going to dive into the details here: you can find out more by reading the tutorial on spatial noise art I linked to earlier. For the current post, all I want to highlight is that we can use this function to add a new column to the canvas that defines our artwork:\n\ncanvas <- new_grid() |> \n  dplyr::mutate(paint = generate_simplex(x, y, seed = seed))\n\ncanvas\n\n# A tibble: 1,000,000 × 3\n       x       y paint\n   <dbl>   <dbl> <dbl>\n 1     0 0       -3.96\n 2     0 0.00100 -3.95\n 3     0 0.00200 -3.95\n 4     0 0.00300 -3.94\n 5     0 0.00400 -3.93\n 6     0 0.00501 -3.92\n 7     0 0.00601 -3.91\n 8     0 0.00701 -3.91\n 9     0 0.00801 -3.90\n10     0 0.00901 -3.89\n# … with 999,990 more rows\n\n\nThis canvas object is structured like a lookup table: it’s a data frame with columns specifying x and y coordinates, and it contains a third column that specifies the colour of “paint” that needs to be applied at each coordinate. However, it’s a very structured data frame because the x and y values form a grid. This makes straightforward to flip from this format to a “bitmap” matrix format:3\n\nbitmap <- canvas |> as.array(value = paint)\nbitmap[1:6, 1:6]\n\n      x\ny           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n  [1,] -3.963129 -3.931507 -3.899957 -3.868550 -3.837357 -3.806447\n  [2,] -3.954987 -3.932781 -3.901242 -3.869845 -3.838662 -3.807762\n  [3,] -3.946846 -3.924674 -3.902573 -3.871211 -3.840061 -3.809194\n  [4,] -3.938708 -3.916594 -3.894550 -3.872644 -3.841552 -3.810740\n  [5,] -3.930575 -3.908541 -3.886577 -3.864752 -3.843132 -3.812398\n  [6,] -3.922448 -3.900516 -3.878655 -3.856932 -3.835414 -3.814166\n\n\nA grid of numbers isn’t very pretty to look at, but we will need to create this matrix representation before passing the data to rayshader later. But I’m getting ahead of myself. For now, we can use the image() function to render an image from matrix-formatted data:\n\n\n\n\ncanvas |> \n  as.array(value = paint) |>\n  image(axes = FALSE, asp = 1, useRaster = TRUE)\n\n\n\n\nWe’re a long way from our goal, but at least we now have an output that looks like art rather than a matrix of numbers. It’s a start!\n\n\n\nCasting shadows across the landscape\nThe next step in the process is to define a render() function that will take an “elevation” matrix as input, but instead of drawing a “heat map” like image() does, it renders it as a three-dimensional topographic map with shadows cast by a hypothetical light source. This is surprisingly easy to do using rayshader. Here’s the function I’ll use in this post:\n\nrender <- function(mat, shades = NULL, zscale = .005) {\n  if(is.null(shades)) {\n    n <- length(unique(mat))\n    shades <- hcl.colors(n, \"YlOrRd\", rev = TRUE)\n  }\n  rayshader::height_shade(\n    heightmap = mat,\n    texture = shades\n  ) |>\n    rayshader::add_shadow(\n      shadowmap = rayshader::ray_shade(\n        heightmap = mat,\n        sunaltitude = 50,\n        sunangle = 80,\n        multicore = TRUE,\n        zscale = zscale\n      ),\n      max_darken = .2\n    ) |>\n    rayshader::plot_map()\n}\n\nI’m not going to go into the specifics: you can find out more by reading the tutorial on rayshader art I linked to earlier. For this post, I’m simply going to show you what it does. Taking the canvas data as input, we first use as.array() to switch from a “data frame style” representation to a “matrix style” representation, and then pass the matrix to render():\n\ncanvas |>\n  as.array(value = paint) |>\n  render()\n\n\n\n\nAgain, still a long way from our desired goal, but we are making progress. Thanks to rayshader, we have output that looks like a shaded topographic map.\n\n\n\nMaking islands from the landscape\nAt this point we have the ability to generate landscapes, but the images just look like a bunch of hills. They don’t have the “coastal” feeling that the original images did. We can create islands by setting a “sea level”. You can do this in a sophisticated way in rayshader using detect_water() and add_water(), but that’s overkill for our purposes. All we really want to do is imagine setting a sea level such that about half the image is “water” and half the image is “land”. To do that we just calculate the median value in the original data:\n\nsea_level <- median(canvas$paint)\n\nFrom there it’s an exercise in using dplyr. Using mutate() we create a new “islands” column whose value is equal to the original value or the sea level, whichever is higher:\n\ncanvas |> \n  dplyr::mutate(\n    islands = dplyr::if_else(\n      condition = paint < sea_level,\n      true = sea_level, \n      false = paint\n    )\n  ) |>\n  as.array(value = islands) |>\n  render()\n\n\n\n\nEt voilà! We have a generative art system that creates fictitious topographic maps of coastal islands. It’s still not quite the same thing as the original, but it’s kind of a nice system in itself. If you want to play with it, the complete source code for generating this image is included in the islands.R script accompanying this post.\n\n\n\nTweaking the spatial noise generator\nIf you do end up playing around, a really useful way to create variations on this system is to modify the function that generates the spatial noise patterns. For example, this generate_fancy_noise() function is awfully similar to the noise generator I used in the Broken Lands series:\n\ngenerate_fancy_noise <- function(x, y, seed = NULL) {\n  if(!is.null(seed)) {\n    set.seed(seed)\n  }\n  z <- ambient::fracture(\n    noise = ambient::gen_worley,\n    fractal = ambient::billow,\n    octaves = 8,\n    freq_init = .1,\n    frequency = ~ . * 2,\n    gain_init = 3,\n    gain = ~ . * .5,\n    value = \"distance2\",\n    x = x,\n    y = y\n  )\n  ambient::fracture(\n    noise = ambient::gen_simplex,\n    fractal = ambient::billow,\n    octaves = 10,\n    freq_init = .02,\n    frequency = ~ . * 2,\n    gain_init = 1,\n    gain = ~ . * .8,\n    x = x + z,\n    y = y + z\n  )\n}\n\nHere it is in action:\n\nnew_grid() |>\n  dplyr::mutate(\n    height = generate_fancy_noise(x, y, seed = seed),\n    islands = dplyr::if_else(\n      condition = height < median(height),\n      true = median(height),\n      false = height\n    )\n  ) |>\n  as.array(value = islands) |>\n  render(zscale = .01)\n\n\n\n\nVery pretty. I suspect that had I isolated this particular noise generator earlier in the artistic process – rather than figuring out in hindsight that this was what I’d been using all along – I might have stopped here, and not bothered with any of the other tricks that I used.4 But of course this is a post-mortem deconstruction, not a description of the bizarrely tangled artistic process I actually followed, so there are more layers to come…"
  },
  {
    "objectID": "posts/2023-03-13_shattered-landscapes/index.html#queering-geography",
    "href": "posts/2023-03-13_shattered-landscapes/index.html#queering-geography",
    "title": "Shattered landscapes",
    "section": "Queering geography",
    "text": "Queering geography\nThe final image in the last section captures something about the overall structure of the Broken Lands images, but it feels wrong in the particulars. It’s too smooth, too fluid, too… natural. It doesn’t have the same feel as the originals. I don’t have the same feeling of alienness that the original pieces have. Where does that not-quite-real feeling come from?\nThe answer to this involves every generative artists favourite trick: curl fields. If you’ve read the tutorial articles I linked to earlier, you’ve encountered these before so I won’t repeat myself by explaining yet again what a curl field is. What I’ll do instead is write a generate_curl() function that takes the original grid of coordinates (in the “base” space) and transforms them to a new set of points (in an “embedding” space) using a curl transformation:5\n\ngenerate_curl <- function(x, y, seed = NULL) {\n  if(!is.null(seed)) {\n    set.seed(seed)\n  }\n  ambient::curl_noise(\n    generator = ambient::fracture,\n    noise = ambient::gen_simplex,\n    fractal = ambient::fbm,\n    octaves = 3,\n    frequency = ~ . * 2,\n    freq_init = .3,\n    gain_init = 1,\n    gain = ~ . * .5,\n    x = x,\n    y = y\n  )\n}\n\nHere’s what happens when we apply this function:\n\ngrid <- new_grid()\ncoords <- generate_curl(grid$x, grid$y, seed = seed)\nhead(coords)\n\n             x        y\n1 1.417494e-07 2.625000\n2 4.275159e-05 2.624981\n3 1.705772e-04 2.624924\n4 3.836068e-04 2.624829\n5 6.818209e-04 2.624697\n6 1.065192e-03 2.624526\n\n\nThe code here is slightly unpleasant, yes, but I’ll do it in a slightly cleaner way in a moment. What matters right now is the fact that the coords data frame is a transformed version of the grid data. The original (x,y) coordinates in the base space have been transformed to some (x,y) coordinates in some new space.\nA slightly cleaner way of doing this – keeping both the original coordinates and the transformed values – would be as follows:\n\ncanvas <- grid |>\n  dplyr::mutate(\n    curl_x = coords$x,\n    curl_y = coords$y\n  )\n\ncanvas\n\n# A tibble: 1,000,000 × 4\n       x       y      curl_x curl_y\n   <dbl>   <dbl>       <dbl>  <dbl>\n 1     0 0       0.000000142   2.62\n 2     0 0.00100 0.0000428     2.62\n 3     0 0.00200 0.000171      2.62\n 4     0 0.00300 0.000384      2.62\n 5     0 0.00400 0.000682      2.62\n 6     0 0.00501 0.00107       2.62\n 7     0 0.00601 0.00153       2.62\n 8     0 0.00701 0.00209       2.62\n 9     0 0.00801 0.00273       2.62\n10     0 0.00901 0.00345       2.62\n# … with 999,990 more rows\n\n\nOkay that’s nice, but what exactly are these “curl transformed” values? What do they look like? Fair question. Here’s a plot showing what has happened to our nice rectangular grid after the transformation…\n\n\n\n\n\nThis image has an evocative feel, right? Like I’ve taken a regular square sheet of fabric and folded or transformed it in some strange way to create an “embedded” manifold? Well, yeah. That’s precisely what I’ve done.\nOur noise operations will be specified on this transformed/embedded manifold, but – to reveal the ending slightly too soon – the final image will be defined on the base space. The code below shows how to apply the noise operations in the embedding space:\n\ncanvas <- canvas |>\n  dplyr::mutate(\n    height = generate_fancy_noise(curl_x, curl_y, seed = seed),\n    islands = dplyr::if_else(\n      condition = height < median(height),\n      true = median(height),\n      false = height\n    )\n  )\n\ncanvas\n\n# A tibble: 1,000,000 × 6\n       x       y      curl_x curl_y height islands\n   <dbl>   <dbl>       <dbl>  <dbl>  <dbl>   <dbl>\n 1     0 0       0.000000142   2.62 -0.676  -0.676\n 2     0 0.00100 0.0000428     2.62 -0.675  -0.675\n 3     0 0.00200 0.000171      2.62 -0.674  -0.674\n 4     0 0.00300 0.000384      2.62 -0.672  -0.672\n 5     0 0.00400 0.000682      2.62 -0.670  -0.670\n 6     0 0.00501 0.00107       2.62 -0.667  -0.667\n 7     0 0.00601 0.00153       2.62 -0.664  -0.664\n 8     0 0.00701 0.00209       2.62 -0.662  -0.662\n 9     0 0.00801 0.00273       2.62 -0.660  -0.660\n10     0 0.00901 0.00345       2.62 -0.658  -0.658\n# … with 999,990 more rows\n\n\nJust to give you a sense of what that looks like in the embedding space, here’s what happens when we redraw the “manifold” plot from above, with each point coloured using the value of the “islands” variable:\n\n\n\n\n\nYou can sort of see what’s going on here. We have a spatial noise pattern that generated the topography that I showed in the last section, but it’s defined on the embedding space. Our base space is like a rectangular fabric that has been laid and folded over and over onto this embedding space, and then we’ve spray painted this pattern onto the fabric.6 7 8 When we unfold the spray painted fabric and lay it flat again, this is what we get:\n\ncanvas |> \n  as.array(value = islands) |>\n  image(axes = FALSE, asp = 1, useRaster = TRUE)\n\n\n\n\nIt’s a bit like tie-dyeing I guess? That’s what it feels like to me. I’m taking something regular, scrunching it up in a strange way, and then applying the colours to the scrunched up object before unfolding it.\nIn any case, we can use our render() function to add shadows with rayshader:\n\ncanvas |> \n  as.array(value = islands) |>\n  render(zscale = .05)\n\n\n\n\nOkay now that feels like an alien geography to me! It still doesn’t look at all like our final images, but it has the right feel to it. Yes, it’s still a geography of sorts, but it feels stretched and twisted in an unnatural way. It feels… well, it feels painful. Nothing like this can occur naturally without the action of some catastrophic process. That’s what it feels like to me. The “brokenness” of the original images is created by this transformation: natural-ish patterns imposed on a twisted space create bizarre and alien patterns when those contortions are unfolded. It feels weird… it feels strange… it feels queer.9"
  },
  {
    "objectID": "posts/2023-03-13_shattered-landscapes/index.html#artistic-trickery",
    "href": "posts/2023-03-13_shattered-landscapes/index.html#artistic-trickery",
    "title": "Shattered landscapes",
    "section": "Artistic trickery",
    "text": "Artistic trickery\nThe last image in the previous section doesn’t look all that much like the Broken Lands pieces, but – perhaps surprisingly – we’re weirdly close to creating something that really does look like those pieces. There are no deep insights left to explore. From here on out, it’s really just a matter of applying a few artistic tricks. To be precise, there are three little tricks left to document.\n\n\nBe discre[et|te]\nThe first trick is discretisation. So far we’ve been creating images in which the “elevation” of the landscapes vary smoothly. The Broken Lands images don’t do that. Instead, there is a distinct feeling that the lands are terraced. In the original pieces there’s that same unnatural terracing that that you see in open cut mining.10 Creating that look in this system is not difficult. First, I’ll define a discretise() function that takes a continuously-varying vector as input, cuts it into n distinct levels that vary in value between 0 and 1, and returns the discretised values:\n\ndiscretise <- function(x, n) {\n  round(ambient::normalise(x) * n) / n\n}\n\nHere’s an example in which 100 normally distributed numbers are sliced into 5 levels:\n\ndiscretise(rnorm(100), 5)\n\n  [1] 0.6 0.8 0.2 0.4 0.6 0.4 0.6 0.4 0.8 0.6 0.4 0.2 0.2 0.4 0.6 0.6 0.6\n [18] 0.4 0.6 0.4 0.8 0.4 0.4 0.6 0.6 0.4 0.2 0.0 0.4 0.6 0.6 0.4 0.0 0.8\n [35] 0.2 0.2 0.4 0.6 0.4 0.4 0.8 0.4 0.4 0.4 0.4 0.2 0.4 0.2 0.6 0.4 0.8\n [52] 0.6 0.6 0.4 0.6 0.6 0.6 0.8 0.4 0.6 0.4 0.4 0.4 0.8 0.6 0.4 0.4 0.8\n [69] 0.4 0.2 0.4 0.2 0.6 0.2 0.2 0.8 0.4 0.4 0.6 0.4 0.6 0.4 0.8 0.6 0.4\n [86] 0.0 0.6 0.4 1.0 0.4 0.6 0.4 0.8 0.0 0.8 0.4 0.8 0.8 0.2 0.6\n\n\nTo create a discretised version of our alien landscapes, all we have to do is liberally pepper our original code with a few calls to discretise(). Here’s an example:\n\ngrid <- new_grid() \ncoords <- generate_curl(grid$x, grid$y, seed = seed)\n\ncanvas <- grid |> \n  dplyr::mutate(\n    curl_x = coords$x |> discretise(50), \n    curl_y = coords$y |> discretise(50),\n    height = generate_fancy_noise(curl_x, curl_y, seed = seed) |> \n      discretise(50),\n    islands = dplyr::if_else(\n      condition = height < median(height),\n      true = median(height),\n      false = height\n    )\n) \n\ncanvas\n\n# A tibble: 1,000,000 × 6\n       x       y curl_x curl_y height islands\n   <dbl>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n 1     0 0         0.32      1   0.72    0.72\n 2     0 0.00100   0.32      1   0.72    0.72\n 3     0 0.00200   0.32      1   0.72    0.72\n 4     0 0.00300   0.32      1   0.72    0.72\n 5     0 0.00400   0.32      1   0.72    0.72\n 6     0 0.00501   0.32      1   0.72    0.72\n 7     0 0.00601   0.32      1   0.72    0.72\n 8     0 0.00701   0.32      1   0.72    0.72\n 9     0 0.00801   0.32      1   0.72    0.72\n10     0 0.00901   0.32      1   0.72    0.72\n# … with 999,990 more rows\n\n\nIf we plot this as a heatmap, the discrete levels are immediately obvious:\n\ncanvas |> \n  as.array(value = islands) |>\n  image(axes = FALSE, asp = 1, useRaster = TRUE) \n\n\n\n\nThis terracing has the effect of levelling out some of the more bizarre features of the alien landscape we plotted earlier. Here’s what we get when we pass this terraced landscape to our render() function:\n\ncanvas |> \n  as.array(value = islands) |>\n  render(zscale = .01) \n\n\n\n\nAh, yes. Now we have something that feels closer to the Broken Lands pieces. The twists and contortions of the alien landscape are preserved, but they have now been forced onto a flatter, controlled geometry. The chaos of the alien land has been tamed. This is a domesticated variant. Safe for children and capitalists alike.\n\n\n\nBe smooth\nAt some level I appreciate the stark feel of the previous piece, but even I am not enough of a masochist11 to truly enjoy the brutality of what I just did. All those intricate alien swirls have been flattened and erased so crudely that we are left with something a little too minimal for my tastes.\nAnd so to the second artistic sleight-of-hand: some of the starkness of the last piece can be ameliorated if we apply noise processes in both the embedding space (i.e., noise is applied to curl_x and curl_y) and in the base space (i.e., to x and y). The code for that might look a little like this:\n\ngrid <- new_grid() \ncoords <- generate_curl(grid$x, grid$y, seed = seed)\n\ncanvas <- grid |> \n  dplyr::mutate(\n    curl_x = coords$x |> discretise(50), \n    curl_y = coords$y |> discretise(50),\n    noise_curl = generate_fancy_noise(curl_x, curl_y, seed = seed),\n    noise_base = generate_simplex(x, y, seed = seed),\n    height = (noise_curl + noise_base) |> discretise(50),\n    islands = dplyr::if_else(\n      condition = height < median(height),\n      true = median(height),\n      false = height\n    )\n) \n\ncanvas |> \n  as.array(value = islands) |>\n  render(zscale = .01) \n\n\n\n\nThere is no principle to this. No deep underlying logic. It is simply an attempt to paper over the cracks, to smooth out some of the raw, sharp edges that were left over when we discretised in the first step.\nThere is probably a life metaphor here, but I choose not to look too closely.\n\n\n\nBe chaotic\nThe final layer of trickery involves the colour palette. Throughout this post I’ve used the default “yellow and red” palette that image() uses to create heat map images, but the render() function I wrote at the beginning lets you choose your own colour scheme. For instance, let’s say I want the land to vary smoothly along a “teal and green” colour palette, while having the water stay white (or thereabouts). It’s surprisingly straightforward to do this, by passing a hand crafted vector of colours to render():\n\nshades <- hcl.colors(50, \"TealGrn\")\nshades[1] <- \"#ffffff\"\n\ncanvas |> \n  as.array(value = islands) |>\n  render(shades = shades) \n\n\n\n\nThis is so very, very close to the style of imagery in the original Broken Lands series. The only thing missing is a slight feeling of chaos to the colours. If you scroll back up to the top of the post you’ll notice that the original images don’t quite adhere to the smoothly-varying-shades feel of a proper topographic map. The reason for this is that I shuffled the colour palette, so each “level” in the discrete map has a randomly sampled colour from the palette. Here’s some code that does precisely that:\n\ngenerate_shades <- function(palette = \"TealGrn\", n = 50, seed = NULL) {\n  if(!is.null(seed)) {\n    set.seed(seed)\n  }\n  shades <- hcl.colors(n, palette)\n  shades <- sample(shades)\n  shades[1] <- \"#ffffff\"\n  shades  \n}\n\ncanvas |> \n  as.array(value = islands) |>\n  render(shades = generate_shades(seed = seed)) \n\n\n\n\n… and there it is. This version of the system isn’t precisely equivalent to the original, but it mirrors it in every respect that matters to me. The magic is all laid bare. There are no artistic secrets left in this system. Everything you need to know about these images is committed to text. Documented. Described. Codified.\nI love the originals no less now that the magic is revealed. There is no art in secrecy."
  },
  {
    "objectID": "posts/2023-03-13_shattered-landscapes/index.html#goodnight-sweet-dreams",
    "href": "posts/2023-03-13_shattered-landscapes/index.html#goodnight-sweet-dreams",
    "title": "Shattered landscapes",
    "section": "Goodnight, sweet dreams",
    "text": "Goodnight, sweet dreams\n\nOne day we’re gonna wake up laughing  Put on your dancing shoes  You won’t believe the tales I tell  That time, Danielle, ain’t mine to choose  Danielle, Danielle, Danielle      – Tex Perkins (and others, but whatever…)12\n\nMuch like the Broken Lands system itself,13 this post has a peculiar genesis. If you read the strange year post I wrote a few months ago, you’d be unsurprised to hear that I am attempting to square a few circles right now. Something broke – rather badly – and I’m trying to work out how to put the pieces together even knowing that the shattered parts can’t go back together in the shape they were before. Aspects to my life that were once central to my sense of self are scattered, and there are little slivers of glass laid everywhere – when I attempt to pick up one of the pieces from the floor I get cut deeply by those tiny transparent needles.\nThis post is one of those attempts. One of the pieces I need to pick up is my writing. The little cataclysm of 2022 broke my writing. I didn’t become a bad writer, or at least I don’t think I did. I hope I didn’t! Rather, I lost my sense of ownership over my writing. I felt like I was writing for others rather than myself, as if my blog were a product to be optimised rather than a thing I write because I love writing. It’s been some work trying to reconnect with the joy of writing for my own purposes.\nAnd so to the point…\nI wrote this post because I loved creating the artwork, and it’s written the way it is written because it makes me happy to write again. That’s it. It’s something I wrote because I want to own my words again. There’s no “take home message”. There’s no “call to action”. You can love it, or hate it, or ignore it. That’s okay. I didn’t write it for you – I wrote it for me."
  },
  {
    "objectID": "posts/2021-10-19_rtistry-posts/index.html",
    "href": "posts/2021-10-19_rtistry-posts/index.html",
    "title": "Generative art resources in R",
    "section": "",
    "text": "People often ask me if I have any words of advice for young people. No wait, that’s not right. Nobody wants to hear my words of advice for young people, largely because I have none. What they often do ask me is if I have any words of advice for aspiring generative artists who want to use R to make pretty pictures. To be honest, I don’t have a lot of advice there either, but I’ll try my best.\nLet’s start with the big picture: there are no books or general purpose introductions out there. There are no books, no CRAN task views, no courses you can take. In fact, until quite recently generative art in R was an extremely niche topic. To my knowledge, the #rtistry hashtag on twitter is where you’ll find the most art and the most posts about the topic, but that hashtag is pretty new.1 There were resources that existed prior to that, of course: how could there not be? After all, Thomas Lin Pedersen has been building a toolkit for generative art in R for quite some time now. In his keynote talk at celebRation2020, he refers to an “art driven development” process that has led him to create several packages that are valuable to the would-be generative artist. For example:\nThese tools are great, but if you’re just getting started it can be helpful to play around in a more constrained environment. If you want something extremely simple, you could play around with the flametree package I wrote. It’s not very flexible (it just draws branching things!) but it does have the advantage that you can get started with something as simple as this:\nPlaying around with a package like flametree – or jasmines if you want something a little more flexible – is a nice way to start drawing things, but at some point you might want to understand the process involved in creating a system like this. I’ve occasionally used art as a way to help teach people how to program in R, so you might find these programming of aRt slides helpful, and the precursor to flametree is discussed in my slides on functional programming.\nResources like mine can help get you started, but there are many other great artists out there who often post tutorials and walkthroughs. For instance, Antonio Sánchez Chinchón has a lot of really useful tutorials on his blog fronkonstin.com. Ijeamaka Anyene has written a lovely and gentle introduction to her system for rectangle subdivision. Will Chase writes about his process on his blog sometimes: here’s an example on a grid system. Jiwan Heo has a wonderful post on how to get started with flow fields in R among many other lovely posts! You can look outside of the R community too: Tyler Hobbs writes a lot of essays about generative art that describe algorithms in fairly general terms. For instance, one of my systems is built from his essay on simulating watercolour paints. And of course there’s also the walkthrough I wrote for one of my systems here and the piece I wrote that talks a little bit about the psychological process of making art in R.\nMy hope is that these resources will point you in the right direction to get started, but more than anything else I would emphasise that it takes time and effort. Art is a skill like any other. I’ve been practicing for about three years now, and while I am happy with the pieces I make, I still have a lot to learn. And that’s okay – one of the big things I always want to stress is that play is a huge part of the process. Making polished systems comes later!\nIn any case, I’ll leave this post as it is for now but if folks would like to suggest additional resources, I can always update it if need be!"
  },
  {
    "objectID": "posts/2021-10-19_rtistry-posts/index.html#postscript",
    "href": "posts/2021-10-19_rtistry-posts/index.html#postscript",
    "title": "Generative art resources in R",
    "section": "Postscript",
    "text": "Postscript\nOkay, I’m going to start adding things. This is just a completely unstructured list for now, but I know how my brain works: if I don’t bookmark the cool posts and resources I see pop up on my timeline I’ll never find them again…\n\nR specific\n\nThinking outside the grid by Meghan Harris\nGradients repository by Sharla Gelfand\nGenerative art package by Nicola Rennie\nVarious art posts by Claus Wilke\nggbenjamini package by Urs Wilke\nGenerative art examples by Pierre Casadebaig\nThe art in randomness by Dorit Geifman\nGenerative art galleries by Jacquie Tran\nArt portfolio site by Ijeamaka Anyene\nMystery curves by George Savva\n\n\n\nMore general\n\nthatcreativecode.page is a great general resource\nthe description of asemi.ca shows a design process in detail\nTyler Hobbs generative art essays"
  },
  {
    "objectID": "posts/2022-10-18_arrow-flight/index.html",
    "href": "posts/2022-10-18_arrow-flight/index.html",
    "title": "Building an Arrow Flight server",
    "section": "",
    "text": "This is a post about Arrow Flight. I will probably tell a whimsical anecdote to open this post. Or not. Who knows. Maybe I’ll leave the introductory paragraph like this. That would be pretty on-brand for me actually."
  },
  {
    "objectID": "posts/2022-10-18_arrow-flight/index.html#the-what-and-why-of-arrow-flight",
    "href": "posts/2022-10-18_arrow-flight/index.html#the-what-and-why-of-arrow-flight",
    "title": "Building an Arrow Flight server",
    "section": "The what and why of Arrow Flight",
    "text": "The what and why of Arrow Flight\nThe central idea behind flight is deceptively simple: it provides a standard protocol for transferring Arrow data over a network. But to understand why this is a Big Deal, you need to have a good sense of what the Arrow ecosystem is all about. For that, I found it helpful to go all the way back1 to the original announcement of flight by Wes McKinney. Here’s how he explained the motivation:\n\nOur design goal for Flight is to create a new protocol for data services that uses the Arrow columnar format as both the over-the-wire data representation as well as the public API presented to developers. In doing so, we reduce or remove the serialization costs associated with data transport and increase the overall efficiency of distributed data systems. Additionally, two systems that are already using Apache Arrow for other purposes can communicate data to each other with extreme efficiency.\n\nTo put this in context, it helps to have a little recap of how the project has grown: Arrow was originally introduced to provide an efficient and language-agnostic standard for representing tabular data in-memory, but as the project has grown it has necessarily expanded in scope. For example, storing data in-memory is not entirely useful if you can’t manipulate it, so Arrow now supplies a powerful compute engine that underpins both the arrow package in R and the pyarrow library in Python, and several others besides. In other words, the compute engine has been developed to solve a practical data science problem.\nArrow Flight evolved from a similar practical concern. It’s pretty trivial to point out that we live in a networked world now, and as consequence it is hard to avoid situations where the data to be analysed are stored on a different machine than the one that does the analysis. In my earlier posts on reticulate and rpy2 I talked about how to efficiently share an Arrow data set between languages, but I implicitly assumed in those posts that the R process and the Python process were running on the same machine. The moment we have processes running on different machines, those tricks don’t work anymore!\nFlight is designed to solve this problem. It’s not a fancypants protocol with lots of different parts. It exists for one purpose: it makes it super easy to transfer Arrow-formatted data. That’s it. It’s pretty flexible though, and you can build other stuff on top of flight, but the design of flight is deliberately simple. It’s meant to be pretty minimal, so you can “just use it” without having to think too hard or do any of the obnoxious implementation work yourself.\n\n\n\n\n\nA biplane in flight. Image by Gerhard from Pixabay\n\n\n\n\n\nPrerequisites\nThere are a couple of prerequisites for this post. Specifically I’ll assume you have the arrow and reticulate packages installed in your R environment, and similarly that your Python environment has pyarrow installed. If you’re only interested in the Python side, you probably don’t need either of the R packages, but R users will need to have the pyarrow installation because the R flight implementation builds on pyarrow."
  },
  {
    "objectID": "posts/2022-10-18_arrow-flight/index.html#an-r-example",
    "href": "posts/2022-10-18_arrow-flight/index.html#an-r-example",
    "title": "Building an Arrow Flight server",
    "section": "An R example",
    "text": "An R example\nThe implementation of Arrow Flight varies a little across languages. In this post I’m going to focus on the two languages I use most – R and Python – but there’s nothing stopping you from using other languages. For example, the book In-Memory Analytics with Apache Arrow by Matt Topol has worked examples using C++ and Go, in addition to Python.\nFor the purposes of this post I’m going to start with R because the arrow package in R exposes a “high-level” interface that will allow us to start using a flight server without having to dive deeply into how it all works. However, as we’ll see, there are some limitations to this approach – not least of which is the fact that the R implementation turns out to secretly be a Python implementation under the hood – and as the post progresses I’ll pivot to Python in order to unpack some of the lower-level functionality.\nTo do this I’ll need access to the arrow and reticulate packages, and I’ll need to make certain that the Python environment is one that has pyarrow installed. For my machine, the commands to do this look like this:\n\nlibrary(arrow)\nlibrary(reticulate)\nuse_miniconda(\"base\")\n\nIt may be a little different for you depending on your configuration. For more information on this, take a look at the reticulate post I wrote recently.\n\n\nThe flight server\nOkay, so let’s get started by thinking about the simplest possible scenario for using a flight server. In this set up all we want the server to do is to act as a “cache” for Arrow tables. Clients can upload tables to the server, download tables from the server, and so on. That’s all we’re really trying to accomplish, and happily for us this use case is supported out of the box in R.\nHere’s how it works. As I mentioned earlier, R doesn’t actually implement the flight protocol itself: it’s just a wrapper around the Python tools. What that means is the underlying flight server is actually written in Python, and if we want to start that server running from R we have to call the load_flight_server() function that will allow us access to this server from R. Conveniently, the arrow R package comes bundled with a “demo” server that already provides the server side functionality that we want, and I can import it like this:\n\nserver_class <- load_flight_server(\"demo_flight_server\")\n\nWhen I do this, all I’ve done is obtain access to the relevant Python code. I haven’t created a server yet and I haven’t started it running either. Create an instance of the “demo server”, I call the DemoFlightServer() method attached to the server_class object:\n\nserver <- server_class$DemoFlightServer(port = 8089)\n\nWe have now defined a server that, once started, will run on port 8089. The server object has a serve() method that I can call to start it running:\n\nserver$serve()\n\nI’ve written a short script called start_demo_server.R that bundles all these operations together:\n\n\n\nstart_demo_server.R\n\n# load R packages and specify the Python environment\nlibrary(arrow)\nlibrary(reticulate)\nuse_miniconda(\"base\")\n\n# load server class, create instance, start serving\nserver_class <- load_flight_server(\"demo_flight_server\")\nserver <- server_class$DemoFlightServer(port = 8089)\nserver$serve()\n\n\nThe easiest way to start a server running in its very own R process would be to execute this script – or a suitably modified version that refers to an appropriate Python environment and server port – at the terminal, which I could do like this:\nRscript start_demo_server.R &\nThis would start an R process as a background job that creates a server and start it running. As an alternative, if you’re comfortable with using the callr package, you can use callr::r_bg() to create a child R process from your current one. The child process will run in the background, and we can start start the server within that R session without blocking the current one. This code will do exactly that:\n\nr_process <- callr::r_bg(function() {\n  reticulate::use_miniconda(\"base\")  \n  demo <- arrow::load_flight_server(\"demo_flight_server\")\n  server <- demo$DemoFlightServer(port = 8089)\n  server$serve()\n})\n\nRegardless of what method you’ve chosen, I’ll assume that the demo server is now running quietly in the background on port 8089.\n\n\n\n\n\nA hummingbird in flight. Image by Pexels from Pixabay\n\n\n\n\n\n\nThe flight client\nNow that I have this server running, I’ll define a flight client in my current R session that can interact with it. To do that, I call flight_connect():\n\nclient <- flight_connect(port = 8089)\n\nPerhaps unsurprisingly, the R object client is a wrapper around a Python flight client. It comes with various methods that implement low-level flight operations, but I’m going to hold off talking about those for a moment because we won’t need to use the low-level interface in this initial example.\nLet’s start by using the client to ask a simple question: what is stored on the server? The way that data sources are conceptualised in Arrow Flight is as a set of “flights”. Each individual “flight” is a data stream from which the client can download data. The precise implementation of this idea (e.g., what data structures are stored in a single flight) varies from server to server, but in both examples in this post one flight corresponds to one Arrow table.\nTo find out what flights are currently available on our server, we can call the list_flights() function:\n\nlist_flights(client)\n\nlist()\n\n\nHm, okay, there’s nothing there. That makes sense because I haven’t actually uploaded anything to the server yet! Okay, well, let’s suppose I want to store a copy of the airquality data as an Arrow table on my server. As R users are probably aware, this is a data set that comes bundled with R, but just so we’re all on the same page here’s the first few rows of the data set:\n\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nThis object is a regular data frame in R: it’s not an Arrow table. Strictly speaking what we want our client to do is send the Arrow table version of this data set to the server, so it will need to be converted. Happily for us, the flight_put() function supplied by the arrow package takes care of that conversion for us. As a result, we can cache an Arrow table copy of the data on the server with one line of code:\n\nflight_put(client, data = airquality, path = \"pollution_data\")\n\nIn this code, the flight_put() function uses the client object to communicate with the server. The data argument specifies the local copy of the data set, and the path argument provides the name for the data on the server. Having uploaded the data we can once again call list_flights(), and we get this as the result:\n\nlist_flights(client)\n\n[1] \"pollution_data\"\n\n\nYay!\nNow, just to prove to you that I’m not cheating, let’s check to make sure that there is no object called pollution_data stored locally within my R session:2\n\npollution_data\n\nError in eval(expr, envir, enclos): object 'pollution_data' not found\n\n\nUnsurprisingly, there is no object called pollution_data available in my current R session. The pollution_data object is stored on the server, not the client. To access that data from the client I can use the flight_get() function:\n\nflight_get(client, \"pollution_data\")\n\nTable\n153 rows x 6 columns\n$Ozone <int32>\n$Solar.R <int32>\n$Wind <double>\n$Temp <int32>\n$Month <int32>\n$Day <int32>\n\nSee $metadata for additional Schema metadata\n\n\nIt works!\n\n\n\n\n\nA flight of stairs. Image by Francis from Pixabay"
  },
  {
    "objectID": "posts/2022-10-18_arrow-flight/index.html#unpacking-the-data-exchange-process",
    "href": "posts/2022-10-18_arrow-flight/index.html#unpacking-the-data-exchange-process",
    "title": "Building an Arrow Flight server",
    "section": "Unpacking the data exchange process",
    "text": "Unpacking the data exchange process\nThe example in the last section is a nice proof-of-concept. It shows that we can use R to start a flight server and use it to upload and download data. But there’s a lot that hasn’t really been explained properly here. The time has come to start digging a little deeper, so we can really get a sense of what’s going on under the hood and how this simple example can be extended. That’s our goal in this section.\nOne thing that I like about the flight functionality exposed through flight_connect(), flight_put(), flight_get(), etc is that it operates at a high level of abstraction. In my day-to-day data analysis work I really don’t want to spend my time thinking about low-level operations. When I tell R to “put” a data set onto the server I want this to happen with one line of code. This high level API is super useful to me on an everyday basis, but it also masks some of the details about how flight works.\nTo give you a sense of what’s being hidden, we can take a closer look at the client object. Here’s a list of some of the methods that are available through the object itself:\nclient$do_put()\nclient$do_get()\nclient$do_action()\nclient$list_action()\nclient$list_flights()\nclient$get_flight_info()\nEach of these methods describes a low level operation available to the flight client. More precisely, these are the actual methods prescribed by the Arrow Flight protocol. Moreover, although I’m showing you this as an R object, in truth these are all Python methods: the R implementation of Arrow Flight is essentially a wrapper around the Python implementation. We can access these methods from R thanks to the magic of reticulate, but – to foreshadow the pivot coming in the next section – eventually we will need to start working with the underlying Python code.\nWhen we look at the names of flight methods, we can see there’s (unsurprisingly) a relationship between those names and the names of the functions exposed in the high-level R interface. As you might expect, the do_put() method for the client is very closely related to the flight_put() function. However, they aren’t the same. The do_put() method doesn’t stream any data to the server: it merely opens a connection to the server, from which we can then stream data with subsequent commands. If calling the do_put() method directly, you would have to take care of the streaming yourself.3 But from the user perspective it’s tiresome to write that code over and over, so the flight_put() function in the R interface provides a convenient high-level wrapper that abstracts over all that.\nIf you’re the analyst working with the data, this is fabulous. But if you’re looking to implement your very own flight server, you probably need to understand what these low level operations are. So that’s where we’re headed next…\n\n\nUnpacking flight_put()\nLet’s start by taking a look at what happens when we call the R function flight_put(). For now, we won’t write any actual code (don’t worry, that will come later!). All we want to do is think about the sequence of operations that takes place. Our goal is to transmit the data to the server, and there’s an Arrow Flight method called do_put() that can do this for us. However, the structure of the interaction is a little more complicated than simply calling do_put(). It’s a multi-step operation that unfolds as shown below:\n\nThe first step in the process occurs when the client calls do_put(), a flight method that takes two arguments: a flight descriptor object that is used to identify the specific data stream that the client wants to be sent – and later on I’ll talk what the descriptor actually looks like – and the schema for the flight data.4 Setting aside the particulars of the syntax – which might be different in every language – here’s what the do_put() function call looks like on the client side:\ndo_put(descriptor, schema)\nPassing the schema on the client side serves a particular purpose: it allows the client to create stream writer and stream reader objects that are returned to the client-side user, and are also passed along to the server. The writer object is the thing that will take care of streaming data to the server, and the reader object is responsible for reading any metadata response that the server happens to send.5\nNow let’s have a look at the server side, where the do_put() method expects three inputs: the flight descriptor, the writer, and the reader. So here’s the signature on the server side:\ndo_put(descriptor, reader, writer)\nAs long as these methods are written appropriately for both the client and the server, we now have a situation where both machines agree on the description of the data and have objects that can take care of the streaming process.\nWe now move to step two in the communication, in which the client streams the data to the server. Once the data arrive on the server side, the do_put() method for the server stores the data along with an appropriate descriptor, so that it can be found later. Optionally, this is followed by a third stage in which the server sends a response containing metadata to the client. In the example server I’ll build in the next section, I won’t bother with that step!\n\n\n\nUnpacking flight_get()\nNext let’s look at flight_get(). When I called this function earlier, it triggered two separate interactions between the client and server. First, the client calls the get_flight_info() method, and the server responds with some information about the data source that includes – among other things – a ticket. Again, the ticket is a particular data structure that I’ll talk more about later, but for now it’s enough to note that it’s a token that uniquely specifies which flight is requested.\nOnce in possession of this ticket, the client can call do_get() to request that the server send the data that matches the ticket, which the server then streams. So the whole exchange looks like this:\n\nSo, in the previous example when I called flight_get(), the process looked like this. On the client side, we used the \"pollution_data\" path to construct a descriptor object and the client used get_flight_info() to request that information about this “flight” from the server:\nget_flight_info(descriptor)\nOn the server side, once the descriptor is received, a flight info object is constructed. The flight info object is comprised of five parts:\n\nThe schema for the data stored by the flight,\nThe flight descriptor object\nA list of one or more endpoints that specify where the data are available for streaming. Each end point includes a location from which to stream, and the associated ticket for that location\nThe total number of records (i.e. rows) stored\nThe total number of bytes to be streamed (i.e., the size of the data)\n\nThis flight info is then returned to the client.\nIt may seem like this arrangement is overly elaborate: why does the client need this much information if only the ticket is needed to request the data? To be honest, for the simple server-client examples I’ve used in this post, this level of complexity is not really needed. However, it’s extremely useful that it’s structured like this when we want to start adopting a more sophisticated setup. One thing it allows, for example, is an arrangement where both the server and client can be distributed across multiple machines, with different endpoints streaming different subsets of the data. Matt Topol discusses some examples where this architecture is employed in In-Memory Analytics with Apache Arrow.\nOnce this flight information has been received by the client, we can extract the ticket from the relevant endpoint (there will be only one endpoint in the server I build in the next section). The client now calls:\ndo_get(ticket)\nThe server then sends a stream reader object that the client can use to receive the stream of data from the server.\n\n\n\n\n\nBalloons in flight. Image by Pexels from Pixabay"
  },
  {
    "objectID": "posts/2022-10-18_arrow-flight/index.html#a-python-example",
    "href": "posts/2022-10-18_arrow-flight/index.html#a-python-example",
    "title": "Building an Arrow Flight server",
    "section": "A Python example",
    "text": "A Python example\nNow that we have a basic understanding of what is happening at a lower level, we can build a flight server of our very own. To do this I’ll switch over to Python. There’s two reasons for doing this. The first reason is that R doesn’t currently have a low level implementation of flight: at the moment it relies on the Python implementation, so it’s easiest to switch completely to Python for the rest of this post.6 The second reason is that Python doesn’t supply a high level API analogous to flight_put(), flight_get() etc, and instead adheres tightly to the Arrow Flight specification. That’s super helpful if you need to design a custom flight server because you get access to the all Arrow Flight functionality, but it also means you need to write a lot more code. To help make that process a little easier, I’ll walk you through how that works in Python now!\n\n\nA tiny flight server\nOur goal in this section is to write our own flight server in Python that does the same job as the one we saw earlier in the R example: it’s a server that allows you to cache copies of Arrow tables. To do so, we’ll start our Python script the way one usually does, with some imports:\n\n\n\ntiny_flight.py\n\nimport pyarrow as pa\nimport pyarrow.flight as flight\n\n\nWhat I’ll do now is define a Python class called TinyServer. The job of this class is to provide server side flight methods for do_get(), do_put(), and others. We’ll be able to use this class to create specific server instances and set them running, in more or less the exact same fashion that we did previously in the R example.\nI’ll explain the code in more detail in a moment after I’ve shown you both the server and the client, but let’s start just by looking at the code. You can find all the code in the tiny_flight.py script that accompanies this post. Here’s the complete code used to define the TinyServer class:\n\n\n\ntiny_flight.py [server]\n\nclass TinyServer(flight.FlightServerBase):\n  \n    def __init__(self, \n                 host = 'localhost', \n                 port = 5678):\n        self.tables = {}\n        self.location = flight                  \\\n                        .Location               \\\n                        .for_grpc_tcp(host, port)\n        super().__init__(self.location)\n    \n    @staticmethod    \n    def server_message(method, name):\n        msg = '(server) '                       \\\n              + method                          \\\n              + ' '                             \\\n              + name.decode('utf-8')\n        print(msg)\n      \n    def do_put(self, context, descriptor, reader, \n               writer):\n        table_name = descriptor.command\n        self.server_message('do_put', table_name)\n        self.tables[table_name] = reader.read_all()\n\n    def do_get(self, context, ticket):\n        table_name = ticket.ticket\n        self.server_message('do_get', table_name)\n        table = self.tables[table_name]\n        return flight.RecordBatchStream(table)\n  \n    def flight_info(self, descriptor):\n        table_name = descriptor.command\n        table = self.tables[table_name]\n\n        ticket = flight.Ticket(table_name)\n        location = self.location.uri.decode('utf-8')\n        endpoint = flight.FlightEndpoint(ticket,\n                                         [location])\n        \n        return flight.FlightInfo(table.schema, \n                                 descriptor, \n                                 [endpoint], \n                                 table.num_rows,\n                                 table.nbytes)\n    \n    def get_flight_info(self, context, descriptor):\n        table_name = descriptor.command\n        self.server_message('get_flight_info',\n                            table_name)\n        return self.flight_info(descriptor)        \n        \n    def list_flights(self, context, criteria):\n        self.server_message('list_flights', b' ')\n        for table_name in self.tables.keys():\n            descriptor = flight                  \\\n                         .FlightDescriptor       \\\n                         .for_command(table_name)\n            yield self.flight_info(descriptor)\n\n    def do_action(self, context, action):\n        if action.type == 'drop_table':\n            table_name = action.body.to_pybytes()\n            del self.tables[table_name]\n            self.server_message('drop_table',\n                                table_name)\n\n        elif action.type == 'shutdown':\n            self.server_message('shutdown', b' ')\n            self.shutdown()\n\n        else:\n            raise KeyError('Unknown action {!r}'.\n                           format(action.type))\n\n    def list_actions(self, context):\n        return [('drop_table', 'Drop table'),\n                ('shutdown', 'Shut down server')]\n\n\nNow, if you’re at all like me this code won’t immediately make sense. Probably you’ll skim over it, read bits of it, and some of it will make sense… but not all of it. There’s a couple of reasons for that. The first and most obvious reason is that it’s a big chunk of code that I haven’t explained yet! The second reason is that (in my opinion) server-side code never makes sense on its own: it only really makes sense when you can place it next to the client-side code so that you can see how the two parts fit together.7 With that in mind, let’s take a quick peek at the client-side code…\n\n\n\nA tiny flight client\nTo accompany a TinyServer, we’ll need a TinyClient that knows how to talk to it. Happily for us, it’s easier to define the client than to define the server, so the source code that defines the TinyClient class is considerably shorter:\n\n\n\ntiny_flight.py [client]\n\nclass TinyClient:\n\n    def __init__(self, host = 'localhost', port = 5678):\n        self.location = flight                      \\\n                        .Location                   \\\n                        .for_grpc_tcp(host, port)\n        self.connection = flight.connect(self.location)\n        self.connection.wait_for_available()\n\n    def put_table(self, name, table):\n        table_name = name.encode('utf8')\n        descriptor = flight                         \\\n                     .FlightDescriptor              \\\n                     .for_command(table_name)\n        writer, reader = self                       \\\n                         .connection                \\\n                         .do_put(descriptor,\n                                 table.schema)\n        writer.write(table)\n        writer.close()\n      \n    def get_table(self, name):\n        table_name = name.encode('utf8')\n        ticket = flight.Ticket(table_name)\n        reader = self.connection.do_get(ticket)\n        return reader.read_all()\n    \n    def list_tables(self):\n        names = []\n        for flight in self.connection.list_flights():\n            table_name = flight.descriptor.command\n            names.append(table_name.decode('utf-8'))\n        return names\n    \n    def drop_table(self, name):\n        table_name = name.encode('utf8')\n        drop = flight.Action('drop_table', table_name) \n        self.connection.do_action(drop)\n\n\nThese two classes are designed to work in concert: the do_put() method for TinyServer is aligned with the do_put() method for TinyClient,8 and the put_table() function I wrote on the client side is a convenient high-level wrapper that manages the whole “put a table on the server” interaction without requiring the user to do anything other than write a single line of code. That’s the reason I started by showing you all the source code for both parts before explaining any of the specific methods: in the next few sections I’ll walk you through the code, placing the relevant snippets from the server code and the client code next to each other so you can more clearly see how they relate to each other.\n\n\n\n\n\nA kite in flight. Image by Anja from Pixabay\n\n\n\n\n\n\nInitialisation\nLet’s start by looking at what happens when the server and client are initialised. When a new TinyServer or TinyClient object is created, the __init__ function is called:\n\n\n\ntiny_flight.py [server]\n\nclass TinyServer(flight.FlightServerBase):\n  \n    def __init__(self, \n                 host = 'localhost', \n                 port = 5678):\n        self.tables = {}\n        self.location = flight                  \\\n                        .Location               \\\n                        .for_grpc_tcp(host, port)\n        super().__init__(self.location)\n\n\n\n\n\ntiny_flight.py [client]\n\nclass TinyClient:\n\n    def __init__(self, host = 'localhost', port = 5678):\n        self.location = flight                      \\\n                        .Location                   \\\n                        .for_grpc_tcp(host, port)\n        self.connection = flight.connect(self.location)\n        self.connection.wait_for_available()\n\n\nSome things to notice here. At start up, the server and client both call the flight.Location.for_grpc_tcp() function to generate a Location object used to specify the address of the server:\n\nloc = flight.Location.for_grpc_tcp('localhost', 5678)\nprint(loc)\n\n<Location b'grpc+tcp://localhost:5678'>\n\n\nThe important thing in this output is the server address. The localhost:5678 part indicates that the server is running locally on port 5678, and the grpc+tcp:// part tells us what communication protocols are being used. For this server, those protocols are gRPC and TCP. TCP is probably familiar to most data scientists since it’s one of the core protocols of the internet, but gRPC (wonderful as it is) is a little more specialised. I’m not going to talk about how gRPC works in this post, but there are some references at the end. For now, it’s sufficient to recognise that this location object does store the server address. If I’d really wanted to, I could have written code that constructs this string manually9 but there’s no need to do that when the pyarrow flight module supplies built-in location classes to do this for us!\nThe rest of the code is used for initialisation. On the server side, we initialise the server object as an instance of the parent class (i.e., FlightServerBase). On the client side, the first action is to call flight.connect(): this is also an initialisation action that returns an instance of the FlightClient class. In other words there’s a kind of symmetry here: the TinyServer is built on top of the FlightServerBase class, and the TinyClient is built on top of the FlightClient class.10\nThe other thing to notice here is the data structures set up in these initialisations. On the server side we create an empty dictionary called tables (referred to as self.tables since it belongs to the instance not the class) that the server uses to store any data sets that it is sent. On the client side, the self.connection object is used to represent our connection to the server: this object is an instance of the FlightClient class, and it comes equipped with client side methods for do_put(), do_get() etc. Finally, notice that the last action that the client takes when it is initialised is to wait for the connection to the server to be established.\n\n\n\nPutting a table\nNext, let’s take a look at the code used to place data on the server. On the server side, we have to specify the do_put() method. In this case, all my code does is store a copy of the data in self.tables and prints a little message to the server console using the server_message() function:\n\n\n\ntiny_flight.py [server]\n\n    @staticmethod    \n    def server_message(method, name):\n        msg = '(server) '                       \\\n              + method                          \\\n              + ' '                             \\\n              + name.decode('utf-8')\n        print(msg)\n      \n    def do_put(self, context, descriptor, reader, \n               writer):\n        table_name = descriptor.command\n        self.server_message('do_put', table_name)\n        self.tables[table_name] = reader.read_all()\n\n\nThere’s a few things to comment on here. First, let’s note that the server_message() function isn’t very interesting for our purposes. It exists solely to print out messages,11 thereby allowing the server to announce what it’s doing, but the server would work just fine without these messages. However, it does give me an opportunity to mention some things about the arguments to the various functions defined in this code:\n\nserver_message() is a static method – which is why it doesn’t take a self argument. The arguments listed in the function definition are exactly the same as the arguments that are included in function calls later.\ndo_put() is a class method, and so it takes self as the first argument. As is typical for object oriented programming systems, the self argument in class methods is passed implicitly. It’s included in the function definition, but not in the function calls. Internally, what’s going on is that a call like object.method(argument) is translated to Class.method(object, argument) and therefore the object itself implicitly becomes the first argument.\ndo_put() is an Arrow Flight method (as well as a Python class method), and because of that it also takes a context argument that, much like self is passed implicitly. This post isn’t the place to have that discussion – it’s too long already – but for now it suffices to note that Arrow will handle the context argument for us, in an analogous fashion to how Python handles self for us.\n\nNow that we have that sorted, let’s have a look at the part of the code that actually does the server-side work. Specifically, it’s this line in do_put():\n\n\n\ntiny_flight.py [server]\n\n        self.tables[table_name] = reader.read_all()\n\n\nLet’s unpack this line one step at a time.\nThe reader object has been passed to the server as one of the arguments to do_put(), and it’s a RecordBatchStreamReader. That is, it’s an object capable of receiving a stream of Arrow data. When the read_all() method is called, it reads all record batches sent by the client and returns the final result as an Arrow table. This table is then stored in the self.tables dictionary.\nNext, notice that the key against which the table is stored as the value is specified by descriptor.command. This part of the code also needs to be explained! What is a “descriptor” object? What is the “command” attribute of a descriptor? That’s not at all obvious from inspection. To resolve our confusion, it helps to realise that this descriptor object is one of the arguments to the the server-side do_put() function, and the code that creates this object is over on the the client side. So let’s look at the code I wrote for the client side:\n\n\n\ntiny_flight.py [client]\n\n    def put_table(self, name, table):\n        table_name = name.encode('utf8')\n        descriptor = flight                         \\\n                     .FlightDescriptor              \\\n                     .for_command(table_name)\n        writer, reader = self                       \\\n                         .connection                \\\n                         .do_put(descriptor,\n                                 table.schema)\n        writer.write(table)\n        writer.close()\n\n\nHere we have a put_table() function written in Python that does roughly the same job that the flight_put() function was doing for us in the R example I presented earlier. It’s a high-level wrapper function that sends a do_put() call to the server, streams the data across, and then stops. This line of code in this function is the one that makes the do_put() call:\n\n\n\ntiny_flight.py [client]\n\n        writer, reader = self                       \\\n                         .connection                \\\n                         .do_put(descriptor,\n                                 table.schema)\n\n\nOkay, so the descriptor on the client side is also the thing that later gets used on the server side to create the key against which the table is stored. If we look at the preceding line of code, we can see that the descriptor object is an instance of the FlightDescriptor class. So let’s actually step into the Python console and run the commands required to create a flight descriptor object:12\n\ntable_name = b'name-of-data'\ndescriptor = flight.FlightDescriptor.for_command(table_name)\nprint(descriptor)\n\n<FlightDescriptor command: b'name-of-data'>\n\n\nPerhaps unsurprisingly, the command attribute is in fact the (byte encoded) string that we used to specify the name. In other words, once we strip back all the layers here it turns out that the server stores the data set using the name that the client gave it!\n\ndescriptor.command\n\nb'name-of-data'\n\n\n\n\n\nGetting a table\nNext, let’s have a look at the code used to get data from the server. Just like last time, I’ll put the relevant sections from the server code and the client side code side by side:\n\n\n\ntiny_flight.py [server]\n\n    def do_get(self, context, ticket):\n        table_name = ticket.ticket\n        self.server_message('do_get', table_name)\n        table = self.tables[table_name]\n        return flight.RecordBatchStream(table)\n\n\n\n\n\ntiny_flight.py [client]\n\n    def get_table(self, name):\n        table_name = name.encode('utf8')\n        ticket = flight.Ticket(table_name)\n        reader = self.connection.do_get(ticket)\n        return reader.read_all()\n\n\nOn the client side, the get_table() helper function that I’ve written does two things. First it creates a Ticket object from the name of the data table to be retrieved. It then calls the do_get() flight method to communicate with the server. Then, using the reader object returned by do_get(), it streams the data from the server. The server side code is the mirror image: when the ticket is received, it uses this ticket to retrieve the specific table from self.tables, and returns a stream.\nLooking at these two code extracts side by side we can see that the ticket object returned client-side by flight.Ticket() gets used server-side to retrieve the requested table. So we should take a look at what happens here. What we hope to see is that this ticket produces the same key used to store the data originally: that is, when the server specifies a storage key with table_name = ticket.ticket in the do_get() method, it should match the key created by do_put() when table_name = descriptor.command was executed.\nLet’s verify that this is true!\nSince I already have a table_name object lying around from earlier, let’s run that line of code shall we?\n\nticket = flight.Ticket(table_name)\nprint(ticket)\n\n<Ticket b'name-of-data'>\n\n\nThat looks promising. If we take a peek at ticket.ticket, we see that – yet again – under the hood the ticket is just an alias for the name of the data set:\n\nticket.ticket\n\nb'name-of-data'\n\n\nWell that’s a relief. In the server-side code, the descriptor.command object and the ticket.ticket object both produce the correct key used to index a table.\n\n\n\n\n\nBoarding a flight. Image by Joshua Woroniecki from Pixabay\n\n\n\n\n\n\nGetting information\nOur journey through the source code continues. On the client side I’ve written a function called list_tables() that returns the names of all tables stored on the server. Here’s what that looks like:\n\n\n\ntiny_flight.py [client]\n\n    def list_tables(self):\n        names = []\n        for flight in self.connection.list_flights():\n            table_name = flight.descriptor.command\n            names.append(table_name.decode('utf-8'))\n        return names\n\n\nThe key part of this function is the call to self.connection.list_flights(). That’s where the client contacts the server and requests information. Everything else in the function is there to extract the one piece of information (the name of the table) that we’re interested in and return it to the user.\nPivoting over to the server code, there are two flight methods that are relevant here. The get_flight_info() function is a flight method that returns information about a single flight – where, in this case, there’s a one-to-one mapping between flights and tables – and the list_flights() method can be used to retrieve information about all flights stored on the server:\n\n\n\ntiny_flight.py [server]\n\n    def get_flight_info(self, context, descriptor):\n        table_name = descriptor.command\n        self.server_message('get_flight_info',\n                            table_name)\n        return self.flight_info(descriptor)        \n        \n    def list_flights(self, context, criteria):\n        self.server_message('list_flights', b' ')\n        for table_name in self.tables.keys():\n            descriptor = flight                  \\\n                         .FlightDescriptor       \\\n                         .for_command(table_name)\n            yield self.flight_info(descriptor)\n\n\nThere’s two things to comment on here. First, note that the list_flight() method iterates13 over all the stored keys in the tables dictionary, uses the key to construct a flight descriptor, and then calls the flight_info() helper function that I’ll explain in a moment. In contrast, the get_flight_info() function receives a flight descriptor directly from the client, so it’s much simpler: it just calls flight_info() directly.\nOkay, so now let’s have a look at the flight_info() helper method. Here’s the code for that one:\n\n\n\ntiny_flight.py [server]\n\n    def flight_info(self, descriptor):\n        table_name = descriptor.command\n        table = self.tables[table_name]\n\n        ticket = flight.Ticket(table_name)\n        location = self.location.uri.decode('utf-8')\n        endpoint = flight.FlightEndpoint(ticket,\n                                         [location])\n        \n        return flight.FlightInfo(table.schema, \n                                 descriptor, \n                                 [endpoint], \n                                 table.num_rows,\n                                 table.nbytes)\n\n\nLet’s start by looking at the return value. It’s a FlightInfo object and as I mentioned earlier there are five things needed to create it:\n\ntable.schema is the Schema for the data stored by the flight\nThe flight descriptor object, which was passed as input\nA list of one or more FlightEndpoint objects – in this cases, [endpoint] is a list containing a single endpoint – that specifies where the data are available for streaming. Each endpoint includes a location from which to stream, and the associated ticket for that location\nThe total number of records can be accessed from table.num_rows\nThe total number of bytes can be accessed from table.nbytes\n\nLooking at the rest of the function, you can see that the list of endpoints requires a little work to construct. We need to call flight.Ticket() to construct a ticket object, we need to extract the server location that we stored when the server was initialised, and then we need to call flight.Endpoint() to put these things together. There’s a little more code involved, but thankfully it’s not conceptually difficult.\n\n\n\nCustom actions\nWe’ve encountered (and implemented) four of the methods defined by the Arrow Flight protocol, and as we’ve seen they server different purposes. The do_put() and do_get() methods are used to stream data to and from the server.14 In contrast, the get_flight_info() and list_flights() methods are used to retrieve metadata about the data stored on the server. In this example, these four methods are sufficient to provide all the core functionality. I could stop here if I absolutely wanted to. But there’s one more method I want to draw your attention to: do_action(). In recognition of the fact that real world applications will always need to perform custom operations that weren’t originally built into the protocol, the do_action() method exists to allow the client to request (and the server to perform) custom actions that you can define however you like.\nTo give you a sense of how this works, we’ll add two custom actions to our server: when the client requests a 'drop_table' action, the corresponding table will be deleted from the server, and when the client requests a 'shutdown' action the server will shut itself down.15 Let’s take a look at the server-side code implementing this:\n\n\n\ntiny_flight.py [server]\n\n    def do_action(self, context, action):\n        if action.type == 'drop_table':\n            table_name = action.body.to_pybytes()\n            del self.tables[table_name]\n            self.server_message('drop_table',\n                                table_name)\n\n        elif action.type == 'shutdown':\n            self.server_message('shutdown', b' ')\n            self.shutdown()\n\n        else:\n            raise KeyError('Unknown action {!r}'.\n                           format(action.type))\n\n\nThe do_action() method expects to receive an Action object as input to the action argument. The action.type attribute is where the name of the action is stored, so the code here uses if-else to decide which action to perform, or raise an error if the action type is not recognised. The code implementing the actions is pretty minimal. On a shutdown action, the server calls the self.shutdown() method: this is inherited from FlightServerBase, I didn’t have to implement it myself. For a drop table action, the server inspects the action.body argument to determine the name of the table to be dropped, and then deletes it from self.tables.16\nHow does the client call this method? To see an example of this, let’s flip over to the client side of the code and look at the drop_table() function:\n\n\n\ntiny_flight.py [client]\n\n    def drop_table(self, name):\n        table_name = name.encode('utf8')\n        drop = flight.Action('drop_table', table_name) \n        self.connection.do_action(drop)\n\n\nHappily, it turns out to be simple: first we call flight.Action() to construct the action object itself, passing the action type and action body as arguments. Then we call the built-in do_action() client-side method, which as usual we can access from the self.connection object.\nVoilà! We are done. The server and client are both ready to go. Let’s take them for a spin, shall we?\n\n\n\n\n\nBee flight. Image by Gary Stearman from Pixabay\n\n\n\n\n\n\nUsing our server\nLet’s start with a few imports. In this demonstration I’m going to read data from csv files, so I’ll import the csv submodule from pyarrow. Obviously, I’ll also need access to the server classes, and since the tiny_flight.py script is bundled with this post I can import that too. Finally, I’m going to start the server running in its own thread, so I’ll import threading too:\n\nimport threading\nimport tiny_flight as tiny\nfrom pyarrow import csv\n\nNext, I’ll initialise a server running on port 9001 and start it running in its own thread. This will turn out to be handy because when the client starts interacting with the server, we’ll see the server messages as well as the client output!\n\nserver = tiny.TinyServer(port = 9001)\nthread = threading.Thread(target = lambda: server.serve(), \n                          daemon = True)\nthread.start()\n\nNow that the server is up and running, let’s instantiate a client and have it connect to the server:\n\nclient = tiny.TinyClient(port = 9001)\n\n(server) list_flights  \n\n\nNotice the message prefixed with (server) here: that part of the output is generated by the server running in the thread. It’s not client-side output. We’re only seeing it here because when the client is initialised by the call to flight.connect(), it calls the list_flights() flight method, and the server prints a message using its internal server_message() function.\nOkay. So far so good. Our next step is to create some Arrow tables client side. To make this simpler I have csv files containing copies of the freely-available 1973 New York city air quality and earthquakes near Fiji since 1964 data sets that are both bundled by the datasets R package. I’ll import them both as pyarrow tables:\n\nfijiquakes = csv.read_csv(\"fijiquakes.csv\")\nairquality = csv.read_csv(\"airquality.csv\")\n\nAt the moment both tables exist on the client, and we’d like to cache them on the server. We can do this by calling the put_table() method:\n\nclient.put_table(\"fijiquakes\", fijiquakes)\nclient.put_table(\"airquality\", airquality)\n\n(server) do_put fijiquakes\n(server) do_put airquality\n\n\nAgain, notice that the server prints messages which make clear that the data have arrived on the server side. Of course, the client doesn’t actually know this because my server-side code for do_put() doesn’t implement a server response for the client. But no matter: the client can check manually by calling list_tables():\n\nclient.list_tables()\n\n(server) list_flights  \n['fijiquakes', 'airquality']\n\n\nHere we see two lines of output: the first one is the server-side log, and the second is the output returned client-side showing that both tables exist on the server.\nWe can take this a step further, of course, by retrieving the data from the server cache. We can do that straightforwardly by calling get_table(), and again see that a server-side message is printed, while the table itself is returned to the client:\n\nclient.get_table(\"fijiquakes\")\n\n(server) do_get fijiquakes\npyarrow.Table\nlat: double\nlong: double\ndepth: int64\nmag: double\nstations: int64\n----\nlat: [[-20.42,-20.62,-26,-17.97,-20.42,...,-25.93,-12.28,-20.13,-17.4,-21.59]]\nlong: [[181.62,181.03,184.1,181.66,181.96,...,179.54,167.06,184.2,187.8,170.56]]\ndepth: [[562,650,42,626,649,...,470,248,244,40,165]]\nmag: [[4.8,4.2,5.4,4.1,4,...,4.4,4.7,4.5,4.5,6]]\nstations: [[41,15,43,19,11,...,22,35,34,14,119]]\n\n\nNow, perhaps we decided that we don’t need a cached copy of the airquality table any longer. We can ask the server to remove it by calling drop_table(), and we can confirm the result by calling list_tables() again:\n\nclient.drop_table(\"airquality\")\nclient.list_tables()\n\n(server) drop_table airquality\n(server) list_flights  \n['fijiquakes']\n\n\nYep, that all looks right!\n\n\n\n\n\nBirds in flight. Image by Gerhard from Pixabay"
  },
  {
    "objectID": "posts/2022-10-18_arrow-flight/index.html#where-to-next",
    "href": "posts/2022-10-18_arrow-flight/index.html#where-to-next",
    "title": "Building an Arrow Flight server",
    "section": "Where to next?",
    "text": "Where to next?\nCompared to the rest of the Apache Arrow project, it’s not so easy to find tutorials and documentation about flight. It’s still a little piecemeal. With that in mind, here’s an annotated reading list that will be helpful if you want to explore flight further:\n\nThe original announcement of flight by Wes McKinney on the Apache Arrow blog gives a very good overview of the motivation for why flight was introduced.\nData transfer at the speed of flight by Tom Drabas, Fernanda Foertter, and David Li. This is a blog post on the Voltron Data blog that provides a concrete example of a working flight server written in Python. The Python code I’ve discussed in this post is an elaboration of the content in that post. It’s a good starting point.\nApache Arrow Flight: A Primer by David Li and Tom Drabas. This is another blog post on the Voltron Data website. This one doesn’t have any working code for you to look at, but it provides a good summary of the technologies that Arrow Flight is built upon. It’s a little intense for novices but is pretty handy for intermediate level users who want to take a peek under the hood.\nThe Python documentation flight vignette is pretty readable and goes into a moderate amount of detail, but be aware it implicitly assumes some familiarity with remote procedure calls.\nThe Python cookbook for Arrow contains the most thorough worked example I’ve seen anywhere. It’s a little dense for novice users, but it’s still the one of the most comprehensive resources I’ve seen, and the only one that talks about issues like authentication (which I have not discussed at all here!)\nThe R documentation flight vignette has a succinct overview of how you can use the high-level interface provided by flight_put(), flight_get(), etc. What it doesn’t do (yet?) is discuss the low-level features. At the moment you won’t find a discussion of say client$do_get() and how it relates to flight_get().\nAlong similar lines there are some examples in the R cookbook, but they are also quite minimal.\nShould you be interested in writing an Arrow Flight service in C++, the documentation pages for the C++ Flight implementation may come in handy!\nIf you’re willing to spend some money I thoroughly recommend the chapter on Arrow flight in Matt Topol’s book In-Memory Analytics with Apache Arrow. I found it really helpful for cementing my own understanding. In addition to the worked examples in Python, C++, and Go, the chapter provides some historical context for understanding the difference between RPC frameworks and REST frameworks, and is also the only resource I’m aware of that goes into detail about how more sophisticated network architectures are supported by flight.\nIf you’re keen to understand what is happening under the hood, at some point you’re going to want to read about gRPC. The flight protocol is built on top of gRPC, and a lot of the advanced content you’ll encounter on flight doesn’t make a lot of sense until you’ve started to wrap your head around it. To that end, I found the introduction to gRPC documentation really helpful. You may also want to take a look at the documentation for protocol buffers because in practice that’s doing a lot of the work for us here!\nIf you want to understand the backdrop against which all this sits, it’s also pretty handy to do a bit of digging around and reading the history around remote procedure call (RPC) approaches to distributed computing and representational state transfer (REST) approaches. Even skimming the two linked Wikipedia articles was helpful for me.\nWhen digging around in source code, I found it handy to take a look at these parts of the code base: source code for the R demo server, a Python example server, and the pyarrow flight implementation.\nFinally, while neither one is ideal as a place to start, once I started getting the hang of what I was doing, I have found it handy to browse through the Python flight API reference pages, and to occasionally dip into the official Arrow flight RPC specification. Regarding the latter, my experience was that the images showing how each of the flight methods operates were handy, and the comments shown in the in the “protocol buffer definitions” are nice because they’re maybe the clearest verbal description of what each of the flight methods expects as input and what objects they will return.\n\nHappy hunting!"
  },
  {
    "objectID": "posts/2022-10-18_arrow-flight/index.html#acknowledgements",
    "href": "posts/2022-10-18_arrow-flight/index.html#acknowledgements",
    "title": "Building an Arrow Flight server",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nMy sincere thanks to Rok Mihevc, David Li, Kae Suarez, and François Michonneau for reviewing earlier versions of this post."
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html",
    "title": "Binding Apache Arrow to R",
    "section": "",
    "text": "So I have a new job.\nIn my previous job as an academic, a large part of my work – my favourite part, if I’m honest – involved creating open access resources to help people use modern open source tools for data analysis. In my totally different role in developer relations at Voltron Data, a large part of my work involves, um … [checks job description] … creating open access resources to help people use modern open source tools for data analysis. Well okay then!\nI’d better get on that, I suppose?\nI’ve been in my current role for a little over a week (or had been when I started writing this post!), and today my first contribution to Apache Arrow was merged. It was very modest contribution: I wrote some code that determines whether any given year is a leap year. It precisely mirrors the behaviour of the leap_year() function in the lubridate package, except that it can be applied to Arrow data and it will behave itself when used in the context of a dplyr pipeline (more on that later). The code itself is not complicated, but it relies on a little magic and a deeper understanding of Arrow than I possessed two weeks ago.\nThis post is the story of how I learned Arrow magic. ✨ 🏹 ✨"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#why-am-i-writing-this",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#why-am-i-writing-this",
    "title": "Binding Apache Arrow to R",
    "section": "Why am I writing this?",
    "text": "Why am I writing this?\n\n\nThe danger of sublimated trauma is a major theme in our story      – The Great God Ember (The Magicians: Season 2, Episode 3)\n\nIt might seem peculiar that I’m writing such a long post about such a tiny contribution to an open source project. After all, it doesn’t actually take a lot of work to figure out how to detect leap years. You can do it in one line of R code:\n\n(year %% 4 == 0) & ((year %% 100 != 0) | (year %% 400 == 0))\n\nThis is a logical expression corresponding to the following rules. If the year is divisible by 4 then it is a leap year (e.g., 1996 was a leap year). But there’s an exception: if year is divisible by 100 then it isn’t a leap year (e.g., 1900 wasn’t a leap year). But there’s also an exception to the exception: if year is divisible by 400 then it is a leap year (e.g., 2000 was a leap year). Yes, the process of mapping the verbally stated rules onto a logical expression is kind of annoying, but it’s not conceptually difficult or unusual. There is no magic in leap year calculation, no mystery that needs unpacking and explaining.\n\n\nAll this assumes years are counted using the Gregorian calendar. There are, of course, other calendars\nThe magic comes in when you start thinking about what the arrow package actually does. It lets you write perfectly ordinary R code for data manipulation that returns perfectly ordinary R data structures, even though the data have never been loaded into R and all the computation is performed externally using Apache Arrow. The code you write with arrow looks and feels like regular R code, but almost none of the work is being done by R. This is deep magic, and it is this magic that needs to be demystified.\n\n\n\n\n\n\nTwo key moments in “The Magicians” when Julia Wicker discovers she can do magic, defying the expectations of others. One moment occurs at the start of Season 1 as a novice, after she had been told she failed the magic exams at Brakebills University; another moment occurs at the end of Season 2 after all magic has supposedly been turned off by the Old Gods or something. The parallel between the two moments is striking. Oh and Quentin Coldwater is in both scenes too I guess. Whatevs. Image via giphy, copyright syfy\n\n\n\n\n\nI have three reasons for wanting to unpack and explain this magic.\n\nThe first reason is personal: I’ve been a professional educator for over 15 years and it has become habit. The moment I learn a new thing my first impulse is to work out how to explain it to someone else.\nThe second reasons is professional: I work for Voltron Data now, and part of my job is to make an educational contribution to the open source Apache Arrow project. Arrow is a pretty cool project, but there’s very little value in magnificent software if you don’t help people learn how to take advantage of it!\nThe third reason is ethical: a readable tutorial/explanation lowers barriers to entry. I mean, let’s be honest: the only reason I was able to work up the courage to contribute to Apache Arrow is that I work for a company that is deeply invested in open source software and in the Arrow project specifically. I had colleagues and friends I could ask for advice. If I failed I knew they would be there to help me. I had a safety net.\n\nThe last of these is huuuuuuugely important from a community point of view. Not everyone has the safety net that I have, and it makes a big difference. In a former life I’ve been on the other side of this divide: I’ve been the person with no support, nobody to ask for help, and I’ve run afoul of capricious gatekeeping in the open source world. It is a deeply unpleasant experience, and one I would not wish upon anyone else. We lose good people when this happens, and I really don’t want that!\nThe quote from the beginning of this section, the one about the danger of sublimated trauma, is relevant here: if we want healthy user communities it is our obligation on the inside to provide safe environments and delightful experiences. Our job is to find and remove barriers to entry. We want to provide that “safety net” that ensures that even if you fall (because we all fall sometimes), you don’t get hurt. Failing safely at something can be a learning experience; suffering trauma, however, is almost never healthy. So yeah, this matters to me. I want to take what I’ve learned now that I’m on the inside and make that knowledge more widely accessible.\n\n\nEveryone deserves a safety net when first learning to walk the tightropes. It’s not a luxury, it’s a necessity\nBefore diving in, I should say something about the “assumed knowledge” for this post.\n\nI’ll do my best to explain R concepts as I go, but the post does assume that the reader is comfortable in R and knows how to use dplyr for data manipulation. If you need a refresher on these topics, I cannot recommend “R for data science” highly enough. It’s a fabulous resource!\nOn the Arrow side it would help a little if you have some vague idea of what Arrow is about. I will of course explain as I go, but if you’re looking for a post that starts at the very beginning, I wrote a post on “Getting started with Apache Arrow” that does exactly this and discusses a lot of the basics.\nFinally, a tiny R warning: later in the post I will do a little excursion into object oriented programming and metaprogramming in R, which will be familiar to some but not all readers. If you’re not comfortable with these topics, you should still be okay to skim those sections and still get the important parts of this post. It’s not essential to understand the main ideas.\n\n\n\n\n\n\n\nThe Great God Ember. Capricious, chaotic, and utterly unreliable unless what you’re looking for is a whimsical death. Pretty much the opposite of what we’d hope for in a healthy open source community really! He is, however, a very entertaining character. Image via giphy, copyright syfy"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#what-is-arrow",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#what-is-arrow",
    "title": "Binding Apache Arrow to R",
    "section": "What is Arrow?",
    "text": "What is Arrow?\n\nIn case you decided not to read the introductory “Getting started with Apache Arrow” post, here’s an extremely condensed version. Apache Arrow is a standard and open source library that represents tabular data efficiently in memory. More generally it refers to a collection of tools used to work with Arrow data. There are libraries supporting Arrow in many different programming languages, including C, C++, C#, Go, Java, JavaScript, Julia, MATLAB, Python, R, Ruby, and Rust. It’s pretty cool."
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#using-arrow-in-r",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#using-arrow-in-r",
    "title": "Binding Apache Arrow to R",
    "section": "Using Arrow in R",
    "text": "Using Arrow in R\n\nA fundamental thing to understand about the arrow package in R is that it doesn’t implement the Apache Arrow standard directly. In fact, it tries very hard not to do any of the heavy lifting itself. There’s a C++ library that does that in a super efficient way, and the job of the R package is to supply bindings that allow the R user to interact with that library using a familiar interface. The C++ library is called libarrow. Although the long term goal is to make the integration so seamless that you can use the arrow R package without ever needing to understand the C++ library, my experience has been that most people want to know something about what’s happening under the hood. It can be unsettling to find yourself programming with tools you don’t quite understand, so I’ll dig a little deeper in this post.\nLet’s start with the C++ library. The role of libarrow is to do all the heavy computational work. It implements all the Arrow standards for representing tabular data in memory, provides support for the Apache “Inter-Process Communication” (IPC) protocol that lets you efficiently transfer data from one application to another, and supplies various compute kernels that allow you to do some data wrangling when your data are represented as an Arrow table.1 It is, fundamentally, the engine that makes everything work.\nWhat about the R package? The role of arrow is to expose the functionality of libarrow to the R user, to make that functionality feel “natural” in R, and to make it easier for R users to write Arrow code that is smoothly interoperable with Arrow code written in other languages (e.g., Python). In order to give you the flexibility you need, the arrow package allows you to interact with libarrow at three different levels of abstraction:\n\nThere’s a heavily abstracted interface that uses the dplyr bindings supplied by arrow. This version strives to make libarrow almost completely invisible, hidden behind an interface that uses familiar R function names.\nThere’s a lightly abstracted interface you can access using the arrow_*() functions. This version exposes the libarrow functions without attempting to exactly mirror any particular R functions, and provides a little syntactic sugar to make your life easier.\nFinally, there’s a minimally abstracted interface using call_function(). This version provides a bare bones interface, without any of the syntactic sugar.\n\nOver the next few sections section I’ll talk about these three levels of abstraction. So let’s load the packages we’re going to need for this post and dive right in!\n\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(arrow)\n\n\n\n\n\n\n\nPenny Adiyodi in the Neitherlands, diving head first into a fountain that transports him to a new and magical world. I cannot stress enough that Penny does not, by and large, make good choices. Impulse control is a virtue, but not one that he possesses in abundance. Image via giphy, copyright syfy"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#using-dplyr-bindings",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#using-dplyr-bindings",
    "title": "Binding Apache Arrow to R",
    "section": "Using “arrowplyr” bindings",
    "text": "Using “arrowplyr” bindings\n\n\nI think I have some special fish magicks.     – Josh Hoberman (The Magicians: Season 4, Episode 13)\n\nWhen I wrote my Getting started with Apache Arrow post, I concluded with an illustration of how you can write dplyr code that will work smoothly in R even when the data themselves are stored in Arrow. Here’s a little recap of how that works, using a tiny data set I pulled from The Magicians Wikipedia page. Here’s what that data set looks like:\n\nmagicians <- read_csv_arrow(\"magicians.csv\", as_data_frame = TRUE)\nmagicians\n\n# A tibble: 65 × 6\n   season episode title                                air_date   rating viewers\n    <int>   <int> <chr>                                <date>      <dbl>   <dbl>\n 1      1       1 Unauthorized Magic                   2015-12-16    0.2    0.92\n 2      1       2 The Source of Magic                  2016-01-25    0.4    1.11\n 3      1       3 Consequences of Advanced Spellcasti… 2016-02-01    0.4    0.9 \n 4      1       4 The World in the Walls               2016-02-08    0.3    0.75\n 5      1       5 Mendings, Major and Minor            2016-02-15    0.3    0.75\n 6      1       6 Impractical Applications             2016-02-22    0.3    0.65\n 7      1       7 The Mayakovsky Circumstance          2016-02-29    0.3    0.7 \n 8      1       8 The Strangled Heart                  2016-03-07    0.3    0.67\n 9      1       9 The Writing Room                     2016-03-14    0.3    0.71\n10      1      10 Homecoming                           2016-03-21    0.3    0.78\n# … with 55 more rows\n\n\nIn the code above I used the read_csv_arrow() function from the arrow package. If you’ve used the read_csv() function from readr this will seem very familiar: although Arrow C++ code is doing a lot of the work under the hood, the Arrow options have been chosen to mirror the familiar readr interface. The as_data_frame argument is specific to arrow though: when it is TRUE the data are imported into R as a data frame or tibble, and when it is FALSE the data are imported into Arrow. Strictly speaking I didn’t need to specify it in this example because TRUE is the default value. I ony included here so that I could draw attention to it.\nOkay, now that we have the data let’s start with a fairly typical data analysis process: computing summary variables. Perhaps I want to know the average popularity and ratings for each season of The Magicians, and extract the year in which the season aired. The dplyr package provides me with the tools I need to do this, using functions like mutate() to create new variables, group_by() to specify grouping variables, and summarise() to aggregate data within group:\n\nmagicians %>% \n  mutate(year = year(air_date)) %>% \n  group_by(season) %>% \n  summarise(\n    viewers = mean(viewers),\n    rating = mean(rating), \n    year = max(year)\n  )\n\n# A tibble: 5 × 4\n  season viewers rating  year\n   <int>   <dbl>  <dbl> <dbl>\n1      1   0.776  0.308  2016\n2      2   0.788  0.323  2017\n3      3   0.696  0.269  2018\n4      4   0.541  0.2    2019\n5      5   0.353  0.111  2020\n\n\nAll of these computations take place within R. The magicians data set is stored in R, and all the calculations are done using this data structure.\nWhat can we do when the data are stored in Arrow? It turns out the code is almost identical, but the first thing I’ll need to do is load the data into Arrow. The simplest way to do this is to set as_data_frame = FALSE when calling arrow_read_csv()\n\narrowmagicks <- read_csv_arrow(\"magicians.csv\", as_data_frame = FALSE)\narrowmagicks\n\nTable\n65 rows x 6 columns\n$season <int64>\n$episode <int64>\n$title <string>\n$air_date <date32[day]>\n$rating <double>\n$viewers <double>\n\n\n\n\nThe “arrowmagicks” variable name is a reference to the quote at the start of the section. For a while Josh was convinced he had been gifted with special magic because he had been a fish. It made sense at the time, I guess? It’s a weird show\nWhen I do this, two things happen. First, a data set is created outside of R in memory allocated to Arrow: all of the computations will be done on that data set. Second, the arrowmagicks variable is created inside R, which consists of a pointer to the actual data along with some handy metadata.\nThe most natural way to work with this data in R is to make sure that both the arrow and dplyr packages are loaded, and then write regular dplyr code. You can do this because the arrow package supplies methods for dplyr functions, and these methods will be called whenever the input data is an Arrow Table. I’ll refer to this data analyses that use this workflow as “arrowplyr pipelines”. Here’s an example of an arrowplyr pipeline:\n\n\nI’ve chosen not to boldface the “arrowplyr” terminology. arrow is a package and dplyr is a package, but arrowplyr isn’t. It’s simply a convenient fiction\n\narrowmagicks %>% \n  mutate(year = year(air_date)) %>% \n  group_by(season) %>% \n  summarise(\n    viewers = mean(viewers),\n    rating = mean(rating), \n    year = max(year)\n  )\n\nTable (query)\nseason: int64\nviewers: double\nrating: double\nyear: int64\n\nSee $.data for the source Arrow object\n\n\nIt looks like a regular dplyr pipeline, but because the input is arrowmagicks (an Arrow Table object), the effect of this is construct a query that can be passed to libarrow to be evaluated.\nIt’s important to realise that at this point, all we have done is define a query: no computations have been performed on the Arrow data. This is a deliberate choice for efficiency purposes: on the C++ side there are a lot of performance optimisations that are only possible because libarrow has access to the entire query before any computations are performed. As a consequence of this, you need to explicitly tell Arrow when you want to pull the trigger and execute the query.\n\n\nLater in the post I’ll talk about Arrow Expressions, the tool that powers this trickery\nThere are two ways to trigger query execution, one using the compute() function and the other using collect(). These two functions behave slightly differently and are useful for different purposes. The compute() function runs the query, but leaves the resulting data inside Arrow:\n\narrowmagicks %>% \n  mutate(year = year(air_date)) %>% \n  group_by(season) %>% \n  summarise(\n    viewers = mean(viewers),\n    rating = mean(rating), \n    year = max(year)\n  ) %>% \n  compute()\n\nTable\n5 rows x 4 columns\n$season <int64>\n$viewers <double>\n$rating <double>\n$year <int64>\n\n\nThis is useful whenever you’re creating an intermediate data set that you want to reuse in Arrow later, but don’t need to use this intermediate data structure inside R. If, however, you want the output to be pulled into R so that you can do R computation with it, use the collect() function:\n\narrowmagicks %>% \n  mutate(year = year(air_date)) %>% \n  group_by(season) %>% \n  summarise(\n    viewers = mean(viewers),\n    rating = mean(rating), \n    year = max(year)\n  ) %>% \n  collect()\n\n# A tibble: 5 × 4\n  season viewers rating  year\n   <int>   <dbl>  <dbl> <int>\n1      1   0.776  0.308  2016\n2      2   0.788  0.323  2017\n3      3   0.696  0.269  2018\n4      4   0.541  0.2    2019\n5      5   0.353  0.111  2020\n\n\nThe nice thing for R users is that all of this feels like regular R code. Under the hood libarrow is doing all the serious computation, but at the R level the user really doesn’t need to worry too much about that. The arrowplyr toolkit works seamlessly and invisibly.\nIn our ideal world, the arrowplyr interface is all you would ever need to use. Internally, the arrow package would intercept all the R function calls you make, and replace them with an equivalent function that performs exactly the same computation using libarrow. You the user would never need to think about what’s happening under the hood.\nReal life, however, is filled with leaky abstractions, and arrowplyr is no exception. Because it’s a huge project that is under active development, there’s a lot of functionality being introduced. As an example, the current version of the package (v6.0.1) has limited support for tidyverse packages like lubridate and stringr. It’s awesome that this functionality is coming online, but because it’s happening so quickly there are gaps. The small contribution that I made today was to fill one of those gaps: currently, you can’t refer to the leap_year() function from lubridate in an arrowplyr pipeline. Well, technically you can, but whenever arrow encounters a function it doesn’t know how to execute in Arrow it throws a warning, pulls the data into R, and completes the query using native R code. Here’s what that looks like:\n\narrowmagicks %>% \n  mutate(\n    year = year(air_date), \n    leap = leap_year(air_date)\n  ) %>% \n  collect()\n\n# A tibble: 65 × 8\n   season episode title                    air_date   rating viewers  year leap \n    <int>   <int> <chr>                    <date>      <dbl>   <dbl> <int> <lgl>\n 1      1       1 Unauthorized Magic       2015-12-16    0.2    0.92  2015 FALSE\n 2      1       2 The Source of Magic      2016-01-25    0.4    1.11  2016 TRUE \n 3      1       3 Consequences of Advance… 2016-02-01    0.4    0.9   2016 TRUE \n 4      1       4 The World in the Walls   2016-02-08    0.3    0.75  2016 TRUE \n 5      1       5 Mendings, Major and Min… 2016-02-15    0.3    0.75  2016 TRUE \n 6      1       6 Impractical Applications 2016-02-22    0.3    0.65  2016 TRUE \n 7      1       7 The Mayakovsky Circumst… 2016-02-29    0.3    0.7   2016 TRUE \n 8      1       8 The Strangled Heart      2016-03-07    0.3    0.67  2016 TRUE \n 9      1       9 The Writing Room         2016-03-14    0.3    0.71  2016 TRUE \n10      1      10 Homecoming               2016-03-21    0.3    0.78  2016 TRUE \n# … with 55 more rows\n\n\n\n\nThis is a bit of an oversimplification. The “warn and pull into R” behaviour shown here is what happens when the data is a Table object. If it is a Dataset object, arrow throws an error\nAn answer has been calculated, but the warning is there to tell you that the computations weren’t performed in Arrow. Realising that it doesn’t know how to interpret leap_year(), the arrow package has tried to “fail gracefully” and pulled everything back into R. The end result of all this is that the code executes as a regular dplyr pipeline and not as an arrowplyr one. It’s not the worst possible outcome, but it still makes me sad. 😭\n\n\n\n\n\n\nQuentin from timeline 40 talking to Alice from timeline 23. Communication across incommensurate universes is difficult. In the show it requires a Tesla Flexion. In Arrow, we use dplyr bindings. Image via giphy, copyright syfy”"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#using-arrow-functions",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#using-arrow-functions",
    "title": "Binding Apache Arrow to R",
    "section": "Calling “arrow-prefix” functions",
    "text": "Calling “arrow-prefix” functions\n\nOkay, let’s dig a little deeper.\nIn the last section I talked about arrowplyr, a collection of dplyr bindings provided by the arrow package. These are designed to mimic their native R equivalents as seamlessly as possible to enable you to write familiar code. Internally, there’s quite a lot going on to make this magic work. In most cases, the arrow developers – which I guess includes me now! 🎉 – have rewritten the R functions that they mimic. We’ve done this in a way that the computations rely only the C++ compute functions provided by libarrow, thereby ensuring that the data never have to enter R. The arrowplyr interface is the way you’d usually interact with Arrow in R, but there are ways in which you can access the C++ compute functions a little more more directly. There are two different ways you can call these compute functions yourself. If you’re working within an arrowplyr pipeline it is (relatively!) straightforward, and that’s what I’ll talk about in this section. However, there is also a more direct method which I’ll discuss later in the post.\nTo see what compute functions are exposed by the C++ libarrow library, you can call list_compute_functions() from R:\n\nlist_compute_functions()\n\n  [1] \"abs\"                             \"abs_checked\"                    \n  [3] \"acos\"                            \"acos_checked\"                   \n  [5] \"add\"                             \"add_checked\"                    \n  [7] \"all\"                             \"and\"                            \n  [9] \"and_kleene\"                      \"and_not\"                        \n [11] \"and_not_kleene\"                  \"any\"                            \n [13] \"approximate_median\"              \"array_filter\"                   \n [15] \"array_sort_indices\"              \"array_take\"                     \n [17] \"ascii_capitalize\"                \"ascii_center\"                   \n [19] \"ascii_is_alnum\"                  \"ascii_is_alpha\"                 \n....\n\n\nThe actual output continues for quite a while: there are currently 240 compute functions, most of which are low level functions needed to perform basic computational operations.\nLet’s imagine you’re writing dplyr code to work with datetime data in a Table object like arrowmagicks. If you were working with native R data like magicians, you can do something like this:\n\nstart_date <- as.Date(\"2015-12-16\")\n\nmagicians %>% \n  mutate(days = air_date - start_date)\n\n# A tibble: 65 × 7\n   season episode title                          air_date   rating viewers days \n    <int>   <int> <chr>                          <date>      <dbl>   <dbl> <drt>\n 1      1       1 Unauthorized Magic             2015-12-16    0.2    0.92  0 d…\n 2      1       2 The Source of Magic            2016-01-25    0.4    1.11 40 d…\n 3      1       3 Consequences of Advanced Spel… 2016-02-01    0.4    0.9  47 d…\n 4      1       4 The World in the Walls         2016-02-08    0.3    0.75 54 d…\n 5      1       5 Mendings, Major and Minor      2016-02-15    0.3    0.75 61 d…\n 6      1       6 Impractical Applications       2016-02-22    0.3    0.65 68 d…\n 7      1       7 The Mayakovsky Circumstance    2016-02-29    0.3    0.7  75 d…\n 8      1       8 The Strangled Heart            2016-03-07    0.3    0.67 82 d…\n 9      1       9 The Writing Room               2016-03-14    0.3    0.71 89 d…\n10      1      10 Homecoming                     2016-03-21    0.3    0.78 96 d…\n# … with 55 more rows\n\n\nHere I’ve created a new days column that counts the number of days that have elapsed between the air_date for an episode and the start_date (December 16th, 2015) when the first episode of Season 1 aired. There are a lot of data analysis situations in which you might want to do something like this, but right now you can’t actually do this using the arrow dplyr bindings because temporal arithmetic is a work in progress. In the not-too-distant future users should be able to expect code like this to work seamlessly, but right now it doesn’t. If you try it right now, you get this error:\n\n\nImproving support for date/time calculations is one of the things I’m working on\n\narrowmagicks %>% \n  mutate(days = air_date - start_date) %>% \n  collect()\n\n# A tibble: 65 × 7\n   season episode title                          air_date   rating viewers days \n    <int>   <int> <chr>                          <date>      <dbl>   <dbl> <drt>\n 1      1       1 Unauthorized Magic             2015-12-16    0.2    0.92     …\n 2      1       2 The Source of Magic            2016-01-25    0.4    1.11 3456…\n 3      1       3 Consequences of Advanced Spel… 2016-02-01    0.4    0.9  4060…\n 4      1       4 The World in the Walls         2016-02-08    0.3    0.75 4665…\n 5      1       5 Mendings, Major and Minor      2016-02-15    0.3    0.75 5270…\n 6      1       6 Impractical Applications       2016-02-22    0.3    0.65 5875…\n 7      1       7 The Mayakovsky Circumstance    2016-02-29    0.3    0.7  6480…\n 8      1       8 The Strangled Heart            2016-03-07    0.3    0.67 7084…\n 9      1       9 The Writing Room               2016-03-14    0.3    0.71 7689…\n10      1      10 Homecoming                     2016-03-21    0.3    0.78 8294…\n# … with 55 more rows\n\n\nRight now, there are no general purpose arithmetic operations in arrow that allow you to subtract one date from another. However, because I chose this example rather carefully to find an edge case where the R package is missing some libarrow functionality, it turns out there is actually a days_between() function in libarrow that we could use to solve this problem, and it’s not too hard to use it. If you want to call one of the libarrow functions inside your dplyr pipeline, all you have to do is add an arrow_ prefix to the function name. For example, the C++ days_between() function becomes arrow_days_between() when called within the arrow dplyr pipeline:\n\narrowmagicks %>% \n  mutate(days = arrow_days_between(start_date, air_date)) %>% \n  collect()\n\n# A tibble: 65 × 7\n   season episode title                          air_date   rating viewers  days\n    <int>   <int> <chr>                          <date>      <dbl>   <dbl> <int>\n 1      1       1 Unauthorized Magic             2015-12-16    0.2    0.92     0\n 2      1       2 The Source of Magic            2016-01-25    0.4    1.11    40\n 3      1       3 Consequences of Advanced Spel… 2016-02-01    0.4    0.9     47\n 4      1       4 The World in the Walls         2016-02-08    0.3    0.75    54\n 5      1       5 Mendings, Major and Minor      2016-02-15    0.3    0.75    61\n 6      1       6 Impractical Applications       2016-02-22    0.3    0.65    68\n 7      1       7 The Mayakovsky Circumstance    2016-02-29    0.3    0.7     75\n 8      1       8 The Strangled Heart            2016-03-07    0.3    0.67    82\n 9      1       9 The Writing Room               2016-03-14    0.3    0.71    89\n10      1      10 Homecoming                     2016-03-21    0.3    0.78    96\n# … with 55 more rows\n\n\nNotice there’s no warning message here? That’s because the computations were done in Arrow and the data have not been pulled into R."
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#a-slightly-evil-digression",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#a-slightly-evil-digression",
    "title": "Binding Apache Arrow to R",
    "section": "A slightly-evil digression",
    "text": "A slightly-evil digression\n\n\nMarina, blatantly lying:     “Hi. I’m Marina. I’m here to help.”  Josh, missing important memories:     “So you’re like some powerful, benevolent White Witch?”  Marina, comically sincere:     “Uh-huh.”      – The Magicians: Season 4, Episode 2\n\n\n\nAt this point in the show everybody except the currently-amnesic main characters knows that Marina has no interest in helping anyone except Marina. I love Marina so much\nOkay, here’s a puzzle. In the previous section I used the arrow_days_between() function in the middle of a dplyr pipe to work around a current limitation in arrow. What happens if I try to call this function in another context?\n\ntoday <- as.Date(\"2022-01-18\")\n\narrow_days_between(start_date, today)\n\nError in arrow_days_between(start_date, today): could not find function \"arrow_days_between\"\n\n\nIt turns out there is no R function called arrow_days_between(). This is … surprising, to say the least. I mean, it really does look like I used this function in the last section, doesn’t it? How does this work? The answer to this requires a slightly deeper understanding of what the dplyr bindings in arrow do, and it’s kind of a two-part answer.\n\nPart one: Object oriented programming\nLet’s consider the mutate() function. dplyr defines mutate() as an S3 generic function, which allows it to display “polymorphism”: it behaves differently depending on what kind of object is passed to to the generic. When you pass a data frame to mutate(), the call is “dispatched” to the mutate.arrow_dplyr_query() methods supplied by (but not exported by) dplyr. The arrow package builds on this by supplying methods that apply for Arrow objects. Specifically, there are internal functions mutate.ArrowTabular(), mutate.Dataset(), and mutate.arrow_dplyr_query() that are used to provide mutate() functionality for Arrow data sets. In other words, the “top level” dplyr functions in arrow are S3 methods, and method dispatch is the mechanism that does the work.\n\n\nPart two: Metaprogramming\nNow let’s consider the leap_year() function that my contribution focused on. Not only is this not a generic function, it’s not even a dplyr function. It’s a regular function in the lubridate package. So how is it possible for arrow to mimic the behaviour of lubridate::leap_year() without messing up lubridate? This is where the dplyr binding part comes in. Let’s imagine that I’d written an actual function called arrowish_leap_year() that performs leap year calculations for Arrow data. If I’d done this inside the arrow package2 then I’d include a line like this to register a binding:\n\n\nI’ll show you how to write your own “arrowish” functions later in the post\n\nregister_binding(\"leap_year\", arrowish_leap_year)\n\nOnce the binding has been registered, whenever leap_year() is encountered within one of the arrow-supplied dplyr functions, R will substitute my arrowish_leap_year() function in place of the lubridate::leap_year() function that would normally be called. This is only possible because R has extremely sophisticated metaprogramming tools: you (the developer) can write functions that “capture” the code that the user input, and if necessary modify that code before R evaluates it. This is a very powerful tool for constructing domain-specific languages within R. The tidyverse uses it extensively, and the arrow package does too. The dplyr bindings inside arrow use metaprogramming tricks to modify the user input in such a way that – in this example – the user input is interpreted as if the user had called arrowish_leap_year() rather than leap_year().\n\n\nCooperative magic\nTaken together, these two pieces give us the answer to our puzzle. The call to arrow_days_between() works in my original example because that call was constructed within the context of an arrow-supplied mutate() function. The interpretation of this code isn’t performed by dplyr it is handled by arrow. Internally, arrow uses metaprogramming magic to ensure that arrow_days_between() is reinterpreted as a call to the libarrow days_between() function. But that metaprogramming magic doesn’t apply anywhere except the arrowplyr context. If you try to call arrow_days_between() from the R console or even in a regular dplyr pipeline, you get an error because technically speaking this function doesn’t exist.\n\n\n\n\n\n\nI guess there’s a connection between slightly-evil-Julia burning down the talking trees and my slightly-evil digression? Sort of. I mean the truth is just that I just love this scene and secretly wish I was her. Of all the characters Julia has the most personally transformative arc (in my opinion), in both good ways and bad. There’s a lot going on with her life, her person, and her body. I relate to that. Image via giphy, copyright syfy"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#using-call-function",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#using-call-function",
    "title": "Binding Apache Arrow to R",
    "section": "Calling libarrow directly",
    "text": "Calling libarrow directly\n\nThe weirdness of that digression leads naturally to a practical question. Given that the “arrow-prefix” function don’t actually exist in the usual sense of the term, and the corresponding bindings can only be called in an arrowplyr context, how the heck does an R developer call the libarrow functions directly? In everyday data analysis you wouldn’t want to do this very often, but from a programming perspective it matters: if you want to write your own functions that play nicely with arrowplyr pipelines, it’s very handy to know how to call libarrow directly.\nSo let’s strip back another level of abstraction!\nShould you ever find yourself wanting to call libarrow compute functions directly from R, call_function() will become your new best friend. It provides a very minimal interface that exposes the libarrow functions to R. The “bare bones” nature of this interface has advantages and disadvantages. The advantage is simplicity: your code doesn’t depend on any of the fancy bells and whistles. Those are fabulous from the user perspective, but from a developer point of view you usually want to keep it simple. The price you pay for this is that you must pass appropriate Arrow objects. You can’t pass a regular R object to a libarrow function and expect it to work. For example:\n\ncall_function(\"days_between\", start_date, today)\n\nError: Argument 1 is of class Date but it must be one of \"Array\", \"ChunkedArray\", \"RecordBatch\", \"Table\", or \"Scalar\"\n\n\nThis doesn’t work because start_date and today are R-native Date objects and do not refer to any data structures in Arrow. The libarrow functions expect to receive pointers to Arrow objects. To fix the previous example, all we need to do is create Arrow Scalars for each date. Here’s how we do that:\n\narrow_start_date <- Scalar$create(start_date)\narrow_today <- Scalar$create(today)\n\narrow_start_date\n\nScalar\n2015-12-16\n\n\nThe arrow_start_date and arrow_today variables are R data structures, but they’re only thin wrappers. The actual data are stored in Arrow, and the R objects are really just pointers to the Arrow data. These objects are suitable for passing to the libarrow days_between() function, and this works:\n\ncall_function(\"days_between\", arrow_start_date, arrow_today)\n\nScalar\n2225\n\n\nHuh. Apparently it took me over 2000 days to write a proper fangirl post about The Magicians. I’m really late to the pop culture party, aren’t I? Oh dear. I’m getting old.\n\n\n\n\n\n\nI’m getting lazier with these connections. Using a Library gif because I’m talking about the C++ library? I mean really, you’d think I’d be better than that wouldn’t you? But no. I am not. Image via giphy, copyright syfy"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#arrow-expressions",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#arrow-expressions",
    "title": "Binding Apache Arrow to R",
    "section": "Arrow expressions",
    "text": "Arrow expressions\n\nThere’s one more foundational topic I should discuss before I can show you how to write arrowplyr-friendly functions, and that’s Arrow Expressions. When I introduced arrowplyr early in the post I noted that most of your code is used to specify a query, and that query doesn’t get evaluated until compute() or collect() is called. If you want to write code that plays nicely with this workflow, you need to ensure that your custom functions return an Arrow Expression.\nThe basic idea behind expressions is probably familiar to R users, since they are what powers the metaprogramming capabilities of the language and are used extensively throughout tidyverse as well as base R. In base R, the quote() function is used to capture a user expression and eval() is used to force it to evaluate. Here’s a simple example where I use quote() to “capture” some R code and prevent it from evaluating:\n\nhead_expr <- quote(head(magicians, n = 3))\nhead_expr\n\nhead(magicians, n = 3)\n\n\nIf I wanted to be clever I could modify the code in head_expr before allowing R to pull the trigger on evaluating it. I could combine a lot of expressions together, change parts of the code as needed, and evaluate them wherever I wanted. As you might imagine, this is super useful for creating domain specific languages within R. But this isn’t a post about metaprogramming so let’s evaluate it now:\n\neval(head_expr)\n\n# A tibble: 3 × 6\n  season episode title                                 air_date   rating viewers\n   <int>   <int> <chr>                                 <date>      <dbl>   <dbl>\n1      1       1 Unauthorized Magic                    2015-12-16    0.2    0.92\n2      1       2 The Source of Magic                   2016-01-25    0.4    1.11\n3      1       3 Consequences of Advanced Spellcasting 2016-02-01    0.4    0.9 \n\n\nThe example above uses native R code. It’s not tied to Arrow in any sense. However, the arrow package provides a mechanism for doing something similar in an Arrow context. For example, here’s me creating a character string as an Arrow Scalar:\n\nfillory <- Scalar$create(\"A world as intricate as filigree\")\nfillory\n\nScalar\nA world as intricate as filigree\n\n\nHere’s me creating the corresponding object within an Arrow Expression:\n\nfillory <- Expression$scalar(\n  Scalar$create(\"A world as intricate as filigree\")\n)\nfillory\n\nExpression\n\"A world as intricate as filigree\"\n\n\nI suspect this would not seem particularly impressive on its own, but you can use the same idea to create function calls that can be evaluated later within the Arrow context:\n\nember <- Expression$create(\"utf8_capitalize\", fillory)\nember\n\nExpression\nutf8_capitalize(\"A world as intricate as filigree\")\n\n\nSo close. We are so very close to the end now.\n\n\n\n\n\nOkay look, I’ll level with you. At this point there is absolutely no connection between the gifs and the content. This post is getting very long and my brain is fried. I need a short break to appreciate the beautiful people, and Kings Idri and Eliot are both very beautiful people. Image via giphy, copyright syfy"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#arrowish-functions",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#arrowish-functions",
    "title": "Binding Apache Arrow to R",
    "section": "Writing arrowplyr functions",
    "text": "Writing arrowplyr functions\nAt long last we have all the ingredients needed to write a function that can be used in an arrowplyr pipeline. Here’s a simple implementation of the base R toupper() function\n\narrowish_toupper <- function(x) {\n  Expression$create(\"utf8_upper\", x)\n}\n\nAs it happens arrowplyr pipelines already support the toupper() function, so there really wasn’t a need for me to write this. However, at present they don’t support the lubridate leap_year() function, which was the purpose of my very small contribution today. An Arrow friendly version of leap_year() looks like this:\n\narrowish_leap_year <- function(date) {\n   year <- Expression$create(\"year\", date)\n  (year %% 4 == 0) & ((year %% 100 != 0) | (year %% 400 == 0))\n}\n\nBefore putting our functions into action, let’s see what happens when we try to write a simple data analysis pipeline without them:\n\narrowmagicks %>% \n  mutate(\n    title = toupper(title),\n    year = year(air_date), \n    leap = leap_year(air_date)\n  ) %>% \n  collect()\n\n# A tibble: 65 × 8\n   season episode title                    air_date   rating viewers  year leap \n    <int>   <int> <chr>                    <date>      <dbl>   <dbl> <int> <lgl>\n 1      1       1 UNAUTHORIZED MAGIC       2015-12-16    0.2    0.92  2015 FALSE\n 2      1       2 THE SOURCE OF MAGIC      2016-01-25    0.4    1.11  2016 TRUE \n 3      1       3 CONSEQUENCES OF ADVANCE… 2016-02-01    0.4    0.9   2016 TRUE \n 4      1       4 THE WORLD IN THE WALLS   2016-02-08    0.3    0.75  2016 TRUE \n 5      1       5 MENDINGS, MAJOR AND MIN… 2016-02-15    0.3    0.75  2016 TRUE \n 6      1       6 IMPRACTICAL APPLICATIONS 2016-02-22    0.3    0.65  2016 TRUE \n 7      1       7 THE MAYAKOVSKY CIRCUMST… 2016-02-29    0.3    0.7   2016 TRUE \n 8      1       8 THE STRANGLED HEART      2016-03-07    0.3    0.67  2016 TRUE \n 9      1       9 THE WRITING ROOM         2016-03-14    0.3    0.71  2016 TRUE \n10      1      10 HOMECOMING               2016-03-21    0.3    0.78  2016 TRUE \n# … with 55 more rows\n\n\n\n\nThe internal arrow function that handles this is called “abandon_ship”. No, I don’t know why I felt the need to mention this`\nYes, it returns the correct answer, but only because arrow detected a function it doesn’t understand and has “abandoned ship”. It pulled the data into R and let dplyr do all the work. Now let’s see what happens when we use our functions instead:\n\narrowmagicks %>% \n  mutate(\n    title = arrowish_toupper(title),\n    year = year(air_date),\n    leap = arrowish_leap_year(air_date)\n  ) %>% \n  collect()\n\n# A tibble: 65 × 8\n   season episode title                    air_date   rating viewers  year leap \n    <int>   <int> <chr>                    <date>      <dbl>   <dbl> <int> <lgl>\n 1      1       1 UNAUTHORIZED MAGIC       2015-12-16    0.2    0.92  2015 FALSE\n 2      1       2 THE SOURCE OF MAGIC      2016-01-25    0.4    1.11  2016 TRUE \n 3      1       3 CONSEQUENCES OF ADVANCE… 2016-02-01    0.4    0.9   2016 TRUE \n 4      1       4 THE WORLD IN THE WALLS   2016-02-08    0.3    0.75  2016 TRUE \n 5      1       5 MENDINGS, MAJOR AND MIN… 2016-02-15    0.3    0.75  2016 TRUE \n 6      1       6 IMPRACTICAL APPLICATIONS 2016-02-22    0.3    0.65  2016 TRUE \n 7      1       7 THE MAYAKOVSKY CIRCUMST… 2016-02-29    0.3    0.7   2016 TRUE \n 8      1       8 THE STRANGLED HEART      2016-03-07    0.3    0.67  2016 TRUE \n 9      1       9 THE WRITING ROOM         2016-03-14    0.3    0.71  2016 TRUE \n10      1      10 HOMECOMING               2016-03-21    0.3    0.78  2016 TRUE \n# … with 55 more rows\n\n\nEverything works perfectly within Arrow. No ships are abandoned, the arrowplyr pipeline springs no leaks, and we all live happily ever after.\nSort of.\nI mean, we’re all still alive.\nThat has to count as a win, right? 🎉\n\n\n\n\n\n\nEliot and Margo applaud your success. They are the best characters, and you are also the best because you have made it to the end of a long and strange blog post. Image via giphy, copyright syfy"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#epilogue-wheres-the-rest-of-the-owl",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#epilogue-wheres-the-rest-of-the-owl",
    "title": "Binding Apache Arrow to R",
    "section": "Epilogue: Where’s the rest of the owl?",
    "text": "Epilogue: Where’s the rest of the owl?\n\n\n\nIn case you don’t know the reference: how to draw an owl\nThe story I’ve told in this post is a little incomplete. I’ve shown you how to write a function like arrowish_leap_year() that can slot into a dplyr pipeline and operate on an Arrow data structure. But I haven’t said anything about the precise workings of how register_binding() works, in part because the details of the metaprogramming magic is one of the mysteries I’m currently unpacking while I dig into the code base.\nBut that’s not the only thing I’ve left unsaid. I haven’t talked about unit tests, for example. I haven’t talked about the social/technical process of getting code merged into the Arrow repository. If you’ve made it to the end of this post and are curious about joining the Arrow developer community, these are things you need to know about. I’ll probably write something about those topics later on, but in the meantime here are some fabulous resources that might be handy:\n\nApache Arrow New Contributors Guide (thank you to Alenka Frim!)\nDevelopers Guide to Writing Bindings (thank you to Nic Crane!)\nApache Arrow R Cookbook (thank you to Nic Crane again)\n\nEnjoy! 🍰"
  },
  {
    "objectID": "posts/2022-11-30_unpacking-arrow-datasets/index.html",
    "href": "posts/2022-11-30_unpacking-arrow-datasets/index.html",
    "title": "Unpacking Arrow Datasets",
    "section": "",
    "text": "Hello again lovely people. I am, once again, blogging about Apache Arrow and I’m not even sorry. Oh well.\nIn an earlier post I wrote about Tables and other in-memory data structures that Arrow uses to represent data objects. That meant the bulk of the post was focused on Record Batch and Table objects and the constituent objects used to define columns in one of these things (Arrays and Chunked Arrays).\nWhat I didn’t really talk about in that post was Datasets, which are used to represent data (typically larger-than-memory data) that are stored on-disk rather than in-memory. Okay, fine, yeah. Technically I did include a section on Datasets at the end of the post, but I was a bit evasive. I gave an example showing how to use Datasets, but I really didn’t talk much about what they are.\nI had a very good reason for this, dear reader, and that reason is this: when I wrote that post I had no f**king idea whatsoever how Datasets worked. I knew how to use them, but if you’d asked me questions about how the magic works I couldn’t have told you.1\nSince that time I’ve learned a few things, and because I’m an annoying person I’m going to tell you about them.2"
  },
  {
    "objectID": "posts/2022-11-30_unpacking-arrow-datasets/index.html#quick-recap-record-batches-and-tables",
    "href": "posts/2022-11-30_unpacking-arrow-datasets/index.html#quick-recap-record-batches-and-tables",
    "title": "Unpacking Arrow Datasets",
    "section": "Quick recap: Record Batches and Tables",
    "text": "Quick recap: Record Batches and Tables\nAt this point I’ve written quite a few posts about Arrow, and it’s not necessarily a good idea for me to assume that you’ve had the misfortune to read all3 of them. So here’s a quick recap of some of the key Arrow data structures that I’ve talked about in other posts…\nLet’s start with Record Batches. A Record Batch is tabular data structure comprised of named Arrays,4 and an accompanying Schema5 that specifies the name and data type associated with each Array. We can create one manually using record_batch()\n\nrb <- record_batch(\n  strs = c(\"hello\", \"amazing\", \"and\", \"cruel\", \"world\"), \n  ints = c(1L, NA, 2L, 4L, 8L),\n  dbls = c(1.1, 3.2, 0.2, NA, 11)\n)\nglimpse(rb)\n\nRecordBatch\n5 rows x 3 columns\n$ strs <string> \"hello\", \"amazing\", \"and\", \"cruel\", \"world\"\n$ ints  <int32> 1, NA, 2, 4, 8\n$ dbls <double> 1.1, 3.2, 0.2, NA, 11.0\n\n\nThis is a Record Batch containing 5 rows and 3 columns. The command rb[1:3, 1:2] extracts the first three rows and the first two columns:\n\nglimpse(rb[1:3, 1:2])\n\nRecordBatch\n3 rows x 2 columns\n$ strs <string> \"hello\", \"amazing\", \"and\"\n$ ints  <int32> 1, NA, 2\n\n\nThe structure of a Record Batch is shown below. In addition to the three Arrays specifying the columns, it includes an explicit Schema object containing relevant metadata:\n\nRecord Batches are a fundamental unit for data interchange in Arrow, but are not typically used for data analysis. The reason for this is that the constituent Arrays that store columns in a Record Batch are immutable: they cannot be modified or extended without creating a new object.6 When data arrive sequentially Record Batches can be inconvenient, because you can’t concatenate them. For that reason Tables are usually more practical…\nSo let’s turn to Tables next. From the user perspective a Table is very similar to a Record Batch but the constituent parts are Chunked Arrays. Chunked Arrays are flexible wrappers enclosing one or more Arrays.7 This makes it possible to concatenate tables. To quickly illustrate this, let’s first convert the rb Record Batch to a Table using arrow_table():\n\ndf1 <- arrow_table(rb)\n\nNow we create a second Table with the same column names and types, again using arrow_table():\n\ndf2 <- arrow_table(\n  strs = c(\"I\", \"love\", \"you\"), \n  ints = c(5L, 0L, 0L),\n  dbls = c(7.1, -0.1, 2)\n)\n\nWe can concatenate these using concat_tables():\n\ndf <- concat_tables(df1, df2)\nglimpse(df)\n\nTable\n8 rows x 3 columns\n$ strs <string> \"hello\", \"amazing\", \"and\", \"cruel\", \"world\", \"I\", \"love\", \"you\"\n$ ints  <int32> 1, NA, 2, 4, 8, 5, 0, 0\n$ dbls <double> 1.1, 3.2, 0.2, NA, 11.0, 7.1, -0.1, 2.0\n\n\nThe structure of this Table object is similar to the structure of the Record Batch object I showed earlier, but the columns are Chunked Arrays rather than simple Arrays:\n You can see this if we print out a single column:\n\ndf$strs\n\nChunkedArray\n<string>\n[\n  [\n    \"hello\",\n    \"amazing\",\n    \"and\",\n    \"cruel\",\n    \"world\"\n  ],\n  [\n    \"I\",\n    \"love\",\n    \"you\"\n  ]\n]\n\n\nThere’s a visual separation there between the different chunks, used to indicated where the boundaries between individual Arrays are. In practice though you actually don’t have to care about this because it’s not semantically meaningful. It’s there for purely technical reasons.\nBut all this is background. So let’s move on, shall we?"
  },
  {
    "objectID": "posts/2022-11-30_unpacking-arrow-datasets/index.html#so-datasets",
    "href": "posts/2022-11-30_unpacking-arrow-datasets/index.html#so-datasets",
    "title": "Unpacking Arrow Datasets",
    "section": "So… Datasets?",
    "text": "So… Datasets?\nOkay, what about Datasets? Like Record Batch and Table objects, a Dataset is used to represent tabular data. At an abstract level, a Dataset can be viewed as an object comprised of rows and columns, and just like Record Batches and Tables, it contains an explicit Schema that specifies the name and data type associated with each column.\nHowever, where Tables and Record Batches are data explicitly represented in-memory, a Dataset is not. Instead, a Dataset is an abstraction that refers to data stored on-disk in one or more files. Reading the data takes place only as needed, and only when a query is executed against the data. In this respect Arrow Datasets are a very different kind of object to Arrow Tables, but the arrow package is written in a way that the dplyr commands used to analyze Tables can also be applied to Datasets."
  },
  {
    "objectID": "posts/2022-11-30_unpacking-arrow-datasets/index.html#what-is-a-dataset-on-disk",
    "href": "posts/2022-11-30_unpacking-arrow-datasets/index.html#what-is-a-dataset-on-disk",
    "title": "Unpacking Arrow Datasets",
    "section": "What is a Dataset on-disk?",
    "text": "What is a Dataset on-disk?\nReduced to its simplest form, the on-disk structure of a Dataset is simply a collection of data files, each storing one subset of the data. These subsets are sometimes referred to as “fragments”, and the partitioning process is sometimes referred to as “sharding”. To illustrate how this works, I’ll write a multi-file dataset to disk manually, without using any of the Arrow Dataset functionality to do the work. I’ll keep it deliberately simple and use three small data frames, each containing one subset of the data we want to store:\n\ndf_a <- data.frame(id = 1:5, value = rnorm(5), subset = \"a\")\ndf_b <- data.frame(id = 6:10, value = rnorm(5), subset = \"b\")\ndf_c <- data.frame(id = 11:15, value = rnorm(5), subset = \"c\")\n\nOur intention is that each of the data frames should be stored in a separate data file. As you can see, this is a quite structured partitioning: all data where subset = \"a\" belong to one file, all data where subset = \"b\" belong to another file, and all data where subset = \"c\" belong to the third file.8\nThe first step is to define and create a folder that will hold all the files:\n\nds_dir <- \"mini-dataset\"\ndir.create(ds_dir)\n\nThe next step is to manually create a “Hive-style”9 folder structure:\n\nds_dir_a <- file.path(ds_dir, \"subset=a\")\nds_dir_b <- file.path(ds_dir, \"subset=b\")\nds_dir_c <- file.path(ds_dir, \"subset=c\")\n\ndir.create(ds_dir_a)\ndir.create(ds_dir_b)\ndir.create(ds_dir_c)\n\nNotice that we have named each folder in a “key=value” format that exactly describes the subset of data that will be written into that folder. This naming structure is the essence of Hive-style partitions.\nNow that we have the folders, we’ll use write_parquet() to create a single parquet file10 for each of the three subsets:\n\nwrite_parquet(df_a, file.path(ds_dir_a, \"part-0.parquet\"))\nwrite_parquet(df_b, file.path(ds_dir_b, \"part-0.parquet\"))\nwrite_parquet(df_c, file.path(ds_dir_c, \"part-0.parquet\"))\n\nIf I’d wanted to, I could have further subdivided the dataset. A folder can contain multiple files (part-0.parquet, part-1.parquet, etc) if we would like it to, though there’s no point whatsoever in doing that with such a tiny dataset. Similarly, there is no requirement to name the files part-0.parquet this way at all: it would have been fine to call these files subset-a.parquet, subset-b.parquet, and subset-c.parquet if I’d wanted to do that. I only chose part-0.parquet because that’s the default filename that the write_dataset() function in the arrow package generates!\nAlong the same lines, it isn’t necessary to use Hive-style partitions to use Arrow Datasets. The default behaviour of write_dataset() is to construct Hive-style partitions, and the default in open_dataset() is to look for Hive-style partitions, but it isn’t required.\nIn any case, I’ve created an on-disk parquet Dataset using Hive-style partitioning. My Dataset is defined by these files:\n\nlist.files(ds_dir, recursive = TRUE)\n\n[1] \"subset=a/part-0.parquet\" \"subset=b/part-0.parquet\"\n[3] \"subset=c/part-0.parquet\"\n\n\nThis is exciting, right? I mean, I’m excited. How could anyone not be completely enthralled by this thrilling exposition?\nAaaaanyway…. to verify that everything has worked, I’ll now try to open the data with open_dataset() and call glimpse() to inspect its contents:\n\nds <- open_dataset(ds_dir)\nglimpse(ds)\n\nFileSystemDataset with 3 Parquet files\n15 rows x 3 columns\n$ id      <int32> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n$ value  <double> -0.08458607, 0.84040013, -0.46348277, -0.55083500, 0.73604043,…\n$ subset <string> \"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c…\nCall `print()` for full schema details\n\n\nAs you can see, the ds Dataset object aggregates the three separate data files. In fact, in this particular case the Dataset is so small that values from all three files appear in the output of glimpse().\nNow, it’s pretty obvious that I wouldn’t use this workflow in my everyday life. Manually writing individual files like this is tiresome, especially when the exact same dataset can be created with the following command:\n\nds |> \n  group_by(subset) |>\n  write_dataset(\"mini-dataset\")\n\nAs an aside, even if ds happens to refer to an on-disk Dataset that is larger than memory, and you’re just wanting to rewrite it with a different file structure, this pipeline should still work without any risk of an out-of-memory error. This is thanks to the Dataset backpressure functionality11 in which the reader will back off and slow down if the writer has fallen too far behind and the memory cache is filling up. Or something like that. Look, I almost managed to make myself care about the details, okay?"
  },
  {
    "objectID": "posts/2022-11-30_unpacking-arrow-datasets/index.html#whats-stored-in-memory-by-the-dataset",
    "href": "posts/2022-11-30_unpacking-arrow-datasets/index.html#whats-stored-in-memory-by-the-dataset",
    "title": "Unpacking Arrow Datasets",
    "section": "What’s stored in-memory by the Dataset?",
    "text": "What’s stored in-memory by the Dataset?\nAssuming I have any readers left at this point in the post, I know what you’re all thinking:\n\nYes okay Danielle that’s fine, I get it, a Dataset is just a bunch of files on disk. But actually I already knew that. There has to be something in-memory though right? What’s that thing? Tell me about that.\n\nFirst off, rude. I was getting to it! Second, yes you are totally right. Sorry. So okay, in the last section I created this the ds object. Like most objects created by the arrow package, it’s an R6 object with a bunch of fields and methods that are used to wrap bindings to the corresponding Arrow C++ dark magic… sorry, um, methods. Anyway, for our purposes there are two things of importance: the ds object has an active binding specifying the Schema of the Dataset, and another one specifying the paths to all the files. That’s pretty much it. Paths to these files are stored in an active binding ds$files:\n\nds$files \n\n[1] \"/home/danielle/GitHub/sites/quarto-blog/posts/2022-11-30_unpacking-arrow-datasets/mini-dataset/subset=a/part-0.parquet\"\n[2] \"/home/danielle/GitHub/sites/quarto-blog/posts/2022-11-30_unpacking-arrow-datasets/mini-dataset/subset=b/part-0.parquet\"\n[3] \"/home/danielle/GitHub/sites/quarto-blog/posts/2022-11-30_unpacking-arrow-datasets/mini-dataset/subset=c/part-0.parquet\"\n\n\nThe Schema is stored as ds$schema:\n\nds$schema\n\nSchema\nid: int32\nvalue: double\nsubset: string\n\nSee $metadata for additional Schema metadata\n\n\nBy default this Schema is inferred by open_dataset() by inspecting the first file only, though it is possible to construct a unified schema after inspecting all files. To do this, set unify_schemas = TRUE when calling open_dataset(). It is also possible to use the schema argument to open_dataset() to specify the Schema explicitly (see the schema() function for details).\nIn any case, in most situations I think it’s reasonable to use this as the mental model of what the ds object contains:"
  },
  {
    "objectID": "posts/2022-11-30_unpacking-arrow-datasets/index.html#how-does-a-dataset-query-work",
    "href": "posts/2022-11-30_unpacking-arrow-datasets/index.html#how-does-a-dataset-query-work",
    "title": "Unpacking Arrow Datasets",
    "section": "How does a Dataset query work?",
    "text": "How does a Dataset query work?\nHm. So if the Dataset object12 is essentially nothing more than a Schema and a list of files, what happens at the time a query has to be evaluated? At some point the data (or at least some of it) do have to be read into memory in order to perform the compute operations! I mean, consider the following dplyr pipeline:\n\nds |>\n  filter(value > 0) |>\n  mutate(new_value = round(100 * value)) |>\n  select(id, subset, new_value) |>\n  collect()\n\n  id subset new_value\n1  2      a        84\n2  5      a        74\n3 12      c        29\n4 13      c        42\n5 15      c         7\n\n\nAt some point in making this happen, data are loaded and computations are performed. At the user level we don’t really think about it much: the dplyr bindings supplied by the arrow package provide us with an abstraction layer for Datasets that completely mask this aspect of the process. That’s super cool because honestly I don’t care enough to spend my time on that sort of thing, but I also find myself curious… what happens when we strip the abstraction away? How would we do this analysis without these abstractions?\nWhen querying a Dataset, we need a strategy for reading data: this is coordinated by a Scanner object constructed for the specific Dataset and the specific query. When analyzing a Dataset using the dplyr interface you never need to construct a Scanner manually, but for explanatory purposes I’ll create one:\n\nscan <- Scanner$create(dataset = ds)\n\nCalling the ToTable() method will materialise the Dataset (on-disk) as a Table (in-memory):13\n\nscan$ToTable()\n\nTable\n15 rows x 3 columns\n$id <int32>\n$value <double>\n$subset <string>\n\nSee $metadata for additional Schema metadata\n\n\nYou can see that this has returned 15 rows (i.e., the whole dataset). If we want to reproduce the behaviour of the dplyr pipeline using the low-level Dataset interface by creating a new scan by specifying the filter and projection arguments to Scanner$create(). The filter argument is used to modify the rows that are returned by the Scanner, and the projection argument is used to modify the columns. These arguments take Arrow Expressions as inputs, which is yet another topic I’ll try to write more about one of these days.\nAnyway, the scanner defined below mimics the dplyr pipeline shown above,\n\nscan <- Scanner$create(\n  dataset = ds, \n  filter = Expression$field_ref(\"value\") > 0,\n  projection = list(\n    id = Expression$field_ref(\"id\"),\n    subset = Expression$field_ref(\"subset\"),\n    new_value = Expression$create(\"round\", 100 * Expression$field_ref(\"value\"))\n  )\n)\n\nWe can check this by calling scan$ToTable() and then converting the result to a data frame so that we get a pretty print out:\n\nscan$ToTable() |> as.data.frame()\n\n  id subset new_value\n1  2      a        84\n2  5      a        74\n3 12      c        29\n4 13      c        42\n5 15      c         7\n\n\nYep, that looks about right.\nWe can dig a little deeper though. To get a better sense of what happens when the query executes, what I’ll call scan$ScanBatches(). Much like the ToTable() method, the ScanBatches() method executes the query separately against each of the files, but it returns a list of Record Batches, one for each file. If we convert each one of those Record Batches to a data frame individually, we get this as a result:\n\nscan$ScanBatches() |> lapply(as.data.frame)\n\n[[1]]\n  id subset new_value\n1  2      a        84\n2  5      a        74\n\n[[2]]\n[1] id        subset    new_value\n<0 rows> (or 0-length row.names)\n\n[[3]]\n  id subset new_value\n1 12      c        29\n2 13      c        42\n3 15      c         7\n\n\nThis version of the result helps you see each part of the Dataset at work in the query. When you pass a query to a Dataset, each file is processed in a separate thread14 and Record Batches will be added as they are returned. The key point here is that Datasets have no notion of row order: if you want the results returned in a particular order you must sort them explicitly.\nA second point to make about the the scanning process is that under the hood, Arrow keeps track of memory usage and doesn’t try to read too many files at once. It will also make use of whatever information it has about the file contents to avoid reading files that it doesn’t have to read: if I filter on subset != \"a\" then the Scanner will ensure that the files in the corresponding folder are never even read.15\nOkay, so now let’s go back to the dplyr query we made earlier, but use compute() to return a Table rather use collect() to return a data frame.\n\ntbl <- ds |>\n  filter(value > 0) |>\n  mutate(new_value = round(100 * value)) |>\n  select(id, subset, new_value) |>\n  compute()\n\nThis Table object has been created by concatenating three Record Batches, one for each of the three data files. As a consequence of this, the Chunked Array that defines a column of the Table has the same partitioning structure present in the data files:16\n\ntbl$subset\n\nChunkedArray\n<string>\n[\n  [],\n  [\n    \"a\",\n    \"a\"\n  ],\n  [\n    \"c\",\n    \"c\",\n    \"c\"\n  ]\n]"
  },
  {
    "objectID": "posts/2022-11-30_unpacking-arrow-datasets/index.html#what-was-the-point",
    "href": "posts/2022-11-30_unpacking-arrow-datasets/index.html#what-was-the-point",
    "title": "Unpacking Arrow Datasets",
    "section": "What was the point?",
    "text": "What was the point?\nDoes any of this matter? Well. That depends, I suppose. If you’re looking to analyse a Dataset using R, you don’t really need to know much of this. Frankly you probably don’t need to know any of it. But also there’s something uncomfortable about using tools when you don’t quite know what they’re doing. It makes me happier when I know just a little bit more than I actually need to know. More importantly, it matters in the sense that it works. Using Datasets leads to shockingly fast performance on data that would not normally be amenable to analysis with R. Which… yeah, that does matter quite a bit!"
  },
  {
    "objectID": "posts/2021-11-19_starting-apache-arrow-in-r/index.html",
    "href": "posts/2021-11-19_starting-apache-arrow-in-r/index.html",
    "title": "Getting started with Apache Arrow",
    "section": "",
    "text": "If you’re like me and spend far too much time talking about R on Twitter you may have come across people talking about how to work with large data sets in R. Perhaps you hear people talking about Parquet files, Apache Arrow, and the arrow package for R, but you’re not really sure what they’re about and are curious? If that’s you, then–\nSo we’re just writing obvious “I want a job in tech, please hire me!” blog posts pitched at potential employers now?\nOh shush. It’s fun and useful too, you know.\nOkay fine, but could you at least be transparent about what you’re doing? Because it’s sort of obnoxious otherwise\nSheesh, what do you think this fake dialogue is for if not making the subtext blatant? Now could you please stop interrupting me and let me talk about Apache Arrow? It is in fact a more interesting subject than our pending unemployment.\nYeah, see how you feel about that in December babe…\nSigh."
  },
  {
    "objectID": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#introduction",
    "href": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#introduction",
    "title": "Getting started with Apache Arrow",
    "section": "Introduction",
    "text": "Introduction\nOkay, where was I? Ah yes…\nIf you’re like me and spend far too much time talking about R on Twitter you may have come across people talking about how to work with large data sets in R. Perhaps you hear people talking about Parquet files, Apache Arrow, and the arrow package for R, but you’re not really sure what they’re about and are curious? If that’s you, then then this blog post is designed to help you get started.\n\nWait… do I actually care?\nLet’s start at the beginning, with the most important question of all: do you actually need to care about this? This might be a long post (or possibly the first post in a long series), so let’s make sure you’re reading for the right reasons!\nFor a lot of people, the answer to the “do I care?” question is going to be “probably not – or at least not right now”. For example, if all your data sets are small and rectangular, then you’re probably working with CSV files and not encountering a lot of problems. Your current workflow uses read.csv() or readr::read_csv() to import data, and everything is fine. Sure, the CSV format has some problems, but it’s simple and it works. If that is you, then right now you don’t need to worry about this.\nBut perhaps that’s not you, or maybe that won’t be you forever. You might be working with larger data sets, either now or in the future, and when that happens you might need to care.\n\n\nOkay… so what’s the problem?\nThanks for a great question! Here are a few scenarios to think about.\n\nScenario 1: Let’s suppose you have a big rectangular data set. An enormous table, basically, and currently it’s stored as a file on your disk. The format of that file could be a plain CSV, a compressed CSV, or it could be something fancier like a Parquet file (I’ll come back to those in a later post, I suspect). It might be a couple of billion rows or so, the kind of thing that you can store on disk but is too big to fit into memory, so it’s not going to be very easy to read this thing into R as a data frame! But your boss wants you to analyse it in R anyway. That’s awkward. R likes to store things in memory. Eek.\nScenario 2: Okay, maybe your data isn’t that big and it fits in memory, but it’s still pretty big, and you need to do something complicated with it. Maybe your analysis needs to start in R but then continue in Python. Or something like that. In your head, you’re thinking okay first I have to read the whole dataset into memory in R, and then it has to be transferred to Python which will have to read its own copy, and… gosh that sounds slow and inefficient. Ugh.\nScenario 3: Honestly, you’re just tired of having to deal with the fact that every language has its own idiosyncratic way of storing data sets in memory and it’s exhausting to have to keep learning new things and you really wish there were some standardised way that programming languages represent data in memory and you’d like a single toolkit that you can use regardless of what language you’re in. Sigh…\n\nIn any of these scenarios, Arrow might be useful to you.\n\n\nFiiiiiine, I’ll keep reading… tell me what Arrow is\nYaaaaay! Green Arrow is a superhero in the DC Comics universe, whose real name is Oliver Queen. He was the subject of an unintentionally hilarious TV show, and–\n\n\nSigh. Apache Arrow please?\nOh right. Apache Arrow is a standardised, language-independent format for storing table-like data in-memory, using a columnar format and supporting zero-copy reads without serialisation overhead.\n\n\nI hate you\nSorry. Let’s unpack each of those terms:\n\nArrow is a standardised and language-independent format. It’s the same thing regardless of what programming language you’re using: a data set accessed from R with Arrow has the same format as the a data set accessed in Python.\nArrow is used to store table-like data, very similar to a data frame or tibble.\nArrow refers to the in-memory format: it’s not talking about how the data are stored on disk, and it’s not talking about file formats. It’s all about how a loaded data set is represented in memory.1\nArrow uses columnar format. Unlike a CSV file, which stores the data row-wise, it represents the data column-wise: this turns out to be a much more efficient way to represent data when you need to subset the data (e.g., by using dplyr::filter() in R or the WHERE clause in SQL).\nArrow supports zero-copy reads without serialisation overhead, which… um… yeah, what the heck does that mean?\n\nSo yeah. Serialisation is one of those terms that those fancy data people know all about, but a regular R user might not be quite as familiar with. It’s worth unpacking this a bit because it’s helpful for understanding the problem that Arrow solves…\n…Hey!\nWait a second, I already wrote a blog post about serialisation! I don’t need to write another one.2 The TL;DR, for folks who quite reasonably don’t want to do a deep dive into how R objects are written to RDS files, is that serialisation is the process of taking an in-memory data structure (like a data frame), and converting it into a sequence of bytes. Those bytes can either be written to disk (when you’re saving a file) or they can be transmitted over some other channel. Regardless of what you want to do with the serialised data, this conversion takes time and resources, and at some point the data will need to be unserialised later. The resources expended in doing so are referred to as the “serialisation overhead”.\nFor small data sets, it doesn’t take R very long to serialise or unserialise. The “serialisation overhead” isn’t a big deal. But when the data set is very large, this is not a trivial operation and you don’t want to do this very often. That’s a problem when a large data set needs to be passed around between multiple platforms. Loading the a CSV into R incurs a serialisation cost; transferring a copy of the data from R to Python incurs a serialisation cost. This happens because R and Python have different structured representations: a data frame in R is a different kind of thing to a panda in Python, so the data has to be serialised, transferred, and then unserialised at the other end in order to pass the data from one to another.\nWouldn’t it be nice if we could avoid that? What if there was just one data structure representing the table in-memory, and R and Python could both agree to use it? That would remove the need to copy and transfer the data, right? And in doing so, it would eliminate those pesky serialisation costs incurred every time. It would be a “zero-copy” mechanism.\nIf only there were a standardised, language-independent format for storing table-like data in-memory, using a columnar format and supporting zero-copy reads without serialisation overhead…\n\n\n\n\n\nArrow image by Possessed Photography. It also has nothing whatsoever to do with the Apache Software Foundation. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#overview-of-arrow",
    "href": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#overview-of-arrow",
    "title": "Getting started with Apache Arrow",
    "section": "Overview of Arrow",
    "text": "Overview of Arrow\nHere’s one of the two big ideas: standardisation prevents wasteful copying. The current situation that most of us are working in looks something like this. Every application and programming language defines its own format for storing data in memory (and often on disk too), and so any time multiple applications require access to the same data, there’s a serialisation cost. The bigger the data, the higher that cost will be. The more applications you connect to the same data, the more times you incur the cost:\n\n\n\n\n\nArrow solves this problem by allocating its own memory to store the data, and providing tools that allow you to access this from any language you like. The goal is to make those tools feel “natural” in whatever language you’re using. For example, if you’re an R user, you may already be familiar with the dplyr grammar for data manipulation and you’d like to be able to manipulate an Arrow Table using dplyr, in exactly the same way you would manipulate a data frame. The arrow R package allows you to do precisely this, and there’s a similar story that applies on the Python side. This allows you to write code that feels natural for the language you’re working in.\nIn this approach, R and Python both have a toolkit that plays nicely with Arrow and feels native to that language. Applications written in R and applications written in Python can both work with the same underlying data (because it’s in Arrow), so you don’t have to serialise the data in order for them to talk to each other:\n\n\n\n\n\nSo that’s the first big idea.\nThe second big idea is that Arrow organises data column-wise in memory and as consequence it can support cool single instruction multiple data (or SIMD) operations that you can do with modern CPUs, which I totally understand 100% and am not just paraphrasing Wikipedia. Anyway, it doesn’t really matter at the user level. All we care about there is that manipulating data with Arrow can be very fast. There’s a very brief discussion of this on the Arrow overview page. (It also has prettier versions of my crappy handwritten diagrams)\n\n\n\n\n\nArrow image by Denise Johnson. Yet again, it has nothing whatsoever to do with the Apache Software Foundation but it is very pretty. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#installing-arrow",
    "href": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#installing-arrow",
    "title": "Getting started with Apache Arrow",
    "section": "Installing Arrow",
    "text": "Installing Arrow\nInstalling Apache Arrow on your local machine as an R user is either extremely easy or mildly tiresome, depending almost entirely on whether you’re on Linux. If you’re using Windows or Mac OS, you shouldn’t need to do anything except install the arrow package in the usual way. It just works:\n\ninstall.packages(\"arrow\")\n\nIf you’re on Linux, there may not be any precompiled C++ binaries for your system, so you’ll have to do it yourself. On my system this was quite time consuming, and the first couple of times I tried it I was convinced that nothing was actually happening because I wasn’t seeing a progress bar or anything, and being impatient I killed the install process before it was finished. If you’re like me and need visual confirmation that something is happening, there’s an ARROW_R_DEV environment variable you can set that will make the process more verbose:\n\nSys.setenv(ARROW_R_DEV = TRUE)\ninstall.packages(\"arrow\")\n\nThis way you get to see all the C++ build information scrolling by on the screen during the installation process. It doesn’t make for very exciting viewing, but at least you have visual confirmation that everything is working!\nThere are quite a few ways you can customise the installation process, and they’re all documented on the installation page. One particularly useful thing to do is to set LIBARROW_MINIMAL to false, which ensures that arrow will install a bunch of optional features like compression libraries and AWS S3 support. It takes longer but you get more stuff! So the actual installation code I used was this:\n\nSys.setenv(\n  ARROW_R_DEV = TRUE,\n  LIBARROW_MINIMAL = FALSE\n)\ninstall.packages(\"arrow\")\n\nThis may take quite a long time if you’re compiling from source so you may want to go make a cup of tea or something while it installs. At the end, hopefully, you’ll have a working version of the package:\n\nlibrary(arrow)\n\nYou can use the arrow_info() function to obtain information about your installation:\n\narrow_info()\n\nArrow package version: 8.0.0\n\nCapabilities:\n               \ndataset    TRUE\nsubstrait FALSE\nparquet    TRUE\njson       TRUE\ns3         TRUE\nutf8proc   TRUE\nre2        TRUE\nsnappy     TRUE\ngzip       TRUE\nbrotli     TRUE\nzstd       TRUE\nlz4        TRUE\nlz4_frame  TRUE\nlzo       FALSE\nbz2        TRUE\njemalloc   TRUE\nmimalloc   TRUE\n\nMemory:\n                  \nAllocator jemalloc\nCurrent    0 bytes\nMax        0 bytes\n\nRuntime:\n                          \nSIMD Level          avx512\nDetected SIMD Level avx512\n\nBuild:\n                          \nC++ Library Version  8.0.0\nC++ Compiler           GNU\nC++ Compiler Version 9.4.0\n\n\nYaaas queen! We are ready to go.\n\n\n\n\n\nArrow image by Frank Busch. Now there are two! There are two arrows. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#does-it-work",
    "href": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#does-it-work",
    "title": "Getting started with Apache Arrow",
    "section": "Does it work?",
    "text": "Does it work?\nMy goal in this post is fairly modest. I wanted to understand why everyone I talk to seems so excited about Arrow, and try to get it configured to work on my machine. Assuming I can be bothered continuing this series, the next step would be to start playing with Arrow and do a proper exploration. For now though, I’ll try something simple, using the diamonds data from the ggplot2 package\n\nlibrary(arrow)\nlibrary(dplyr)\nlibrary(ggplot2)\ndiamonds\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# … with 53,930 more rows\n\n\n\nExample 1: Arrow data sets aren’t stored in R memory\nOkay, so the first thing I want to investigate is this idea that Arrow holds the data in its own memory, not in the memory allocated to R. As things currently stand the diamonds tibble has 53940 rows stored in R memory, and that occupies about 3.3MB of memory:\n\nlobstr::obj_size(diamonds)\n\n3,456,344 B\n\n\nWhat happens when we move the data into Arrow? To do this we would construct a “Table” object using the arrow_table() function, like this:\n\ndiamonds2 <- arrow_table(diamonds)\ndiamonds2\n\nTable\n53940 rows x 10 columns\n$carat <double>\n$cut <dictionary<values=string, indices=int8, ordered>>\n$color <dictionary<values=string, indices=int8, ordered>>\n$clarity <dictionary<values=string, indices=int8, ordered>>\n$depth <double>\n$table <double>\n$price <int32>\n$x <double>\n$y <double>\n$z <double>\n\n\nIt’s printed a little differently, but it’s the same tabular data structure consisting of 53940 rows and 10 columns. So how much R memory does diamonds2 occupy?\n\nlobstr::obj_size(diamonds2)\n\n284,632 B\n\n\nOnly 279KB. The reason why it occupies so little memory is that diamonds2 doesn’t contain all the data. The data are stored elsewhere, using memory allocated to Arrow. If a Python program wanted to access the diamonds2 data, it could do so without having to serialise the data again. It can link to the same data structure in Arrow memory that I just created. Neat!\n\n\nExample 2: Arrow plays nicely with dplyr\nOne neat thing about dplyr is that it cleanly separates the API from the backend. So you can use the dbplyr package to work with databases using dplyr code, or the dtplyr package to use a data.table backend, and so on. The arrow package does the same thing for Apache Arrow.\nHere’s an example. If I were working with the original diamonds tibble, I might write a simple dplyr pipe to tabulate the clarity of premium-cut diamonds:\n\ndiamonds %>% \n  filter(cut == \"Premium\") %>% \n  count(clarity)\n\n# A tibble: 8 × 2\n  clarity     n\n  <ord>   <int>\n1 I1        205\n2 SI2      2949\n3 SI1      3575\n4 VS2      3357\n5 VS1      1989\n6 VVS2      870\n7 VVS1      616\n8 IF        230\n\n\nCan I do the same thing using the diamonds2 Table? Let’s try:\n\ndiamonds2 %>% \n  filter(cut == \"Premium\") %>% \n  count(clarity)\n\nTable (query)\nclarity: dictionary<values=string, indices=int8, ordered>\nn: int32\n\nSee $.data for the source Arrow object\n\n\nOkay, perhaps not what we were expecting. In order to optimise performance, the query doesn’t get evaluated immediately (more on this in a later post perhaps) You have to tell it either to compute() the result, which will return another Table, or to collect() the result into a data frame\n\ndiamonds2 %>% \n  filter(cut == \"Premium\") %>% \n  count(clarity) %>% \n  collect()\n\n# A tibble: 8 × 2\n  clarity     n\n  <ord>   <int>\n1 SI1      3575\n2 VS2      3357\n3 SI2      2949\n4 I1        205\n5 VS1      1989\n6 VVS1      616\n7 VVS2      870\n8 IF        230\n\n\nAt no point has the full data set been loaded into R memory. The diamonds2 object doesn’t contain any new information. It’s still the same size:\n\nlobstr::obj_size(diamonds2)\n\n284,632 B\n\n\nMy example is trivial, of course, because the diamonds data set isn’t very big. But if you start reading the Arrow documentation, they give an example using the NYC taxi data which is about 37GB in size. That’s… a teeensy bit bigger than I’d want to try loading into memory on my laptop, so I wouldn’t be able to load it into R at all much less use dplyr. However, because Arrow supplies a dplyr back end, it is possible to write dplyr code for the NYC taxi data.\nOld and jaded though I may be, I have to admit that’s pretty cool.\n\n\n\n\n\nOkay yeah, this one actually does have something to do with the Apache Software Foundation. It’s, like, a registered trademark or something. I’m guessing this counts as fair use though."
  },
  {
    "objectID": "posts/2022-12-26_strange-year/index.html",
    "href": "posts/2022-12-26_strange-year/index.html",
    "title": "A very strange year",
    "section": "",
    "text": "Warning\n\n\n\nContent note: This post refers briefly to sexual assault\nWe all saw this back in 2020, right? It seemed pretty funny to me at the time – 2020 was such a messed up year that it seemed hard to believe that 2022 could really be even stranger. Oh how very wrong I was.\nMy personal life has taken the biggest upheavals. After leaving academia in late 2021, I started my first ever tech job in January 2022… and was let go from my first ever tech job in December 2022. Not gonna lie, that one stings a bit, but mostly because it’s recent and I still haven’t really emotionally processed it all. I’m objective enough to recognise that the situation I’m in now really isn’t so bad. I lost a job that I was enjoying. I feel sad and hurt about losing it because I’m a normal human being who feels sad about things that are sad. It’ll pass. I’ll heal.\nBesides, let’s be honest. In a year when I find myself in the situation where I can say things like “okay sure I’ve been sexually assaulted four times in the last 12 months, but technically speaking, only two of the men raped me” in a conversation and have it be entirely true, losing a job just doesn’t carry the level of trauma that it probably should.\nOn reflection, that is a very horrible sentence to have written.\nBut it is true. Sexual assault has been a recurring theme in 2022 for me, and while I don’t have much desire to talk about what it feels like to have been repeatedly violated in my personal life, I also don’t feel like I should be ashamed that it happened. I’m not going to make a secret of something that was not my fault. It happened, and I cannot change the fact that it happened. I’ve picked up the pieces as best I can and gone about my life again. What else can I do?\nBut let’s turn to happier topics, shall we? I’ve said the thing I wanted to say about the dark topics and there’s no need to dwell."
  },
  {
    "objectID": "posts/2022-12-26_strange-year/index.html#contributing-to-the-ggplot2-book",
    "href": "posts/2022-12-26_strange-year/index.html#contributing-to-the-ggplot2-book",
    "title": "A very strange year",
    "section": "Contributing to the ggplot2 book",
    "text": "Contributing to the ggplot2 book\n\n\n\n\n\n\n\nI keep a little log of things that I’ve been doing with my time, and it’s interesting to look back at what I was doing in January 2022. It feels like a lifetime ago: on January 9th I merged a big pull request into the work-in-progress 3rd edition of the ggplot2 book that reorganised the scales chapters. I haven’t had time to do anything else on that since January, but I really like how the writing worked out for that (plus it’s always fun to work with Hadley!) The book now has four separate chapters on scales. Three of the chapters focus on the practicalities of working with scales:\n\nPosition scales: ggplot2-book.org/scale-position.html\nColour scales: ggplot2-book.org/scale-colour.html\nScales for other aesthetics: ggplot2-book.org/scale-other.html\n\nThere’s a fourth one too, which talks more about the underlying theory:\n\nScales and guides ggplot2-book.org/scales-guides.html\n\nHaving done earlier work helping out with revising the Maps and Annotations chapters, it felt really nice to be able to work on that. It’s possible I’ll have more time to revisit in 2023, but at this point all my plans are up in the air so who knows."
  },
  {
    "objectID": "posts/2022-12-26_strange-year/index.html#writing-about-apache-arrow",
    "href": "posts/2022-12-26_strange-year/index.html#writing-about-apache-arrow",
    "title": "A very strange year",
    "section": "Writing about Apache Arrow",
    "text": "Writing about Apache Arrow\n\n\n\nFor most of the year I was gainfully employed to work on open source projects – Apache Arrow in particular – and I’ve written a lot over the last year about it. On this blog alone here’s what I wrote this year:\n\nGetting started with Apache Arrow in R. A now slightly dated primer on how to get started. Okay this one was technically November 2021, but it’s the logical beginning of the sequence of posts and I’m including anyway\nBinding Apache Arrow to R. A post about how the dplyr bindings in the arrow R package work and how you can write them yourself\nData types in Arrow and R. A very long post that walks you through the low level data types used by Arrow and R, and some of the subtle details around translating from one to the other\nArrays and tables in Arrow. This post builds on the previous one and talks about some of the higher level data structures used in Apache Arrow (Arrays, Tables, Record Batches, etc), and the ways in which they are similar to and different from similar data structures in R (vectors, data frames, etc).\nHow to visualise a billion rows of data in R with Apache Arrow. This one was a practical post, walking you through the process of plotting a very large data set\nPassing data between R and Python with reticulate Part one of a two-part series about how you can use Arrow to pass data between R and Python without incurring serialisation costs. In part one I talked about it from an R-centric perspective, using the reticulate R package as the primary tool\nPassing data between Python and R with rpy2 Part two of the same series. This one takes a Pythonic perspective and uses the rpy2 Python library as the primary tool\nBuilding an Arrow Flight server. One of the underrated features of the Arrow toolkit is that is the Flight RPC protocol: you can use it to efficiently communicate Arrow data over a network. This post is a walkthrough of how to do that in R and Python (mostly Python, really)\nThe Arrow Dataset API. The last Arrow post I wrote this year talked about the Arrow Dataset API in more detail than I had done previously.\n\nA lot of these posts are… well, they’re long and they’re detailed. The intention was always to try to create a collection of useful resources with code walkthoughs that I could later fold back into documentation, books, workshops and so on. The nice thing is that this actually did happen. For example…\n\nI wrote an entire workshop on Larger than memory data workflows with Apache Arrow for the R community. The slides, walkthrough, tutorial, etc are all up on the website\nI contributed a chapter on Arrow to the 2nd edition of R for Data Science. Given the centrality of R4DS in the R community I kind of feel like that’s probably one of the more useful things I actually managed to get done!\n\nBut probably the biggest thing is that a lot of the content from my other writing worked its way into a big pull request I wrote updating the documentation for the arrow R package. It hasn’t quite gone properly live yet, and I don’t think it will migrate to the front page until the 11.0.0 release in January, but it’s currently available on the dev version of the documentation.\n\nI completely rewrote the Get started page so that it is now more novice friendly and helps orient new users\nI added a new article highlighting the read/write capabilities of Arrow\nI added a new article talking about the data wrangling using the dplyr interface that was partly new material, and partly reworked existing content\nI tidied up the article on multi-file Dataset objects\nI added a new article on data objects in Arrow that reworked a lot of content I’d originally written for my blog\nI added a new article on data types in Arrow that, again, reworked a lot of content I’d written for my blog\nI laid the groundwork for a tidier discussion of metadata in Arrow that I’d intended to expand on later\n\nOh, and I also wrote the Arrow Visual Identity page and all the code for generating the various logos!\nThere was a lot more I wanted to do with Arrow, to be honest. For example, the R package doesn’t handle Arrow Flight correctly at the moment (it works in special cases but it badly needs updating), and – before I found myself unemployed – I had talked about writing bindings so that the R package implemented the Arrow Flight protocol correctly. Maybe one day I’ll write them. I don’t think it’s actually very hard because the hard work is already done in the C++ library, but… oddly, I think I’ll take a bit of a break from Arrow work while I let my emotions about my employment situation settle."
  },
  {
    "objectID": "posts/2022-12-26_strange-year/index.html#art",
    "href": "posts/2022-12-26_strange-year/index.html#art",
    "title": "A very strange year",
    "section": "Art",
    "text": "Art\n\n\n\nI made a lot of generative art in 2022, though maybe not quite as much as in 2021. I added nine new galleries on my art website art.djnavarro.net, but really the big thing for me in art this year was being invited to give a generative art workshop at rstudio::conf. So, thanks to the support of folks at Posit, there is now a fully fledged freely available tutorial – okay it’s closer to being an entire book, really – on how to make generative art with R. It’s online at art-from-code.netlify.app. It makes me really happy that I was lucky enough to be able to write that one. I was also interviewed for the DSxD book on The Future of Data Science for my artistic work! Oh, and one of my art pieces was used – with my permission freely given to the authors because they are lovely – as a the cover of a book on transgender sexual health, which I think is really cool."
  },
  {
    "objectID": "posts/2022-12-26_strange-year/index.html#academia",
    "href": "posts/2022-12-26_strange-year/index.html#academia",
    "title": "A very strange year",
    "section": "Academia",
    "text": "Academia\nSomehow, despite the fact that I am no longer in academia, I managed to publish some academic papers… in my spare time, like normal people do. To be fair though I didn’t actually do much of the work this year: these were all project that I’d committed to while I was still in academia. They were mostly papers that had been accepted or in the final stages of revision at the start of 2021 and have been slowly emerging from the pipeline one by one. It’s hard to know what to say about my academic output given that I’m no longer invested in the peculiar norms of the academia. It’s not my world anymore. I care a lot about some of the values, and so I’m still maintaining a personal archive as well as contributing my papers to institutional and other public archives, but… look, if ever you feel a need to ask “wait, what was Danielle’s academic research all about?” I moved all that to papers.djnavarro.net at the end of 2022."
  },
  {
    "objectID": "posts/2022-12-26_strange-year/index.html#what-else",
    "href": "posts/2022-12-26_strange-year/index.html#what-else",
    "title": "A very strange year",
    "section": "What else?",
    "text": "What else?\n\n\n\nI did a lot of other things in 2022. There’s really no need to try to go through all of them. But here are some other personal favourites that made an appearance on this blog:\n\nI wrote an R package on multi-threaded task queues: blog.djnavarro.net/queue\nI wrote about Crayola crayon colours and some fun data wrangling problems: blog.djnavarro.net/crayola-crayon-colours\nI wrote an absurdly popular blog post about mastodon: blog.djnavarro.net/what-i-know-about-mastodon/\nI wrote a post that I thought was absurdly-popular (until the mastodon thing happened) about porting this blog from distill to quarto: blog.djnavarro.net/porting-to-quarto"
  },
  {
    "objectID": "posts/2022-12-26_strange-year/index.html#so-what-happens-next",
    "href": "posts/2022-12-26_strange-year/index.html#so-what-happens-next",
    "title": "A very strange year",
    "section": "So what happens next?",
    "text": "So what happens next?\nArriving at the end of this post, I find myself very unsure about what comes next. It’s been such a strange year. I’m proud of the work that I’ve done and the projects I’ve completed. It feels good to be able to look at the list above and think yeah that’s actually a pretty decent body of work, and I could have added more if I’d wanted to. I think I’ve used my time well? I’ve accomplished a lot and learned a lot. But at the same time these things sit against a stunningly horrible backdrop, and it hasn’t been easy coping with that. In general I try not to talk about those kinds of topics on this blog – this is a data science blog, after all, and it’s not really a place for talking about sexual assault and what can do to your sense of self worth – but I suppose I would like it if 2023 goes a little differently. Not sure I can manage another year like this one."
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html",
    "title": "Data types in Arrow and R",
    "section": "",
    "text": "Manuals for translating one language into another can be set up in divergent ways, all compatible with the totality of speech dispositions, yet incompatible with one another      – William Van Orman Quine, 1960, Word and Object\nAt the 2018 useR! conference in Brisbane, Roger Peng gave a fabulous keynote talk on teaching R to new users in which he provided an overview of the history of the language and how it is used in the broader community. One thing that stood out to me in his talk – and I’ve seen reflected in other data – is that R is unusual as a language because it’s not designed primarily for programmers. Software engineering practices have now become widespread in the R community, and that’s a good thing. Nevertheless, a very large proportion of the R community don’t have a traditional computer science background – and that’s okay! In fact, given the goals of the language that’s a good thing too.\nR is a language designed with a practical goal in mind: it is a tool for statistical programming and data analysis. Because of this design focus, R users tend to care most deeply about the tasks that make up their day to day jobs. Few of us care about the IEEE 754 standard for encoding floating point numbers. R users are not typically interested in the big-endian/little-endian distinction. The purpose of R as a high level statistical programming environment is to abstract away from these things, and to allow users to focus on data cleaning, wrangling, and visualisation. R tries to help you get to your data as easily as possible, build models for your data, report those models reliably, and so on. Because that’s the job.\nBut.\nThere’s always a “but”, isn’t there?\nOne of the huge changes in the data science ecosystem in recent years is the change in scale of our data sets. Data sets can now easily encompass billions of rows, and surpass the ability of your machine (and R) to hold in memory. Another huge change in the ecosystem is the proliferation of tools. Data sets have to be passed from one system to another, and when those data sets are large, problems follow. Apache Arrow solves these problems by providing a multi-language toolbox for data exchange and data analysis. It’s a toolbox designed for a big data environment, and a many-language environment. From the perspective of an R user, it supplies the arrow package that provides an interface to Apache Arrow, and through that package allows you to have access to all the other magic that Arrow exposes. It’s an extremely powerful toolbox… but to use it effectively you do need to learn more of those low-level concepts that we as R users like to skim over.\nThis post is an attempt to fill that gap for you! It’s a long form post, closer to a full length article than a typical blog. My goals in this post are to:\nThis post isn’t intended to be read in isolation. It’s the third part of a series I have been writing on Apache Arrow and R, and it probably works best if you’ve read the previous two. I’ve made every effort to make this post self-contained and self-explanatory, but it does assume you’re comfortable in R and have a little bit of knowledge about what the arrow package does. If you’re not at all familiar with arrow, you may find it valuable to read the first post in the series, which is a getting started post, and possibly the second one that talks about the arrow dplyr backend.\nStill keen to read? I haven’t scared you off?\nNo?\nFabulous! Then read on, my loves!"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#regarding-magic",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#regarding-magic",
    "title": "Data types in Arrow and R",
    "section": "Regarding magic",
    "text": "Regarding magic\nConsider this piece of magic. I have a csv file storing a data set. I import the data set into R using whatever my favourite csv reader function happens to be:\n\nmagicians <- read_csv_arrow(\"magicians.csv\")\nmagicians\n\n# A tibble: 65 × 6\n   season episode title                                air_date   rating viewers\n    <int>   <int> <chr>                                <date>      <dbl>   <dbl>\n 1      1       1 Unauthorized Magic                   2015-12-16    0.2    0.92\n 2      1       2 The Source of Magic                  2016-01-25    0.4    1.11\n 3      1       3 Consequences of Advanced Spellcasti… 2016-02-01    0.4    0.9 \n 4      1       4 The World in the Walls               2016-02-08    0.3    0.75\n 5      1       5 Mendings, Major and Minor            2016-02-15    0.3    0.75\n 6      1       6 Impractical Applications             2016-02-22    0.3    0.65\n 7      1       7 The Mayakovsky Circumstance          2016-02-29    0.3    0.7 \n 8      1       8 The Strangled Heart                  2016-03-07    0.3    0.67\n 9      1       9 The Writing Room                     2016-03-14    0.3    0.71\n10      1      10 Homecoming                           2016-03-21    0.3    0.78\n# … with 55 more rows\n\n\nThen I decide to “copy the data into Arrow”.1 I do that in a very predictable way using the arrow_table() function supplied by the arrow package:\n\narrowmagicks <- arrow_table(magicians)\narrowmagicks\n\nTable\n65 rows x 6 columns\n$season <int32>\n$episode <int32>\n$title <string>\n$air_date <date32[day]>\n$rating <double>\n$viewers <double>\n\n\nThis is exactly the output I should expect, but the longer I think about it the more it seems to me that something quite remarkable is going on. Some magic is in play here, and I want to know how it works.\nTo understand why I’m so curious, consider the two objects I now have. The magicians data set is a data frame (a tibble, technically) stored in R. The arrowmagicks data set, however, is a pointer to a data structure stored in Arrow. That data structure is a Table object. Table objects in Arrow are roughly analogous to data frames – both represent tabular data with columns that may be of different types – but they are not the same. The columns of a Table are built from objects called ChunkedArrays that are in turn constructed from Arrays, and those Arrays can contain Scalar objects. In other words, to move data from one language to another an act of translation is required, illustrated below:\n\nIt’s not standard, but since this is a post about data types, I’ll italicise the names of data types in both R and Arrow (e.g., data.frame, Table). It gets a bit tiresome, but I think it’s helpful\n\n\n\n\n\n\nA miniature translation guide. On the left a data frame in R is shown: it is comprised of three columns. Each columns is an R vector. We use the term ‘element’ to refer to any length-1 constituent of a vector, even though it isn’t really a distinct object in its own right. On the right is a Table in Arrow: it too is comprised of three columns, encoded as ChunkedArrays. Each ChunkedArray is comprised of one or more Arrays, and each Array contains one or more Scalars, which (unlike elements of R vectors) are distinct objects. The data structure that translates one into the other is called a Schema.\n\n\n\n\nIn this post I’m not going to talk much about the difference between Arrays and ChunkedArrays, or why Arrow organises Tables this way (that will be the topic of a later post). For now it’s enough to recognise that Arrow does have this additional structure: the Table data type in Arrow is not equivalent to the data frame class in R, so a little work is required to map one to the other.\nA similar story applies when we look at the contents of the data set. The translation process doesn’t just apply to the “container” object (i.e., the data frame in R and the Table in Arrow), it also applies to the values that the object contains. If we look at the how the columns of magicians and arrowmagicks are labelled, we see evidence of this translation. The integer columns in R have been mapped to int32 columns in Arrow, Date columns in R become date32 columns in Arrow, and so on.\n\nVariable names like arrowmagicks and function calls like arrow_table() are shown in monospace typewriter font\n\nThere’s quite a lot of complexity to the translation process, yet it all seems to work seamlessly, and it works both ways. I can pull the arrowmagicks data back into R and recover the original data:\n\ncollect(arrowmagicks)\n\n# A tibble: 65 × 6\n   season episode title                                air_date   rating viewers\n    <int>   <int> <chr>                                <date>      <dbl>   <dbl>\n 1      1       1 Unauthorized Magic                   2015-12-16    0.2    0.92\n 2      1       2 The Source of Magic                  2016-01-25    0.4    1.11\n 3      1       3 Consequences of Advanced Spellcasti… 2016-02-01    0.4    0.9 \n 4      1       4 The World in the Walls               2016-02-08    0.3    0.75\n 5      1       5 Mendings, Major and Minor            2016-02-15    0.3    0.75\n 6      1       6 Impractical Applications             2016-02-22    0.3    0.65\n 7      1       7 The Mayakovsky Circumstance          2016-02-29    0.3    0.7 \n 8      1       8 The Strangled Heart                  2016-03-07    0.3    0.67\n 9      1       9 The Writing Room                     2016-03-14    0.3    0.71\n10      1      10 Homecoming                           2016-03-21    0.3    0.78\n# … with 55 more rows\n\n\nIn this example the translation back and forth “just works”. You really don’t have to think too much about the subtle differences in how Arrow and R “think about the world” and how their data structures are organised. And in general that’s what we want in a multi-language toolbox: we want the data analyst to be thinking about the data, not the cross-linguistic subtleties of the data structures!\nThat being said, it’s also valuable to give the data analyst flexibility. And that means we’re going to need to talk about Schemas. As shown in the “translation diagram” above, Schemas are the data structure arrow uses to govern the translation between R and Arrow, and since I’m going to be talking about data “on the R side” and data “on the Arrow side” a lot, it will be helpful to have some visual conventions to make it a little clearer. Throughout the post you’ll see diagrams showing the default mappings that the arrow package uses when converting data columns from R to Arrow and vice versa. In each case I’ll show R data types on the left hand side (against a blue background) and Arrow data types on the right hand side (against an orange background), like this:\n\n\n\n\n\nIllustration of the graphical convention used in the later diagrams, showing R on the left side (against a blue background) and Arrow on the right side (against an orange background)."
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#defining-schemas",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#defining-schemas",
    "title": "Data types in Arrow and R",
    "section": "Defining Schemas",
    "text": "Defining Schemas\nThe arrow package makes very sensible default choices about how to translate an R data structure into an Arrow data structure, but those choices can never be more than defaults because of the fundamental fact that the languages are inherently different. The quote about the indeterminacy of translation at the top of this post was originally written about natural languages, but I think it applies in programming too. There’s no single rulebook that tells you how to translate between R and Arrow: there can’t be.2\nSuppose that I knew that there would in fact be a “Season 5.1648” coming, consisting of a single episode that would air not only on a specific date, but at a specific time that would – for some bizarre reason3 – be important to encode in the data. Knowing that this new data point is coming, I’d perhaps want my Arrow data to encode season as a numeric variable, and I’d need to encode the air_date field using a date type that implicitly encodes time of day. I can do this with the schema() function:\n\ntranslation <- schema(\n  season = float64(), # not the default\n  episode = int32(),\n  title = utf8(), \n  air_date = date64(), # not the default\n  rating = float64(),\n  viewers = float64()\n)\n\nNow I can use my schema to govern the translation:\n\narrowmagicks2 <- arrow_table(magicians, schema = translation)\narrowmagicks2\n\nTable\n65 rows x 6 columns\n$season <double>\n$episode <int32>\n$title <string>\n$air_date <date64[ms]>\n$rating <double>\n$viewers <double>\n\n\nThe output may not make complete sense at this point, but hopefully the gist of what I’ve done should be clear. The season is no longer stored as an integer (it’s now a numeric type), and the air_date no longer uses “day” as the unit of encoding, it uses “ms” (i.e., millisecond). I’ve accomplished my goals. Yay!\nThis is of course a toy example, as are all the other examples you’ll encounter in this post. But the underlying issues are important ones!\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#why-mapping-languages-is-hard",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#why-mapping-languages-is-hard",
    "title": "Data types in Arrow and R",
    "section": "Why mapping languages is hard",
    "text": "Why mapping languages is hard\n\nOrganising the world into concepts (or data structures) is hard.4 We define ontologies that impose order on a chaotic world, but those structures are rarely adequate to describe the world as it is. While doing background research for this post I spent a little time reading various sections from An Essay Towards a Real Character, and a Philosophical Language, a monograph written by John Wilkins in 1668 that makes a valiant (but doomed… oh so doomed) attempt to organise all the categories of things and propose a mechanism by which we could describe them within a single universal language. The classification systems he came up with were… not great. For example, he divided BEASTS into two categories: VIVIPAROUS beasts are those that bear live young, whereas OVIPAROUS beasts are those that lay eggs. The viviparous ones could be subdivided into WHOLE-FOOTED ones and CLOVEN-FOOTED ones. The cloven-footed beasts could be subdivided into those that were RAPACIOUS and those that were not. RAPACIOUS types could be of the CAT-KIND or the DOG-KIND.\nSuffice it to say the poor man had never encountered a kangaroo.\nThe problem with trying to construct universal ontologies is that these things are made by humans, and humans have a perspective that is tied to their own experience and history. As a 17th century English gentleman, Wilkins saw the world in a particular way, and the structure of the language he tried to construct reflected that fact.\nI am of course hardly the first person to notice this. In 1952 the Argentinian author Jorge Luis Borges published a wonderful essay called The Analytical Language of John Wilkins that both praises Wilkins’ ambition and then carefully illustrates why it is necessarily doomed to fail. Borges’ essay describes a classification system from an fictitious “Celestial Emporium of Benevolent Knowledge” which carves up the beasts as follows:\n\nIn its remote pages it is written that the animals are divided into: (a) belonging to the emperor, (b) embalmed, (c) tame, (d) sucking pigs, (e) sirens, (f) fabulous, (g) stray dogs, (h) included in the present classification, (i) frenzied, (j) innumerable, (k) drawn with a very fine camelhair brush, (l) et cetera, (m) having just broken the water pitcher, (n) that from a long way off look like flies\n\nNow, it’s pretty unlikely that any human language would produce a classification system quite as chaotic as Borges’ fictional example, but the point is well made. Actual classification systems used in different languages and cultures are very different to one another and often feel very alien when translated. It’s a pretty fundamental point, and I think it applies to programming languages too.5 Every language carries with it a set of assumptions and structures that it considers “natural”, and translation across the boundaries between languages is necessarily a tricky business.6"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#a-little-bit-of-big-picture",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#a-little-bit-of-big-picture",
    "title": "Data types in Arrow and R",
    "section": "A little bit of big picture",
    "text": "A little bit of big picture\nBefore we get to “moving data around” part it’s helpful to step back a little and recognise that R and Arrow are designed quite differently. For starters, the libarrow library to which the arrow package provides bindings is written in C++, and C++ is itself a different kind of language than R. And in a sense, that’s actually the natural place to start because it influences a lot of things in the design of arrow.\n\n\nObject oriented programming in arrow\nOne of ways in which C++ and R differ is in how each language approaches object oriented programming (OOP). The approach taken in C++ is an encapsulated OOP model that is common to many programming languages: methods belong to objects. Anyone coming from outside R is probably most familiar with this style of OOP.\nThe approach taken in R is… chaotic. R has several different OOP systems that have different philosophies, and each system has its own strengths and weaknesses.7 The most commonly used system is S3, which is a functional OOP model: methods belong to generic functions like print(). Most R users will be comfortable with S3 because it’s what we see most often. That being said, there are several other systems out there, some of which adopt the more conventional encapsulated OOP paradigm. One of the most popular ones is R6, and it works more like the OOP systems seen in other languages.\nThe arrow package uses both S3 and R6, but it uses them for quite different things. Whenever arrow does something in an “R-native” way, S3 methods get used a lot. For example, in my earlier post on dplyr bindings for Arrow I talked about how arrow supplies a dplyr engine: this works in part by supplying S3 methods for various dplyr functions that are called whenever a suitable Arrow object gets passed to dplyr. The interface between arrow and dplyr uses S3 because this context is “R like”. However, this isn’t a post about that aspect of arrow, so we won’t need to talk about S3 again in this post.\nHowever, arrow has a second task, which is to interact with libarrow, the Arrow C++ library. Because the data structures there all use encapsulated OOP as is conventional in C++, it is convenient to adhere to those conventions within the arrow package. Whenever arrow has to interact with libarrow, it’s useful to be as “C++ like” as possible, and this in turn means that the interface between arrow and libarrow is accomplished using R6. So we will be seeing R6 objects appear quite often in this post.8\n\n\n\nTable, ChunkedArray, and Scalar\nYou may be wondering what I mean when I say that R6 objects are used to supply the interface between R and Arrow. I’ll try to give some concrete examples. Let’s think about the arrow_table() function. At the start of the post I used this function to translate an R data frame into an Arrow Table, like this:\n\narrow_table(magicians)\n\nThis is a natural way of thinking about things in R, but the arrow_table() function doesn’t actually do the work. It’s actually just a wrapper function. Within the arrow package is an R6 class generator object called Table,9 and its job is to create tables, modify tables, and so on. You can create a table by using the create() method for Table. In other words, instead of calling arrow_table() I could have done this:\n\nTable$create(magicians)\n\nand I would have ended up with the same result.\nThe same pattern appears throughout the arrow package. When I used the schema() function earlier, the same pattern was in play. There is an R6 class generator called Schema, and it too has a create() method. I could have accomplished the same thing by calling Schema$create().\nI could go on like this for some time. Though I won’t talk about all of them in this post, there are R6 objects for Dataset, RecordBatch, Array, ChunkedArray, Scalar, and more. Each of these provides an interface to a data structure in Arrow, and while you can often solve all your problems without ever interacting with these objects, it’s very handy to know about them and feel comfortable using them. As the post goes on, you’ll see me doing that from time to time.\nBut enough of that! It’s time to start moving data around…\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#logical-types",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#logical-types",
    "title": "Data types in Arrow and R",
    "section": "Logical types",
    "text": "Logical types\nAt long last we arrive at the point where I’m talking about the data values themselves, and the simplest kind of data to talk about are those used to represent truth values. In R, these are called logical data and can take on three possible values: TRUE and FALSE are the two truth values, and NA is used to denote missing data.10 In a moment I’ll show you how to directly pass individual values from R to Arrow, but for the moment let’s stick to what we know and pass the data across as part of a tabular data structure. Here’s a tiny tibble, with one column of logical values:\n\ndat <- tibble(values = c(TRUE, FALSE, NA))\ndat\n\n# A tibble: 3 × 1\n  values\n  <lgl> \n1 TRUE  \n2 FALSE \n3 NA    \n\n\nWe’re going to pass this across to Arrow using arrow_table() but before we do let’s talk about what we expect to happen when the data arrive at the other side.\nIn this case, it’s quite straightforward. Arrow has a boolean type that has truth values true and false that behave the same way as their cousins in R. Just like R, Arrow allows missing values, though they’re called null values in Arrow. Unlike basically every other example we’re going to see in this post, this one is straightforward because the mapping is perfect. Unless you do something to override it, the arrow package will map an R logical to an Arrow boolean and vice versa. Here’s the diagram I use to describe it:\n\n\n\n\n\nDefault mappings for logical types\n\n\n\n\nSeems to make sense, right? So let’s stop talking about it and create the corresponding Table in Arrow:\n\ntbl <- arrow_table(dat)\ntbl\n\nTable\n3 rows x 1 columns\n$values <bool>\n\n\nHm. Okay that’s a little underwhelming as output goes? I’d like to see the actual values please. Happily the arrow package supplies a $ operator for Table objects so we can extract an individual column from tbl the same way we can from the original R object dat. Let’s try that:\n\ntbl$values\n\nChunkedArray\n[\n  [\n    true,\n    false,\n    null\n  ]\n]\n\n\nThe output looks a little different to what we’d get when printing out a single column of a tibble (or data frame), but it’s pretty clear that we’ve extracted the right thing. A single column inside an Arrow Table is stored as a ChunkedArray, so this looks right.\nYay us!\nAt this point, it’s handy to remember that the arrow_table() function that I used to move the data into Arrow is really just a wrapper that allows you to access some of the Table functionality without having to think about R6 too much. I also mentioned there’s a class generator called ChunkedArray object and a chunked_array() wrapper function. In hindsight, I probably didn’t need to bother creating the tibble and porting that over as a Table. I could have created a logical vector in R and port that over as a ChunkedArray directly:\n\nvalues <- c(TRUE, FALSE, NA)\nchunked_array(values)\n\nChunkedArray\n[\n  [\n    true,\n    false,\n    null\n  ]\n]\n\n\nThat’s a cleaner way of doing things. If you want a Table, use Table and its wrappers. If you want a ChunkedArray, use ChunkedArray and its wrappers. There’s no need to over-complicate things.\nSpeaking of which… later in the post, I’ll often want to send single values to Arrow. In those cases I don’t want to create a ChunkedArray, or even the simpler unchunked Array type. What I want to pass is a Scalar.\nIt’s worth unpacking this a little. Unlike some languages, R doesn’t really make a strong distinction between “vectors” and “scalars”: an R “scalar” is just a vector of length one. Arrow is stricter, however. A ChunkedArray is a container object with one or more Arrays, and an Array is also a container object with one or more Scalars. If it helps, you can think of it a little bit like working with lists in R: if I have a list lst, then lst[1] is still a list. It doesn’t return the contents of the list. If I want to extract the contents I have to use lst[[1]] to pull them out. Arrow Arrays contain Scalars in a fashion that we would call “list-like” in R.\nIn any case, the important thing to recognise is that arrow contains a class generator object called Scalar, and it works the same way as the other ones. The one difference is that there aren’t any wrapper functions for Scalar, so I’ll have to use Scalar$create() directly:\n\nScalar$create(TRUE, type = boolean())\n\nScalar\ntrue\n\n\nIn this example I didn’t really need to explicitly specify that I wanted to import the data as type = boolean(). The value TRUE is an R logical, and the arrow default is to map logicals onto booleans. I only included it here because I wanted to call attention to the type argument. Any time that you want to import data as a non-default type, you need to specify the type argument. If you look at the list of Apache Arrow data types on the arrow documentation page, you’ll see quite a lot of options. For now, the key thing to note is that the type argument expects you to call one of these functions.\nAnyway, that’s everything I had to say about logicals. Before moving on though, I’m going to write my own wrapper function, and define scalar() as an alias for Scalar$create():\n\nscalar <- function(x, type = NULL) {\n  Scalar$create(x, type)\n}\n\nThe main reason I’m doing that is for convenience, because in this post I’m actually going to need this wrapper function a lot. So I should probably check… does it work?\n\nscalar(TRUE)\n\nScalar\ntrue\n\n\nAwesome!\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#integer-types",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#integer-types",
    "title": "Data types in Arrow and R",
    "section": "Integer types",
    "text": "Integer types\nWhen translating R logicals to Arrow booleans, there aren’t a lot of conceptual difficulties. R has one data structure and Arrow has one data structure, and they’re basically identical. This is easy. Integers, however, are a little trickier because there’s no longer an exact mapping between the two languages. Base R provides one integer type, but Arrow provides eight distinct integer types that it inherits from C++. As a consequence it will no longer be possible to provide one-to-one mappings between R and Arrow, and some choices have to be made. As we’ll see in this section, the arrow package tries very hard to set sensible default choices, and in most cases these will work seamlessly. It’s not something you actually have to think about much. But, as my dear friend Dan Simpson11 reminds me over and over with all things technical, “God is present in the sweeping gestures but the Devil is in the details”.\nIt is wise to look carefully at the details, so let’s do that.\n\n\n[Arrow] Eight types of integer\nTo make sense of the different types, it helps to take a moment to think about how integers are represented in a binary format. Let’s suppose we allocate 8 bits to specify an integer. If we do that, then there are \\(2^8 = 256\\) unique binary patterns we can create with these bits. Because of this, there is a fundamental constraint: no matter how we choose to set it up, 8-bit integers can only represent 256 distinct numbers. Technically, we could choose any 256 numbers we like, but in practice there are only two schemes used for 8-bit integers: “unsigned” 8-bit integers (uint8) use those bits to represent integers from 0 to 255, whereas “signed” 8-bit integers (int8) can represent integers from -128 to 127.\nMore generally, an unsigned n-bit integer can represent integers from 0 to \\(2^n - 1\\), whereas a signed n-bit integer can represent integers from \\(-2^{n-1}\\) to \\(2^{n-1} - 1\\). Here’s what that looks like for all the integer types supported by Arrow:\n\n\n\n\n\n\n\n\n\nDescription\nName\nSmallest Value\nLargest Value\n\n\n\n\n8 bit unsigned\nuint8\n0\n255\n\n\n16 bit unsigned\nuint16\n0\n65535\n\n\n32 bit unsigned\nuint32\n0\n4294967295\n\n\n64 bit unsigned\nuint64\n0\n18446744073709551615\n\n\n8 bit signed\nint8\n-128\n127\n\n\n16 bit signed\nint16\n-32768\n32767\n\n\n32 bit signed\nint32\n-2147483648\n2147483647\n\n\n64 bit signed\nint64\n-9223372036854775808\n9223372036854775807\n\n\n\n\n\n\n[R] One integer class\nOn the R side, the integer type supplied by base R is a 32 bit signed integer, and has a natural one-to-one mapping to the Arrow int32 type. Because of this, the arrow default is to convert an R integer to an Arrow int32 and vice versa. Here’s an example. I’ve been watching Snowpiercer lately, and the train is currently 1029 cars long so let’s pass the integer 1029L from R over to Arrow\n\nsnowpiercer <- scalar(1029L)\nsnowpiercer\n\nScalar\n1029\n\n\nLet’s inspect the type field of the snowpiercer object in order to determine what type of object has arrived in Arrow:\n\nsnowpiercer$type\n\nInt32\nint32\n\n\nWe can apply the S3 generic function as.vector() to snowpiercer to pull the data back into R,12 and hopefully it comes as no surprise to see that we get the same number back:\n\nas.vector(snowpiercer)\n\n[1] 1029\n\n\nWe can take this one step further to check that the returned object is actually an R integer by checking its class(), and again there are no surprises:\n\nsnowpiercer %>% \n  as.vector() %>% \n  class()\n\n[1] \"integer\"\n\n\nAs you can see, the default behaviour in arrow is to translate an R integer into an Arrow int32, and vice versa. That part, at least, is not too complicated.\nThat being said, it’s worth unpacking some of the mechanics of what I’m doing with the code here. Everything I’ve shown above is R code, so it’s important to keep it firmly in mind that when I create the snowpiercer object there are two different things happening: a data object is created inside Arrow, and a pointer to that object is created inside R. The snowpiercer object is that pointer (it’s actually an R6 object). When I called snowpiercer$type in R, the output is telling me that the data object in Arrow has type int32. There’s a division of responsibility between R and Arrow that always needs to be kept in mind.\nNow, in this particular example there’s an element of silliness because my data object is so tiny. There was never a good reason to put the data in Arrow, and the only reason I’m doing it here is for explanatory purposes. But in real life (like in the TV shoe), snowpiercer might in fact be a gargantuan monstrosity over which you have perilously little control due to it’s staggering size. In that case it makes a big difference where the data object is stored. Placing the data object in Arrow is a little bit like powering your 1029-car long train using the fictitious perpetual motion engine from the show: it is a really, really good idea when you have gargantuan data.13\n\n\n\nWhen integer translation is easy\nWhat about the other seven C++ integer types? This is where it gets a little trickier. The table above illustrates that some integer types are fully contained within others: unsurprisingly, every number representable by int16 can also be represented by int32, so we can say that the int16 numbers are fully “contained” by (i.e. are a proper subset of) the int32 numbers. Similarly, uint16 is contained by uint32. There are many cases where an unsigned type is contained by a signed type: for instance, int32 contains all the uint16 numbers. However, because the unsigned integers cannot represent negative numbers, the reverse is never true. So we can map out the relationships between the different types like this:\n\n\n\n\n\nContainment relationships between the integer types.\n\n\n\n\nWhenever type A contains type B, it’s possible to transform an object of type B into an object of type A without losing information or requiring any special handling. R integers are 32 bit signed integers, which means it’s possible to convert Arrow data of types int32, int16, int8, uint16, and uint8 to R integers completely painlessly. So for these data types the arrow defaults give us this relationship:\n\n\n\n\n\nDefault mappings for some integer types\n\n\n\n\nThese are the cases where it is easy.\n\n\n\nWhen integer translation is hard\nOther integer types are messier. To keep things nice and simple, what we’d like to do is to map the Arrow uint32, uint64, and int64 types onto the R integer type. Sometimes that’s possible: if all the stored values fall within the range of values representable by R integers (i.e., are between -2147483648 and 2147483647) then we can do this, and that’s what arrow does by default. However, if there are values that “overflow” this range, then arrow will import the data as a different type. That leads to a rather messy diagram, I’m afraid:\n\n\n\n\n\nDefault mappings for other integer types. The asterisk notation here is intended to indicate that the path arrow follows can depend on the data values and other settings.\n\n\n\n\nTranslations become messy when the boxes in one language don’t quite match up to the content expressed in another. Sometimes it’s just easier to see the system in action, so let’s write a little helper function:\n\ntranslate_integer <- function(value, type) {\n  fn <- function(value, type) {\n    tibble(\n      value = value,\n      arrow_type = scalar(value, type)$type$name,\n      r_class = scalar(value, type) %>% as.vector() %>% class()\n    )\n  }\n  purrr::map2_dfr(value, type, fn)\n}\n\nThe translate_integer() function takes a value vector and a type list as input, and it returns a tibble that tells you what Arrow type was created from each input, and what R class gets returned when we import that Arrow object back into R. I’ll pass the inputs in as doubles originally, but as you’ll see they always get imported to Arrow as integer types because that’s what I’m telling arrow to do. So let’s start with an easy case. The number 10 is unproblematic because it’s very small, and arrow never encounters any problem trying to pull it back as an R integer:\n\ntranslate_integer(\n  value = c(10, 10, 10, 10), \n  type = list(uint8(), uint32(), uint64(), int64())\n)\n\n# A tibble: 4 × 3\n  value arrow_type r_class\n  <dbl> <chr>      <chr>  \n1    10 uint8      integer\n2    10 uint32     integer\n3    10 uint64     integer\n4    10 int64      integer\n\n\nOkay, that makes sense. If the numbers can be represented using the R integer class then that’s what arrow will do. Why make life unnecessarily difficult for the user?\nNow let’s increase the number to a value that is too big to store as a signed 32-bit integer. This is a value that R cannot represent as an integer, but Arrow can store as a uint32, uint64 or int64. What happens when we try to pull that object back into R?\n\ntranslate_integer(\n  value = c(3000000000, 3000000000, 3000000000), \n  type = list(uint32(), uint64(), int64())\n)\n\n# A tibble: 3 × 3\n       value arrow_type r_class  \n       <dbl> <chr>      <chr>    \n1 3000000000 uint32     numeric  \n2 3000000000 uint64     numeric  \n3 3000000000 int64      integer64\n\n\nThe first two rows seem intuitive. In base R, whenever an integer overflows and becomes too large to store, R will coerce it to a double. This is exactly the same behaviour we’d observe if the data had never left R at all. The third row, however, might come as a bit of a surprise. It certainly surprised me the first time I encountered it. Until very recently I did not know that R even had an integer64 class. This class is supplied by the bit64 package, and although I’m not going to talk about it in any detail here, it provides a mechanism to represent signed 64-bit integers in R. However, the one thing I will mention is the fact that the existence of the integer64 class opens up the possibility of forcing arrow to always map the integer64 class to the int64 type and vice versa. If you set\n\noptions(arrow.int64_downcast = FALSE)\n\nit will change the arrow default so that int64 types are always returned as integer64 classes, even when the values are small enough that the data could have been mapped to a regular R integer. This can be helpful in situations where you need to guarantee type stability when working with int64 data. Now that I’ve altered the global options, I can repeat my earlier command with the number 10.\n\ntranslate_integer(\n  value = c(10, 10, 10, 10), \n  type = list(uint8(), uint32(), uint64(), int64())\n)\n\n# A tibble: 4 × 3\n  value arrow_type r_class  \n  <dbl> <chr>      <chr>    \n1    10 uint8      integer  \n2    10 uint32     integer  \n3    10 uint64     integer  \n4    10 int64      integer64\n\n\nNotice that the results change for the int64 type only. The “int64_downcast” option pertains only to the int64 type, and does not affect the other integer types.\nAnd that’s it for integers. Next up we’ll talk about numeric types, but first I’ll be a good girl and restore my options to their previous state:\n\noptions(arrow.int64_downcast = NULL)\n\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#numeric-types",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#numeric-types",
    "title": "Data types in Arrow and R",
    "section": "Numeric types",
    "text": "Numeric types\nIn the last section I talked about the rather extensive range of data types that Arrow has to represent integers. Sure, there’s a practical benefit to having all these different data types, but at the same time its wild that we even need so many different data structures to represent something so simple. Integers aren’t complicated things. We learn them as kids even before we go to school, and we get taught the arithmetic rules to operate on them very early in childhood.\nThe problem, though, is that there are A LOT of integers. It’s a tad inconvenient sometimes, but the set of integers is infinite in size,14 so it doesn’t matter how many bits you allocate to your “int” type, there will always be integers that your machine cannot represent. But this is obvious, so why am I saying it? Mostly to foreshadow that things get worse when we encounter…\n\n\nFloating point numbers and the desert of the reals\n\nTo dissimulate is to pretend not to have what one has. To simulate is to feign to have what one doesn’t have. One implies a presence, the other an absence. But it is more complicated than that because simulating is not pretending: “Whoever fakes an illness can simply stay in bed and make everyone believe he is ill. Whoever simulates an illness produces in himself some of the symptoms” (Littré). Therefore, pretending, or dissimulating, leaves the principle of reality intact: the difference is always clear, it is simply masked, whereas simulation threatens the difference between the “true” and the “false,” the “real” and the “imaginary.”      – Jean Baudrillard, 1981, Simulacra and Simulation15\n\nThe real numbers correspond to our intuitive concept of the continuous number line. Just like the integers, the real line extends infinitely far in both directions, but unlike the integers the reals are continuous: for any two real numbers – no matter how close they are to each other – there is always another real number in between. This, quite frankly, sucks. Because the moment you accept that this is true, something ugly happens. If I accept that there must exist a number between 1.01 and 1.02, which I’ll call 1.015, then I have to accept that there is a number between 1.01 and 1.015, which I’ll call 1.0075, and then I have to accept that… oh shit this is going to go on forever. In other words, the reals have the obnoxious property that there between any two real numbers there are an infinity of other real numbers.16\nTry shoving all that into your finite-precision machine.\nStepping away from the mathematics for a moment, most of us already know how programming languages attempt to solve the problem. They use floating point numbers as a crude tool to approximate the real numbers using a finite-precision machine, and it… sort of works, as long as you never forget that floating point numbers don’t always obey the normal rules of arithmetic. I imagine most people reading this post already know this but for those that don’t, I’ll show you the most famous example:\n\n0.1 + 0.2 == 0.3\n\n[1] FALSE\n\n\nThis is not a bug in R. It happens because 0.1, 0.2, and 0.3 are not real numbers in the mathematical sense. Rather, they are encoded in R as objects of type double, and a double is a 64-bit floating point number that adheres to the IEEE 754 standard. It’s a bit beyond the scope of this post to dig all the way into the IEEE standard, but it does help a lot to have a general sense of how a floating point number (approximately) encodes a real number, so in the next section I’m going to take a look under the hood of R doubles. I’ll show you how they’re represented as binary objects, and why they misbehave sometimes. I’m doing this for two reasons: firstly it’s just a handy thing to know, but secondly, understanding the misbehaviour of the “standard” binary floating point number representation used in R helps motivate why Arrow and some other platforms expose other options to the user.\n\n\n\n[R] The numeric class\nTo give you a better feel for what a double looks like when represented as a set of bits, I’ve written a little extractor function called unpack_double() that decomposes the object into its constituent bits and prints it out in a visually helpful way (source code here). In truth, it’s just a wrapper around the numTobits() function provided by base R, but one that gives slightly prettier output. Armed with this, let’s take a look at the format. To start out, I’ll do the most boring thing possible and show you the binary representation of 0 as a floating point number. You will, I imagine, be entirely unshocked to discover that it is in fact a sequence of 64 zeros:\n\nunpack_double(0)\n\n0 00000000000 0000000000000000000000000000000000000000000000000000 \n\n\nTruly amazing.\nReally, the only thing that matters here is to notice the spacing. The sequence of 64 bits are divided into three meaningful chunks. The “first” bit17 represents the “sign”: is this a positive number (first bit equals 0) or a negative number (first bit equals 1), where zero is treated as if it were a positive number. The next 11 bits are used to specify an “exponent”: you can think of these bits as if they describe a signed “int11” type, and can be used to store any number between -1022 and 1023.18 The remaining 53 bits are used to represent the “mantissa”.19\nThese three components carve up a real number by using this this decomposition:\n\\[\n(\\mbox{real number}) = (\\mbox{sign}) \\times (\\mbox{mantissa}) \\times 2 ^ {\\mbox{(exponent)}}\n\\]\nAny real number can be decomposed in this way, so long as you have enough digits to express your mantissa and your exponent. Of course, on a finite precision machine we won’t always have enough digits, and this representation doesn’t allow us to fit “more” numbers into the machine: there’s a fundamental limit on what you can accomplish with 64 bits. What it can do for you, however, is let you use your limited resources wisely. The neat thing about adopting the decomposed format that floating-point relies on is that we can describe very large magnitudes and very small magnitudes with a fixed-length mantissa.\nTo give a concrete example of how floating point works, let’s take a look at the internal representation of -9.832, which I am told is the approximate rate of acceleration experienced by a falling object in the Earth’s polar regions:\n\npolar_g <- unpack_double(-9.832)\npolar_g\n\n1 10000000010 0011101010011111101111100111011011001000101101000100 \n\n\nI wrote some extractor functions that convert those binary components to the sign, exponent, and mantissa values that they represent, so let’s take a look at those:\n\nextract_sign(polar_g)\nextract_exponent(polar_g)\nextract_mantissa(polar_g)\n\n[1] -1\n[1] 3\n[1] 1.229\n\n\nNotice that the sign is always represented exactly: it can only be -1 or 1. The exponent is also represented exactly, as long as it’s not too large or too small: the number is always an integer value between -1022 and 1023. The mantissa, however, is a fractional value. When you encounter floating point errors it’s generally going to be because the stored mantissa doesn’t represent the true mantissa with sufficient precision.20 In any case, let’s check that the formula works:\n\nsign <- extract_sign(polar_g)\nexponent <- extract_exponent(polar_g)\nmantissa <- extract_mantissa(polar_g)\n\nsign * mantissa * 2 ^ exponent\n\n[1] -9.832\n\n\nYay!\nJust to prove to you that this isn’t a fluke, I also included a repack_double() function that automates this calculation. It takes the deconstructed representation of an R double and packs it up again, so repack_double(unpack_double(x)) should return x. Here are a few examples:\n\nsanity_check <- function(x) {\n  x == repack_double(unpack_double(x))\n}\nsanity_check(12)\nsanity_check(1345234623462342)\nsanity_check(0.000000002345345234523)\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nNow that we have some deeper knowledge of how R doubles are represented internally, let’s return to the numbers in the famous example of floating point numbers misbehaving:\n\nunpack_double(.1)\nunpack_double(.2)\nunpack_double(.3)\n\n0 01111111011 1001100110011001100110011001100110011001100110011010 \n0 01111111100 1001100110011001100110011001100110011001100110011010 \n0 01111111101 0011001100110011001100110011001100110011001100110011 \n\n\nAlthough these are clean numbers with a very simple decimal expansion, they are not at all simple when written in a binary floating point representation. In particular, notice that 0.1 and 0.2 share the same mantissa but 0.3 has a different mantissa, and that’s where the truncation errors occur. Let’s take a peek at 0.6 and 0.9:\n\nunpack_double(.6)\nunpack_double(.8)\nunpack_double(.9)\n\n0 01111111110 0011001100110011001100110011001100110011001100110011 \n0 01111111110 1001100110011001100110011001100110011001100110011010 \n0 01111111110 1100110011001100110011001100110011001100110011001101 \n\n\nSo it turns out that 0.6 has the same mantissa as 0.3, and 0.8 has the same mantissa as 0.1 and 0.2, but 0.9 has a different mantissa from all of them. So what we might expect is that floating point errors can happen for these cases:21\n\n0.1 + 0.2 == 0.3\n0.3 + 0.6 == 0.9\n\n[1] FALSE\n[1] FALSE\n\n\nbut not these ones:\n\n0.1 + 0.1 == 0.2\n0.3 + 0.3 == 0.6\n0.2 + 0.6 == 0.8\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nOkay that checks out! Now, it’s important to recognise that these errors are very small. So when I say that floating point arithmetic doesn’t actually “work”, a little care is needed. It does am impressively good job of approximating something very complicated using a quite limited tool:\n\n0.1 + 0.2 - 0.3\n\n[1] 5.551115e-17\n\n\nUltimately, floating point numbers are a simulation in the sense described by Baudrillard at the start of this section. They are a pretense, an attempt to act as if we can encode a thing (the reals) that we cannot encode. Floating point numbers are a fiction, but they are an extraordinarily useful one because they allow us to “cover” a very wide span of numbers across the real line, at a pretty high level of precision, without using too much memory.\nWe pretend that machines can do arithmetic on the reals. They can’t, but it’s a very powerful lie.\n\n\n\n[Arrow] The float64 type\nOkay. That was terribly long-winded, and I do apologise. Nevertheless, I promise there is a point to this story and it’s time we switched back over to the Arrow side of things to think about what happens there.\nBy now you’re probably getting used to the fact that Arrow tends to have more primitive types than R in most situations. Floating point numbers are no exception. R has only a single class, usually referred to as numeric but sometimes called double. In contrast, Arrow has three: float64, float32 and float16.22 It also has another numeric type called decimal that I’ll discuss later.\nThe easiest of these to discuss is float64, because it adopts the same conventions as the R double class. Just like R, it uses 64 bits to represent a floating point number.23 Because the data structures are so similar, the default behaviour in arrow is to translate an R double into an Arrow float64 and vice versa.\nAs always, I’ve got a little diagram summarising all the default mappings:\n\n\n\n\n\nDefault mappings for numeric types\n\n\n\n\nLet’s have a look at the Arrow float64 type. It’s a little anticlimactic in a sense, because it’s the same data structure as the R double type, so all we’re going to “learn” is that it behaves the same way! First, let’s create one:\n\nfloat_01 <- scalar(0.1)\nfloat_01\n\nScalar\n0.1\n\n\nAs always, we’ll verify that the created object has the type we’re expecting…\n\nfloat_01$type\n\nFloat64\ndouble\n\n\n… and it does, but you might be slightly puzzled by the output this time. What’s going on with the top line and the bottom line? Why does one say “Float64” and the other say “double”?\nWe’ve seen the “two lines of output” pattern earlier in the post when printing out an int32, but last time the two lines both said the same thing so I didn’t bother to comment on it. This time, however, there’s something to unpack. The distinction here refers to the name of the object type at the R level and and the C++ level. The first line of the output reads “Float64” because that’s what this data structure is called at the R level (i.e., according to arrow). The second line reads “double” because that’s what this data structure is called at the C++ level (i.e., in libarrow). There are a few cases where the arrow package adopts a slightly different naming scheme to libarrow, and so you’ll see this happen from time to time later in the post. There are some good reasons for this difference in nomenclature, and it’s nothing to be concerned about!\nAnyway, getting back to the main thread… since we’ve created the value 0.1 as a float64 in Arrow, let’s go through the same exercise we did in R and show that Arrow floats produce the same floating point errors. We’ll create new variables for 0.2 and 0.3:\n\nfloat_02 <- scalar(0.2)\nfloat_03 <- scalar(0.3)\n\nJust like we saw in R, the logical test of equality gives a counterintuitive answer:\n\nfloat_01 + float_02 == float_03\n\nScalar\nfalse\n\n\n… and just like we saw in R, the reason for it is that there’s a very small rounding error:\n\nfloat_01 + float_02 - float_03\n\nScalar\n5.551115123125783e-17\n\n\nJust so you don’t have to scroll up to check, yes, the rounding error is the same as the one that R produces:\n\n0.1 + 0.2 - 0.3\n\n[1] 5.551115e-17\n\n\nR and Arrow implement the same standard for floating point arithmetic, and so they “fail” in the same way because the failure occurs at the level of the standard. But we don’t blame IEEE 754 for that, because it’s literally impossible to define any standard that will encode the real numbers in an error-free way on a finite-precision machine.\n\n\n\n[Arrow] The float32 and float16 types\nThe float64 type provides an excellent, high precision floating point representation of numeric data. As data types go it is a good type. However, it is a 64-bit type, and sometimes you don’t need to store your data at a high degree of precision. With that in mind, because Arrow places a strong emphasis on both scalability and efficiency, it also provides the float32 type and the float16 type (though float16 hasn’t really been implemented yet, as far as I know). Encoding numeric data in these formats will save space, but will come at a cost of precision. As always, the decision of what encoding works best for your application will depend on what your needs are.\nAs far as the arrow package is concerned, there are no difficulties in passing data back and forth between R doubles and Arrow float32 types, but at present it’s not really possible to do this with float16 because this isn’t implemented. Still, we can briefly take a look at how it works for float32. Here’s an example of me passing an R double to Arrow:\n\nfloat32_01 <- scalar(.1, type = float32())\nfloat32_01\n\nScalar\n0.1\n\n\nLet’s quickly verify that it is in fact a 32-bit float:\n\nfloat32_01$type\n\nFloat32\nfloat\n\n\nAnd now let’s pull it back into R where it will be, once again, encoded as a double:\n\nas.vector(float32_01)\n\n[1] 0.1\n\n\nYay! It works!\n\n\n\nDecimal floating point numbers?\nIt’s time to talk about decimals. This is a fun topic, but I need to start with a warning: I mentioned that Arrow has a decimal type, and your first instinct as an R programmer might be to assume that this is another variation of floating point numbers. Fight this instinct: it’s not quite right.\nOkay, ready?\nEarlier in this section I promised that the Baudrillard quote from Simulacra and Simulation was going to be relevant? Well, that time has arrived. It’s also the moment at which the quote from Word and Object by Quine that opened this blog post becomes painfully relevant. Stripped of their fancy language, here’s what the two authors are telling us in these passages:\n\nThe Baudrillard quote emphasises that floating point numbers are a simulation. They are the mechanism by which we pretend to encode real numbers on computers. It’s a lie, but it’s a powerful lie that almost works.\nThe Quine quote emphasises that translation (and, I would argue, simulation also) is underdetermined. For any complicated thing there are many ways to simulate, or translate, or approximate it. These approximations can be extremely accurate and still be inconsistent with each other.\n\nQuine’s truism applies to floating point numbers, and it is the reason why “decimal floating point” numbers exist in addition “binary floating point” numbers. All floating point systems are simulations in the Baudrillard sense of the term: lies, strictly speaking, but close enough to true that the distinction between lies and truth gets a little blurry.\nLet’s see how that plays out with floating point numbers. When discussing doubles in R, I mentioned that they represent the real numbers using a decomposition that looks like this:\n\\[\n(\\mbox{real number}) = (\\mbox{sign}) \\times (\\mbox{mantissa}) \\times 2 ^ {\\mbox{(exponent)}}\n\\]\nThe number “2” pops out here, doesn’t it? Is there any reason to think that “2” is a pre-ordained necessity when approximating the real numbers on a finite-precision machine? Programmers have a tendency to like using “2” as the base unit for everything because it lines up nicely with binary representations, and that’s often a good instinct when dealing with machines.\nUnfortunately, life consists of more than machines.\nIn particular, binary representations create problems for floating point arithmetic because the world contains entities known as “humans”, who have a habit of writing numbers in decimal notation24. Numbers that look simple in decimal notation often look complicated in binary notation and vice versa. As we saw earlier, a “simple” decimal number like 0.1 doesn’t have a short binary expansion and so cannot be represented cleanly in a finite-precision binary floating point number system. Rounding errors are introduced every time a machine uses (base 2) floating point to encode data that were originally stored as a (base 10) number in human text.\nA natural solution to this is to design floating point data types that use other bases. It is entirely possible to adopt decimal floating point types that are essentially equivalent to the more familiar binary floating point numbers, but they rely on a base 10 decomposition:\n\\[\n(\\mbox{real number}) = (\\mbox{sign}) \\times (\\mbox{mantissa}) \\times 10 ^ {\\mbox{(exponent)}}\n\\]\nThe virtues of decimal floating point seem enticing, and it’s tempting to think that this must be what Arrow implements. However, as we’ll see in the next section, that’s not true.\nInstead of using floating-point decimals, it supplies “fixed-point” decimal types. In a floating-point representation, the exponent is chosen automatically, and it is a property of the value itself. The number -9.832 will always have an exponent of 3 when encoded as a binary floating-point number (as we saw in the polar_g example earlier), and that exponent will never be influenced by the values of other numbers stored in the same data set.\nA fixed-point representation is different. The exponent – and in a decimal representation, remember that the exponent is just “the location of the decimal point” – is chosen by the user. You have to specify where the decimal point is located manually, and this location will be applied to each value stored in the object. In other words, the exponent – which is now called the “scale”, and is parameterised slightly differently – becomes a property of the type, not the value.\nSigh. Nothing in life is simple, is it? It’ll become a little clearer in the next section, I promise!\n\n\n\n[Arrow] The decimal fixed-point types\nArrow has two decimal types, a decimal128 type that (shockingly) uses 128 bits to store a floating point decimal number, and a decimal256 type that uses 256 bits. As usual arrow package supplies type functions decimal128() and decimal256() that allow you to specify decimal types. Both functions have two arguments that you must supply:\n\nprecision specifies the number of significant digits to store, similar to setting the length of the mantissa in a floating-point representation.\nscale specifies the number of digits that should be stored after the decimal point. If you set scale = 2, exactly two digits will be stored after the decimal point. If you set scale = 0, values will be rounded to the nearest whole number. Negative scales are also permitted (handy when dealing with extremely large numbers), so scale = -2 stores the value to the nearest 100.\n\nOne convenience that exists both in the arrow R package and within libarrow itself is that it can automatically decide whether you need a decimal128 or a decimal256 simply by looking at the value of the precision argument. If the precision is 38 or less, you can encode the data with a decimal128 type. Larger values require a decimal256. If you would like to take advantage of this – as I will do in this post – you can use the decimal() type function which will automatically create the appropriate type based on the specified precision.\nOne inconvenience that I have in this post, however, is that R doesn’t have any analog of a fixed-point decimal, and consequently I don’t have any way to create an “R decimal” that I can then import into Arrow. What I’ll do instead is create a floating point array in Arrow, and then explicitly cast it to a decimal type. Step one, create the floating point numbers in Arrow:\n\nfloats <- chunked_array(c(.01, .1, 1, 10, 100), type = float32())\nfloats\n\nChunkedArray\n[\n  [\n    0.01,\n    0.1,\n    1,\n    10,\n    100\n  ]\n]\n\n\nStep two, cast the float32 numbers to decimals:\n\ndecimals <- floats$cast(decimal(precision = 5, scale = 2))\ndecimals\n\nChunkedArray\n[\n  [\n    0.01,\n    0.10,\n    1.00,\n    10.00,\n    100.00\n  ]\n]\n\n\nThese two arrays look almost the same (especially because I chose the scale judiciously!), but the underlying encoding is different. The original floats array is a familiar float32 type, but if we have a look at the decimals object we see that it adopts a quite different encoding:\n\ndecimals$type\n\nDecimal128Type\ndecimal128(5, 2)\n\n\nTo illustrate that these do behave differently, let’s have fun making floating point numbers misbehave again:\n\nsad_floats <- chunked_array(c(.1, .2, .3))\nsum(sad_floats)\n\nScalar\n0.6000000000000001\n\n\nOh noes. Okay, let’s take a sad float32 and turn it into a happy decimal. I’ll store it as a high precision decimal to make it a little easier to compare the results:\n\nhappy_decimals <- sad_floats$cast(decimal(20, 16))\n\nNow let’s look at the two sums side by side:\n\nsum(sad_floats)\nsum(happy_decimals)\n\nScalar\n0.6000000000000001\nScalar\n0.6000000000000000\n\n\nYay!\nAs a final note before moving on, it is (of course!!!) the case that fixed-point decimals aren’t a universal solution to the problems of binary floating-point numbers. They have limitations of their own and there are good reasons why floats remain the default numeric type in most languages. But they have their uses: binary and decimal systems provide different ways to simulate the reals, as do fixed and floating point systems. Each such system is a lie, of course: the reals are too big to be captured in any finite system we create. They are, however, useful.\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#character-types",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#character-types",
    "title": "Data types in Arrow and R",
    "section": "Character types",
    "text": "Character types\nOur journey continues. We now leave behind the world of number and enter the domain of text. Such times we shall have! What sights we shall see! (And what terrors lie within?)\nStrings are an interesting case. R uses a single data type to represent strings (character vectors) but Arrow has two types, known as strings and large_strings. When using the arrow package, Arrow strings are specified using the utf8() function, and large strings correspond to the large_utf8() type. The default mapping is to assume that an R character vector maps onto the Arrow utf8() type, as shown below:\n\n\n\n\n\nDefault mappings for character types\n\n\n\n\nThere’s a little more than meets the eye here though, and you might be wondering about the difference between strings and large_strings in Arrow, and when you might prefer one to the other. As you might expect, the large string type is suitable when you’re storing large amounts of text, but to understand it properly I need to talk in more depth about how R and Arrow store strings, and I’ll use this partial list of people that – according to the lyrics of Jung Talent Time by TISM – were perhaps granted slightly more fame than they had earned on merit:\nBert Newton\nBilly Ray Cyrus\nWarwick Capper\nUri Geller\nSamantha Fox\n\n\n[R] The character class\nSuppose I want to store this as a character vector in R, storing only the family names for the sake of brevity and visual clarity.\n\njung_talent <- c(\"Newton\", \"Cyrus\", \"Capper\", \"Geller\", \"Fox\")\n\nEach element of the jung_talent vector is a variable-length string, and is stored internally by R as an array of individual characters25 So, to a first approximation, your mental model of how R stores the jung_talent variable might look something like this:\n\n\n\n\n\nSimplified representation of how character vectors are represented in R\n\n\n\n\nHere, the jung_talent variable is an object26 that contains five elements shown as the orange boxes. Internally, each of those orange boxes is itself an array of individual characters shown as the purple boxes. As a description of what R actually does this is a bit of an oversimplification because it ignores the global string pool, but it will be sufficient for the current purposes.\nThe key thing to understand conceptually is that R treats the elements of a character vector as the fundamental unit. The jung_talent vector is constructed from five distinct strings, \"Newton\", \"Cyrus\", etc. The \"Newton\" string is assigned to position 1, the \"Cyrus\" string is assigned to position 2, and so on.\n\n\n\n[Arrow] The utf8 type\nThe approach taken in Arrow is rather different. Instead of carving up the character vector into strings (and internally treating the strings as character arrays), it concatenates everything into one long buffer. The text itself is dumped into one long string, like this:\nNewtonCyrusCapperGellerFox\nThe first element of this buffer – the letter \"N\" – is stored at “offset 0” (indexing in Arrow starts at 0), the second element is stored at offset 1, and so on. This long array of text is referred to as the “data buffer”, and it does not specify where the boundaries between array elements are. Those are stored separately. If I were to create an Arrow string array called jung_talent_arrow, it would be comprised of a data buffer, and an “offset buffer” that specifies the positions at which each element of the string array begins. In other words, we’d have a mental model that looks a bit like this:\n\n\n\n\n\nSimplified representation of how character vectors are represented in Arrow\n\n\n\n\nHow are each of these buffers encoded?\n\nThe contents of the data buffer are stored as UTF-8 text, which is itself a variable length encoding: some characters are encoded using only 8 bits while others require 32 bits. This blog post on unicode is a nice explainer.\nThe contents of the offset buffer are stored as unsigned integers, either 32 bit or 64 bit, depending on which of the two Arrow string array types you’re using. I’ll unpack this in the next section.\n\nSheesh. That was long. Let’s give ourselves a small round of applause for surviving, and now actually DO something. We’ll port the jung_talent vector over to Arrow.\n\njung_talent_arrow <- chunked_array(jung_talent)\njung_talent_arrow\n\nChunkedArray\n[\n  [\n    \"Newton\",\n    \"Cyrus\",\n    \"Capper\",\n    \"Geller\",\n    \"Fox\"\n  ]\n]\n\n\nThat certainly looks like text to me! Let’s take a look at the data type:\n\njung_talent_arrow$type\n\nUtf8\nstring\n\n\nYep. Definitely text!\n\n\n\n[Arrow] The large_utf8 type\nOkay, so as I mentioned, Arrow has two different string types: it has strings (also called utf8) and large_strings (also called large_utf8).27 The default in arrow is to translate character data in R to the utf8 data type in Arrow, but we can override this if we want to. In order to help you make an informed choice, I’ll dig a little deeper into the difference between the two types.\nThe first thing to recognise is that the nature of the data buffer is the same for utf8 and large_utf8: the difference between the two lies in how the offset buffers are encoded. When character data are encoded as utf8 type, every offset value is stored as an unsigned 32-bit integer. That means that – as shown in the table of integer types earlier in the post – you cannot store an offset value larger than 4294967295. This constrain places a practical cap on the total length of the data buffer: if total amount of text stored in the data buffer is greater than about 2GiB, the offset buffer can’t encode the locations within it! Switching to large_utf8 means that the offset buffer will store every offset value as an unsigned 64-bit integer. This means that the offset buffer now takes up twice as much space, but it allows you to encode offset values up to… um… 18446744073709551615. And if you’ve got so much text that your data buffer is going to exceed that limit, well, frankly you have bigger problems.\nIn short, if you’re not going to exceed 2GiB of text in your array, you don’t need large_utf8. Once you start getting near that limit, you might want to think about switching:\n\njung_talent_arrow_big <- chunked_array(jung_talent, type = large_utf8())\njung_talent_arrow_big$type\n\nLargeUtf8\nlarge_string\n\n\nBefore moving on, I’ll mention one additional complexity. This is a situation where the distinction between Arrays and ChunkedArrays begins to matter. Strictly speaking, I lied earlier when I said there’s only one data buffer. A more precise statement would be to say that there is one data buffer per chunk (where each chunk in a ChunkedArray is an Array). ChunkedArrays are designed to allow a block (or “chunk”) of contiguous rows in a table to be stored together in a single location (or file). There are good reasons for doing that28, but they aren’t immediately relevant. What matters is to recognise that in a ChunkedArray, the 2GiB limit on utf8 type data applies on a per-chunk basis. The net result of this is that you probably don’t need large_utf8 except in very specific cases.\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#datetime-types",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#datetime-types",
    "title": "Data types in Arrow and R",
    "section": "Date/time types",
    "text": "Date/time types\nNext up on our tour of data types are dates and times. Internally, R and Arrow both adopt the convention of measuring time in terms of the time elapsed since a specific moment in time known as the unix epoch. The unix epoch is the time 00:00:00 UTC on 1 January 1970. It was a Thursday.\nDespite agreeing on fundamentals, there are some oddities in the particulars. Base R has three date/time classes (Date, POSIXct, and POSIXlt), and while Arrow also has three date/time classes (date32, date64, and timestamp), the default mappings between them are a little puzzling unless you are deeply familiar with what all these data types are and what they represent. I’ll do the deep dive in a moment, but to give you the big picture here’s how the mapping works:\n\n\n\n\n\nDefault mappings for date/time types\n\n\n\n\n\n\n[R] The Date class\nOn the R side of things, a Date object is represented internally as a numeric value, counting the number of days since the unix epoch. Here is today as a Date:\n\ntoday <- Sys.Date()\ntoday\n\n[1] \"2022-08-23\"\n\n\nIf I use unclass() to see what it looks like under the hood:\n\nunclass(today)\n\n[1] 19227\n\n\nFundamentally, a Date object is a number:29 it counts the number of days that have elapsed since a fixed date. It does not care what the year is, what the month is, or what day of the month it is. It does not care how the date is displayed to the user. All those things are supplied by the print() method, and are not part of the Date itself.\n\n\n\n[R] The POSIXct class\nA date is a comparatively simple thing. When we want to represent dates and time together, we need to know the time of day, and we might need to store information about the timezone as well (more on that later). Base R has two different classes for representing this, POSIXct and POSIXlt. These names used to confuse me a lot. POSIX stands for “portable operating system interface”, and it’s a set of standards used to help operating systems remain compatible with each other. In this context though, it’s not very meaningful: all it says “yup we use unix time.”\nThe more important part of the name is actually the “ct” versus “lt” part. Let’s start with POSIXct. The “ct” in POSIXct stands for “calendar time”: internally, R stores the number of seconds30 that have elapsed since 1970-01-01 00:00 UTC.\n\nnow <- Sys.time()\nnow\n\n[1] \"2022-08-23 13:05:52 AEST\"\n\n\nIf I peek under the hood using unclass() here’s what I see:\n\nunclass(now)\n\n[1] 1661223953\n\n\nThere are no attributes attached to this object, it is simply a count of the number of seconds since that particular moment in time. However, it doesn’t necessarily have to be this way: a POSIXct object is permitted to have a “tzone” attribute, a character string that specifies the timezone that is used when printing the object will be preserved when it is converted to a POSIXlt.\nNevertheless, when I created the now object by calling Sys.time(), no timezone information was stored in the object. The fact that it appears when I print out now occurs because the print() method for POSIXct objects prints the time with respect to a particular timezone. The default is to use the system timezone, which you can check by calling Sys.timezone(), but you can override this behaviour by specifying the timezone explicitly (for a list of timezone names, see OlsonNames()). So if I wanted to print the time in Berlin, I could do this:\n\nprint(now, tz = \"Europe/Berlin\")\n\n[1] \"2022-08-23 05:05:52 CEST\"\n\n\nIf you want to record the timezone as part of your POSIXct object rather than relying on the print method to do the work, you can do so by setting the tzone attribute. To illustrate this, let’s pretend I’m in Tokyo:\n\nattr(now, \"tzone\") <- \"Asia/Tokyo\"\nnow\n\n[1] \"2022-08-23 12:05:52 JST\"\n\n\nThe important thing here is that the timezone is metadata used to change the how the time is displayed. Changing the timezone does not alter the number of seconds stored in the now object:\n\nunclass(now)\n\n[1] 1661223953\nattr(,\"tzone\")\n[1] \"Asia/Tokyo\"\n\n\n\n\n\n[R] The POSIXlt class\nWhat about POSIXlt? It turns out that this is a quite different kind of data structure, and it “thinks” about time in a very different way. The “lt” in POSIXlt stands for “local time”, and internally a POSIXlt object is a list that stores information about the time in a way that more closely mirrors how humans think about it. Here’s what now looks like when I coerce it to a POSIXlt object:\n\nnow_lt <- as.POSIXlt(now)\nnow_lt\n\n[1] \"2022-08-23 12:05:52 JST\"\n\n\nIt looks exactly the same, but this is an illusion produced by the print() method. Internally, the now_lt object is a very different kind of thing. To see this, let’s see what happens if we print it as if it were a regular list:\n\nunclass(now_lt)\n\n$sec\n[1] 52.72954\n\n$min\n[1] 5\n\n$hour\n[1] 12\n\n$mday\n[1] 23\n\n$mon\n[1] 7\n\n$year\n[1] 122\n\n$wday\n[1] 2\n\n$yday\n[1] 234\n\n$isdst\n[1] 0\n\n$zone\n[1] \"JST\"\n\n$gmtoff\n[1] 32400\n\nattr(,\"tzone\")\n[1] \"Asia/Tokyo\" \"JST\"        \"JDT\"       \n\n\nAs you can see, this object separately stores the year (counted from 1900), the month (where January is month 0 and December is month 11), the day of the month (starting at day 1), etc.31 The timezone is stored, as is the day of the week (Sunday is day 0), it specifies whether daylight savings time is in effect, and so on. Time, as represented in the POSIXlt class, uses a collection of categories that are approximately the same as those that humans use when we talk about time.\nIt is not a compact representation, and it’s useful for quite different things than POSIXct. What matters for the current purposes is that POSIXlt is, fundamentally, a list structure, and is not in any sense a “timestamp”.\n\n\n\n[Arrow] The date32 type\nOkay, now let’s pivot over to the Arrow side and see what we have to work with. The date32 type is similar – but not identical – to the R Date class. Just like the R Date class, it counts the number of days since 1970-01-01. To see this, let’s create an analog of the today Date object inside Arrow, and represent it as a date32 type:\n\ntoday_date32 <- scalar(today, type = date32())\ntoday_date32\n\nScalar\n2022-08-23\n\n\nWe can expose the internal structure of this object by casting it to an int32:\n\ntoday_date32$cast(int32())\n\nScalar\n19227\n\n\nThis is the same answer we got earlier when I used unclass() to take a peek at the internals of the today object. That being said, there is a subtle difference: in Arrow, the date32 type is explicitly a 32-bit integer. If you read through the help documentation for date/time classes in R you’ll see that R has something a little more complicated going on. The details don’t matter for this post, but you should be aware that Dates (and POSIXct objects) are stored as doubles. They aren’t stored as integers:\n\ntypeof(today)\ntypeof(now)\n\n[1] \"double\"\n[1] \"double\"\n\n\nIn any case, given that the Arrow date32 type and the R Date class are so similar to each other in structure and intended usage, it is natural to map R Dates to Arrow date32 types and vice versa, and that’s what the arrow package does by default.\n\n\n\n[Arrow] The date64 type\nThe date64 type is similar to the date32 type, but instead of storing the number of days since 1970-01-01 as a 32-bit integer, it stores the number of milliseconds since 1970-01-01 00:00:00 UTC as a 64-bit integer. It’s similar to the POSIXct class in R, except that (1) it uses milliseconds instead of seconds; (2) the internal storage is an int64, not a double; and (3) it does not have metadata and cannot represent timezones.\nAs you might have guessed, the date64 type in Arrow isn’t very similar to the Date class in R. Because it represents time at the millisecond level, the intended use of the date64 class is in situations where you want to keep track of units of time smaller than one day. Sure, I CAN create date64 objects from R Date objects if I want to…\n\nscalar(today, date64())\n\nScalar\n2022-08-23\n\n\n…but this is quite wasteful. Why use a 64-bit representation that tracks time at the millisecond level when all I’m doing is storing the date? Although POSIXct and date64 aren’t exact matches, they’re more closely related to each other than Date and date64. So let’s create an Arrow analog of now as a date64 object:\n\nnow_date64 <- scalar(now, date64())\nnow_date64\n\nScalar\n2022-08-23\n\n\nThe output is printed as a date, but this is a little bit misleading because it doesn’t give you a good sense of the level of precision in the data. Again we can peek under the hood by explicitly casting this to a 64-bit integer:\n\nnow_date64$cast(int64())\n\nScalar\n1661223952729\n\n\nThis isn’t a count of the number of days since the unix epoch, it’s a count of the number of milliseconds. It is essentially the same number, divided by 1000, as the one we obtained when I typed unclass(now).\nHowever, there’s a puzzle here that we need to solve. Let’s take another look at unclass(now):\n\nunclass(now)\n\n[1] 1661223953\nattr(,\"tzone\")\n[1] \"Asia/Tokyo\"\n\n\nThis might strike you as very weird. On the face of it, what has happened is that I have taken now (which ostensibly represents time at “second-level” precision), ported it over to Arrow, and created an object now_date64 that apparently knows what millisecond it is???? How is that possible? Does Arrow have magic powers?\nNot really. R is playing tricks here. Remember how I said that POSIXct objects are secretly doubles and not integers? Well, this is where that becomes relevant. It’s quite hard to get R to confess that a POSIXct object actually knows the time at a more precise level than “to the nearest second” but you can get it do to so by coercing it to a POSIXlt object and then taking a peek at the sec variable:\n\nas.POSIXlt(now)$sec\n\n[1] 52.72954\n\n\nAha! The first few digits of the decimal expansion are the same ones stored as the least significant digits in now_date64. The data was there all along. Even though unclass(now) produces an output that has been rounded to the nearest second, the original now variable is indeed a double, and it does store the time a higher precision! Ultimately, the accuracy of the time depends on the system clock itself, but the key thing to know here is that even though POSIXct times are almost always displayed to the nearest second, they do have the ability to represent more precise times.\nBecause of this, the default behaviour in arrow is to convert date64 types (64-bit integers interpreted as counts of milliseconds) to POSIXct classes (which are secretly 64-bit doubles interpreted as counts of seconds).\nRight. Moving on.\n\n\n\n[Arrow] The timestamp type\nThe last of the Arrow date/time types is the timestamp. The core data structure is a 64-bit integer used to count the number of time units that have passed since the unix epoch, and this is associated with two additional pieces of metadata: the time unit used (e.g., “seconds”, “milliseconds,”microseconds”, “nanoseconds”), and the timezone. As with the POSIXct class in R, the timezone metadata is optional, but the time unit is necessary. The default is to use microseconds (i.e., unit = \"us\"):\n\nscalar(now)\nscalar(now, timestamp(unit = \"us\"))\n\nScalar\n2022-08-23 03:05:52.729535\nScalar\n2022-08-23 03:05:52.729535\n\n\nAlternatively, we could use seconds:\n\nscalar(now, timestamp(unit = \"s\"))\n\nScalar\n2022-08-23 03:05:52\n\n\nIt’s important to recognise that changing the unit does more than change the precision at which the timestamp is printed. It changes “the thing that is counted”, so the numbers that get stored in the timestamp are quite different depending on the unit. Compare the numbers that are stored when the units are seconds versus when the units are nanoseconds:\n\nscalar(now, timestamp(unit = \"s\"))$cast(int64())\nscalar(now, timestamp(unit = \"ns\"))$cast(int64())\n\nScalar\n1661223952\nScalar\n1661223952729535744\n\n\nOkay, what about timezone?\nRecall that now has a timezone attached to it, because I explicitly recorded the tzone attribute earlier. Admittedly I lied and I said I was in Tokyo and not in Sydney, but still, that information is in the now object:\n\nnow\n\n[1] \"2022-08-23 12:05:52 JST\"\n\n\nWhen I print the R object it displays the time in the relevant time zone. The output for the Arrow object doesn’t do that: the time as displayed is shown in UTC. However, that doesn’t mean that the metadata isn’t there:\n\nnow_timestamp <- scalar(now)\nnow_timestamp$type\n\nTimestamp\ntimestamp[us, tz=Asia/Tokyo]\n\n\nI mention this because this caused me a considerable amount of panic at one point when I thought my timezone information had been lost when importing data from POSIXct into Arrow. Nothing was lost, it is simply that the arrow R package prints all timestamps in the corresponding UTC time regardless of what timezone is specified in the metadata.\nThere is, however, a catch. This worked last time because I was diligent and ensured that my now variable encoded the timezone. By default, a POSIXct object created by Sys.time() will not include the timezone. It’s easy to forget this because the print() method for POSIXct objects will inspect the system timezone if the POSIXct object doesn’t contain any timezone information, so it can often look like you have a timezone stored in your POSIXct object when actually you don’t. When that happens, Arrow can’t help you. Because the POSIXct object does not have a timezone (all appearances to the contrary), the object that arrives in Arrow won’t have a timezone either. Here’s what I mean:\n\n# a POSIXct object with no timezone\nnew_now <- Sys.time() # has no time zone...\nnew_now               # ... but appears to!\n\n[1] \"2022-08-23 13:05:52 AEST\"\n\n# an Arrow timestamp with no timezone\nnew_now_timestamp <- scalar(new_now)\nnew_now_timestamp$type\n\nTimestamp\ntimestamp[us, tz=Australia/Sydney]\n\n\nThe take-home message in all this is that if you’re going to be working in both Arrow and R, and using the arrow package for data interchange, you’d be well advised to be careful with your POSIXct objects and timezones. They are trickier than they look, and can lead to subtle translation errors if you are not careful!\n\n\n\nUm, but what about POSIXlt?\nAt long last we come to POSIXlt, which has no clear analog in Arrow. The key idea behind POSIXlt is to represent temporal information in terms of multiple different units: days, weeks, years, seconds, timezones, and so on. It’s a very different kind of thing to a POSIXct object in R or a timestamp in Arrow. In R terms, it’s essentially a list, and as a consequence the default behaviour in arrow is to import it as a struct (which serves essentially the same purpose). Here’s how that looks:\n\nscalar(now_lt)\n\nScalar\n...\n\n\nThe struct object contains named fields that are identical to their POSIXlt equivalents, and whose types have been translated according to the default mappings: sec is a double, min is an int32, hour is an int32, zone is a string, and so on.\nThis arrangement, where POSIXct maps to timestamp and POSIXlt maps to struct, makes perfect sense when you think about the underlying data structures that POSIXct and POSIXlt encode. Where things can be tricky for the R user is in the “mental account keeping”. In order to be helpful, R displays POSIXct and POSIXlt objects in exactly the same way:\n\nnow\nnow_lt\n\n[1] \"2022-08-23 12:05:52 JST\"\n[1] \"2022-08-23 12:05:52 JST\"\n\n\nNot only that, because POSIXct and POSIXlt are both subclasses of the POSIXt class, R allows you to perform temporal arithmetic on objects of different types:\n\nnow - now_lt\n\nTime difference of 0 secs\n\n\nThis is very convenient from a data analysis perspective, since calculations performed with date/time classes “just work” even though POSIXct objects are secretly doubles and POSIXlt objects are secretly lists. However, all this happens invisibly. In much the same way that it’s easy to forget that POSIXct objects may not encode a timezone even though they look like they do, it can be easy to forget that POSIXct and POSIXlt are fundamentally different objects, and they map onto quite different data structures in Arrow.\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#duration-types",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#duration-types",
    "title": "Data types in Arrow and R",
    "section": "Duration types",
    "text": "Duration types\nAny discussion of temporal data is incomplete without a discussion of duration types, which are used to describe a length of time without reference to any fixed origin. The figure below shows the default mappings used by arrow:\n\n\n\n\n\nDefault mappings for duration types\n\n\n\n\n\n\n[R] The difftime class\nIn base R, the difference between two date/time objects is stored as a difftime object. To give a better illustration of a difftime object, let’s create diff, a variable that stores the amount of time elapsed between executing the R markdown chunk that first computed the now variable, and executing the R markdown chunk below:\n\nnew_now <- Sys.time()\nrmd_time <- new_now - now\nrmd_time\n\nTime difference of 0.5508101 secs\n\n\nNow let’s take a look at how it’s actually stored:\n\nunclass(rmd_time)\n\n[1] 0.5508101\nattr(,\"units\")\n[1] \"secs\"\n\n\nThe duration is represented as a double variable, and the \"units\" attribute is used to specify the time unit that it represents: “secs”, “mins”, “hours”, “days” or “weeks”. Unless the user specifies exactly which unit is to be used, R will attempt to make a sensible choice. For instance, if I were to do this,\n\nhedy_lamarr <- as.POSIXct(\"1914-11-09 19:30:00\", tz = \"Europe/Vienna\")\nhedy_age <- now - hedy_lamarr\nhedy_age\n\nTime difference of 39368.36 days\n\n\nI would learn that it has been 39368 days since Hedy Lamarr was born.32 More to the point, R has guessed that the length of time is sufficiently long that “seconds” aren’t the appropriate encoding unit:\n\nunclass(hedy_age)\n\n[1] 39368.36\nattr(,\"units\")\n[1] \"days\"\n\n\n\n\n\n[Arrow] The duration type\nThe difftime class in R has a natural analog in Arrow, the duration type. As usual though, they are not exactly equivalent to one another. An R difftime object stores the value as a double, so it has no problems storing 0.55 as the value and setting the units to be seconds. This doesn’t work very well in Arrow because the value is stored as a signed 64 bit integer (int64), and a value of 0.55 seconds will simply round down to a duration of zero seconds. When importing my duration data into Arrow, then, I should be careful to ensure I choose a higher precision unit. If I don’t, things can go a little awry:\n\nrmd_time_arrow <- scalar(rmd_time)\nrmd_time_arrow\n\nScalar\n0\n\n\nHm. Zero seconds was not exactly the answer I was looking for. It helps a little to take a peek at the data type and see what precisely it is that I have just created:\n\nrmd_time_arrow$type\n\nDurationType\nduration[s]\n\n\nThis reveals my mistake. I’ve encoded the time rounded to the nearest second, which is not very useful in this instance. What I really should have done is specify a higher level of precision. To import a duration into Arrow rounded to the nearest microsecond, I can do this:\n\nrmd_time_arrow <- scalar(rmd_time, duration(unit = \"us\"))\nrmd_time_arrow\n\nScalar\n550810\n\n\nThat’s a little better! Again, I can inspect the data type and see that the unit of encoding is now set to microseconds:\n\nrmd_time_arrow$type\n\nDurationType\nduration[us]\n\n\n\n\n\n[R] The hms::hms class\nSo where are we up to in our voyage through the world of dates, times, and durations in the R world? We’ve talked about situations where you can specify a fixed date (with the Date class) and situations where you can specify a fixed moment in time (with POSIXct and POSIXlt classes). We’ve also talked about situations where you can specify an amount of time without fixing it to a specific date or time (with the difftime class). What we haven’t talked about is how to store the time of day. In base R you can talk about a date without needing to specify a time, or you can talk about times and dates together, but what you can’t do is specify a time on its own without a date.\nThe hms package fixes this by supplying the hms class. Internally, it’s just a difftime object that counts the number of seconds elapsed since midnight. As I type these words the current time is 14:05:25, and I could create an hms object representing this like so:\n\nhms_time <- hms::hms(seconds = 25, minutes = 5, hours = 14)\nhms_time\n\n14:05:25\n\n\nThe nice thing about hms times is that they inherit from difftime, which we can see by checking the class vector for our hms_time object\n\nclass(hms_time)\n\n[1] \"hms\"      \"difftime\"\n\n\nJust to show that there really isn’t anything fancy going on, let’s strip the class attribute away and let R print out the raw object. As the output here shows, an hms object has the same underlying structure as a regular difftime object:\n\nunclass(hms_time)\n\n[1] 50725\nattr(,\"units\")\n[1] \"secs\"\n\n\n\n\n\n[Arrow] The time32 and time64 types\nWhat about Arrow?\nAt a technical level, it would be perfectly possible to translate an hms object in R into an Arrow duration object, but that feels slightly unnatural. The entire reason why the hms class exists in R is that we – the human users – attach special meaning to the “duration of time that has elapsed since midnight on an arbitrary day”. We call it the time of day, and while technically it is possible to represent the time of day as a duration (or an hms as a difftime), human beings like to treat special things as special for a reason.\nBecause of this, Arrow supplies two data types that are roughly analogous to the hms class in R, called time32 and time64. The time32 type stores the time of day as a signed 32-bit integer, which represents the number of seconds (or alternatively, milliseconds) since midnight. By default, the arrow package will translate an hms object to a time32 type, using seconds as the unit:\n\nhms_time32_s <- scalar(hms_time)\nhms_time32_s\n\nScalar\n14:05:25\n\n\nAs usual, let’s just verify that the encoding unit is as expected:\n\nhms_time32_s$type\n\nTime32\ntime32[s]\n\n\nYep, we’re all good! To switch to milliseconds, I would use a command like this:\n\nhms_time32_ms <- scalar(hms_time, time32(unit = \"ms\"))\nhms_time32_ms\n\nScalar\n14:05:25.000\n\n\nNotice that the output shows the time in a little more resolution. I find this a helpful touch, since it provides a visual cue letting me know what the unit is. But just to confirm, let’s inspect the type explicitly:\n\nhms_time32_ms$type\n\nTime32\ntime32[ms]\n\n\nIf you need to represent the time of day at a higher degree of precision, you’ll want to use the time64 type, which (shockingly!) represents the time of day as a signed 64-bit integer. When using the time64 class you can choose microseconds (unit = \"us\") or nanoseconds (the default, unit = \"ns\") as the unit:\n\nhms_time64_us <- scalar(hms_time, time64())\nhms_time64_us\n\nScalar\n14:05:25.000000000\n\n\nThe display is showing more trailing zeros, so you can already be sure that the encoding unit has changed. So the only real question you might have pertains to the author. Will she be tediously predictable and check the data type yet again to verify that the encoding unit is nanoseconds, just like she has done every time before? Yes. Yes she will:\n\nhms_time64_us$type\n\nTime64\ntime64[ns]\n\n\nShe is quite tiresome at times.\nIn essence, the arrow defaults are set up such that if you choose time32() when going from R to Arrow without specifying a unit, you will end up with the lowest precision representation of time (rounded to the nearest second), whereas if you do the same with time64() you end up with the highest precision (nanosecond level) representation. When going the other way, arrow will map time32 types and time64 types to hms objects, and the end result is that the time of day will be stored as a double.\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#other-types",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#other-types",
    "title": "Data types in Arrow and R",
    "section": "Other types",
    "text": "Other types\nAs with any story told by humans, this one is incomplete. When I started writing this post I had the ambition to cover every single line in the Table of R/Arrow mappings shown in the arrow documentation. I didn’t quite get there in the end, and there are a few missing cases. I’ll briefly mention them here:\n\nArrow possesses a “null” value used to represent missing data, and behaves similarly to NA in R. In base R there are several different NA values, corresponding to the different atomic types: NA_logical, NA_character. The way this is handled in arrow is to rely on the vctrs package. Specifically, in vctrs there is a vctrs_unspecified class that works very well here, so Arrow nulls map to vctrs_unspecified and vice versa. In practice, this is where NA values enter into the picture.\nIn R there is the concept of the raw type used to represent bytes. Arrow doesn’t have a natural equivalent of this, but the closest is an unsigned 8-bit integer, so the default is to map raw to uint8.\nI haven’t talked about factors at all, and frankly I probably should have. My only excuse is exhaustion. The post was getting very long and I ran out of energy. The analog of factors in Arrow is the dictionary type. They’re not exact mirrors of each other so it’s worth reading the documentation, but it’s close enough that factors are mapped to dictionaries and vice versa.\nR and Arrow both allow more complicated data structures to be included as columns within a data frame (or Table). For example, in R each element of a column can itself be a data frame. In such cases, the default in arrow is to map each R data frame onto an Arrow struct. Again, this is one where it’s worth reading the documentation, because there are some subtleties with how things like list columns are handled.\n\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#the-magic-goes-away",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#the-magic-goes-away",
    "title": "Data types in Arrow and R",
    "section": "The magic goes away",
    "text": "The magic goes away\n\n“Being in love ruins my judgement. It takes my mind off what I’m doing, and I ruin spells”     – Mirandee, from The Magic Goes Away by Larry Niven\n\nWhen I first started using arrow, it was the magic I loved most. Everything just worked. I could move data between R and Arrow without having to think, I could manipulate enormous data sets using dplyr syntax that I’d never even be able to load into R, and I never had to look under the hood. Magic is always compelling. It is delightful. It makes the user feel joy, and it’s the experience the developer wants to provide.\nBut as any teacher will tell you, the magic always goes away. There comes a time when you have to sit down and read the manuals. You have to understand how the magic works, and when you do understand you realise there is no magic. At best, there is design. A system can work using all the default settings because it has been thoughtfully designed, but you will eventually encounter situations when the defaults don’t apply to you. It’s taken me some time to piece all this together, and at the end of the process I feel a lot more confident in my judgment. Having a deeper understanding of data types in Arrow and R is useful to me, even if I’m only using the default schemas.\nI hope the post is helpful for anyone else following a similar path.33"
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html",
    "href": "posts/2023-01-10_kubernetes/index.html",
    "title": "Deploying R with kubernetes",
    "section": "",
    "text": "Edited to add: Okay yes the demo app in this post isn’t working right now, but that’s not because the code doesn’t work, it’s because I haven’t set up payments on google cloud account yet so the cluster isn’t live! (March 15, 2023)\nStory time. There was a very weird moment in machine learning history, about 20 years ago, when the probabilistic AI folks were completely obsessed with Bayesian nonparametrics, and a disproportionate number of papers at NeurIPS had titles like “[Cutesy prefix]: An infinite dimensional model of [something really boring]”. In most cases, you’d dig into the paper and discover that they hadn’t done anything very special. All they’d done is implement a Bayesian model of [boring thing] that was ambiguous about the number of [components], and instead of thinking about what prior constraints make sense for the problem they were trying to solve, the authors used a Chinese restaurant process (CRP) to specify the conditional prior distribution over allocations of observations to components. The CRP has the mildly-interesting property that for any finite sample size there is a non-negligible conditional probability that the next observation belongs to a hitherto unobserved component, and asymptotically the partitions over observations it generates have a countably infinite number of components. Alas, exactly zero of these papers happened to have an infinitely large data set to train the model on, and without fail the results in the papers didn’t appear to have anything “infinite dimensional” about them whatsoever.\nI say this with love and gentleness, dear reader, because I wrote quite a few of those papers myself.\nWhy do I tell this story in a blog post that has absolutely nothing to do with machine learning, statistics, or Bayesian inference? Because in a fit of pique, somewhere around 2006, I decided to do the damned reading myself and learned quite a lot of Bayesian nonparametrics. Not because I thought it would be useful, but because I was curious and I was getting extremely irritated at overconfident machine learning boys telling me that as a mere psychologist I couldn’t possibly understand the depth of their thinking.\nWhich brings me, naturally enough, to kubernetes."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#ggplot2-on-kubernetes",
    "href": "posts/2023-01-10_kubernetes/index.html#ggplot2-on-kubernetes",
    "title": "Deploying R with kubernetes",
    "section": "ggplot2 on kubernetes",
    "text": "ggplot2 on kubernetes\nLet’s start at the ending, shall we? The art shown below is generated at donut.djnavarro.net, and it is more-or-less unique. The site is designed to serve a different image every time it is accessed, using the timestamp as the seed to a generative art system written in R with ggplot2. If you refresh this page, the artwork will change:\n\n\nUnder the hood, the site is a kubernetes app running containerised R code with google kubernetes engine. Sounds fancy, right?\nWell, maybe. Shall we take a look at how it works? Perhaps, like so many other things in this world, it will turn out not to be anywhere near as complicated as it is made out to be."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#um.-what-is-kubernetes-do-i-care",
    "href": "posts/2023-01-10_kubernetes/index.html#um.-what-is-kubernetes-do-i-care",
    "title": "Deploying R with kubernetes",
    "section": "Um. What is kubernetes? Do I care?",
    "text": "Um. What is kubernetes? Do I care?\nThere’s nothing I love more than looking at the website for a software tool and trying to work out what it does by reading how the developers have chosen to describe it. On the kubernetes website they’ve gone with the headline “Production-Grade Container Orchestration”, and started with this:\n\nKubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.\n\nAs opening lines go it wouldn’t get you a lot of attention on grindr1 but context makes a difference and it’s not so terrible as a description of what kubernetes does. It’s a useful tool if you need to deploy an application on a cluster2 and have that application run smoothly as you “scale” your cluster by adding more “nodes”3 to the cluster. For the application I’m about to write, kubernetes is overkill. I don’t actually need kubernetes to run something this simple, but this is a learning exercise. I’m doing it so that I can familiarise myself with core concepts. When I get to the part of the post that actually does something with kubernetes I’ll start introducing terminology, but for now that’s enough for us.\nShould you care as an R user? I mean, probably not. If you want a proper answer, Roel Hogervorst has an excellent blog post called “WTF is Kubernetes and Should I Care as R User?” I won’t duplicate content here: you should read the original post. But the short answer is that you probably won’t need to run your own application using kubernetes, but you might need to contribute code to a larger application that uses it. If so, you may want to play around with kubernetes to make sure you understand what it does."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#write-the-r-code",
    "href": "posts/2023-01-10_kubernetes/index.html#write-the-r-code",
    "title": "Deploying R with kubernetes",
    "section": "Write the R code",
    "text": "Write the R code\n\nLikes to watch me in the glass room, bathroom  Chateau Marmont, slippin’ on my red dress, puttin’ on my makeup  Glass room, perfume, cognac, lilac fumes  Says it feels like heaven to him    – Lana Del Rey4\n\nLet’s begin at the beginning. Anytime you want to write something, it helps to have something to say. There is nothing more tiresome than an op-ed writer trying to fill out 1000 words to make the Saturday paper deadline, but tech writing that tries to demonstrate a tool5 without anything even remotely resembling an application runs a very close second. So let’s at least pretend we have a use case for this yeah?\nIn real life I am an unemployed 40-something woman who smokes and drinks too much and makes very poor choices around men, but in my spare moments I make generative art using R. It’s an extremely unprofitable hobby6 but it’s not completely without market value. Among other things the lovely folks at Posit were kind enough to pay me to put together an “art from code” workshop last year, and thanks to their kindness and my weird priorities there is now a nice little online tutorial that you can use to learn how to make generative art in R. What I’m going to do in this post is build a little kubernetes app that creates generative in R. It won’t be very fancy, but hopefully you can see how an app like this could be expanded7 to create a platform for “long form generative art” with R, not dissimilar to what artblocks or fxhash allow generative artists to do with javascript. The artist supplies code (in this case using R) that generates artwork, and the server uses that code to generate an arbitrary number of pieces that… idk, I guess you could sell them? Whatever. Do I look like a capitalist to you?\nTo build something like this we’ll need some R code that creates generative art. I won’t try to make anything too fancy here. In fact, I’ll reuse code for the “donuts” system I used in my multi-threaded task queues post. It’s a good choice for this application because the donuts system is something that is extremely easy to implement in R because the ggplot2 package provides tooling for creating data visualisations that use polar geometry8\nHere’s how you build the system. I won’t go into detail because this system is a very minor variation on this one in my art-from-code tutorial, but here’s the gist. First, the plot is going to need a colour scheme, so we’ll define a function that samples a palette randomly with the assistance of the ggthemes package:\n\n\n\nserver.R\n\nsample_canva <- function(seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]]\n}\n\n\nNext, we’ll have a function that generates a table full of random numbers that we will later on map onto various plot aesthetics to make a pretty picture:\n\n\n\nserver.R\n\nsample_data <- function(seed = NULL, n = 100){\n  if(!is.null(seed)) set.seed(seed)\n  dat <- tibble::tibble(\n    x0 = stats::runif(n),\n    y0 = stats::runif(n),\n    x1 = x0 + stats::runif(n, min = -.2, max = .2),\n    y1 = y0 + stats::runif(n, min = -.2, max = .2),\n    shade = stats::runif(n),\n    size = stats::runif(n),\n    shape = factor(sample(0:22, size = n, replace = TRUE))\n  )\n}\n\n\nNow comes the part of the system that does most of the artistic work, by defining a visual layout for any plots that are created using the system:\n\n\n\nserver.R\n\ndonut_style <- function(data = NULL, palette) {\n  ggplot2::ggplot(\n    data = data,\n    mapping = ggplot2::aes(\n      x = x0,\n      y = y0,\n      xend = x1,\n      yend = y1,\n      colour = shade,\n      linewidth = size\n    )) +\n    ggplot2::coord_polar(clip = \"off\") +\n    ggplot2::scale_y_continuous(\n      expand = c(0, 0),\n      limits = c(-1, 1),\n      oob = scales::oob_keep\n    ) +\n    ggplot2::scale_x_continuous(\n      expand = c(0, 0),\n      limits = c(0, 1),\n      oob = scales::oob_keep\n    ) +\n    ggplot2::scale_colour_gradientn(colours = palette) +\n    ggplot2::scale_linewidth(range = c(0, 6)) +\n    ggplot2::theme_void() +\n    ggplot2::theme(\n      panel.background = ggplot2::element_rect(\n        fill = palette[1], colour = palette[1]\n      )\n    ) +\n    ggplot2::guides(\n      colour = ggplot2::guide_none(),\n      linewidth = ggplot2::guide_none(),\n      fill = ggplot2::guide_none(),\n      shape = ggplot2::guide_none()\n    )\n}\n\n\nThe last step is a function that puts it all together. The donut() function takes a single integer-valued input and returns a plot object that happens to look slightly pretty:\n\n\n\nserver.R\n\ndonut <- function(seed) {\n\n  dat <- sample_data(n = 200, seed = seed) |>\n    dplyr::mutate(y1 = y0, size = size / 3)\n\n  line_spec <- sample(c(\"331311\", \"11\", \"111115\"), 1)\n\n  pic <- donut_style(palette = sample_canva(seed = seed)) +\n    ggplot2::geom_segment(data = dat, linetype = line_spec)\n\n  if(stats::runif(1) < .5) {\n    pic <- pic +\n      ggplot2::geom_segment(\n        data = dat |> dplyr::mutate(y1 = y1 - .2, y0 = y0 - .2),\n        linetype = line_spec\n      )\n  }\n  if(stats::runif(1) < .5) {\n    pic <- pic +\n      ggplot2::geom_segment(\n        data = dat |> dplyr::mutate(y1 = y1 - .4, y0 = y0 - .4),\n        linetype = line_spec\n      )\n  }\n\n  pic\n}\n\n\nHere it is in action:\n\nfor(seed in 1:6) plot(donut(seed))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot my finest work, but still pretty enough to be fun."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#expose-an-api",
    "href": "posts/2023-01-10_kubernetes/index.html#expose-an-api",
    "title": "Deploying R with kubernetes",
    "section": "Expose an API",
    "text": "Expose an API\n\nAre you posting hole on main again?    – Everyone who knows me, eventually\n\nThe next step in the process is to define a public API that specifies how visitors to the website can interact with the underlying R code. That’s not something we typically do with R code because we aren’t usually in the business of writing web applications in R, but thanks to endless joy that is the plumber this task can be accomplished with a few lines of code decoration:\n\n\n\nserver.R\n\n#* draws a donut plot\n#* @serializer svg list(width = 10, height = 10)\n#* @get /\nfunction(seed = NA) {\n  if(is.na(seed)) {\n    seed <- as.integer(Sys.time())\n  }\n  print(donut(seed))\n}\n\n\nThere are a few things to note here:\n\nThe code decoration on line 93 specifies that the function defined in lines 94-99 will be called whenever an HTML GET request is sent to the / endpoint. Or, in simpler language, whenever someone visits the main page for the website that eventually ended up being hosted at donut.djnavarro.net.\nThe code decoration on line 92 how the output from the R function (an in-memory data structure) will be serialised (to a binary stream) and transmitted to the user.9 In this case, the output is a plot object that would normally be handled by the R graphics device. What I’ve used plumber to do here, is have this output converted to an svg file. It’s that svg file that the website will serve to the user.\nFinally, notice that the function does take a seed argument.10 I’ve set NA as the default value rather than the more conventional NULL because plumber won’t accept a NULL default in this context.\n\nNoting that all the code I’ve presented so far belongs to a file called server.R (the link goes to the github repo for this “donut” side-project), I can start the web server running locally on port 3456 like this:\n\nplumber::plumb(file=\"server.R\")$run(port = 3456)"
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#containerise-it",
    "href": "posts/2023-01-10_kubernetes/index.html#containerise-it",
    "title": "Deploying R with kubernetes",
    "section": "Containerise it",
    "text": "Containerise it\nAt this point in the process I have a perfectly functional webserver… that only runs on my machine which just happens to have the dependencies installed, and is only accessible locally from that machine. We’ll need to fix both of those problems.\nLet’s start by fixing the first one by running the website from within a docker container. Under normal circumstances I’d walk you through that process, but since I wrote a long blog post about docker just the other day, I’ll jump straight to showing you the Dockerfile:\n\n\n\n\nDockerfile\n\nFROM rocker/r-ver:4.2.2\n\nLABEL org.opencontainers.image.source \"https://github.com/djnavarro/donut\"\nLABEL org.opencontainers.image.authors \"Danielle Navarro <djnavarro@protonmail.com>\"\nLABEL org.opencontainers.image.description DESCRIPTION\nLABEL org.opencontainers.image.licenses \"MIT\"\n\nRUN Rscript -e 'install.packages(c(\"ggplot2\", \"scales\", \"tibble\", \"dplyr\", \"plumber\", \"ggthemes\"))'\nCOPY server.R /home/server.R\nEXPOSE 80\nCMD Rscript -e 'plumber::plumb(file=\"/home/server.R\")$run(host=\"0.0.0.0\", port = 80)'\n\n\n\nEvery instruction in this dockerfile is something I covered in the last post, except for the EXPOSE instruction on line 10. That one tells the container to listen on port 80. It doesn’t necessarily publish the output anywhere accessible from outside the container11 but it does mean that the plumber web server running inside the container is listening on port 80 and can create a response when it receives a request. I’ll deal with the publishing issue later."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#push-it-to-the-registry",
    "href": "posts/2023-01-10_kubernetes/index.html#push-it-to-the-registry",
    "title": "Deploying R with kubernetes",
    "section": "Push it to the registry",
    "text": "Push it to the registry\nThe next step in the process is to host the image created from this dockerfile on a public registry.12 I talked about that process in the last post too, so again I’ll keep things simple. The process I followed for the donut project is essentially identical to the one I used in this section of the docker post. I’ve created a github actions workflow that automatically builds the image on github and hosts it with the github container registry. The resulting image name is ghcr.io/djnavarro/donut:main and here’s the build-image.yaml workflow I’m using:\n\n\n\n\n.github/workflows/build-image.yaml\n\nname: publish donut image\n\non:\n  push:\n    branches: ['main']\n\nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ${{ github.repository }}\n\njobs:\n  build-and-push-image:\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        include:\n          - dockerfile: ./Dockerfile\n            image: ghcr.io/djnavarro/donut\n\n    permissions:\n      contents: read\n      packages: write\n\n    steps:\n      - name: checkout repository\n        uses: actions/checkout@v2\n\n      - name: login to the container registry\n        uses: docker/login-action@f054a8b539a109f9f41c372932f1ae047eff08c9\n        with:\n          registry: ${{ env.REGISTRY }}\n          username: ${{ github.actor }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: extract metadata (tags, labels) for docker\n        id: meta\n        uses: docker/metadata-action@98669ae865ea3cffbcbaa878cf57c20bbf1c6c38\n        with:\n          images: ${{ matrix.image }}\n\n      - name: build and push docker image\n        uses: docker/build-push-action@ad44023a93711e3deb337508980b4b5e9bcdc5dc\n        with:\n          context: .\n          file: ${{ matrix.dockerfile }}\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n\n\n\nAt long, long last we have all the precursors in place. We have a little web application that runs inside a docker container, and the image describing that container is hosted on a registry.13 We can get started on the kubernetes side of things…"
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#create-a-kubernetes-cluster",
    "href": "posts/2023-01-10_kubernetes/index.html#create-a-kubernetes-cluster",
    "title": "Deploying R with kubernetes",
    "section": "Create a kubernetes cluster",
    "text": "Create a kubernetes cluster\nAt the risk of stating the bloody obvious, if you want to use kubernetes to deploy an application on a cluster… you’re probably going to need a cluster running kubernetes. You can get yourself one of these in lots of different ways but the way I’m going to do it is with GKE, the google kubernetes engine. You’ll need a google account to do this, and yes this is something that they charge actual money for, but the good news is that when you sign up for google cloud services you get a few hundred dollars of credit to start with. That’s pretty useful for novices: it’s nice to be able to play around and learn the basics before you have to start worrying about what it’s going to cost.\nIf you go down that path you can access your projects from the cloud console, located at console.cloud.google.com. Once there you can navigate to the various pages you’ll need by clicking on links and menu items, but google offers a lot of different cloud services and it does take a little while for the interface to start feeling familiar, so I’ll link to the pages you need directly as well.\nBefore you can create a cluster of your very own, you need to create a project. Pretty much everything you do with google cloud services is organised into projects so that’s where we’ll start. To create a project, go to console.cloud.google.com/projectcreate and follow the prompts. Give your project a fancy name that makes you sound cool: I called mine donut-art.\nNow that you have a project, you’ll need to enable the specific google cloud services that your project will need access to. In this example the only thing I’ll need is GKE itself, but in other situations you might need access to google cloud storage or something like that. To enable GKE on your current project go to console.cloud.google.com/kubernetes/. If it hasn’t already been enabled for the project the page will ask if you want to. Even more conveniently, if you don’t have a cluster running it will ask if you want to create one.14 It will give you two options: an “autopilot” cluster is one where google will automatically manage the configuration for you, whereas for a “standard” cluster you’ll have to be more explicit about how many nodes you want and how they are organised. There are situations where you need to use the standard cluster,15 but this is not one of those so I went with the autopilot approach because it’s simpler. I didn’t need to change any of the defaults: I called my cluster donut-cluster, and created it in the region australia-southeast1.16\nHere’s a screenshot showing you what the relevant bit of the google kubernetes engine console looks like for me now that I have a cluster up and running:\n\nIf I click on the “donut-cluster” link it takes me to a page with a lot more detail, but you can see that some of the information is the same:"
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#command-line-tools",
    "href": "posts/2023-01-10_kubernetes/index.html#command-line-tools",
    "title": "Deploying R with kubernetes",
    "section": "Command line tools",
    "text": "Command line tools\nTime for a little digression.\nTake a look at the menu shown in the last screenshot. If I click on the “connect” button and it will reveal a command I can use to connect to the cluster from the terminal on my laptop… but it requires me to have the gcloud command line tools installed. Now, if you don’t want to install the tools locally you can avoid this by selecting the “run in cloud shell” option that also appears on the dialog box. However, I dislike the cloud shell and prefer to work from my own terminal. So, the next step is to install the command line tools. For this project, the two things I need are gcloud (to interact with google cloud services) and kubectl (to interact with kubernetes).\n\nInstalling gcloud\nIt turns out that installing the command line tools is relatively straightforward, and is made a lot easier thanks to the gcloud installation instructions which are detailed and not too hard to follow. In addition to the basic tools, I installed the “gke-gcloud-auth-plugin” which are needed for authentication. Once the command line tools are installed, authentication from your terminal is a one-line command:\n\ngcloud auth login\n\nI can now interact with my google cloud projects from my command line.\n\n\nInstalling kubectl\nThe second tool I need for this project is kubectl, a command line tool used to control a kubernetes cluster. You can find installation instructions for different operating systems by visiting the kubernetes install tools page. I’m doing this from a linux machine, so I also found it useful to enable autocompletion of kubectl commands within the bash shell. The instructions for this are included in the kubectl install page for linux.\n\n\nConnect to the cluster\nNow that I have gcloud and kubectl running, I can connect to my cluster. The first thing to do is use gcloud to get the credentials needed to connect to my cluster:\n\n  export USE_GKE_GCLOUD_AUTH_PLUGIN=True\n  gcloud container clusters get-credentials donut-cluster \\\n    --zone australia-southeast1 \\\n    --project donut-art\n\nFetching cluster endpoint and auth data.\nkubeconfig entry generated for donut-cluster.\nThen I can use kubectl to verify that it can connect to my cluster:\n\nkubectl cluster-info\n\nKubernetes control plane is running at blah blah blah\nGLBCDefaultBackend is running at blah blah blah\nKubeDNS is running at blah blah blah\nKubeDNSUpstream is running at blah blah blah\nMetrics-server is running at blah blah blah\nOkay, I may have edited the output slightly. The missing bits are the various URLs. They aren’t very interesting… the main thing to notice is that yes, kubectl can connect to my cluster and the cluster is up and running."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#kubernetes-terminology",
    "href": "posts/2023-01-10_kubernetes/index.html#kubernetes-terminology",
    "title": "Deploying R with kubernetes",
    "section": "Kubernetes terminology",
    "text": "Kubernetes terminology\nNot surprisingly, kubernetes has a lot of terminology. That’s quite daunting when you’re getting started and I’m not going to attempt a complete glossary here. Instead, let’s start with these four terms, since we’ll use them a lot:\n\nContainer. This has the same meaning it has in other contexts: a container is a self-contained executable that bundles up dependencies and runs isolated from other processes on the machine. Kubernetes supports other types of containers besides docker containers, but let’s pretend we’re only talking about docker here.\nPod. A pod is the smallest deployable unit you can create: it is an abstraction that refers to one or more containers working together. An application can have many pods running on many machines (nodes) but each pod runs on one machine. Pods are considered ephemeral. Kubernetes will have no qualms about shutting down a pod if it doesn’t seem to be doing its job, or creating new pods to replace it if it needs to,\n\nDeployment. A deployment is an abstraction that specifies a collection of pods that your application runs. Essentially it describes your “desired state” for the application. When you “apply” a deployment kubernetes will start the application running (more or less), and try to make the thing that’s actually running look like your stated deployment configuration.\nService. A service is an abstraction that specifies how the pods running on your kubernetes cluster communicate with the outside world. They’re awfully handy things to have if you want your application to be accessible on the web.\n\nConceptually, it’s also helpful to know these terms at the very beginning, even though frankly I’m not going to do anything with them here:\n\nNode. A node refers one of the machines running in your cluster.\nControl Plane. The control plane refers to a collection of processes that run together on a single node and are in charge of actually running the whole thing. We won’t need to do anything to the control plane in this post other than leave it alone and let it do its job, but it does help to know the term because it shows up everywhere.\n\nMore terms will appears as we go along – and I’ll try to explain all those when they pop up – but these are the ones that I wish I’d understood properly before I started trying to play with kubernetes clusters."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#create-a-deployment",
    "href": "posts/2023-01-10_kubernetes/index.html#create-a-deployment",
    "title": "Deploying R with kubernetes",
    "section": "Create a deployment",
    "text": "Create a deployment\nThe way to configure your kubernetes cluster is with manifest files that are written in yaml format and use the kubectl apply command to update your cluster using the instructions laid out in the manifest file. You can use a manifest to modify any aspect to your cluster configuration, and later in this post I’ll show a few more manifests, but for now here’s the deployment.yaml file I’m using to specify a deployment for the donuts application:\n\n\n\ndeployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: donut-example\n  name: donut\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: donut-example\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: donut-example\n    spec:\n      containers:\n        - name: donut\n          image: ghcr.io/djnavarro/donut:main\n          imagePullPolicy: Always\n          resources:\n            requests:\n              memory: \"64Mi\"\n              cpu: \"250m\"\n            limits:\n              memory: \"128Mi\"\n              cpu: \"500m\"\n          ports:\n            - containerPort: 80\n\n\nFor the moment, let’s censor the metadata and everything that uses the metadata so that we can focus on what the rest of the manifest is doing:\n\n\n\ndeployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata: \n  [some metadata]\nspec:\n  replicas: 2\n  selector:\n    [use some metadata]\n  template:\n    metadata:\n      [some metadata]\n    spec:\n      containers:\n        - name: [names are metadata]\n          image: ghcr.io/djnavarro/donut:main\n          imagePullPolicy: Always\n          resources:\n            requests:\n              memory: \"64Mi\"\n              cpu: \"250m\"\n            limits:\n              memory: \"128Mi\"\n              cpu: \"500m\"\n          ports:\n            - containerPort: 80\n\n\nThere’s a lot going on here, so for the moment let’s focus on the bottom.17 The section of the code from lines 13-25 is used to specify the docker containers18 that my cluster is going to deploy. There’s only one container listed in this section, and the line that reads image: ghcr.io/djnavarro/donut:main is the way that I’ve specified the docker image to use when creating the container. There are other settings I’ve used to set up this container: I’ve asked kubernetes to allocate memory and cpu resources to the container, and I’ve exposed container port 80 (which, if you can remember back that far, is where the plumber web API is running inside the container). I’ve also set imagePullPolicy: Always to ensure that every time I update this deployment kubernetes will pull the image from the registry afresh. I did that because more often than not while I was writing code for the kubernetes deployment I was tweaking the image too, and I wanted to make sure that I was always trying to deploy the most recent version of the image.\nOkay, now that we know what’s going on in that section of the code, let’s collapse that part and think about the manifest file like this:\n\n\n\ndeployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata: \n  [some metadata]\nspec:\n  replicas: 2\n  selector:\n    [use some metadata]\n  template:\n    metadata:\n      [some metadata]\n    spec:\n      [details of one or more containers]\n\n\nOkay, so now let’s think about the bottom part of this code too. Lines 9-13 of this condensed pseudo-manifest describe some kind of template. But a template for what? Well, as clearly stated on line 13 in human(ish) language, it’s a template for “one or more containers”. In kubernetes terminology, a deployable thing that holds one or more containers is a pod… so this section of the code is describing a pod template. It’s an instruction to kubernetes that says… “hey, when you create a pod as part of this deployment, here’s the template you should use”. So we can simplify again:\n\n\n\ndeployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata: \n  [some metadata]\nspec:\n  replicas: 2\n  selector:\n    [use some metadata]\n  template:\n    [details of the pod template]\n\n\nIn this condensed form we can see that lines 5-10 provide a specification of the deployment itself. I’ve given it a pod template that tells kubernetes what the pods should look like, and I’ve specified the number of “replicas”. How many copies of this pod do I want it to run in this deployment: for no good reason at all I decided to run two (i.e., two replicas).\nIf we simplify the manifest yet again, we can see the top-level description:\n\n\n\ndeployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata: \n  [some metadata]\nspec:\n  [details of the deployment]\n\n\nThe only other things to point out right now is that the “kind” field is used to tell kubernetes what type of object to create (a Deployment), and the “apiVersion” field is used to specify which version of the kubernetes API to use when interpreting the manifest. That’s handy to note because later on I’ll be using APIs that are a bit more specific to the google kubernetes engine.\nOkay, so now that we have some sense of what’s going on in the deployment.yaml file (ignoring the fact that I’ve glossed over the metadata bits), let’s actually apply it to our cluster:19\n\nkubectl apply -f deployment.yaml\n\nTo see if it’s working we can use kubectl get deployments:\n\nkubectl get deployments\n\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\ndonut   2/2     2            2           19h\nIt is alive.\nFor more details on this part of the process, check out the kubernetes documentation on deploying a stateless application. You may also want to look at the page on managing container resources at this point."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#expose-the-deployment-no-https",
    "href": "posts/2023-01-10_kubernetes/index.html#expose-the-deployment-no-https",
    "title": "Deploying R with kubernetes",
    "section": "Expose the deployment (no https)",
    "text": "Expose the deployment (no https)\nAt this point my little donut application is running happily on the cluster, but it doesn’t have a public IP address. No-one can visit it. To expose the deployment to the world you’ll need to start a service running that takes care of this for you. Exactly how you go about doing this depends on whether you want to enable https on the website. If you’re not too fussed about https the process is fairly simple and you can find details on how to do it by reading the tutorial on exposing an external IP address, and you may find the kubernetes documentation on services helpful too. The TL;DR is that it’s simple enough that you don’t even need to bother with a manifest file:\n\nkubectl expose deployment donut --type=LoadBalancer --name=donut-service\n\nThe application is now online. It has a public IP address that you can find, and if you own a domain that you want to map to that IP address all you have to do is create a DNS record that points the URL at the appropriate IP. With any luck your domain provider will have some decent documentation for this. For instance, I use google domains for djnavarro.net domain, and they have some pretty decent instructions on configuring DNS records that I could use to point donut.djnavarro.net at the IP address for my kubernetes application.\nUnfortunately for me, I am a masochist, and as such I chose the option that delivers pain."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#expose-the-deployment-with-https",
    "href": "posts/2023-01-10_kubernetes/index.html#expose-the-deployment-with-https",
    "title": "Deploying R with kubernetes",
    "section": "Expose the deployment (with https)",
    "text": "Expose the deployment (with https)\n\nI like the kick in the face  And the things you do to me  I love the way that it hurts  I don’t miss you, I miss the misery    – Halestorm\n\nConfiguring the kubernetes application to use https is a bit of a pain in the ass.20 I’m deploying all this through google kubernetes engine, so the approach I took was to follow the guide for using google-managed ssl certificates. The guide is excellent… apart from a couple of minor… issues… that really fucked me21 when I tried to follow it. I’ll mention those as I go.\n\nGet a static IP\nThe first thing you have to do is create a static IP address that you’ll later use for your cluster. The gcloud compute addresses create command does that for you. Here’s how I did that for my cluster:\n\ngcloud compute addresses create donut-ip-address \\\n  --global \\\n  --project donut-art\n\n\n\n\n\n\nThis prints out a very boring message that informs you that the ID address has been created. More helpfully, now that the IP address exists you can ask google to tell you what it is:\n\ngcloud compute addresses describe donut-ip-address \\\n  --global \\\n  --project donut-art\n\nWhen you do this, the output prints out the IP address and some other details. Later on, this is the address you’ll create a DNS record for so that – in my case – the https://donut.djnavarro.net/ address points to the correct location.\n\n\nGet a certificate\nThe next step in the process is to create a managed certificate. Somebody needs to certify that my website is what it says it is.22 I’m going to need a manifest file for this, which I’ve saved as managed-cert.yaml:\n\n\n\nmanaged-cert.yaml\n\napiVersion: networking.gke.io/v1\nkind: ManagedCertificate\nmetadata:\n  name: managed-cert\nspec:\n  domains:\n    - donut.djnavarro.net\n\n\nNotice that the apiVersion field here is using something specific to GKE: I’m using google infrastructure here and they’ve kindly23 provided an API that makes it easy to use their managed certificates. The yaml here is pretty simple: I’m asking google to supply me with a certificate for my kubernetes application, which will be valid for the domain donut.djnavarro.net (you can list more than one here but I didn’t).\nNow that I have a manifest, I apply it to my cluster in the usual way:\n\nkubectl apply -f managed-cert.yaml\n\nOkay, at this point in the guide it warns you that it might take an hour or so for the certificate to be provisioned. I manage so many websites now that I’d stopped paying attention to this warning because like, 90% of the time, the thing actually happens in 20 seconds. Yeah nah, not this time babe. This one actually took an hour. We’ll come back to it. I mean, if you want to check you can try this command:\n\nkubectl describe managedcertificate managed-cert\n\nIt will print out a bunch of stuff, but when you scroll through the output you’ll very likely come across something that says that the certificate is “Provisioning”. It did work for me but it took a while so let’s move on while that is happening.\n\n\nCreate a service\nNext up is the step that fucked me in the worst possible way. It failed, because I did a copy-paste on a bit of code that I needed to edit. I did that because the guide on the google website doesn’t flag this as something you need to edit. Worse yet, it failed silently because kubernetes had no bloody way to know my manifest was fucked up. Worst of all, for at least three hours I was convinced that my error was in the later step because this step failed silently.\nSiiiiiiiiiiiiiigh. Computers were a mistake.\nAnyway, let’s start by looking at the manifest file, which I’ve called mc-service.yaml:\n\n\n\nmc-service.yaml\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: mc-service\nspec:\n  selector:\n    app.kubernetes.io/name: donut-example\n  type: NodePort\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n\n\nAt this point in the process I freely admit I’m at the edge of my own knowledge, and I don’t want to say too much about something I only barely understand myself, but there are two things I will point out about this:\n\nNotice that under spec.selector (lines 5-7) I’m referring to the name of my deployment (donut-example). That’s what the kubernetes docs tell you to do when setting up a NodePort service (see here), but the guide on the the corresponding google page that I linked to earlier (i.e., this one) misled me. It made me think I was supposed to use the name of the service (mc-service). You want this service to point at your deployment!\nNotice I’m doing everything on port 80? You probably don’t need to do this, but I found some discussion online about an old issue with kubernetes where they were hardcoding port 80 somewhere. I’m pretty certain that’s been properly resolved now and I don’t need to use port 80 for everything, but it was one of the tweaks I made on the way to figuring out the problem with spec.selector and… well… fuck it. The current version works and I’m new to kubernetes so I’m not changing it today.\n\nIn any case, now that I have a manifest file I can apply it to the cluster:\n\nkubectl apply -f mc-service.yaml\n\n\n\nCreate the DNS record\nThe next step in the process was to create a DNS record (with google domains in my case) for donut.djnavarro.net that points this subdomain to the appropriate IP address. I talked about this earlier in the post, so let’s move on…\n\n\nCreate an ingress\nIf the gods were kind we would be done, but of course we are not. I have a managed certificate and I have a service that exposes my deployment. That doesn’t mean that my application is configured to serve pages over https. To do this I need to create an ingress that manages external access to the service I created earlier and handles the SSL bit. Which is the thing I really need in order to make https work. Again…\nSiiiiiiiiiiiigh.\nOkay, here’s my mc-ingress.yaml manifest file for that:\n\n\n\nmc-ingress.yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: mc-ingress\n  annotations:\n    kubernetes.io/ingress.global-static-ip-name: donut-ip-address\n    networking.gke.io/managed-certificates: managed-cert\n    kubernetes.io/ingress.class: \"gce\"\nspec:\n  defaultBackend:\n    service:\n      name: mc-service\n      port:\n        number: 80\n\n\nThis manifest uses the static IP address I created (donut-ip-address), as well as the TLS certificate that I’ve asked google to provide me (managed-cert), and it specifies the mc-service I created as the backend. These things together give me https… apparently.\nAs usual, I apply the manifest to my cluster:\n\nkubectl apply -f mc-ingress.yaml \n\nI can inspect the results with kubectl get ingress:\n\nkubectl get ingress\n\nNAME         CLASS    HOSTS   ADDRESS         PORTS   AGE\nmc-ingress   <none>   *       34.149.195.33   80      98s\nI might still have to wait for the certificate provisioning to finish, so I’d better check again:\n\nkubectl describe managedcertificate managed-cert\n\nHere’s the relevant bit of the output showing what it looks like once it’s all working:\nSpec:\n  Domains:\n    donut.djnavarro.net\nStatus:\n  Certificate Name:    mcrt-b2204ff4-ad92-4811-a56d-f007190bb659\n  Certificate Status:  Active\n  Domain Status:\n    Domain:     donut.djnavarro.net\n    Status:     Active\nAt last. I have https. It works, which I can verify simply by visiting https://donut.djnavarro.net and seeing if my app is working. Obviously I know that it is, because the embedded image at the start of the post is doing what it’s supposed to, but just for fun I’ll do it again:\n\n\nExcellent. It works."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#epilogue",
    "href": "posts/2023-01-10_kubernetes/index.html#epilogue",
    "title": "Deploying R with kubernetes",
    "section": "Epilogue",
    "text": "Epilogue\nThe application I built is very limited, and I think it’s important to point to the things that I know it does poorly. I am sure there are others, but the big one is storage. As currently designed, the app generates a new image every time the site is visited. That’s wasteful, especially if you’re going to reuse images. You can do better than this by enabling google cloud storage, connecting to it as a volume, and writing generated images to storage when they are create. The plumber app would then check for the relevant files before trying to generate a new one. There’s a useful tutorial on volumes if you want to explore this with the google kubernetes engine.\nIt’s also worth pointing out that I have completely ignored helm, the package manager for kubernetes. Helm is excellent, and when the time comes that you want to deploy an application that someone else has designed properly, the thing you actually do is use a helm “chart”. For example, the one time I actually got spark running properly on a kubernetes cluster, the way I did it was using a terribly-useful helm chart provided by Bitnami: bitnami.com/stack/spark/helm. There’s a lot of useful tooling built up around kubernetes, and you might as well take advantage of that!\nThat being said… yes there’s a lot more to talk about, but I’m done with this post. I’m tired and unemployed, and since nobody is paying me for any of this I’m going to call it quits for today."
  },
  {
    "objectID": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html",
    "href": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html",
    "title": "R scripts for twitter mutes and blocks",
    "section": "",
    "text": "Twitter is a complicated place. I’ve met some of my closest friends through twitter, it’s the place where I keep in touch with my professional community, and it’s an important part of my work as a developer advocate at Voltron Data. But it’s also a general purpose social media site, and there is a lot of content out there I prefer not to see. In particular, because I’m transgender and have absolutely no desire to participate in or even view the public debate that surrounds trans lives, I want to keep that kind of content off my twitter feed. This is particularly salient to me today as a member of the Australian LGBT community. Most people who follow me on twitter probably wouldn’t be aware of it, but it’s been a rough week for LGBT folks in Australia courtesy of a rather intense political fight over LGBT rights and the ostensible (and in my personal opinion, largely fictitious) conflict with religious freedom. The details of Australian politics don’t matter for this post, however. What matters is that these kinds of political disputes necessarily spill over into my twitter feed, and it is often distressing. Events like this one are quite commonplace in my online life, and as a consequence I’ve found it helpful to partially automate my use of twitter safety features such as muting and blocking. Because my experience is not unique, I thought it might be useful to write a short post talking about the the scripts I use to manage twitter safety features from R using the rtweet package."
  },
  {
    "objectID": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#warning-off-label-usage",
    "href": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#warning-off-label-usage",
    "title": "R scripts for twitter mutes and blocks",
    "section": "Warning: off-label usage",
    "text": "Warning: off-label usage\nLet’s make one thing very clear at the outset. A lot of what I’m going to be doing in this blog post is “off label” use of the rtweet package. You’ll see me use it in ways that the writers of the package didn’t really intend it to be used (I think), and you’ll see me dig into the internals of the package and rely on unexported functions.\nThis is almost always a bad idea.\nIf you haven’t seen it, Hadley Wickham gave a very good talk about maintaining R packages as part the rstudio::global 2021 conference. At about the 19 minute mark he talks about the “off label” metaphor. In the context of medication, “off label” refers to any use of a medication that it’s not officially approved for. It might work, but there could be unknown consequences because maybe it hasn’t been fully explored in this context. When applied to software, “off label” use means you’re doing something with a function or package that the designer doesn’t really intend. Your code might work now, but if you’re relying on “incidental” properties of the function to achieve your desired ends, you’re taking a risk. Package maintainers will usually go to considerable lengths to make sure that updates to their packages don’t break your code when you’re using it for its intended purpose… but if you’re doing something “off label” there’s a good chance that the maintainers won’t have thought about your particular use case and they might unintentionally break your code.\nIn short: you go off-label at your own risk. In this particular instance it is a risk I’ve chosen to take and I’m perfectly happy to fix my scripts if (or, let’s be realistic, when) future updates to rtweet cause them to break. Or possibly abandon my scripts. I knew the risks when I went off-label.\nBut if you follow me along this path you need to be aware of the risk too… don’t go off-label lightly! In my case I didn’t do this on a whim: I chose this path about a year ago out of personal desperation, and I’ve had to rewrite the scripts a lot in that time. So, please be careful."
  },
  {
    "objectID": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#setting-up",
    "href": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#setting-up",
    "title": "R scripts for twitter mutes and blocks",
    "section": "Setting up",
    "text": "Setting up\nThe first step is to make sure you have the developer version of rtweet: the scripts I’ve been using rely on the dev version of the package and won’t work with the current CRAN version. To be precise, I’m currently using rtweet version 0.7.0.9011. If you don’t have it, this is the command to install:\n\nremotes::install_github(\"ropensci/rtweet\")\n\nThe second step is to authenticate so that rtweet can access private information about your twitter account. The good news here is that the authentication mechanism in the dev version of rtweet is a little more streamlined than it used to be. You only need to authenticate once on your machine, and the command is as simple as this:\n\nauth_setup_default()\n\nWith that, you should be ready to start!"
  },
  {
    "objectID": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#write-a-blockmute-function",
    "href": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#write-a-blockmute-function",
    "title": "R scripts for twitter mutes and blocks",
    "section": "Write a block/mute function",
    "text": "Write a block/mute function\nOur first task will be to write a function that can be used either to block or to mute a twitter account. A little whimsically I decided to call it cancel_user(). Quite obviously the name is a personal joke, since it does not “cancel” anyone: the only thing blocking or muting accomplishes is to give you a little distance from the account you’re muting or blocking.\nThe reason for wanting one function that can switch between muting and blocking is that I typically run every process twice, once on my primary account (where, with one exception, I don’t block anyone but mute extensively) and once on my private account (where I block quite aggressively). I’d like to be able to reuse my code in both contexts, so I’ll design the core function to handle both blocks and mutes. Here’s the code:\n\ncancel_user <- function(user, type) {\n  api <- c(\n    \"block\" = \"/1.1/blocks/create\",\n    \"mute\" = \"/1.1/mutes/users/create\"\n  )\n  rtweet:::TWIT_post(\n    token = NULL,\n    api = api[type],\n    params = list(user_id = user)\n  )\n}\n\nThere’s quite a bit to unpack here.\nFirst notice that I have called the internal function rtweet:::TWIT_post(). This is the clearest indication that I’m working off-label. If I were interested only in muting users and never blocking, I’d be able to do this without going off-label because rtweet has an exported function called post_mute() that you can use to mute an account. However, there is no corresponding post_block() function (possibly for good reasons) so I’ve written cancel_user() as my personal workaround.\nSecond, let’s look at the interface to the function. Unlike the more sophisticated functions provided by rtweet this is a bare-bones interface. The user argument must be the numerical identifier corresponding to the account you want to block/mute, and type should be either be \"mute\" or \"block\" depending on which action you wish to take.\nFinding the numeric user id code for any given user is straightforward with rtweet. It provides a handy lookup_users() function that you can employ for this. The actual output of the function is a data frame containing a lot of public information about the user, but the relevant information is the user_id variable. So, if you hate me enough to want to mute or block me on twitter, I’ll make it easy on you. Here’s my numeric user id:\n\nlookup_users(\"djnavarro\")$user_id\n\n\n\n[1] \"108899342\"\n\n\nAs it turns out, for the particular scripts I use, I rarely need to rely on lookup_users() but it is a very handy thing to know about."
  },
  {
    "objectID": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#preparing-to-scale",
    "href": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#preparing-to-scale",
    "title": "R scripts for twitter mutes and blocks",
    "section": "Preparing to scale",
    "text": "Preparing to scale\nAs written there’s nothing particularly wrong with the cancel_user() function, but it’s also not very useful. I can use it to block or mute an individual account, sure, but if that were the problem I wanted to solve it would be a lot easier to do that using the block/mute buttons on twitter. I don’t need to write an R function to do that.\nThe only real reason to implement this as an R function is if you intend to automate it in some fashion and repeat the operation on a scale that would be impossible to do manually. To give a sense of the scale at which I’ve had to implement this I currently have about 220000 accounts blocked from my private account, and a similar number muted from my main account. There’s no way I could possibly do that manually, so I’m going to need to be a little more thoughtful about my cancel_user() function.\n\nPractice safe cancellation\nThe first step in making sure the function works well “at scale”1 is error handling. If I have a list of 50000 account I want to block but for one reason or another cancel_user() throws an error on the 5th account, I don’t want to prevent R from attempting to block the remaining 49995 accounts. Better to catch the error and move on. My preferred way to do this is to use purrr::safely():\n\ncancel_safely <- purrr::safely(cancel_user)\n\nThe cancel_safely() function operates the same way as cancel_user() with one exception. It never throws an error: it always returns a list with two elements, result and error. One of these is always NULL. If cancel_user() throws an error then result is NULL and error contains the error object; if it doesn’t then error is null and result contains the output from cancel_user().\nNot surprisingly, the cancel_safely() function is much more useful when we’re trying to block or mute large numbers of accounts on twitter.\n\n\nCheck your quotas my loves\nOne thing that has always puzzled me about the twitter API is that it places rate limits on how many mutes you can post in any 15 minute period, but doesn’t seem to impose any limits on the number of blocks you can post. I’m sure they have their reasons for doing it, but it’s inconvenient. One consequence of this is that there are lots of tools that exist already for blocking large numbers of accounts. You don’t actually need to write a custom R script for that! But if you want to mute large numbers of accounts, it’s a lot harder: you have to write a script that keeps posting mutes until the rate limits are exceeded, then pauses until they reset, and then starts posting mutes again. Speaking from experience, this takes a very long time. As a purely practical matter, you don’t want to be in the business of muting large numbers of accounts unless you are patient and have a very good reason. In my case, I did.\nIn any case, one thing we’ll need to write a rate_exceeded() function that returns TRUE if we’ve hit the rate limit and FALSE if we haven’t. That’s actually pretty easy to do, as it turns out, because any time our attempt to mute (or block) fails, the cancel_safely() function will catch the error and capture the error message. So all we have to do to write a rate_exceeded() function is to check to see if there’s an error message, and if there is a message, see if that message informs us that the rate limite has been exceeded. This function accomplishes that goal:\n\nrate_exceeded <- function(out) {\n  if(is.null(out$error)) return(FALSE)\n  if(grepl(\"limit exceeded\", out$error$message)) return(TRUE)\n  return(FALSE)\n}\n\nBecause blocks are not rate limited, in practice this function only applies when you’re trying to mute accounts.\n\n\nBe chatty babes\nThe last step in preparing the cancellation function to work well at scale is to make it chatty. In practice, a mass block/mute operation is something you leave running in its own R session, so you want it to leave behind an audit trail that describes its actions. A moderately verbose function is good here. You could make this as sophisticated as you like, but I find this works nicely for me:\n\ncancel_verbosely <- function(user, type) {\n\n  # notify user attempt has started\n  msg <- c(\n    \"block\" = \"blocking user id\",\n    \"mute\" = \"muting user id\"\n  )\n  withr::local_options(scipen = 14)\n  cli::cli_process_start(paste(msg[type], user))\n\n  # make the attempt; wait 5 mins if rate limits \n  # exceeded and try again\n  repeat {\n    out <- cancel_safely(user, type)\n    if(rate_exceeded(out)) {\n      Sys.sleep(300)\n    } else {\n      break\n    }\n  }\n\n  # notify user of the outcome\n  if(is.null(out$result)) {\n    cli::cli_process_failed()\n  } else{\n    cli::cli_process_done()\n  }\n}\n\nHere’s what the output looks like when it successfully blocks a user. Not fancy, but it shows one line per account, specifies whether the action was a block or a mute, and makes clear whether the attempt succeeded or failed:\n✓ blocking user id 15xxxx66 ... done\n(where, in the real output the user id for the blocked account is of course not censored). In this function I’ve used the lovely cli package to create pretty messages at the R command line, but there’s nothing stopping you from using simpler tools if you’d prefer."
  },
  {
    "objectID": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#scaling-up",
    "href": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#scaling-up",
    "title": "R scripts for twitter mutes and blocks",
    "section": "Scaling up",
    "text": "Scaling up\nNow that we have a version of our block/mute function that is suitable for use on a larger scale, it’s time to put it into practice. Let’s say I have a list of 50000 users represented as numeric ids and I want to block (or mute) all these accounts. To do this, I’ll need a vectorised version of cancellation function. Thanks to the functional programming tools in the purrr package, this is not too difficult. Here’s the cancel_users() function that I use:\n\ncancel_users <- function(users, type) {\n  msg <- c(\"block\" = \"blocking \", \"mute\" = \"muting \")\n  cat(msg[type], length(users), \" users\\n...\")\n  purrr::walk(users, cancel_verbosely, type = type)\n}\n\nWhen given a vector of user ids, the cancel_users() function will attempt to block/mute them all one at a time. When rate limits are exceeded it will pause and wait for them to reset, and then continue with the process. For mass muting in particular it can take a long time, so it’s the kind of thing you run in its own session while you go do something else with your life. If you want to be clever about it you can make it a background job and sink the output to a log file but honestly I’m usually too lazy to bother with that: all I’m trying to do is sanitise my twitter experience, I’m not deploying production code here.\nThe trickier question is “where do I get this block list from?”\nHere, I’m not going to be too specific, for a couple of reasons. Firstly, I don’t want to be in the business of teaching people how to track down hidden networks of users embedded in social media. That’s not something I’m comfortable doing. Secondly, if you’re doing this defensively (i.e., you’re protecting yourself from attack) then you probably already know something about where the attacks are coming from. You already have your own list of key names, because they’re the people who keep harassing you. Really, your only goal is to block them and their followers, because the thing that’s happening is they’re targeting you and they’re using their follower base as a weapon. Right? I mean if that’s not the situation you’re in, and what you’re actually trying to do is seek out a hidden population to potentially target them… yeah I’m not sure I want to be telling you the other tricks I know. So let’s keep it very simple.\nThe easiest trick in the book (and, honestly, one of the most powerful when you’re trying to block out harassment from a centralised “hub-and-spokes” network), is simply to find every account that follows more than \\(k\\) of the \\(n\\) of the key actors, and block/mute them. Actually, in the case of “astroturf” organisations that don’t have real grassroots support, you can probably just pick a few of the big names and block (or mute) all their followers. That will eliminate the vast majority of the horrible traffic that you’re trying to avoid. (Yes, I am speaking from experience here!)\nThe rtweet package makes this fairly painless courtesy of the get_followers() function. Twitter makes follower lists public whenever the account itself is public, so you can use get_followers() to return a tibble that contains the user ids for all followers of a particular account.2 Here’s an example showing how you an write a wrapper around get_followers() and use it to block/mute everyone who follows a particular account:\n\ncancel_followers <- function(user, type = \"block\", n_max = 50000, precancelled = numeric(0)) {\n\n  followers <- get_followers(user, n = n_max, retryonratelimit = TRUE)\n  followers <- followers$from_id\n\n  uncancelled <- setdiff(followers, precancelled)\n  uncancelled <- sort(as.numeric(uncancelled))\n\n  cancel_users(uncancelled, type = type)\n}\n\nNote the precancelled argument to this function. If you have a vector of numeric ids containing users that you’ve already blocked/muted, there’s no point wasting time and bandwidth trying to block them again, so the function will ignore anything on that list. You could use the same idea to build a whitelist of accounts that would never get blocked or muted regardless of who they follow.\nWe’re almost at the end of the post. There’s only one other thing I want to show here, and that’s how to extract a list of all the accounts you currently have muted or blocked. Again this particular bit of functionality isn’t exposed by rtweet directly, so you’ll notice that I’ve had to go off-label again and call an unexported function!\n\nlist_cancelled <- function(type, n_max, ...) {\n  api <- c(\n    \"block\" = \"/1.1/blocks/ids\",\n    \"mute\" = \"/1.1/mutes/users/ids\"\n  )\n  params <- list(\n    include_entities = \"false\",\n    skip_status = \"true\"\n  )\n  resp <- rtweet:::TWIT_paginate_cursor(NULL, api[type], params, n = n_max, ...)\n  users <- unlist(lapply(resp, function(x) x$ids))\n  return(users)\n}\n\nI’m not going to expand on this one, other than to mention that when you get to the point where you have hundreds of thousands of blocked or muted accounts, it’s handy to use a function like this from time to time, and to save the results locally so that you can be a little more efficient whenever you next need to refresh your block/mute lists."
  },
  {
    "objectID": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#epilogue",
    "href": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#epilogue",
    "title": "R scripts for twitter mutes and blocks",
    "section": "Epilogue",
    "text": "Epilogue\nI wrote this post in two minds. On the one hand, the rtweet developers made a decision not to support blocklists for a reason, and presumably the twitter developers have some reason for making it difficult to mute large numbers of accounts. It’s very rarely a good idea to write code that works against the clear intent of the tools you’re relying on. It is almost certain to break later on.\nOn the other hand, this is functionality that I personally need. On my primary account I’ve made the deliberate decision not to block anyone3 but to keep my twitter feed free of a particular type of content I have had to mute an extremely large number of accounts. Twitter makes that difficult to do, but with the help of these scripts I managed to automate the process. After a month or two, with a little manual intervention, the problematic content was gone from my feed, and I was able to get back to doing my job. So, if anyone else finds themselves in a similar situation, hopefully this blog post will help."
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html",
    "title": "Unpredictable paintings",
    "section": "",
    "text": "Almost two years (2020-01-15) ago I wrote this blog post as an introduction to generative art in R. The idea behind the post was to start making a new generative art system from scratch, and write the blog post at the same time. By doing it that way the reader can see how the process unfolds and how many false starts and discarded ideas a generative artist tends to go through, even for a simple system like this one. The post disappeared when I moved my blog to its own subdomain and its own repository, but I’ve now managed to rescue it! Hope it’s helpful…"
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html#introduction",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html#introduction",
    "title": "Unpredictable paintings",
    "section": "Introduction",
    "text": "Introduction\nOver the last year or so I’ve taken up generative artwork as a hobby, and I am occasionally asked to write an introduction to the subject… how does one get started in generative art? When I started posting the code for my generative art to my “rosemary” repository I has this to say about my subjective experience when making artwork,\n\nMaking generative artwork reminds me a lot of gardening. Both are aesthetic exercise, creating a pleasant and relaxing evironment that the artist/gardener can enjoy no less than anyone visiting the space. Both are hard work, too. Learning how to prune, learning which plants will thrive in the land that you have, knowing what nutrients differnt plants need, taking care of the garden in hard times, et cetera, none of these are easy. At the end you might have a sustainable native garden that blends in seamlessly with the environment and brings you joy, but growing the garden is itself a technical and sometimes physically demanding exercise. The analogy between gardening and generative artwork feels solid to me, but it’s not amazingly helpful if you want to start making this kind of art. If you want to start gardening, you probably don’t really want a fancy gardener to talk about their overall philosophy of gardens, you’d like a few tips on what to plant, how often to water and so on. This post is an attempt to do that, and like so many things in life, it is entirely Mathew Ling’s fault.\n\nThe first thing to say about generative artwork is that it’s really up to you how you go about it. I do most of my programming using R, so that’s the language I use for my artwork. Most of the artwork I’ve been making lately has relied on the ambient package for the “generative” component, but to be honest you don’t have to rely on fancy multidimensional noise generators or anything like that. You can use the standard pseudorandom number generators built into R to do the work. Since the point of this post is to talk about “how to get started”, this is exactly what I’ll do!\nIn fact, what I’m going to do in this post is build a new system for generative art… I’m not sure what I’m going to end up with or if it will be any good, but let’s see where it goes! For the purposes of this post I’m assuming that you’re somewhat familiar with the tidyverse generally and ggplot2 specifically, and are comfortable writing functions in R. There’s a couple of spots where I do something slightly more complex, but I’ll explain those when they pop up. So, here goes…"
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html#do-something-anything",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html#do-something-anything",
    "title": "Unpredictable paintings",
    "section": "Do something, anything",
    "text": "Do something, anything\nTruth be told, I almost never have a plan when I start building a new system. What I do is start playing with pictures that visualise random data in some fashion, and see where that takes me. So, okay… I’ll start out creating a data frame that contains random numbers: each row in the data frame is a single “point”, and each column specifies an attribute: an x variable specifying a horizontal co-ordinate, a y variable specifying the vertical location, and a g variable that randomly assigns each of point to a “group” of some kind. At this point in time I have no idea how I’m going to use this information:\n\nlibrary(tidyverse)\nset.seed(1)\nobj <- tibble(\n  x = rnorm(100), \n  y = rnorm(100), \n  g = sample(10, 100, TRUE)\n)\nobj\n\n# A tibble: 100 × 3\n        x       y     g\n    <dbl>   <dbl> <int>\n 1 -0.626 -0.620      1\n 2  0.184  0.0421     3\n 3 -0.836 -0.911     10\n 4  1.60   0.158      7\n 5  0.330 -0.655      4\n 6 -0.820  1.77       1\n 7  0.487  0.717      9\n 8  0.738  0.910      7\n 9  0.576  0.384      6\n10 -0.305  1.68       4\n# … with 90 more rows\n\n\nSomething to note about this code is that I used set.seed(1) to set the state of the random number generator in R. This will ensure that every time I call the same “random” code I will always get the same output. To get a different output, I change the seed to something different.\nSo I guess the first thing I’ll do is try a scatterplot:\n\nggplot(obj, aes(x, y, colour = g)) +\n  geom_point(show.legend = FALSE) + \n  coord_equal() + \n  theme_void()\n\n\n\n\nOkay, yeah that’s scatterplot. I’m not feeling inspired here, but it does occur to me that I’ve seen some very pretty hexbin plots in the past and maybe there’s some fun I could have playing with those?\n\nggplot(obj, aes(x, y)) +\n  geom_hex(show.legend = FALSE) + \n  coord_equal() + \n  theme_void()\n\nWarning: Computation failed in `stat_binhex()`:\n\n\n\n\n\nHm. Interesting? Maybe I could split this by group and try overlaying different hexagonal shapes? That sometimes makes for a neat three-dimensional feel when two hexagonal grids are offset from one another… okay let’s pursue that for a bit…\n[an hour passes in which I draw many boring plots]\n…yeah, okay I’ve got nothing. It seemed like a good idea but I couldn’t make anything I really liked. This is, in my experience, really common. I go down quite a few blind alleys when making a generative system, discard a lot of things that don’t seem to do what I want. It’s an exploration process and sometimes when you explore you get lost. Oh well, let’s try something else. Instead of drawing a scatterplot, let’s connect the dots and draw some lines:\n\nggplot(obj, aes(x, y, colour = g)) +\n  geom_path(show.legend = FALSE) + \n  coord_equal() + \n  theme_void()\n\n\n\n\nHm. A bit scribbly, but there’s something aesthetically pleasing there. Okay, what if I decided to turn the paths into polygons?\n\nggplot(obj, aes(x, y, fill = g, group = g)) +\n  geom_polygon(show.legend = FALSE) + \n  coord_equal() + \n  theme_void()\n\n\n\n\nOkay, this feels promising. It reminds me a bit of the time I accidentally drew some really pretty pictures by setting axis limits inappropriately when drawing kernel density estimates with ggplot2, and ended up using it as a way to explore the scico package. Let’s run with this…"
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html#mix-it-up-a-bit",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html#mix-it-up-a-bit",
    "title": "Unpredictable paintings",
    "section": "Mix it up a bit",
    "text": "Mix it up a bit\nTo try to get a sense of what you can do with a particular approach, it’s usually helpful to try out some variations. For example, the previous plot uses the ggplot2 default palette, which isn’t the most appealing colour scheme. So let’s modify the code to use palettes from the scico package. One of my favourites is the lajolla palette:\n\nlibrary(scico)\nggplot(obj, aes(x,y, fill = g, group = g)) +\n  geom_polygon(show.legend = FALSE) + \n  coord_equal() + \n  theme_void() + \n  scale_fill_scico(palette = \"lajolla\")\n\n\n\n\nThis is definitely neat. I do like the “jagged little polygons” feel to this, but to be honest I’m getting a bit bored. I’ve done a few different art pieces that exploit this effect before, and this isn’t the most exciting thing for me, so I want to push things in a different direction. Speaking of which, I’m not sure I want all the polygons to lie on top of each other so much, so what I’ll do is create a small tibble called grp that specifies a random “offset” or “shift” for each group, and then using full_join() from dplyr to merge it into the data object:\n\ngrp <- tibble(\n  g = 1:10,\n  x_shift = rnorm(10),\n  y_shift = rnorm(10)\n)\nobj <- full_join(obj, grp)\nobj\n\n# A tibble: 100 × 5\n        x       y     g x_shift y_shift\n    <dbl>   <dbl> <int>   <dbl>   <dbl>\n 1 -0.626 -0.620      1   1.13   -1.00 \n 2  0.184  0.0421     3   0.741   0.945\n 3 -0.836 -0.911     10  -0.581   1.78 \n 4  1.60   0.158      7  -0.408   0.376\n 5  0.330 -0.655      4  -1.32    0.434\n 6 -0.820  1.77       1   1.13   -1.00 \n 7  0.487  0.717      9  -0.701  -1.43 \n 8  0.738  0.910      7  -0.408   0.376\n 9  0.576  0.384      6   0.398  -0.390\n10 -0.305  1.68       4  -1.32    0.434\n# … with 90 more rows\n\n\nSo now I can adjust my ggplot2 code like this. Instead of defining each polygon in terms of the x and y columns, I’ll add the x_shift and y_shift values so that each polygon gets moved some distance away from the origin. This is kind of helpful, because now I can see more clearly what my objects actually look like!\n\nggplot(\n  data = obj, \n  mapping = aes(\n    x = x + x_shift, \n    y = y + y_shift, \n    fill = g, \n    group = g\n  )\n) +\n  geom_polygon(show.legend = FALSE) + \n  coord_equal() + \n  theme_void() + \n  scale_fill_scico(palette = \"lajolla\")\n\n\n\n\nVery pretty! But as I said, I’m bored with the “jagged little polygon” look, so what I want to do is find some way of changing the appearance of the shapes."
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html#cran-is-a-girls-best-friend",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html#cran-is-a-girls-best-friend",
    "title": "Unpredictable paintings",
    "section": "CRAN is a girl’s best friend",
    "text": "CRAN is a girl’s best friend\nAt this point in my process I was a bit lost for ideas. I want to do something different, and I think what I want to do is turn each point set into more of a regular shape, something without holes in it. It then occurred to me that way back in 1998 I did my honours thesis on combinatorial optimisation problems in neuropsychological testing and had played around with things like the Travelling Salesperson Problem (TSP) and remembered that the solutions to two-dimensional planar TSPs can sometimes be quite pretty. A few minutes on google uncovers the TSP package, and a few more minutes playing around with the API gives me a sense of what I need to do in order to work out what order to connect the points in order to generate a TSP solution:\n\nlibrary(TSP)\ntour <- function(obj) {\n  obj$tour <- unname(c(solve_TSP(ETSP(obj[, c(\"x\", \"y\")]))))\n  arrange(obj, order(tour))\n}\n\nThe code here is very ugly because I wrote it in a rush. The gist of it is that what you want to do normally is feed a data frame to the ETSP() function, which creates the data structure needed to solve the corresponding optimisation problem. The output is then passed to solve_TSP() which can produce an approximate solution via one of many different algorithms, and that then returns a data structure (as an S3 object) that specifies the order in which the points need to be connected, along with some handy metadata (e.g., the length of the tour). But I don’t want any of that information, so I use c() and unname() to strip all that information out, append the resulting information to the data object, and then use the arrange() function from the dplyr package to order the data in the desired fashion.\nNext, because I want to apply the tour() function separately to each group rather than to compute a TSP solution for the overall data structure, I use group_split() to split the data set into a list of data frames, one for each group, and then map_dfr() to apply the tour() function to each element of that list and bind the results together into a data frame:\n\nobj <- obj %>%\n  group_split(g) %>%\n  map_dfr(~tour(.x))\nobj\n\n# A tibble: 100 × 6\n         x      y     g x_shift y_shift  tour\n     <dbl>  <dbl> <int>   <dbl>   <dbl> <int>\n 1 -0.626  -0.620     1    1.13   -1.00     1\n 2 -0.165  -1.91      1    1.13   -1.00     9\n 3  0.267  -0.926     1    1.13   -1.00     3\n 4 -0.103  -0.589     1    1.13   -1.00     8\n 5  0.370  -0.430     1    1.13   -1.00     4\n 6  0.557  -0.464     1    1.13   -1.00     5\n 7  2.17    0.208     1    1.13   -1.00     2\n 8  0.821   0.494     1    1.13   -1.00    10\n 9 -0.820   1.77      1    1.13   -1.00     7\n10 -0.0162 -0.320     1    1.13   -1.00     6\n# … with 90 more rows\n\n\nNow when I apply the same plotting code to the new data object, here’s what I get:\n\nggplot(\n  data = obj, \n  mapping = aes(\n    x = x + x_shift, \n    y = y + y_shift, \n    fill = g, \n    group = g\n  )\n) +\n  geom_polygon(show.legend = FALSE) + \n  coord_equal() + \n  theme_void() + \n  scale_fill_scico(palette = \"lajolla\")\n\n\n\n\nOoh, I like this."
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html#formalise-a-system",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html#formalise-a-system",
    "title": "Unpredictable paintings",
    "section": "Formalise a system…",
    "text": "Formalise a system…\nThe next step in the process is to take all the moving parts and write a system. The exact details of what consitutes a generative art system is a little vague, but I usually think of it as a collection of functions that capture the essence of the process. If I’m being fancy I’ll convert this set of functions to a full-fledged R package, but let’s not bother with that for this simple system. So what do we need? First, I’ll state the dependencies:\n\nlibrary(tidyverse)\nlibrary(scico)\nlibrary(TSP)\n\nNext, let’s keep the tour() function as a separate thing. It’s one way of organising the points that belong to the same group, but there might be others:\n\ntour <- function(obj) {\n  tsp <- ETSP(obj[,c(\"x\",\"y\")])\n  obj$tour <- unname(c(solve_TSP(tsp)))\n  arrange(obj, order(tour))\n}\n\nMy personal style is to separate the “builder” functions that generate the underlying data structure from the “styling” functions that render that data structure as an image. For the current project, our builder function is build_art() and defined as follows:\n\nbuild_art <- function(\n  points = 100,   # total number of points\n  groups = 10,    # number of groups\n  polygon = tour, # function used to organise points\n  gap = 1,        # standard deviation of the \"shift\" separating groups\n  seed = 1        # numeric seed to use\n) {\n  \n  # set the seed\n  set.seed(seed)\n  \n  # create the initial data frame\n  obj <- tibble(\n    x = rnorm(points), \n    y = rnorm(points), \n    g = sample(groups, points, TRUE)\n  )\n  \n  # create the offset for each group\n  grp <- tibble(\n    g = 1:groups,\n    x_shift = rnorm(groups) * gap,\n    y_shift = rnorm(groups) * gap\n  )\n  \n  # merge obj with grp\n  obj <- full_join(obj, grp, by = \"g\") \n  \n  # split obj by group and apply the \"polygon\" mapping\n  # function separately to each group\n  obj <- obj %>%\n    group_split(g) %>%\n    map_dfr(~polygon(.x))\n  \n  return(obj) # output\n}\n\nAs you can see, it’s more or less the same as the code I developed for my original example, just written with a little more abstraction so that I can feed in different parameter values later. The draw_art() function takes this object as input, and creates a plot using the same ggplot2 code. The only free “parameter” here is the ... that I can use to pass arguments to the palette function:\n\ndraw_art <- function(obj, ...) {\n  ggplot(\n    data = obj, \n    mapping = aes(\n      x = x + x_shift, \n      y = y + y_shift, \n      fill = g, \n      group = g\n    )\n  ) +\n    geom_polygon(show.legend = FALSE) + \n    coord_equal() + \n    theme_void() + \n    scale_fill_scico(...)\n}\n\nNow we’re ready to go! Because I set it up so that every parameter has a default value that corresponds to the same parameters I used to draw the original picture, this code reproduces the original image:\n\nbuild_art() %>% \n  draw_art()\n\n\n\n\n… well, almost!"
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html#vary-parameters",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html#vary-parameters",
    "title": "Unpredictable paintings",
    "section": "Vary parameters…",
    "text": "Vary parameters…\nOkay, the one thing that I didn’t do is specify the default palette. In the scico package the default palette is “bilbao”, and the original artwork I produced used the “lajolla” palette. So the default output of the system is identical to this:\n\nbuild_art(seed = 1) %>% \n  draw_art(palette = \"bilbao\")\n\n\n\n\nIf I’d set palette = \"lajolla\" I’d have obtained exactly the same result as before. But let’s play around a little bit. If I switch to the “vik” palette I get output with the same shapes, just with a different colours scheme:\n\nbuild_art(seed = 1) %>% \n  draw_art(palette = \"vik\")\n\n\n\n\nHowever, if I modify the seed argument as well I get different random points, and so the resulting shapes are different.\n\nbuild_art(seed = 2) %>% \n  draw_art(palette = \"vik\")\n\n\n\n\nMore generally, I can play around with my new system and find out what it is capable of. Here’s a version with 1000 points divided into 5 groups with a fairly modest offset:\n\nbuild_art(\n  points = 1000, \n  groups = 5,\n  gap = 2\n) %>% \n  draw_art(\n    palette = \"vik\", \n    alpha = .8\n  )\n\n\n\n\nThe shapes aren’t quite what I was expecting: I’m not used to seeing TSP solutions rendered as polygons, because they’re usually drawn as paths, and they make me think of crazy shuriken or maybe really screwed up snowflakes. Not as organic as I thought it might look, but still neat. Notice that I’ve also made the shapes slightly transparent by setting the alpha argument that gets passed to scale_fill_scico(). Okay, let’s play around a bit more:\n\nbuild_art(\n  points = 5000, \n  groups = 20,\n  gap = 7\n) %>% \n  draw_art(\n    palette = \"bamako\", \n    alpha = .8\n  )\n\n\n\n\nThis is kind of neat too, but I want to try something different. The general pattern for a TSP solution is that they take on this snowflake/shuriken look when there are many points, but not when there are fewer data points. So this time I’ll have 10000 points in total, but divide them among 1000 groups so that on average each polygon is defined by 10 vertices. I’ll space them out a little bit more too, and…\n\nbuild_art(\n  points = 10000, \n  groups = 1000,\n  gap = 15, \n  seed = 10\n) %>% \n  draw_art(palette = \"tokyo\")\n\n\n\n\nI kind of love it!"
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html#have-fun-exploiting-loopholes",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html#have-fun-exploiting-loopholes",
    "title": "Unpredictable paintings",
    "section": "Have fun exploiting loopholes",
    "text": "Have fun exploiting loopholes\nAt the beginning, when I created the system, I set tour() to be the default polygon function used to modify each polygon. My original plan was that this function was really just supposed to be used to order the points, but there’s actually nothing in the system that prevents me from doing something fancier. For example, here’s a sneaky trick where the function calls dplyr::mutate() before passing the data for that group to the tour() function. In this case, what I’ve done is a dilation transformation: the overall size of each group is multiplied by the group number g, so now the shapes will lie on top of each other with different scales. It also, in another slightly sneaky trick, flips the sign of the group number which will ensure that when the data gets passed to draw_art() the order of the colours will be reversed. The result…\n\nshift_tour <- function(obj) {\n  obj %>% \n    mutate(\n      x = x * g, \n      y = y * g, \n      g = -g\n    ) %>%\n    tour()\n}\n\nbuild_art(\n  points = 5000,\n  groups = 200,\n  gap = 0,\n  polygon = shift_tour\n) %>% draw_art(palette = \"oslo\")\n\n\n\n\n… is really quite lovely. Later on, I might decide that this little trick is worth bundling into another function, the system gains new flexibility, and the range of things you can do by playing around with it expands. But I think this is quite enough for now, so it’s time to move on to the most important step of all …"
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html#tweet-it",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html#tweet-it",
    "title": "Unpredictable paintings",
    "section": "Tweet it!",
    "text": "Tweet it!\nBecause what’s the point of making art if you can’t share it with people?"
  },
  {
    "objectID": "posts/2021-04-19_bs4cards-in-distill/index.html",
    "href": "posts/2021-04-19_bs4cards-in-distill/index.html",
    "title": "Bootstrap cards in distill",
    "section": "",
    "text": "When creating R markdown websites, I often find myself wanting to organise content into a nice-looking grid of links. For example, in a recent project I wanted to be able to create something like this:\nIt bothered me that this wasn’t as straightforward as I was expecting, so for one of my side projects I’ve been putting together a small package called bs4cards to make this a little easier inside an R markdown document or website. There are some introductory articles posted on the bs4cards package website showing how the package works, and there’s no need to duplicate that content here. However, because this website uses the distill package (Allaire et al. 2021) and the package website is built using pkgdown (Wickham, Hesselberth, and Salmon 2021), it seems like a good idea to have at least one post on both sites that uses bs4cards."
  },
  {
    "objectID": "posts/2021-04-19_bs4cards-in-distill/index.html#enabling-bootstrap-4",
    "href": "posts/2021-04-19_bs4cards-in-distill/index.html#enabling-bootstrap-4",
    "title": "Bootstrap cards in distill",
    "section": "Enabling bootstrap 4",
    "text": "Enabling bootstrap 4\nThe reason for doing this is that the first step in using the package is to make sure that your R markdown document uses version 4 of bootstrap: the bs4cards package takes its name from the cards system introduced in bootstrap version 4, and will not work properly if used in R markdown documents that rely on bootstrap version 3, or don’t use bootstrap at all. To ensure that you are using bootstrap 4, you need to edit the YAML header for your document to specify which version of bootstrap you want to use. The instructions are slightly different depending on what kind of document you’re creating:\n\nVanilla R markdown\nFor a plain R markdown document or website (i.e., one where the output format is html_document) here is the relevant section of YAML you might use:\noutput:\n  html_document:\n    theme:\n      version: 4\nThis overrides the R markdown defaults (Xie, Dervieux, and Riederer 2020) to ensure that the output is built using bootstrap 4.5.\n\n\nPkgdown\nTo enable bootstrap 4 in a pkgdown site, the process is similar but not identical. Edit the _pkgdown.yml file to include the following\ntemplate:\n  bootstrap: 4\nNote that this relies on a currently-in-development feature, so you may need to update to the development version of pkgdown to make this work.\n\n\nDistill\nDistill R markdown does not use bootstrap, which is a little inconvenient if you want to use bs4cards with distill. With a little effort it is possible to enable the entire bootstrap library in a distill site, but this can lead to undesirable side-effects because bootstrap has a lot of styling that doesn’t look visually appealing when mixed with the istill styling. The solution I’ve adopted for this is to use a custom bootstrap build that includes a minimal number of bootstrap components. If you want to try the same approach, you can download the strapless.css file to the same folder as the distill post you want to enable it for, and include the following YAML in the post header:\noutput:\n  distill::distill_article:\n    css: \"strapless.css\"\nIf you want to enable strapless for the entire site, this markup goes in the _site.yml file and the css file should go in the home folder for the project. Once that’s done you should be ready to go. That being said, you’d be wise to be careful when adopting this approach: the strapless build is a crude hack, and I haven’t tested it very thoroughly."
  },
  {
    "objectID": "posts/2021-04-19_bs4cards-in-distill/index.html#testing-with-pretty-pictures",
    "href": "posts/2021-04-19_bs4cards-in-distill/index.html#testing-with-pretty-pictures",
    "title": "Bootstrap cards in distill",
    "section": "Testing with pretty pictures",
    "text": "Testing with pretty pictures\nJust to make certain, let’s check that it does what we want by generating cards using the galleries data that comes bundled with the bs4cards package:\n\nlibrary(bs4cards)\ngalleries %>% \n  cards(title = long_name, image = image_url)\n\n\n\n\n\n\nAsh Cloud and Blood\n\n\n\n\n\nGhosts on Marble Paper\n\n\n\n\n\nIce Floes\n\n\n\n\n\nNative Flora\n\n\n\n\n\nSilhouettes\n\n\n\n\n\nTrack Marks\n\n\n\n\n\nViewports\n\n\n\n\n\n\n\nLooks about right to me?"
  },
  {
    "objectID": "posts/2021-04-19_bs4cards-in-distill/index.html#last-updated",
    "href": "posts/2021-04-19_bs4cards-in-distill/index.html#last-updated",
    "title": "Bootstrap cards in distill",
    "section": "Last updated",
    "text": "Last updated\n\n2022-08-23 13:12:17 AEST"
  },
  {
    "objectID": "posts/2021-04-19_bs4cards-in-distill/index.html#details",
    "href": "posts/2021-04-19_bs4cards-in-distill/index.html#details",
    "title": "Bootstrap cards in distill",
    "section": "Details",
    "text": "Details\n\nsource code, R environment"
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html",
    "title": "How to mint digital art on HEN",
    "section": "",
    "text": "Cryptoart can be a touchy subject for generative artists, and it’s something a lot of us have messy feelings about. In my case it is no secret that I feel conflicted, and I completely understand why a lot of us are uncomfortable with it. I genuinely believe there are many perfectly good reasons why a generative artist would choose not to participate. On the other hand, I also recognise that there are some very sensible reasons why a generative artist would want (or need) to sell NFTs: artists have to pay rent, for example. So this post isn’t about passing judgment one way or the other. It’s intended to be a guide to help other artists get started in this area, particularly artists in the R community, if they should decide to try it out. That’s all.\nThis post is also not supposed to be an introduction to blockchains or cryptocurrencies. It doesn’t dive into the details on what these things are or even what an NFT is. I make art: I don’t care about any of these subjects. What I’m assuming is that you’re coming to this world from a similar position to me: you have a vague understanding of what blockchain is, what cryptocurrencies are about, and have a similarly vague notion that an NFT is kind of like a “digitally signed copy” of your art that you can sell to other people. That’s all you need."
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#prologue-barriers-to-entry",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#prologue-barriers-to-entry",
    "title": "How to mint digital art on HEN",
    "section": "Prologue: Barriers to entry",
    "text": "Prologue: Barriers to entry\nOne thing I have noticed about the world of cryptoart is that there are many barriers to entry. Some barriers are obvious: if you want to sell art on Foundation, for example, you need to be invited. To be invited, you need to know someone who can and will invite you. As anyone who has ever been excluded from a fancy venue by virtue of their race, gender, sexual orientation, transgender status etc can attest, an invitation requirement is a non-trivial and frequently discriminatory barrier. “By invitation” systems create entry barriers by design: for good or ill, they are inherently exclusionary. But there are other ways in which cryptoart creates barriers to entry.\n\nEnvironmental costs matter\nAnother kind of barrier comes from the nature of cryptoart. Blockchains were not designed to be energy efficient, and they can be extraordinarily wasteful (much more than you’d think). Environmental considerations also create barriers to entry, albeit indirect barriers. For example, the biggest cryptocurrencies like Bitcoin and Ethereum operate on a “proof of work” principle (often abbreviated to “PoW”) and as the name suggests, operations on those chains require a lot of computational work. A lot. They are staggeringly wasteful, and as a consequence the total energy consumption of these chains is so high that an NFT minted on one of these chains has a very high carbon footprint. Proof of work chains are an environmental disaster, and so (in my mind) they are socially irresponsible. Don’t use them if you can avoid it.\nThis poses a problem for artists, unfortunately. The biggest cryptoart markets are based on the Ethereum chain, and Ethereum is a proof of work chain. True, there are plans to change this and make Ethereum more ethical, but it hasn’t happened yet and I personally am unwilling to participate until that switch actually occurs. This is deeply unfortunate from artistic point of view, because it rules out OpenSea. It sucks because OpenSea is the largest marketplace and it’s very easy to get started there. For instance, I have an unused account that I set up in a few minutes before I realised the problem. But for me the end-user convenience wasn’t worth the environmental costs, so I abandoned this idea at the outset. On the plus side, OpenSea have announced that they are planning to support the Tezos blockchain (see below), and when that day comes I will probably make use of my OpenSea account: the thing I take moral issue with is not OpenSea, it is with Ethereum (or more precisely, with proof-of-work chains). Personally, I don’t want to touch the stuff.\nSo what are the alternatives?\n\n\nThere are alternatives\nThe main alternative to the “proof of work” blockchains are the “proof of stake” (PoS) blockchains. These don’t require anywhere near as much computation, and as a consequence are much more energy efficient. For that reason, NFTs on those chains are often called “clean NFTs”. There are a multiple proof of stake chains (Tezos, Solana, etc), but the one I’m most familiar with is Tezos. To give you a sense of just how extreme the difference is, this is a screenshot that popped up on one of the sites while I was doing my initial exploration:\n\n\n\n\n\n\n\n\n\nEven if this claim is somewhat exaggerated for marketing purposes, the sheer scale of it is remarkable. A multiplicative factor of 1.5 million is… enormous. I could literally mint NFTs on Tezos for every single image that I have ever created for the rest of my life, and it would still be several orders of magnitude more energy efficient than minting one piece on Ethereum. To my way of thinking, that makes a massive difference to the moral calculus associated with minting NFTs. In fact, the difference between Tezos and Ethereum is so extreme that there is actually one art marketplace there – Bazaar – that is not just carbon neutral but is actually carbon negative. That’s only possible because Tezos is so much more efficient than Ethereum, and it becomes practical for the developers to impose a carbon tax on minting: the transaction costs are used to purchase sufficient carbon offsets to ensure the system as a whole remains carbon negative. Right now I wouldn’t recommend setting up on Bazaar because it’s so early in development that it’s hard to use, but I’m absolutely keeping an eye on it for the future!\nSetting up on the Tezos blockchain is particularly appealing because it has an established digital art marketplace called “hic et nunc”. The name is Latin in origin and translates to “here and now”. You’ll usually see it abbreviated to “HEN”, which is what I’ll call it in this post, but some people use “H=N”, I guess because it looks visually similar to the HEN logo. The HEN marketplace is completely open: you don’t need an invitation. There’s no super-secret club to be invited into (as far as I know!), and to my mind that’s a huge positive. Better yet, a few folks from the R art community are already there. I’m entirely certain that there are others I don’t know about yet, but so far on HEN I’ve already found Thomas Lin Pedersen, Will Chase, Antonio S. Chinchón, and George Savva. As of a few days ago, I’m there too.\nOpenness! Community! Yay!\nIf there’s one thing I have learned from the lovely R folks on twitter, everything is better when you are part of a supportive team of people who actually care about each other and work to build each other up. From my perspective, this makes HEN a very attractive option.\nThere is, unfortunately, a catch. There is always a catch.\n\n\nIt can be confusing\nOne big limitation to HEN is that it isn’t easy to get started there unless you are already somewhat enmeshed in the crypto world generally, or the cryptoart scene specifically. The ecosystem is distributed over several sites that have weird names without enough vowels, the user interfaces on the sites tend to be unconventional (often pointlessly so in my opinion), and the “how to” guides aren’t very easy to read. The overall aesthetic and typology screams out WE ARE THE COOL KIDS in capital letters. It doesn’t even have the good grace to be subtle about it. Taken together, all these little things add up, and it annoys me. I have been a professional educator for 15 years now, and I can absolutely guarantee that the overall effect of this is to create a de facto entry barrier. All these things act as signals to exclude people who aren’t already part of the clique. It feels disproportionately uncomfortable if you’re an outsider. It tells you that you’re not welcome if you’re not one of the cool kids. Are you one of the cool kids? No? Then sorry. No HEN for you babe.\nWell, fuck.\nYet again, there are barriers to entry to HEN, and that makes me uncomfortable. However, unlike the other cryptoart options I looked at, there’s something I can do to improve the situation: I can write a blog post explaining the process. This blog post.\n\n\nLet’s demystify it\nLet’s assume you’re not one of the cool kids. Let’s assume you’re just a regular human being who likes to make generative art in R, and are a little curious. You have a vague idea of what cryptocurrencies are (yeah, yeah, digital currency blah blah blah). You have a vague idea of what an NFT is (digitally signed copy of the art, whatever dude). Maaaaaybe you’ve sort of heard of HEN … but that’s only because you’ve seen some R people posting about it on twitter. And that’s it. That’s all you know. But maybe you want to try it out, just to see if it’s for you? Just to try. But you really, really, reaaaaaalllllllly don’t want to wade into all the details and you’re secretly worried that it’s all too complicated and you won’t be able to do it. Your impostor sydrome is going wild. Is that you? Do you feel the same way I felt?\nIf so, this post is written for you."
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#get-an-overview",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#get-an-overview",
    "title": "How to mint digital art on HEN",
    "section": "1: Get an overview",
    "text": "1: Get an overview\nWhen I started setting up on, I wandered around the Tezos cryptoart landscape in confusion, wandering aimlessly over the terrain. It was all deeply unsettling. Eventually I pieced together some overall view of things, but I wouldn’t recommend doing things the same way I did. I think the best thing to do first is to “zoom out” and look at the landscape as a whole. The best site I’ve found for doing that is tezos.art. If you click on the link it will take you to a page with the following three sections:\n\nMarketplaces: Sites where you can mint, buy, and sell art\nWallets: Tools that handle your identity and store your funds\nCommunity: Places where you can go for help\n\nIt’s worth taking a quick look at this page because it gives you a feel for what all the moving parts are, but doesn’t dive into details. You’ve taken a quick peek, yes? Cool. Let’s get started…"
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#create-a-wallet",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#create-a-wallet",
    "title": "How to mint digital art on HEN",
    "section": "2: Create a wallet",
    "text": "2: Create a wallet\nIt’s a little counterintuitive, but the natural place to start is not the art marketplaces: the first thing you need is a wallet. The reason for this is that your wallet serves two distinct purposes. As the name suggests, the wallet provides a method for storing funds: the currency itself is referred to as “tezos”, which you’ll see abbreviated to “tez” or denoted “ꜩ”. However, it also serves as your unique identifier on the Tezos blockchain. On blockchains as in life it is rather hard to do anything interesting without a public identity, so you need to create one first.\nOkaaaay… at this point you’d probably be wondering “where do I sign up for one of these wallets?” Excellent question. As you will have noticed by peeking at the tezos.art website, you have a few different options. Being offered choices is nice, of course, but it can also be anxiety-provoking when you don’t even know what the differences between the options are. So, for whatever it’s worth, I’ll mention that I chose Temple Wallet. I made that choice for two reasons and only two reasons. First, it was one of the options listed on the HEN wiki. Second, I was complaining privately to Will Chase about how confused I was and he told me uses Temple and I copied what he did. That being said, I suspect the choice is arbitrary.\nFor the sake of argument, I’ll assume you decided to use Temple too. So now you’re clicking through the link above in order to open an account with Temple Wallet and… wait, it’s just a browser extension? Yup. This seems to be very common in blockchain land, and initially it struck me as bizarre. The longer I hang around there, however, the more I realise it does make a kind of sense. Once you start doing things on Tezos, you’ll find that you have to validate everything you do. Any time you ask a website to undertake some action on your behalf, the first thing that will happen is that you’ll be asked to authorise the action using your public identity. What that means is that you have to use your wallet all the time, even for things that don’t cost money. A browser extension makes this a little easier. When the website asks you to authenticate, the wallet browser extension will create a little popup window that asks you to confirm the transaction. There’s a bit of friction to the process sometimes, and it feels a little alien, but it does start to feel normal after a while.\nMoving on… the next little strangeness is that when you set up the wallet you don’t create a username, only the password, and you’ll be given a “recovery phrase”, which is a sequence of 12 random words. Don’t lose either of these things. Here, as always, I strongly recommend that you use a password manager to store your password, because there aren’t that many options for recovery if you start losing passwords. Personally, I’ve been using 1password for a few years and I really like it. So yes. Use a password manager, store your wallet password there and store your recovery phrase there too.\nAt the end of this process you are assigned a public identity, which is a long string of complete gibberish. For example, this is me:\ntz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7\nNaturally, the first thing I did when seeing this is groan. The second thing I did is notice the Srmojf substring and it made me think of Smurfs. So I secretly think of this gibberish identifier as the Smurf, and that’s how I’ll refer to it for the rest of this post. Of course, in the long run you probably don’t want to be a random string of digits, you want to have a name! This is possible to do, and I’ll walk you through that later. But right now that’s not a complication you need to care about.\nWe’ll get to that a little bit later but the key thing for now is that your equivalent of the Smurf is both a public identifier and a bank account number. If someone wants to send you some tez, all they need to know is that string."
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#tell-hen-who-you-are",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#tell-hen-who-you-are",
    "title": "How to mint digital art on HEN",
    "section": "3: Tell HEN who you are",
    "text": "3: Tell HEN who you are\n\nSynchronise with your wallet\nWhen you go to the HEN website you’ll see a little bit of text on the top right hand side that has a link that says “sync”. Click on that:\n\n\n\n\n\n\n\n\n\nThis will bring up an overlay that looks like this:\n\n\n\n\n\n\n\n\n\nIf you chose a Temple wallet choose the “Temple - Tezos Wallet (ex. Thanos)” option. It might ask for your password at this point but it probably won’t if you’re already logged in. What you’re more likely to see is a screen like this:\n\n\n\n\n\n\n\n\n\nThis is a message from your wallet asking you to confirm that yes, you do want to synchronise with HEN (it also shows you that I currently have a balance of 11 tez, which I guess is something like US$60). Click on connect, and HEN will now be synchronised with your identity. You can see that because the menu at the top now looks something like this:\n\n\n\n\n\n\n\n\n\nYou’re now synced: in effect, you are now logged in to HEN. You still don’t have a username, but you have authenticated yourself and you can now change some settings.\n\n\nThe HEN menu is weird\nOkay, let’s move to the next step. To the right of your Smurf, you’ll see the “hamburger” menu. It behaves pretty much the same as any menu you’d encounter on the internet, but some of the options have very non-intuitive names. Here’s what the menu looks like, with my annotations added:\n\n\n\n\n\n\n\n\n\nAs with everything about HEN, it’s very minimalist. Some of the options are easy to understand, but others are not. The options I’ve been using most are these:\n\nsearch takes you to the HEN search page\nedit profile allows you add some information about yourself (see next section)\nmanage assets will take you to your profile page (it took me a long time to realise this)\nOBJKT (mint) is the option you select when you want to create art. I’ll talk bout that later\n\n\n\nName, avatar and bio\nThe time has come to give yourself a name. If you do things in the right order and with the right mental model of what’s going on, this is pretty easy to do, but it’s easy to get a little confused because there are actually multiple things going on here, and you always have to keep in mind that your equivalent of my Smurf string is your actual identity.\nSo… your first step is to tell HEN to link your Smurf string to a name, bio and avatar. Click on “edit profile”. This brings up another slightly unconventional looking screen that has several options you can set. Here’s what mine currently looks like:\n\n\n\n\n\n\n\n\n\nThere are three things you can do immediately without any major hassle:\n\nFirst, if you click on “choose file” you can upload an image to give yourself a profile image.\nSecond, you can give yourself a username. The advice I read on the relevant HEN wiki page suggested that you should avoid spaces and special characters, and should stick to lower case letters because usernames are case sensitive.\nThird, you can write a brief description of yourself. It doesn’t have to be very thorough. Most people say something about who they are and what they do, but you don’t have to. For example, I’ve had a habit of identifying myself as “an object of type closure” on all my social media websites. It’s intended as a silly reference to the classic R error message:\n::: {.cell}\nidentity[]\n::: {.cell-output .cell-output-error} Error in identity[]: object of type 'closure' is not subsettable ::: :::\nAs it happens, this allowed me to make an even sillier double-layered joke in my HEN bio. When you create art on HEN the tokens that you generate are referred to as OBJKTs, so now I refer to myself as “an OBJKT of type closure”. I’m so funny.\n\nAaaaanyway… once you’ve done those three things, click on “save profile”, and you’re done for now. Ignore everything below the “save profile” button. All that stuff is useful, and it will let you do things like link to your twitter profile and your github profile, but it’s surprisingly finicky to set up and it costs money, so we’ll leave that until later.\n\n\nCheck out your profile\nBefore moving on, take a quick look at your profile. As I mentioned earlier, you can do this through the menu system, by selecting the “manage assets” option. Personally I wish they’d chosen a better name: I’m not an investor and I don’t think of my art as “assets”. The page that displays my art is my homepage on HEN, and it bothers me a little that the site frames it in such mercenary terms. It’s irritating. But whatever, it’s not a dealbreaker.\nIt’s worth quickly commenting on the URL for your profile. When you click on the “manage assets” link, it will take you to a URL that identifies you using the Smurf. For me, that URL is:\nhttps://www.hicetnunc.xyz/tz/tz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7/\nAs long as you have your very own Smurf in your wallet, you’ll have this URL. However, if you followed the instructions in the last section, HEN is kind enough to arrange it so that the ugly Smurf based URL will automatically redirect to one based on your username. For me, that URL is:\nhttps://www.hicetnunc.xyz/djnavarro/\nAt this point, you exist on HEN! Yaaaay!"
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#intermission-follow-people",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#intermission-follow-people",
    "title": "How to mint digital art on HEN",
    "section": "Intermission: Follow people",
    "text": "Intermission: Follow people\nThere’s more stuff you can do to get your account set up, but you might want to take a little breather and look for some art. Maybe you want to search for someone you know in the R community who might be on HEN, and you’d like to find them. As I mentioned earlier, the HEN site does have a search page, but there are some limitations. It’s okay if you want to search by keywords to find art or artists, but what it won’t let you do is follow them. Personally, I quite like being able to follow artists whose work I love, and it would be pretty cool to have a feed where I can see what they’ve posted, arranged in chronological order. That’s where the the “HEN explorer” website is handy:\nhttps://www.henext.xyz/\nLike HEN itself, the HEN explorer site has browsing and search capability. It’s a little clunky in places (on my browser, there seems to be a bug where the search box only works when you’re on the home page), but it does the job.\nTo use HEN explorer, you’ll need to synchronise with your wallet (i.e., log in). To do that you can click on the “profile” icon in the nav bar (the one that looks like a little person), or just visit\nhttps://henext.xyz/profile\nThat will bring up a screen that looks like this\n\n\n\n\n\n\n\n\n\nClick on the “connect wallet” button, and it will take you through the same steps that were involved when you connected your wallet to the HEN site.\nOnce you’ve done that, you’re logged in to HEN explorer, and you’re able to find artists you like and follow them! If you would like to follow me, you can search for “djnavarro” on the HEN explorer search box, or you can visit my HEN explorer profile page directly:\nhttps://www.henext.xyz/tz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7\nAdd a few artists you like, and you’ll get a sense of what the feed looks like. The location of the feed is\nhttps://www.henext.xyz/following\nHappy browsing!"
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#get-a-little-money",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#get-a-little-money",
    "title": "How to mint digital art on HEN",
    "section": "4: Get a little money",
    "text": "4: Get a little money\nOne slightly frustrating thing about this process is that it’s hard accomplish very much in this arena without spending money, and we’re rapidly reaching the point where you’ll need a little bit. Thankfully, if you’re an artist wanting to create your own art, and aren’t looking to collect anyone else’s, you don’t need very much to get started. If you’re in the R community there’s a good chance you can ask one of the other R folks on HEN to help out. That’s what I did, and I’m grateful to the people who sent me a few tez, and the others who spontaneously offered. R people are lovely.\nIf the “ask a friend” approach is an option for you, I’d recommend it for artists. The reason I say this is that you have a bigger set up cost (in terms of your time and effort) than someone who is joining in order to purchase art, so from the perspective of the artist all you need – right now – is a little start up fund. To use myself as the example, I made a lot of weird mistakes setting up and wasted quite a lot of transactions, but even with all that I think I only spent about 1 tez in total (at the exchange rate at the time that was about US$5).\nAssuming that you can solve the problem that way, you can take care of the other financials later (and there’s a guide on how to do that coming later in the post). There’s a part of me that hopes that if the R art community does end up with a larger presence on HEN, we’ll look after our own. We’re R folks, and we pay it forward because we care for each other.\nThat being said, I’m also not naive, and I know perfectly well that it doesn’t always work that way, so I’ll briefly mention other options. For example, the HEN website has some suggestions for other places you can ask for help. Alternatively if you have a Visa card, one possibility is to buy through https://tzkt.io/buy-tezos (the tzkt.io site will come up later in the post!), though you’ll need identification documents for this (or any other option) because it’s a financial institution. Finally, you can sign up at a currency exchange, which you’ll probably want to do later anyway because that’s going to be how you convert the funds from your HEN sales to regular currency. I’ll talk about that later on.\nRegardless of how you solve this part of the problem, I’m hoping that at this point you have a few tez to start out!"
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#release-your-art",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#release-your-art",
    "title": "How to mint digital art on HEN",
    "section": "5: Release your art!",
    "text": "5: Release your art!\n\nMinting the art\nSurprisingly, the process of releasing your art on HEN is quite easy, at least when compared to how complicated everything else is. If you open the menu and click on the “OBJKT (mint)” option, it will take you to the minting page, which looks like this:\n\n\n\n\n\n\n\n\n\nAt this stage in the process you upload the file, give it a name and a description, and make some decisions about (a) how many tokens you want to create, and (b) your royalties, the percentage of future sales that are returned to you. Here’s me filling one out:\n\n\n\n\n\n\n\n\n\nClick on the preview button, and it will show you a preview of what the page will look like when it goes live. If you’re happy with it you can proceed and click the “mint OBJKT” button. You’ll be asked by your wallet to confirm the minting operation (this costs a small amount of tez), and then after a short time the OBJKT (i.e., the token) exists. In this case, here’s the page displaying the OBJKT that I’ve just created:\nhttps://www.hicetnunc.xyz/objkt/359761\n\n\nPutting OBJKTs up for sale\nThe tokens now exist, but as yet they have not been placed on the market. People can’t buy them from you. To place the token for sale, go to the page showing the token (i.e., the link above). It will look something like this:\n\n\n\n\n\n\n\n\n\nIf you want to put the art on sale, click on the “swap” link that I’ve highlighted here (and if you change your mind and want to destroy it, click on the “burn” link next to it). The interface will look like this:\n\n\n\n\n\n\n\n\n\nIt will then let you decide you many of your tokens you want to put up for sale, and set the price for each one. For this particular piece I’d decided to create a lot of tokens (there are 50 of them), and I’m going to put them all on sale at the very low price of 2 tez. I honestly know nothing about pricing, but I’m playing around with it at the moment: some pieces I mint only a single token and set the price high, others I mint a large number of tokens and set the price low. In any case, when you’re happy press the “swap” button, confirm with your wallet, and the pieces will now be on sale!\n\n\nCreating auctions\nThe mechanism I’ve shown above is the simplest way to put art on sale: you list a price and wait for someone to purchase it. However, if you want to try more exotic options like auctions, you can check out objkt.com.\n\n\nSome art…\nHere are the Native Flora pieces I posted while writing this post. They’re all available for purchase!\n\n\n\n\n\n\n\n\n\n\nOBJKT 359814\n\n\n\n\n\n\n\n\n\nOBJKT 359795\n\n\n\n\n\n\n\n\n\nOBJKT 359761\n\n\n\n\n\n\n\n\n\nOBJKT 359745"
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#share-on-social-media",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#share-on-social-media",
    "title": "How to mint digital art on HEN",
    "section": "6: Share on social media",
    "text": "6: Share on social media\nAt some point you’ll probably want to advertise the fact that the artwork is available for purchase. You don’t have to, of course, and I’m honestly not sure how much of my online life I want to spend advertising art for sale, but it’s handy to have the option, and that probably means sharing on social media.\nMost of us in the R community who make art are primarily sharing on twitter. Yes, I have seen some people post on reddit, others on instagram, and no doubt many other places besides, but my social media world is dominated by twitter, and I’d like to be able to post to twitter. To my mild irritation, the HEN website doesn’t seem to do twitter cards properly, so if you share the link on its own, people won’t see a nice preview image.\nThere are a couple of ways to get around this. The first is to post the link on twitter and attach your art as an image: that way folks on twitter will get the link and and the image. But they won’t get an actual twitter card displaying the title of the piece.\nThe second solution is to use the hic.art website. At the moment, if you visit the website it will tell you that signups are closed, but that actually doesn’t matter. You don’t need to sign up to use the service. All you have to do is provide the OBJKT identifier. For instance, here’s one of my pieces on HEN:\nhttps://www.hicetnunc.xyz/objkt/354474\nThe identifier here is 354474. If I share the link above on twitter, it won’t display a very nice twitter preview. However, if I tweet this link\nhttps://hic.art/354474\nIt will display a very lovely looking twitter preview, and when the user clicks on the link or the preview it will automatically redirect to the official HEN page. It’s a nice service!\nHere’s an example from Antonio Sánchez Chinchón:\n\n\nMondrianomie 28Basic cellular multiorganism grown according to neoplasticism assumptions (2033 x 2400 PNG)3 ed - 28 tez at @hicetnunc2000https://t.co/TyNvt1zMBu#HEN #hicetnunc #hicetnunc2000 #nft #NFTs #nftcommunity #nftcollectors #cleannft #nftart #tezos #tezosart\n\n— Antonio Sánchez Chinchón (@aschinchon) September 25, 2021"
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#manage-your-identity",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#manage-your-identity",
    "title": "How to mint digital art on HEN",
    "section": "7: Manage your identity",
    "text": "7: Manage your identity\nThere are at least three additional tools that may be useful to you in managing your identity in the peculiar world of cryptoart on the Tezos blockchain: (1) you can set up a Tezos Profile, (2) you can establish an alias on the Tezos Blockchain Explorer, and/or (3) you can purchase a Tezos Domain. None of these are strictly necessary, but all of them offer some value to you as an artist on HEN so I’ll discuss each one.\n\nEstablishing a Tezos Profile\nEarlier in this post I mentioned that it’s possible to connect your twitter profile, github account, website, etc with your HEN profile? You can do this with the assistance of Tezos Profiles. So lets go back HEN, open the menu, click on the option that says “edit profile” and then take a closer look at the window that pops up. It’s almost impossible to notice, but the text that reads “Tezos Profiles” is in fact a link:\n\n\n\n\n\n\n\n\n\nClicking on that link will take you to https://tzprofiles.com/, where you will see a very prominent “connect wallet” button. Click on that button, confirm with your wallet that you want to allow tzprofiles to connect (the little popup window will appear, like it always does), and then you’ll see a screen that looks like this:\n\n\n\n\n\n\n\n\n\nThere are several different things you can do here, and any of them that you verify on tzprofiles will eventually end up on HEN. For example, if you want to verify your twitter account, you’ll go through a series of elaborate steps (which, yes, will have to be confirmed with your wallet) and in the end you’ll be forced to send a tweet like this one:\n\n\nI am attesting that this twitter handle @djnavarro is linked to the Tezos account tz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7 for @tzprofilessig:edsigtaH3nvbQjpiAfMCnT4zcQESZefXoVLPf2NEYaZeUfhwHjzRYp4oeBiiyDFLdrUAUvjBhvepyDFoxuyE2ynVYxd7TvV9fj6\n\n— Danielle Navarro (@djnavarro) September 21, 2021\n\n\nTo verify your GitHub account it’s pretty similar, except that it forces you to create a gist, using your GitHub account, that includes a signature block similar to the one in the tweet. For a website, it’s the same idea except you have to insert it as a DNS record (which I found extremely irritating to do). You can verify as many or as few of these as you like, but there is some value to doing them. Because Tezos Profiles forces you to go through the clunky verification process, other people can check your HEN profile and verify for themselves that it really is you posting your artwork onto the site, and not someone else who has stolen your piece (apparently, that happens way too often)\nOnce you’re done verifying your accounts, you may need to use your wallet to confirm again so that the updated Tezos Profile information can be accessed by the HEN website. After that’s been done, you’ll see icons appear on your HEN page, linking to your twitter account, github account, etc:\n\n\n\n\n\n\n\n\n\nAt this point your HEN profile is meaningfully linked to your other public identities, and any artwork you mint on HEN can be traced back to you, the original artist.\n\n\nCreating an alias on Tezos Blockchain Explorer\nAll right. If you’re like me you’ve probably been exploring as you go and you’ve been encountering other sites that seem connected to this ecosystem. In particular, you may have clicked on links associated with transactions and it has taken you to the Tezos Blockchain Explorer website. As the name suggests, the role of this website is to publicly display transactions that take place on the Tezos blockchain. For example, here’s the page showing all the transactions that have involved me in some fashion:\nhttps://tzkt.io/tz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7/operations/\nWhen I first started (oh so many days ago…) it looked something like this:\n\n\n\n\n\n\n\n\n\nA lot of it is gibberish, but you can kind of see what’s going on here. Yet again you can see my Smurf, there’s a bunch of transactions that show me minting NFTs, etc. It makes a kind of sense.\nWhat might be surprising, particularly if you’ve just gone to the effort of setting up a Tezos Profile, is that the account information doesn’t show my avatar. It doesn’t include my name, or a bio, and it doesn’t include my social media links. Instead, all I have is a cartoon image of a very suspicious looking cartoon cat. Unlike HEN, the tzkt.io site doesn’t pull information from your Tezos Profile.\nThe mystery deepens a little when you start noticing that the exact same cartoon cat appears on various other sites. For example, this was how my profile looked on objkt.com at the time:\n\n\n\n\n\n\n\n\n\nThe weird cryptocat was following me around across all these websites. Hm. The suspicious kitty is cute and everything, but honestly I’d prefer my usual name and profile image to follow me around instead.\nAs it turns out, the source for all these skeptical cats is the blockchain explorer site, tzkt.io, and you can submit an application to the people who run that site to create an alias for you. The process is described in this post on the “Baking Bad” blog (don’t let the name and silly images fool you, the blog is associated with the people who run the site). The post will take you to a Google Form that you can fill out, in order to have your alias created. When you do this, it won’t update immediately: there is a manual verification process that takes about three days, so you’ll need to be patient.\nOnce that happens you’ll discover that your links have appeared on your tzkt.io page, and more importantly perhaps, you have an avatar and description on other sites that make use of this alias. This is what my profile page on objkt.com looks like now:\n\n\n\n\n\n\n\n\n\nMine is a deliberately vague because I’m a peculiar person, but you can see a slightly more informative version if you look at Thomas Lin Pedersen’s profile:\n\n\n\n\n\n\n\n\n\n\n\nPurchasing a Tezos Domain\nWhen you look at the two profiles above, there’s something slightly peculiar. Notice how Thomas’ profile now links to thomasp85.tez and mine links to djnavarro.tez? That’s something slightly different again. Those addresses aren’t created by the Tezos Profile, nor are they created when you set your alias on the Tezos Blockchain Explorer. Those are Tezos Domains. The idea is very sensible: human beings don’t really enjoy memorising long strings of random digits. It would be much more convenient if I could message someone and say “hey send tez to me at djnavarro.tez, because that’s me!”. It’s certainly nicer than trying to say “send tez to me at tz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7, because that’s me!”\nIf you’d like to do this, visit tezos.domains and follow the instructions there: it costs 1 tez per year to set one up."
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#convert-tez-to-dollars",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#convert-tez-to-dollars",
    "title": "How to mint digital art on HEN",
    "section": "8: Convert tez to dollars",
    "text": "8: Convert tez to dollars\nAt some point, hopefully very soon, you’ll sell some artwork and you’ll want to get paid. To do that, you’ll probably need to sign up with one of the currency exchanges. Although you likely have no desire to be a currency trader, it’s a necessity if you want to get paid in real money. Yes, cryptocurrencies sound cool, but coolness does not pay the rent. My landlord expects to be paid in Australian dollars, and – by extension – so do I. That means exchanging your tez for regular money. The HEN wiki lists a couple of options along with the standard warning that you should definitely do your own research, because this is a thing that will depend a bit on where you live. I looked into one of their suggested options (Kraken) and it seemed fairly standard, but in the end used an Australian provider, CoinSpot. The sign up process was fairly standard, requiring identification documents for verification. Once that was completed, I was able to send money to my bank account. It ended up being a three-step process:\n\nSend tezos from the Temple wallet associated with my public identity (i.e., the one I’ve been using on HEN etc), to a tezos wallet that is provided for me through my CoinSpot account\nOn CoinSpot, sell the tezos in exchange for Australian dollars\nWithdraw money from my CoinSpot account and deposit it in my Australian bank account\n\nOnce I figured it all out it was surprisingly smooth. I imagine the process varies a lot from country to country and from exchange to exchange, but hopefully the description of my process is at least somewhat helpful."
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#epilogue-is-all-it-worth-it",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#epilogue-is-all-it-worth-it",
    "title": "How to mint digital art on HEN",
    "section": "Epilogue: Is all it worth it?",
    "text": "Epilogue: Is all it worth it?\nI haven’t been doing this for very long, but I’m a little surprised to find that I’m enjoying the process of minting art on HEN. I’ve sold three pieces to people who know me, and it is a nice feeling. I’m not making mountains of money, and I don’t expect that I will any time soon, but it is still quite satisfying. The fact that I’m doing it on HEN makes a big difference to how I feel about it too: the environmental costs worry me a lot and don’t think I could make myself use a site that relied on Ethereum. And to be honest, it really is nice to get paid for my art. Praise is nice, sure, but you can’t live off that.\nI suppose the other thing I’m noticing already is that I feel a little less constrained on HEN. When I post art to twitter or instagram, it’s always with the knowledge that the big social media websites are also places where my professional networks exist, and I’m obliged to behave, to be nice, to be the “good girl”. I might swear and be grumpy on twitter sometimes, but for the most part I try not to let other parts of my personality and my life show on those sites. That’s okay, and it’s probably how it should be. Twitter is a place where it’s okay to mix some parts of your personal life with some parts of your work life, but there’s a tacit understanding that you probably ought to keep some things carefully sequestered from the bird site. There are a lot of things about a person’s life that their employer and professional colleagues may not want to know.\nWhere that runs into some difficulty, for me at least, is that a lot of my generative art is deeply entwined with my personal life, with my moods, and my experiences. When done well, art is always intimate, and the intimacy of creating and sharing the art often entails personal disclosures that might not be welcome on twitter. Consider these pieces, for example:\n\n\n\n\n\n\n\n\n\n\nOBJKT 341833\n\n\n\n\n\n\n\n\n\nOBJKT 341852\n\n\n\n\n\n\n\n\n\nOBJKT 341868\n\n\n\n\n\n\n\n\n\nOBJKT 341880\n\n\n\n\n\n\n\nI am very fond of these pieces, but they aren’t the easiest ones to share on twitter. The title of the series is Bruises are how sadists kiss, and the pieces are tagged with “sadomasochism” on my HEN profile. The title isn’t deliberately intended to be provocative or anything of the sort. That’s not really my preferred style. It’s much more prosaic: those things are part of my world and part of my life, and sometimes they show up in my art. The emotional experience expressed through the art (via the code) was one in which a very polite sadist had turned up in my life after a long absence. I was reminiscing, trying to work out what he meant to me, and I wrote the code while I was thinking about it. This was the system that emerged.\nOn twitter I would not dream of referring to those parts of my world so overtly (nor would I typically do so on this blog, focused as it is on technical topics). On HEN though, it feels a little more natural: art is often raw, it is often personal, and those subjects do come up if you spend a little time exploring the cryptoart space. It feels like a place where that version of me is permitted to have an online existence. As it turns out, that’s a thing that has some value to me."
  },
  {
    "objectID": "posts/2021-09-14_tidy-tuesday-billboard/index.html",
    "href": "posts/2021-09-14_tidy-tuesday-billboard/index.html",
    "title": "Visualising the hits of Queen Britney",
    "section": "",
    "text": "I’ve never participated in Tidy Tuesday before, but because I’ve now joined a slack that does, it is high time I did something about that poor track record. I wasn’t sure what I wanted to do with this week’s “Billboard” data, other than I wanted it to have something to do with Britney Spears (because she’s awesome). After going back and forward for a while, I decided what I’d do is put together a couple of plots showing the chart performance of all her songs and – more importantly – write it up as a blog post in which I try to “over-explain” all my choices. There are a lot of people in our slack who haven’t used R very much, and I want to “unpack” some of the bits and pieces that are involved. This post is pitched at beginners who are hoping for a little bit of extra scaffolding to explain some of the processes…"
  },
  {
    "objectID": "posts/2021-09-14_tidy-tuesday-billboard/index.html#finding-the-data-on-github",
    "href": "posts/2021-09-14_tidy-tuesday-billboard/index.html#finding-the-data-on-github",
    "title": "Visualising the hits of Queen Britney",
    "section": "Finding the data on GitHub",
    "text": "Finding the data on GitHub\nEvery week the Tidy Tuesday data are posted online, and the first step in participating is generally to import the data. After a little bit of hunting online, you might discover that the link to the billboard data looks like this:\nhttps://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-14/billboard.csv\nLet’s start by unpacking this link. There is a lot of assumed knowledge buried here, and while it is entirely possible for you to get started without understanding it all, for most of us in the slack group the goal is to learn new data science skills. At some point you are probably going to want to learn the “version control” magic. This post is not the place to learn this sorcery, but I am going to start foreshadowing some important concepts because they will be useful later.\n\nGitHub repositories\nThe place to start in understanding this link is the peculiar bit at the beginning: what is this “github” nonsense? The long answer is very long, but the short answer is that https://github.com is a website that programmers use to store their code. GitHub is one of several sites (e.g., https://gitlab.org, https://bitbucket.com) that are all built on top of a version control system called “git”. Git is a powerful tool that lets you collaborate with other people when writing code, allows you to keep track of the history of your code, and to backup your code online in case your laptop mysteriously catches on fire.\n\n\nIn the R community, “laptop fires” are universally understood to be a reference to what happens to you when you foolishly ignore the wise advice of Jenny Bryan\nGit is a complicated tool and it takes quite some time to get the hang of (I’m still learning, quite frankly), but it is worth your effort. When you have time, I recommend starting a free GitHub account. You can sign up using an email address, and if you have a university email address you get the educational discount (basically you get the “pro” version for free). My username on GitHub is djnavarro, and you can find my profile page here:\nhttps://github.com/djnavarro\nThe Tidy Tuesday project originated in the “R for data science” learning community, and there is a profile page for that community too:\nhttps://github.com/rfordatascience\n\n\nR for data science is a wonderful book by Hadley Wickham and Garrett Grolemund\nOkay, so that’s part of the link explained. The next thing to understand is that when you create projects using git and post them to GitHub, they are organised in a “repository” (“repo” for short). Each repo has its own page. The Tidy Tuesday repo is here:\nhttps://github.com/rfordatascience/tidytuesday\nIf you click on this link, you’ll find that there’s a nice description of the whole project, links to data sets, and a whole lot of other things besides.\nMost of the work organising this is done by Thomas Mock, and it’s very very cool.\n\n\nRepositories have branches\nWhenever someone creates a git repository, it will automatically have at least one “branch” (usually called “master” or “main”). The idea behind it is really sensible: suppose you’re working on a project and you think “ooooh, I have a cool idea I want to try but maybe it won’t work”. What you can do is create a new “branch” and try out all your new ideas in the new branch all without ever affecting the master branch. It’s a safe way to explore: if your new idea works you can “merge” the changes into the master branch, but if it fails you can switch back to the master branch and pick up where you left off. No harm done. If you have lots of branches, you effectively have a “tree”, and it’s a suuuuuuper handy feature. Later on as you develop your data science skills you’ll learn how to do this yourself, but for now this is enough information. The key thing is that what you’re looking at when you visit the Tidy Tuesday page on GitHub is actually the master branch on the tree:\nhttps://github.com/rfordatascience/tidytuesday/tree/master\n\n\nRepositories are usually organised\nThe Tidy Tuesday repository has a lot of different content, and it’s all nicely organised into folders (no different to the folders you’d have on your own computer). One of the folders is called “data”, and inside the “data” folder there is a “2021” folder:\nhttps://github.com/rfordatascience/tidytuesday/tree/master/data/2021\nInside that folder you find lots more folders, one for every week this year. If you scroll down to the current week and click on the link, it will take you here:\nhttps://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-09-14\nBeing the kind soul that he is, Thomas has included a “readme” file in this folder: it’s a plain markdown file that gets displayed in a nice human readable format on the github page. Whenever you’re doing a Tidy Tuesday analysis, it’s super helpful to look at the readme file, because it will provide you a lot of the context you need to understand the data. Whenever doing your own projects, I’d strongly recommend creating readme files yourself: they’re reeeeaaaaaally helpful to anyone using your work, even if that’s just you several months later after you’ve forgotten what you were doing. Over and over again when I pick up an old project I curse the me-from-six-months ago when she was lazy and didn’t write one, or feel deeply grateful to her for taking the time to write one.\n\n\nReadme files are your best friend. Seriously\nIn any case, one of the things you’ll see on that page is a link to the “billboard.csv” data. If you click on that link it will take you here:\nhttps://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-09-14/billboard.csv\nNotice that this doesn’t take you to the data file itself: it goes to a webpage! Specifically, it takes you to the “blob” link that displays some information about the file (notice the “blob” that has sneakily inserted itself into the link above?). In this case, the page won’t show you very much information at all because the csv file is 43.7MB in size and GitHub doesn’t try to display files that big! However, what it does give you is a link that tells you where they’ve hidden the raw file! If you click on it (which I don’t recommend), it will take you to the “raw” file located at…\nhttps://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-14/billboard.csv\nThis is the link that you might have discovered if you’d been googling to find the Billboard data. It’s a GitHub link, but GitHub uses the “raw.githubusercontent.com” site as the mechanism for making raw files accessible, which is why that part of the link has changed.\n\n\nI didn’t intend for this section to be this long, honest\n\n\nThe anatomy of the data link\nAll of this tedious exposition should (I hope) help you make sense of what you’re actually looking at when you see this link. In real life I would never bother to do this, but if you wanted to you could decompose the link into its parts. In the snippet below I’ll create separate variables in R, one for each component of the link:\n\nsite <- \"https://raw.githubusercontent.com\"\nuser <- \"rfordatascience\"\nrepo <- \"tidytuesday\"\nbranch <- \"master\"\nfolder1 <- \"data\"\nfolder2 <- \"2021\" \nfolder3 <- \"2021-09-14\"\nfile <- \"billboard.csv\"\n\nArgh. Wait. There’s something slightly off-topic that I should point out… one thing you might be wondering when you look at this snippet, is where that pretty “arrow” character comes from. Don’t be fooled. It’s not a special arrow character, it’s two ordinary characters. What I’ve actually typed is <-, but this blog uses a fancypants font that contains a special ligature that makes <- appear to be a single smooth arrow. The font is called “Fira Code”, and a lot of programmers use it on their blogs. Once you know the trick, it’s really nice because it does make the code a little easier to read, but it can be confusing if you’re completely new to programming! It’s one of those little things that people forget to tell you about :-)\nAnyway, getting back on topic. The URL (a.k.a. “link”) for the Billboard data is what you get when you paste() all these components together, separated by the “/” character:\n\ndata_url <- paste(\n  site, \n  user, \n  repo, \n  branch,\n  folder1, \n  folder2, \n  folder3, \n  file, \n  sep = \"/\"\n)\n\ndata_url\n\n[1] \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-14/billboard.csv\"\n\n\nExciting stuff."
  },
  {
    "objectID": "posts/2021-09-14_tidy-tuesday-billboard/index.html#attaching-packages",
    "href": "posts/2021-09-14_tidy-tuesday-billboard/index.html#attaching-packages",
    "title": "Visualising the hits of Queen Britney",
    "section": "Attaching packages",
    "text": "Attaching packages\nI’m relatively certain that everyone in the slack has been exposed to the idea of an “R package”. A package is a collection of R functions and data sets that don’t automatically come bundled with R, but are freely available online. The tidyverse, for example, is a collection of R packages that a lot people find helpful for data analysis, and you can install all of them onto your machine (or your RStudio Cloud project) by using this command:\n\ninstall.packages(\"tidyverse\")\n\nThis can take quite a while to complete because there are a lot of packages that make up the tidyverse! Once the process is completed, you will now be able to use the tidyverse tools. However, it’s important to recognise that just because you’ve “installed” the packages, it doesn’t mean R will automatically use them. You have to be explicit. There are three tidyverse packages that I’m going to use a lot in this post (dplyr, stringr, and ggplot2), so I’ll use the library() function to “attach” the packages (i.e. tell R to make them available):\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/2021-09-14_tidy-tuesday-billboard/index.html#importing-the-data",
    "href": "posts/2021-09-14_tidy-tuesday-billboard/index.html#importing-the-data",
    "title": "Visualising the hits of Queen Britney",
    "section": "Importing the data",
    "text": "Importing the data\nAt this point we know where the data set is located, and we have some R tools that we can use to play around with it. The next step is reading the data into R. The readr package is part of the tidyverse, and it contains a useful function called read_csv() that can go online for you, retrive the billboard data, and load it into R. That’s cool and all but if you look at the library() commands above, I didn’t actually attach them. I didn’t want to do this because honestly I’m only going to use the readr package once, and it feels a bit silly to attach the whole package. Instead, what I’ll do is use the “double colon” notation :: to refer to the function more directly. When I write readr::read_csv() in R, what I’m doing is telling R to use the read_csv() function inside the readr package. As long as I have readr on my computer, this will work even if I haven’t attached it using library(). The technical name for this is “namespacing”, and if you hang around enough R programmers long enough that’s a word that will pop up from time to time. The way to think about it is that every package (e.g., readr) contains a collection of things, each of which has a name (e.g., “read_csv” is the name of the read_csv() function). So you can think of a “space” of these names… and hence the boring term “namespace”.\nOkay, let’s use a “namespaced” command to import the data, and assign it to a variable (i.e., give the data a name). I’ll call the data billboard:\n\nbillboard <- readr::read_csv(data_url)\n\nRows: 327895 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): url, week_id, song, performer, song_id\ndbl (5): week_position, instance, previous_week_position, peak_position, wee...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe billboard data is a nice, rectangular data set. Every row refers to a specific song on a specific date, and tells you its position in the charts on that date. We can type print(billboard) to take a look at the first few rows and columns. In most situations (not all), you can print something out just by typing its name:\n\nbillboard\n\n# A tibble: 327,895 × 10\n   url     week_id week_…¹ song  perfo…² song_id insta…³ previ…⁴ peak_…⁵ weeks…⁶\n   <chr>   <chr>     <dbl> <chr> <chr>   <chr>     <dbl>   <dbl>   <dbl>   <dbl>\n 1 http:/… 7/17/1…      34 Don'… Patty … Don't …       1      45      34       4\n 2 http:/… 7/24/1…      22 Don'… Patty … Don't …       1      34      22       5\n 3 http:/… 7/31/1…      14 Don'… Patty … Don't …       1      22      14       6\n 4 http:/… 8/7/19…      10 Don'… Patty … Don't …       1      14      10       7\n 5 http:/… 8/14/1…       8 Don'… Patty … Don't …       1      10       8       8\n 6 http:/… 8/21/1…       8 Don'… Patty … Don't …       1       8       8       9\n 7 http:/… 8/28/1…      14 Don'… Patty … Don't …       1       8       8      10\n 8 http:/… 9/4/19…      36 Don'… Patty … Don't …       1      14       8      11\n 9 http:/… 4/19/1…      97 Don'… Teddy … Don't …       1      NA      97       1\n10 http:/… 4/26/1…      90 Don'… Teddy … Don't …       1      97      90       2\n# … with 327,885 more rows, and abbreviated variable names ¹​week_position,\n#   ²​performer, ³​instance, ⁴​previous_week_position, ⁵​peak_position,\n#   ⁶​weeks_on_chart\n\n\n\n\nFinally, some data!\nThis view helps you see the data in its “native” orientation: each column is a variable, each row is an observation. It’s a bit frustrating though because a lot of the columns get chopped off in the printout. It’s often more useful to use dplyr::glimpse() to take a peek. When “glimpsing” the data, R rotates the data on its side and shows you a list of all the variables, along with the first few entries for that variable:\n\nglimpse(billboard)\n\nRows: 327,895\nColumns: 10\n$ url                    <chr> \"http://www.billboard.com/charts/hot-100/1965-0…\n$ week_id                <chr> \"7/17/1965\", \"7/24/1965\", \"7/31/1965\", \"8/7/196…\n$ week_position          <dbl> 34, 22, 14, 10, 8, 8, 14, 36, 97, 90, 97, 97, 9…\n$ song                   <chr> \"Don't Just Stand There\", \"Don't Just Stand The…\n$ performer              <chr> \"Patty Duke\", \"Patty Duke\", \"Patty Duke\", \"Patt…\n$ song_id                <chr> \"Don't Just Stand TherePatty Duke\", \"Don't Just…\n$ instance               <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ previous_week_position <dbl> 45, 34, 22, 14, 10, 8, 8, 14, NA, 97, 90, 97, 9…\n$ peak_position          <dbl> 34, 22, 14, 10, 8, 8, 8, 8, 97, 90, 90, 90, 90,…\n$ weeks_on_chart         <dbl> 4, 5, 6, 7, 8, 9, 10, 11, 1, 2, 3, 4, 5, 6, 1, …\n\n\nNotice that this time I just typed glimpse rather than dplyr::glimpse. I didn’t need to tell R to look in the dplyr namespace because I’d already attached it when I typed library(dplyr) earlier."
  },
  {
    "objectID": "posts/2021-09-14_tidy-tuesday-billboard/index.html#finding-britney",
    "href": "posts/2021-09-14_tidy-tuesday-billboard/index.html#finding-britney",
    "title": "Visualising the hits of Queen Britney",
    "section": "Finding Britney",
    "text": "Finding Britney\nTime to start analysing the data. I have made a decision that today I have love in my heart only for Britney. So what I want to do is find the rows in billboard that correspond to Britney Spears songs. The natural way to do this would be to pull out the “performer” column and then try to find entries that refer to Britney. The slightly tricky aspect to this is that Britney doesn’t appear solely as “Britney Spears”. For example, “Me Against The Music” features Madonna, and the entry in the performer column is “Britney Spears Featuring Madonna”. So we’re going to have to search in a slightly smarter way. Before turning this into R code, I can sketch out my plan like this:\nget the billboard data, THEN\n  pull out the performer column, THEN\n  search for britney, THEN\n  tidy up a bit\nThis kind of workflow is naturally suited to the “pipe”, which is written %>%. You’ll see referred to either as the “magrittr pipe” (referring to the magrittr package where it originally came from) or the “dplyr pipe” (because dplyr made it famous!). I’m sure you’ve seen it before, but since one goal of this post is to be a refresher, I’ll explain it again. The pipe does the same job as the word “THEN” in the pseudo-code I wrote above. Its job is to take the output of one function (whatever is on the left) and then pass it on as the input to the next one (on the right). So here’s that plan re-written in an “R-like” format:\nthe_billboard_data %>% \n  pull_out_the_performer_column() %>% \n  search_for_britney() %>% \n  tidy_it_up()\nIn fact that’s pretty close to what the actual R code is going to look like! The dplyr package has a function dplyr::pull() that will extract a column from the data set (e.g., all 327,895 listings in the performer column), and base R has a function called unique() that will ignore repeat entries, showing you only the unique elements of a column. So our code is going to look almost exactly like this\nbillboard %>% \n  pull(performer) %>% \n  search_for_britney() %>% \n  unique()\n\nPattern matching for text data\nInexcusably, however, R does not come with a search_for_britney() function, so we’re going to have to do it manually. This is where the stringr package is very helpful. It contains a lot of functions that are very helpful in searching for text and manipulating text. The actual function I’m going to use here is stringr::str_subset() which will return the subset of values that “match” a particular pattern. Here’s a very simple example, where the “pattern” is just the letter “a”. I’ll quickly define a variable animals containing the names of a few different animals:\n\nanimals <- c(\"cat\", \"dog\", \"rat\", \"ant\", \"bug\")\n\nTo retain only those strings that contain the letter \"a\" we do this:\n\nstr_subset(string = animals, pattern = \"a\")\n\n[1] \"cat\" \"rat\" \"ant\"\n\n\nAlternatively we could write this using the pipe:\n\nanimals %>% \n  str_subset(pattern = \"a\")\n\n[1] \"cat\" \"rat\" \"ant\"\n\n\nI’m not sure this second version is any nicer than the first version, but it can be helpful to see the two versions side by side in order to remind yourself of what the pipe actually does!\nWe can use the same tool to find all the Britney songs. In real life, whenever you’re working with text data you need to be wary of the possibility of mispellings and other errors in the raw data. Wild caught data are often very messy, but thankfully for us the Tidy Tuesday data sets tend to be a little kinder. With that in mind can safely assume that any song by Britney Spears will include the pattern “Britney” in it somewhere.\nSo let’s do just try this and see what we get:\n\nbillboard %>% \n  pull(performer) %>% \n  str_subset(\"Britney\") %>% \n  unique()\n\n[1] \"Britney Spears\"                              \n[2] \"Rihanna Featuring Britney Spears\"            \n[3] \"will.i.am & Britney Spears\"                  \n[4] \"Britney Spears & Iggy Azalea\"                \n[5] \"Britney Spears Featuring G-Eazy\"             \n[6] \"Britney Spears Featuring Madonna\"            \n[7] \"Britney Spears Featuring Tinashe\"            \n[8] \"Britney Spears Featuring Nicki Minaj & Ke$ha\"\n\n\n\n\nAt this point I was sorely tempted to get distracted by Ke$ha and Rihanna, but somehow managed to stay on topic. Somehow\nOkay, so it turns out that Britney is listed in eight different ways. For the sake of this post, I’m happy to include cases where another artist features on a Britney track, but I don’t want to include the two cases where Britney is the featuring artist. Looking at the output above, it seems like I can find those cases by keeping only those rows that start with the word “Britney”.\nNow our question becomes “how do we write down a pattern like that?” and the answer usually involves crying for a bit because the solution is to use a regular expression, or “regex”.\nRegular expressions are a tool used a lot in programming: they provide a compact way to represent patterns in text. They’re very flexible, but can often be quite hard to wrap your head around because there are a lot of special characters that have particular meanings. Thankfully, for our purposes today we only need to know one of them: the ^ character is used to mean “the start of the string”. So when interpreted as a regular expression, \"^Britney\" translates to “any string that begins with ‘Britney’”. Now that we have our regular expression, this works nicely:\n\nbillboard %>% \n  pull(performer) %>% \n  str_subset(\"^Britney\") %>% \n  unique()\n\n[1] \"Britney Spears\"                              \n[2] \"Britney Spears & Iggy Azalea\"                \n[3] \"Britney Spears Featuring G-Eazy\"             \n[4] \"Britney Spears Featuring Madonna\"            \n[5] \"Britney Spears Featuring Tinashe\"            \n[6] \"Britney Spears Featuring Nicki Minaj & Ke$ha\"\n\n\nRegular expressions are one of those things you’ll slowly pick up as you go along, and although they can be a huuuuuuge headache to learn, the reward is worth the effort.\n\n\nIn my mental list of “stuff I hatelove in data science”, git and regexes are tied for first place\n\n\nCreating the Britney data\nOkay so now we’re in a position to filter the billboard data, keeping only the rows that correspond to Britney songs. Most people in our slack group have taken an introductory class before, so you’ll be expecting that dplyr::filter() is the tool we need. The kind of filtering you’ve seen before looks like this:\n\nbritney <- billboard %>% \n  filter(performer == \"Britney Spears\")\n\nHowever, this doesn’t work the way we want. The bit of code that reads performer == \"Britney Spears\" is a logical expression (i.e., a code snippet that only returns TRUE and FALSE values) that will only detect exact matches. It’s too literal for our purposes. We can’t use the == operator to detect our regular expression either: that will only detect cases where the performer is literally listed as “^Britney”. What we actually want is something that works like the == test, but uses a regular expression to determine if it’s a match or not.\nThat’s where the str_detect() function from the stringr package is really handy. Instead of using performer == \"Britney Spears\" to detect exact matches, we’ll use str_detect(performer, \"^Britney\") to match using the regular expression:\n\nbritney <- billboard %>% \n  filter(str_detect(performer, \"^Britney\"))\n\n\n\nA confession. I didn’t technically need to use a regex here, because stringr has a handy str_starts() function. But half the point of our slack group is to accidentally-on-purpose reveal new tools and also I forgot that str_starts() exists so… regex it is\nThis version works the way we want it to, but it’s usually a good idea in practice to check that we haven’t made any mistakes. Perhaps I have forgotten what str_detect() actually does or I’ve made an error in my use of filter(), for example. So let’s take a look at the performer column in the britney data and check that it contains the same six unique strings:\n\nbritney %>% \n  pull(performer) %>% \n  unique()\n\n[1] \"Britney Spears\"                              \n[2] \"Britney Spears & Iggy Azalea\"                \n[3] \"Britney Spears Featuring G-Eazy\"             \n[4] \"Britney Spears Featuring Madonna\"            \n[5] \"Britney Spears Featuring Tinashe\"            \n[6] \"Britney Spears Featuring Nicki Minaj & Ke$ha\"\n\n\nThat’s reassuring. So let’s take a quick peek at the results of our data wrangling:\n\nglimpse(britney)\n\nRows: 468\nColumns: 10\n$ url                    <chr> \"http://www.billboard.com/charts/hot-100/2000-0…\n$ week_id                <chr> \"4/22/2000\", \"10/24/2009\", \"12/20/2008\", \"12/2/…\n$ week_position          <dbl> 67, 1, 3, 70, 70, 21, 17, 29, 76, 1, 26, 62, 65…\n$ song                   <chr> \"Oops!...I Did It Again\", \"3\", \"Circus\", \"Stron…\n$ performer              <chr> \"Britney Spears\", \"Britney Spears\", \"Britney Sp…\n$ song_id                <chr> \"Oops!...I Did It AgainBritney Spears\", \"3Britn…\n$ instance               <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ previous_week_position <dbl> NA, NA, NA, NA, NA, 45, NA, NA, NA, NA, 27, NA,…\n$ peak_position          <dbl> 67, 1, 3, 70, 70, 21, 17, 29, 76, 1, 26, 62, 65…\n$ weeks_on_chart         <dbl> 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1,…\n\n\nThat looks good to me…\n\n\nFixing the dates\n…or does it? Looking at the week_id column is enough to make any data analyst sigh in mild irritation. This column encodes the date, but the first two entries are \"4/22/2000\" and \"10/24/2009\". They are encoded in a “month/day/year” format. Nobody on this planet except Americans writes dates this way. Most countries use “day/month/year” as their standard way of writing dates, and most programming style guides strongly recommend “year/month/day” (there are good reasons for this, mostly to do with sorting chronologically). Worse yet, it’s just a character string. R doesn’t know that this column corresponds to a date, and unlike Excel it is smart enough not to try. Trying to guess what is and is not a date is notoriously difficult, so R makes that your job as the data analyst. Thankfully, the lubridate package exists to make it a little bit easier. In this case, where we have data in month/day/year format, the lubridate::mdy() function will do the conversion for us. You’ll be completely unsurprised to learn that there are lubridate::dmy() and lubridate::ymd() functions that handle other kinds of date formats.\nSo let’s do this. I’ll use the dplyr::mutate() function to modify the britney data, like so:\n\nbritney <- britney %>% \n  mutate(week_id = lubridate::mdy(week_id))\n\nglimpse(britney)\n\nRows: 468\nColumns: 10\n$ url                    <chr> \"http://www.billboard.com/charts/hot-100/2000-0…\n$ week_id                <date> 2000-04-22, 2009-10-24, 2008-12-20, 2000-12-02…\n$ week_position          <dbl> 67, 1, 3, 70, 70, 21, 17, 29, 76, 1, 26, 62, 65…\n$ song                   <chr> \"Oops!...I Did It Again\", \"3\", \"Circus\", \"Stron…\n$ performer              <chr> \"Britney Spears\", \"Britney Spears\", \"Britney Sp…\n$ song_id                <chr> \"Oops!...I Did It AgainBritney Spears\", \"3Britn…\n$ instance               <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ previous_week_position <dbl> NA, NA, NA, NA, NA, 45, NA, NA, NA, NA, 27, NA,…\n$ peak_position          <dbl> 67, 1, 3, 70, 70, 21, 17, 29, 76, 1, 26, 62, 65…\n$ weeks_on_chart         <dbl> 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1,…\n\n\nMuch better!"
  },
  {
    "objectID": "posts/2021-09-14_tidy-tuesday-billboard/index.html#visualising-a-queen",
    "href": "posts/2021-09-14_tidy-tuesday-billboard/index.html#visualising-a-queen",
    "title": "Visualising the hits of Queen Britney",
    "section": "Visualising a queen",
    "text": "Visualising a queen\nI’m now at the point that I have a britney data set I can visualise. However, being the queen she is, Britney has quite a few songs that appear in the Billboard Top 100, so the first thing I’ll do is specify a few favourites that we’ll highlight in the plots:\n\nhighlights <- c(\"Work B**ch!\", \"...Baby One More Time\", \"Toxic\")\n\nMost people in our slack will probably have encountered the ggplot2 package before, and at least have some experience in creating data visualisations using it. So we might write some code like this, which draws a plot showing the date on the horizontal axis (the mapping x = week_id) and the position of the song on the vertical axis (represented by the mapping y = week_position). We’ll also map the colour to the song by setting colour = song. Then we’ll add some points and lines:\n\nggplot(\n  data = britney,\n  mapping = aes(\n    x = week_id,\n    y = week_position,\n    colour = song\n  )\n) + \n  geom_line(show.legend = FALSE) + \n  geom_point(show.legend = FALSE)\n\n\n\n\nThe reason I’ve included show.legend = FALSE here is that there are quite a few different songs in the data, and if they were all added to a legend it wouldn’t leave any room for the data!\nWe can improve on this in a couple of ways. First up, let’s use scale_y_reverse() to flip the y-axis. That way, a top ranked song appears at the top, and a 100th ranked song appears at the bottom:\n\nbritney %>% \n  ggplot(aes(\n    x = week_id, \n    y = week_position, \n    colour = song\n  )) + \n  geom_line(show.legend = FALSE) + \n  geom_point(show.legend = FALSE) + \n  scale_y_reverse()\n\n\n\n\nNotice that I’ve switched to using the pipe here. I take the britney data, pipe it with %>% to the ggplot() function where I set up the mapping, and then add things to the plot with +. It’s a matter of personal style though. Other people write their code differently!\nOkay, it’s time to do something about the lack of labels. My real interest here is in the three songs I listed in the highlights so I’m going to use the gghighlight package, like this:\n\nbritney %>% \n  ggplot(aes(\n    x = week_id, \n    y = week_position, \n    colour = song\n  )) + \n  geom_line() + \n  geom_point() + \n  scale_y_reverse() + \n  gghighlight::gghighlight(song %in% highlights)\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\nTried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: song\n\n\n\n\n\nWhen the data are plotted like this, you get a strong sense of the chronology of Britney’s career, but the downside is that you can’t easily see how the chart performance of “…Baby One More Time” compares to the performance of “Toxic” and “Work B**ch!“. To give a better sense of that, it’s better to plot weeks_on_chart on the horizontal axis:\n\nbritney %>% \n  ggplot(aes(\n    x = weeks_on_chart, \n    y = week_position, \n    group = song,\n    colour = song\n  )) + \n  geom_line() + \n  geom_point() + \n  scale_y_reverse() + \n  gghighlight::gghighlight(song %in% highlights)\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\nTried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: song\n\n\n\n\n\nShown this way, you get a really strong sense of just how much of an impact “…Baby One More Time” had. It wasn’t just Britney’s first hit, it was also her biggest. It’s quite an outlier on the chart!\nIf we’re doing exploratory data analysis, and the only goal is to have a picture to show a colleague, that’s good enough. However, if we wanted to share it more widely, you’d probably want to spend a little more time fiddling with the details, adding text, colour and other things that actually matter a lot in real life!\n\nbritney %>% \n  ggplot(aes(\n    x = weeks_on_chart, \n    y = week_position, \n    group = song,\n    colour = song\n  )) + \n  geom_line(size = 1.5) + \n  scale_y_reverse() + \n  scale_color_brewer(palette = \"Dark2\") + \n  gghighlight::gghighlight(song %in% highlights, \n    unhighlighted_params = list(size = .5)) + \n  theme_minimal() +\n  labs(\n    title = \"Britney Spears' first hit was also her biggest\",\n    subtitle = \"Chart performance of Britney Spears' songs\",\n    x = \"Weeks in Billboard Top 100\",\n    y = \"Chart Position\"\n  )\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: song\n\n\n\n\n\nIf I were less lazy I would also make sure that the chart includes a reference to the original data source, and something that credits myself as the creator of the plot. That’s generally good etiquette if you’re planning on sharing the image on the interwebs. There’s quite a lot you could do to tinker with the plot to get it to publication quality, but this is good enough for my goals today!\n\n\n\n\n\nHer Royal Highness Britney Spears, performing in Las Vegas, January 2014. Figure from wikimedia commons, released under a CC-BY-2.0 licence by Rhys Adams"
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#what-is-grid",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#what-is-grid",
    "title": "Generative art with grid",
    "section": "What is grid?",
    "text": "What is grid?\nThe grid package provides the underlying graphics system upon which ggplot2 is built. It’s one of two quite different drawing systems that are included in base R: base graphics and grid. Base graphics has an imperative “pen-on-paper” model: every function immediately draws something on the graphics device. Much like ggplot2 itself, grid takes a more declarative approach where you build up a description of the graphic as an object, which is later rendered. This declarative approach allows us to create objects that exist independently of the graphic device and can be passed around, analysed, and modified. Importantly, parts of a graphical object can refer to other parts, which allows you to do things like define rectangle A to have width equal to the length of text string B, and so on.\nThis blog post – and the corresponding section in the book, should we decide to include it – isn’t intended to be a comprehensive introduction to grid. But it does cover many of the core concepts and introduces key terms like grobs, viewports, graphical parameters, and units. Hopefully it will make sense even if you’re completely new to grid."
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#pushing-viewpoints",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#pushing-viewpoints",
    "title": "Generative art with grid",
    "section": "Pushing viewpoints",
    "text": "Pushing viewpoints\n\nvp <- viewport(angle = 17, width = .8, height = .8)\nbox <- rectGrob(width = 1, height = 1)\ngrid.newpage()\nfor(i in 1:20) {\n  pushViewport(vp)\n  grid.draw(box)\n}\n\n\n\n\nA quirky side effect:\n\nset.seed(1)\nn <- 15\nsurprises <- surpriseGrob(\n  x = runif(n, min = .1, max = .9),\n  y = runif(n, min = .1, max = .9),\n  size = runif(n, min = 0, max = .15),\n  threshold = 1\n)\n\nvp_spiral <- viewport(width = .95, height = .95, angle = 10)\n\ngrid.newpage()\nfor(i in 1:30) {\n  pushViewport(vp_spiral)\n  grid.draw(surprises)\n}"
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#pushing-viewports",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#pushing-viewports",
    "title": "Generative art with grid",
    "section": "Pushing viewports",
    "text": "Pushing viewports\nA nice feature of grid is that viewports can be nested within other viewports. At the top level there is always “root” viewport that encompasses the entire image. By default user-created viewports are children of the root viewport, and inherit properties from it. However, there’s nothing stopping you from assigning new viewports to be children of previous user-generated viewports. In the simplest case4, we can use this to create a viewport stack in which each new viewport is the child of the previous one. The pushViewport() function allows us to do this.\nHere’s an example:\n\nvp <- viewport(angle = 17, width = .8, height = .8)\nbox <- rectGrob(width = 1, height = 1)\ngrid.newpage()\nfor(i in 1:20) {\n  pushViewport(vp)\n  grid.draw(box)\n}\n\n\n\n\nIn this code I define vp to be a viewport that shrinks the width and height of the current viewport to be 80% of its parent, and rotates the frame by 17 degrees.b Then I repeatedly push vp to the viewport stack, and draw a border (the box grob) showing the edges of that viewport. The effects of each push to the stack are cumulative, as the image shows.\nIt’s also quite pretty.\nThe grid package has a lot of tools for working with viewport lists, stacks, and trees. You can assign names to viewports, navigate back and forth between different viewports during plot construction, and so on. But that’s a topic for another day."
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#piecing-it-together",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#piecing-it-together",
    "title": "Generative art with grid",
    "section": "Piecing it together",
    "text": "Piecing it together\n\nspiral_surprise <- function(seed) {\n  \n  set.seed(seed)\n  n <- 15\n  surprises <- surpriseGrob(\n    x = runif(n, min = .1, max = .9),\n    y = runif(n, min = .1, max = .9),\n    size = runif(n, min = 0, max = .15),\n    threshold = 1\n  )\n  \n  vp_spiral <- viewport(width = .95, height = .95, angle = 10)\n  \n  grid.newpage()\n  for(i in 1:30) {\n    pushViewport(vp_spiral)\n    grid.draw(surprises)\n  }\n}\n\n\nspiral_surprise(1)\n\n\n\n\n\nspiral_surprise(2)\n\n\n\n\n\nspiral_surprise(3)"
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#spiralling-surprises",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#spiralling-surprises",
    "title": "Generative art with grid",
    "section": "Spiralling surprises",
    "text": "Spiralling surprises\nI’m pleasantly surprised at how easy it is to create interesting generative art with grid. As a really simple example, here’s a system that combines two tricks: it uses the surpriseGrob() that we defined earlier, and it uses a viewport stack to create spiraling images:\n\nspiral_surprise <- function(seed) {\n  \n  set.seed(seed)\n  n <- 15\n  surprises <- surpriseGrob(\n    x = runif(n, min = .1, max = .9),\n    y = runif(n, min = .1, max = .9),\n    size = runif(n, min = 0, max = .15),\n    threshold = 1\n  )\n  \n  vp_spiral <- viewport(width = .95, height = .95, angle = 10)\n  \n  grid.newpage()\n  for(i in 1:30) {\n    pushViewport(vp_spiral)\n    grid.draw(surprises)\n  }\n}\n\nIt produces output like this…\n\nspiral_surprise(1)\n\n\n\n\n… and this …\n\nspiral_surprise(2)\n\n\n\n\n… and this.\n\nspiral_surprise(3)"
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#trans-spirals",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#trans-spirals",
    "title": "Generative art with grid",
    "section": "Trans spirals",
    "text": "Trans spirals\nI’m pleasantly surprised at how easy it is to create interesting generative art with grid. As a really simple example, here’s a system that combines two tricks: it uses the transGrob() that we defined earlier, and it uses a viewport stack to create spiraling images:\n\ntrans_spiral <- function(seed) {\n  \n  set.seed(seed)\n  n <- 15\n  trans <- transGrob(\n    x = runif(n, min = .1, max = .9),\n    y = runif(n, min = .1, max = .9), \n    size = runif(n, min = 0, max = .15),\n    threshold = 1\n  )\n  \n  cols <- sample(c(\"#5BCEFA\", \"#F5A9B8\", \"#FFFFFF\"), 30, TRUE)\n  \n  vp_spiral <- viewport(width = .95, height = .95, angle = 10)\n  \n  grid.newpage()\n  for(i in 1:30) {\n    pushViewport(vp_spiral)\n    grid.draw(grobTree(trans, gp = gpar(fill = cols[i])))\n  }\n}\n\nIt produces output like this…\n\ntrans_spiral(1)\n\n\n\n\n… and this …\n\ntrans_spiral(2)\n\n\n\n\n… and this.\n\ntrans_spiral(3)"
  }
]