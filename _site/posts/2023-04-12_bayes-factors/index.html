<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Danielle Navarro">
<meta name="dcterms.date" content="2023-04-12">
<meta name="description" content="In which the dead return to life and the author has opinions">

<title>Notes from a data witch - A personal essay on Bayes factors</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>
    .quarto-title-block .quarto-title-banner {
      background: #eeeeee;
    }
    </style>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Notes from a data witch - A personal essay on Bayes factors">
<meta property="og:description" content="In which the dead return to life and the author has opinions">
<meta property="og:image" content="https://blog.djnavarro.net/posts/2023-04-12_bayes-factors/katy.jpg">
<meta property="og:site-name" content="Notes from a data witch">
<meta name="twitter:title" content="Notes from a data witch - A personal essay on Bayes factors">
<meta name="twitter:description" content="In which the dead return to life and the author has opinions">
<meta name="twitter:image" content="https://blog.djnavarro.net/posts/2023-04-12_bayes-factors/katy.jpg">
<meta name="twitter:creator" content="@djnavarro">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Notes from a data witch</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://fosstodon.org/@djnavarro"><i class="bi bi-mastodon" role="img" aria-label="mastodon">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/djnavarro"><i class="bi bi-github" role="img" aria-label="github">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://djnavarro.net"><i class="bi bi-person-circle" role="img" aria-label="website">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://art.djnavarro.net"><i class="bi bi-palette-fill" role="img" aria-label="art">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">A personal essay on Bayes factors</h1>
                  <div>
        <div class="description">
          In which the dead return to life and the author has opinions
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Statistics</div>
                <div class="quarto-category">Bayes</div>
              </div>
                  </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-heading">Affiliation</div>
    
      <div class="quarto-title-meta-contents">
      <a href="https://djnavarro.net">Danielle Navarro</a> <a href="https://orcid.org/0000-0001-7648-6578" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              <a href="https://www.youtube.com/watch?v=j58V2vC9EPc">
              I’m on smoko
              </a>
            </p>
        </div>
      </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 12, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#the-post" id="toc-the-post" class="nav-link active" data-scroll-target="#the-post">The post…</a></li>
  <li><a href="#love-at-first-sight" id="toc-love-at-first-sight" class="nav-link" data-scroll-target="#love-at-first-sight">Love at first sight</a></li>
  <li><a href="#seeds-of-doubt" id="toc-seeds-of-doubt" class="nav-link" data-scroll-target="#seeds-of-doubt">Seeds of doubt</a></li>
  <li><a href="#seeing-other-statistics-at-least-thats-what-i-said-i-was-doing" id="toc-seeing-other-statistics-at-least-thats-what-i-said-i-was-doing" class="nav-link" data-scroll-target="#seeing-other-statistics-at-least-thats-what-i-said-i-was-doing">Seeing other statistics (at least that’s what I said I was doing)</a></li>
  <li><a href="#what-problems-do-we-study" id="toc-what-problems-do-we-study" class="nav-link" data-scroll-target="#what-problems-do-we-study">What problems do we study?</a></li>
  <li><a href="#its-the-little-things" id="toc-its-the-little-things" class="nav-link" data-scroll-target="#its-the-little-things">It’s the little things</a></li>
  <li><a href="#what-to-do" id="toc-what-to-do" class="nav-link" data-scroll-target="#what-to-do">What to do?</a></li>
  </ul>
</nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">




<!--------------- my typical setup ----------------->
<!--------------- post begins here ----------------->
<blockquote class="blockquote">
<p>You like my gin and tonic kisses ’cause you know they taste so sweet<br> And I know you got your missus, but there ain’t no one like me<br> &nbsp;&nbsp; –Elle King and Miranda Lambert</p>
</blockquote>
<p>So. As my loyal<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> readers may have worked out, I have decided that now is the time to resurrect blog posts from my dark past. Oh sure, I can walk in the bright and blessed world of data science and adorn myself in <a href="https://www.imdb.com/title/tt9079692/">all that hard glossy armour</a> that is the perquisite of trade, but I have a dark past. Once upon a time I <a href="https://www.quotes.net/mquote/56448">delved too deep</a> into the dark realms of statistics. I fought with dragons (frequentists), trolls (objective Bayesians), and balrogs (subjective Bayesians). Occasionally I emerged victorious, in the sense that I am not yet dead. In this respect I am proud of myself, but oh my… to survive this in perilous land, I too have had to scorch the earth and burn all the bridges. These are dangerous paths to tread, and there are few things a statistician loves more than a whimsical death.</p>
<p>Yeah, anyway. I have opinions about <a href="https://en.wikipedia.org/wiki/Bayes_factor">Bayes factors</a>, and since I am no longer required to give a fuck about academic norms, I shall raise them from the dead. Or, to use normal language, I’m gonna repost a rant I wrote a few years back because I bloody well can…</p>
<div class="column-page">
<p><img src="katy.jpg" class="img-fluid"></p>
</div>
<p><br></p>
<section id="the-post" class="level2">
<h2 class="anchored" data-anchor-id="the-post">The post…</h2>
<blockquote class="blockquote">
<p>’Cause you’re hot then you’re cold <br> You’re yes then you’re no <br> You’re in then you’re out <br> You’re up then you’re down <br> You’re wrong when it’s right <br> It’s black and it’s white<br> &nbsp;&nbsp; –Katy Perry</p>
</blockquote>
<p>I have mixed feelings about Bayes factors. As Katy Perry once observed, it’s <em>extremely</em> <a href="https://www.youtube.com/watch?v=kTHNpusq654">hard to make valid inferences</a> when you aren’t even sure what the hell it is you’re trying to make inferences about. Oh sure, it’s easy enough to tell pretty stories about rational reasoning with respect to a prior, but if we’re going to have a serious statistical relationship you need to bring more than a <a href="https://en.wikipedia.org/wiki/Dutch_book">Dutch book argument</a> to the table.</p>
<p><br></p>
</section>
<section id="love-at-first-sight" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="love-at-first-sight">Love at first sight</h2>
<p>I first discovered Bayes factors in 1999, as a new Ph.D.&nbsp;student working on problems in similarity judgment and categorisation. I read <a href="https://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476572">Kass and Raftery (1995)</a> and was head over heels. As long as one accepts the principle of inverse probability, a Bayesian reasoner can evaluate a hypothesis <span class="math inline">\(h\)</span> in light of data <span class="math inline">\(d\)</span> in a simple fashion:</p>
<p><span class="math display">\[
P(h|d) = \frac{P(d|h) P(h)}{P(d)}
\]</span></p>
<p>I suspect most people reading this post already knows what Bayes rule says, but not everyone who follows me cares that much so let’s break this down:<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<ul>
<li><span class="math inline">\(P(h)\)</span> is my <em>prior belief</em>: how plausible was <span class="math inline">\(h\)</span> before the data arrived?</li>
<li><span class="math inline">\(P(h|d)\)</span> is my <em>posterior belief</em>: how plausible is <span class="math inline">\(h\)</span> now that I’ve seen the data?</li>
<li><span class="math inline">\(P(d|h)\)</span> is the <em>likelihood</em>: the probability that we would have observed data <span class="math inline">\(d\)</span> if the hypothesis <span class="math inline">\(h\)</span> describes the true data generating mechanism <a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></li>
</ul>
<p>When comparing two competing hypotheses <span class="math inline">\(h_0\)</span> and <span class="math inline">\(h_1\)</span>, I can compute the posterior odds favouring one over the other simply by dividing the two posterior probabilities,</p>
<p><span class="math display">\[
\frac{P(h_1 | d)}{P(h_0 | d)} = \frac{P(d|h_1)}{P(d|h_0)} \times \frac{P(h_1)}{P(h_0)}
\]</span></p>
<p>Or, in something closer to every day language:</p>
<p><span class="math display">\[
\mbox{posterior odds} = \mbox{Bayes factor} \times \mbox{prior odds}
\]</span></p>
<p>Thus the Bayes factor (BF) is defined by the ratio of the two likelihoods, and it has a natural interpretation as a <em>weight of evidence</em>. It tells me how I need to adjust my beliefs in light of data. And it’s so simple…</p>
<p><span class="math display">\[
\mbox{BF} = \frac{P(d|h_1)}{P(d|h_0)}
\]</span></p>
<p>What’s not to love?</p>
<p>Better yet, it even extends naturally from simple hypotheses to full fledged models. Suppose I have a theoretically meaningful computational model <span class="math inline">\(\mathcal{M}\)</span> for some psychological phenomenon, with parameter(s) <span class="math inline">\(\theta\)</span>. For any choice of parameter values <span class="math inline">\(\theta\)</span> my model provides me with a likelihood function for the data <span class="math inline">\(P(d|\theta)\)</span>, and my researcher knowledge of the world provides a prior <span class="math inline">\(P(\theta|\mathcal{M})\)</span> belief about the relative plausibility of different parameters. So the <em>a priori</em> prediction that my model makes about the probability of observing data <span class="math inline">\(d\)</span> in my experiment is calculated with the <em>marginal likelihood</em> <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p><span class="math display">\[
P(d | \mathcal{M}) = \sum_\theta P(d | \theta) P(\theta | \mathcal{M})
\]</span></p>
<p>The intuition is dead simple (or so I thought at the time)…. if I don’t know which parameter <span class="math inline">\(\theta\)</span> is the right one, I should hedge my bets by constructing an appropriate weighted average. Easy-peasy. This gives me a Bayes factor that I can use to compare two computational models like so:</p>
<p><span class="math display">\[
\mbox{BF} = \frac{P(d|\mathcal{M}_1)}{P(d|\mathcal{M}_0)}
\]</span></p>
<p>Honestly, I don’t see why this “statistics business” is so hard, I thought. All you have to do to scale up from simple hypotheses to serious theory evaluation is turn an italicised <span class="math inline">\(h\)</span> into a squiggly <span class="math inline">\(\mathcal{M}\)</span> and you’re done! I read the Myung and Pitt (1997) paper on <a href="https://link.springer.com/article/10.3758/BF03210778">model selection with Bayes factors</a> and thought yep, this is it. Problem solved. Easy!</p>
<p>Oh, you sweet summer child.</p>
<p><br></p>
<div class="column-page">
<p><img src="lana.jpg" class="img-fluid"></p>
</div>
</section>
<section id="seeds-of-doubt" class="level2">
<h2 class="anchored" data-anchor-id="seeds-of-doubt">Seeds of doubt</h2>
<blockquote class="blockquote">
<p>I’m feelin’ electric tonight <br> Cruisin’ down the coast, goin’ about 99<br> Got my bad baby by my heavenly side<br> I know if I go, I’ll die happy tonight<br> &nbsp;&nbsp; – Lana Del Rey</p>
</blockquote>
<p>During my PhD I used Bayes factors (or similar tools) a <em>lot</em>. One of my very first papers (<a href="https://papers.djnavarro.net/2003_hybridsimilarity.">this one</a> sought to combine multidimensional scaling methods with overlapping clustering methods in a way that would allow someone to estimate stimulus representations that have both continuous and discrete parts (e.g., our intuitions about number are partly continuous insofar as they pertain to magnitude, but also discrete when they pertain to other arithmetic properties), using Laplace approximations to the Bayes factor to automatically determine the appropriate number of clusters and dimensions. The technique had some problems. Collections of weighted binary features (as used in featural reprentations; <a href="https://doi.org/10.1037/0033-295X.84.4.327">Tversky 1977</a>, <a href="https://doi.org/10.1037/0033-295X.86.2.87">Shepard and Arabie 1979</a>) induce a <em>qualitatively different parameter space</em> than co-ordinates in a Minkowski space<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> (<a href="https://doi.org/10.1007/BF02291665">Shepard 1974</a>), and so when you try to mix them together into a hybrid similarity representation you get… weirdness.</p>
<p>Any time you compute the marginal likelihood <span class="math inline">\(P(d | \mathcal{M})\)</span> you are implicitly introducing a penalty for excess complexity, so the Bayes factor incorporates a form of automatic Ockham’s razor. But when I built the hybrid model I found that the (implied) penalty term for <em>“adding one more continuous dimension for an MDS solution”</em> doesn’t seem to be commensurate with the (implied) penalty term for <em>“adding one more discrete feature”</em> and while I could get some decent solutions in some cases (the numbers example worked pretty well…) I never did find a general version that would “just work”.</p>
<p>I put it down to the fact that the priors <span class="math inline">\(P(\theta|\mathcal{M})\)</span> were kind of ad hoc… after all, I didn’t know what would make sense as a plausible prior that would render continuous things and discrete things commensurate with one another in a way that made sense for the psychological problems I wanted to solve. I assumed the right answer would come to me one day.</p>
<p>It hasn’t yet, but I’m still hoping it will.</p>
<p><br></p>
</section>
<section id="seeing-other-statistics-at-least-thats-what-i-said-i-was-doing" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="seeing-other-statistics-at-least-thats-what-i-said-i-was-doing">Seeing other statistics (at least that’s what I said I was doing)</h2>
<blockquote class="blockquote">
<p>We lay on the bed there <br> Kissing just for practice <br> Could we please be objective? <br> ’Cause the other boys are queuing up behind us <br> &nbsp;&nbsp; – Belle &amp; Sebastian</p>
</blockquote>
<p>At about this point in time, I became fascinated with some of Jay Myung and Mark Pitt’s other papers on alternative ways to do model selection. For instance, in <a href="https://doi.org/10.1037/0033-295X.109.3.472">2003</a> they advocated the use of model selection by minimum description length (MDL). The MDL approach to statistical inference comes out of algorithmic information theory and can be viewed as a stripped down form of Kolmogorov complexity (KC). In KC we would say something like this…</p>
<blockquote class="blockquote">
<p>The Kolmogorov complexity of a string S with respect to programming language L is the length (in bits) of the shortest program P that prints S and then halts.</p>
</blockquote>
<p>… so the idea would be to think of a model <span class="math inline">\(\mathcal{M}\)</span> as a program and use it as a tool to compress the data <span class="math inline">\(d\)</span>. Whichever model compresses the data the most is the winner. Strictly speaking, KC is useless in real life because it’s uncomputable<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> but there are many ways of taking the idea and transforming it to something that you can use. The best known (I think?) is <a href="https://doi.org/10.1109/18.481776">Jorma Rissanen’s</a> stochastic complexity approach (borrowing from work by Shtarkov) but I’ve always had a soft spot for <a href="https://academic.oup.com/comjnl/article-abstract/42/4/270/558949">Wallace and Dowe’s</a> explicitly Bayesian formulation of the problem.</p>
<p>As you can probably tell, during the early 2000s I read a lot of statistics papers that I didn’t understand all that well.</p>
<p>What I did notice though is that many of these techniques end up constructing some version of the marginal likelihood <span class="math inline">\(P(d|\mathcal{M})\)</span>. They all have different motivations and not all of them allow a clear probabilistic interpretation (Rissanen doesn’t endorse a Bayesian interpretation of MDL, for instance), but they have more in common with one another than I’d originally thought. I even started reading some <a href="https://doi.org/10.1073/pnas.170283897">information geometry</a> and found roughly the same thing. A large number of these model selection criteria can be viewed as series expansions of <span class="math inline">\(\ln P(d|\mathcal{M})\)</span>, with “small” terms omitted (usually <span class="math inline">\(O(1)\)</span>). Yay, I thought! This is fantastic. Particulars notwithstanding, there is a strong theoretical justification for basing my inferences on the marginal likelihood.</p>
<p>It didn’t take long for my enthusiasm to fade again. The first time I tried to use this for model selection in the wild (selecting between different retention functions in recall memory tasks) I broke it <a href="https://www.mitpressjournals.org/doi/10.1162/0899766041336378">pretty badly</a>. It turns out that <span class="math inline">\(O(1)\)</span> terms can be <em>very fucking large</em> in practice, and you can get all sorts of absurd results (e.g., a nested model that is judged to be more complex than the full one) when you use these model selection criteria with “small” (say, a mere 1000 or so observations) samples.</p>
<p>I expanded my dating pool further. I had an on again off again thing with Bayesian nonparametrics (<a href="http://dx.doi.org/10.1016/j.jmp.2005.11.006">here</a>, <a href="http://dx.doi.org/10.1162/neco.2008.04-07-504">here</a>, <a href="https://doi.org/10.1037/rev0000077">here</a>), I dated <a href="http://dx.doi.org/10.1016/j.jmp.2005.06.008">normalised maximum likelihood</a>, and various other things besides. They all let me down somehow. It turns out that NML is mostly useless in real life, Bayesian nonparametric models don’t converge to anything sensible in some situations, and so on.</p>
<p>I never dated a p-value though. I do have standards.</p>
<p><br></p>
<p><br></p>
<div class="column-page">
<p><img src="taylor.jpg" class="img-fluid"></p>
</div>
</section>
<section id="what-problems-do-we-study" class="level2">
<h2 class="anchored" data-anchor-id="what-problems-do-we-study">What problems do we study?</h2>
<blockquote class="blockquote">
<p>But I got smarter, I got harder in the nick of time <br> Honey, I rose up from the dead, I do it all the time <br> I’ve got a list of names and yours is in red, underlined <br> &nbsp;&nbsp; – Taylor Swift</p>
</blockquote>
<p>Just lately I’ve been wondering how many of the practical problems I’ve encountered stem from the fact that almost no statistical problems worth caring about are <span class="math inline">\(\mathcal{M}\)</span>-closed (this time around it’s <a href="https://twitter.com/dan_p_simpson/status/991621450837741569">Dan Simpson’s fault</a> I’m thinking about this, but it’s been a recurring theme in my thoughts for a long time). At the moment I’m reading <a href="https://projecteuclid.org/download/pdfview_1/euclid.ba/1378729923">this paper</a> by Clarke, Clarke and Yu (2013), and I’ll steal their words. The first paragraph of the paper starts with this</p>
<blockquote class="blockquote">
<p>Prediction problems naturally fall into three classes, namely M-closed, M-complete, and M-open, based on the properties of the data generator (DG) (Bernardo and Smith 2000). Briefly, M-closed problems are those where it is reasonable to assume that the true model is one of the models under consideration, i.e., the true model is actually on the model list (at least in the sense that error due to model mis-specification is negligible compared to any other source of error). This class of problems is comparatively simple and well studied.</p>
</blockquote>
<p>Ouch. That’s about 99% of the statistical methodology that I was taught (and see in the psychological literature) and they’ve discarded it as too simplistic to be bothered talking about. It’d hurt less if they weren’t entirely correct. Almost all of what we talk about in psychology frames the problem of inference as one of “choosing the true model”, and it’s implicit that one of the models is presumed to be correct.</p>
<p>This is never accurate in real life. We often hand wave this way by quoting George Box’s famous aphorism <em>all models are wrong but some are useful</em>, yet we are rarely explicit in psychology in saying what we mean by “useful”. At one point I tried formulating what I thought I meant: for many cognitive science experiments that are designed to be “operationalised” versions of a more complex real world situation, I think it makes little sense to bother making predictions about low-level features of the data, and a model is most useful if when makes the <a href="http://dx.doi.org/10.1037/0033-295X.113.1.57">correct a priori predictions about theoretically-relevant ordinal patterns in the data</a>. But that’s not a very generalisable criterion, it doesn’t apply in situations where you actually do have to care about all the features in the data, and so on. I’ve never seen anyone come up with anything that I found compelling either.</p>
<p>That’s the thing about stepping outside of the <span class="math inline">\(\mathcal{M}\)</span>-closed world… nothing really works the way it’s supposed to. In the most difficult case you have <span class="math inline">\(\mathcal{M}\)</span>-open problems:</p>
<blockquote class="blockquote">
<p>M-open problems are those in which the DG does not admit a true model. The DG is so complex (in some sense) that there is no true model that we can even imagine. For instance, one can regard the Collected Works of William Shakespeare as a sequence of letters. Unarguably this data set had a DG (William Shakespeare), but it makes no sense to model the mechanism by which the data was generated. One might try to use the first n letters to predict the n + 1 letter and do better than merely guessing, but one should not expect such a predictor, or any model associated with it, to generate more great literature. The same point applies to the nucleotide sequence in a chromosome, the purchases of a consumer over time, and many other settings. In these cases, we are only able to compare different predictors without reference to a true model.</p>
</blockquote>
<p>Oh yes. <span class="math inline">\(\mathcal{M}\)</span>-open problems are <em>nasty</em>. You have to find some sensible way to discuss what it means to make good prediction that doesn’t rely on any notion of “true models”, because there is no sense in which the data generating mechanism can possibly be mapped to anything that you or I would <em>ever</em> call a “model”. I suspect that this is part of the reason why some of the MDL people (e.g.&nbsp;Jorma Rissanen) <em>don’t</em> want to formulate their model selection procedures with reference to any notion of a “true model”. The moment you allow yourself the “crutch” of assuming that a true model exists, you’re left unable to justify any claims in an <span class="math inline">\(\mathcal{M}\)</span>-open world. Clarke et al comment on that actually…</p>
<blockquote class="blockquote">
<p>From a log-loss point of view, the Shtarkov solution (Shtarkov 1987) has also been extensively studied in the M-open case, see Cesa-Bianchi and Lugosi (2006), but has not caught on partially because the conditions for it to exist are so narrow</p>
</blockquote>
<p>… where (assuming it’s the paper I’m thinking of) Shtarkov’s work is linked to Rissanen’s approach to MDL that some folks in psychology (such as myself, once upon a time!) had argued for. But it’s like Clarke et al say, this approach is basically useless in real life because there are so few scenarios where you can <em>do</em> anything with it.</p>
<p>On the other hand, there’s a sense in which the <span class="math inline">\(\mathcal{M}\)</span>-open scenario above is more pessimistic than it needs to be. Not every statistical problem is as hard as generating new Shakespeare novels….</p>
<blockquote class="blockquote">
<p>By contrast, M-complete problems are those where the DG has a true model that can be imagined but is not identifiable in any closed form. Inability to write a model explicitly may arise because the model is too complicated or because its constituent pieces are not known. The key point for an M-complete problem is that it is plausible to assume that a true model - also called a “belief model” - exists because this enables its use in reasoning even if a prior cannot be meaningfully assigned in the usual way. For instance, if a true model exists a bias-variance decomposition can be developed, at least in principle, even when the true model is not explicitly known.</p>
</blockquote>
<p>I think this is where most of our practical problems in science lie. If we knew enough about a phenomenon to put us in <span class="math inline">\(\mathcal{M}\)</span>-closed world we wouldn’t bother to study it, and if we knew so little that we couldn’t even imagine a true model (putting us in <span class="math inline">\(\mathcal{M}\)</span>-open land) it would be foolish to try. So in practice we live in the land of <span class="math inline">\(\mathcal{M}\)</span>-complete inference problems. There are interesting results in this situation. I haven’t read much about this in a long time, but my recollection from earlier reading was that in this situation a Bayes factor selection procedure will asymptotically converge to the model <span class="math inline">\(\mathcal{M}\)</span> that is closest to the true distribution in Kullback-Leibler divergence.</p>
<p>I used to find this reassuring. I’m less sure now.</p>
<p><br></p>
</section>
<section id="its-the-little-things" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="its-the-little-things">It’s the little things</h2>
<blockquote class="blockquote">
<p>Oh, life is bigger <br> It’s bigger <br> Than you and you are not me <br> The lengths that I will go to <br> The distance in your eyes <br> Oh no, I’ve said too much <br> I set it up &nbsp;&nbsp; – R.E.M.</p>
</blockquote>
<p>The worry I have with leaning so heavily on “convergence in KL terms” comes from a few sources. For one thing I’m starting to go beyond the limits of my own skill. You actually have to have a very good grasp of the theory to know what the hell this actually means, and I’m not sure I do. I’m a little unsure about what <em>practical</em> conclusions I should draw about a model if all I can say is that it is closer to the truth in the sense of a very specific information distance measure defined over distributions.</p>
<p>The impression I have had when working with KL divergence is that it really does seem to depend on every property of the distributions, but as a researcher I often don’t care about every little thing in the data. Worse, to the extent that Bayes factors specifically depend on the <em>prior</em> to specify the marginal distribution in question, I have this intuition that even modest mistakes in you specify the prior (especially the tails) could do very strange things. Looking back over the various papers I’ve written about in this post, I feel like it’s been a recurring theme that the details really matter. Just in this little reminiscence…</p>
<ul>
<li>When thinking about similarity modelling, I found stimulus features and stimulus dimensions don’t seem to have commensurate complexity as judged by the most sensible Bayesian method I could think of</li>
<li>When doing very simple memory modelling, the best approximations I knew of (Fisher information approximation to MDL) gave absurd predictions because of the weird structure of the models</li>
<li>In categorisation, when using “infinite dimensional” nonparametric Bayesian models … oh, don’t even get me started.</li>
</ul>
<p>… and the thing is <em>these issues have caused my inferences to misbehave every single time I have tried to automate them</em>.</p>
<p>In real world data analysis, nothing works the way it’s supposed to and I have grown deeply skeptical that <em>any</em> rule governed approach to automating statistical inference makes much sense.</p>
<p><br></p>
<div class="column-page">
<p><img src="florence.jpg" class="img-fluid"></p>
</div>
</section>
<section id="what-to-do" class="level2">
<h2 class="anchored" data-anchor-id="what-to-do">What to do?</h2>
<blockquote class="blockquote">
<p>’Cause I’m gonna be free and I’m gonna be fine <br> (Holding on for your call) <br> ’Cause I’m gonna be free and I’m gonna be fine <br> (Maybe not tonight) <br> &nbsp;&nbsp; – Florence and the Machine</p>
</blockquote>
<p>Honestly, I don’t know. I like Bayesian inference a great deal, and I still find Bayes factors useful in those circumstances where I (1) trust the prior, (2) understand the models and (3) have faith in the (either numerical or analytic) approximations used to estimate it. I don’t have a better alternative, and I’m certainly not going to countenance a return to using p-values<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. More than anything else, the one thing I don’t want to see happen is to have the current revival off Bayesian methods in psychology ossify into something like what happened with p-values.</p>
<p>What I think happened there is not necessarily that p-values are inherently useless and that’s why our statistics went bad. Rather, it’s that introductory methods classes taught students that there was A RIGHT WAY TO DO THINGS and those students became professors and taught other students and eventually we ended up with an absurd dogs breakfast of an inference system that (I suspect) even Fisher or Neyman would have found ridiculous. If I’ve learned nothing else from my research on <a href="http://dx.doi.org/10.1111/cogs.12667">cultural evolution and iterated learning</a> it’s that a collection of perfectly-rational learners can in fact ratchet themselves into believing foolish things, and that it’s the agents with most extreme biases that tend to dominate how the system evolves.</p>
<p>Whatever we do with Bayesian methods, whatever role Bayes factors play, whether we use default or informed priors, the one thing I feel strongly about is this… we should try to avoid anything that resembles a prescriptive approach to inference that instructs scientists THIS IS HOW WE DO IT and instills in them the same fear of the Bayesian gods that I was once taught to have for the frequentist deities.</p>
<p>It doesn’t help anyone, and it makes science worse.</p>
<!--------------- appendices go here ----------------->


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>lol<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Throughout this post I will follow my usual tendency to ignore things like the difference between densities and probabilities, and I will absolutely <em>not</em> waste everyone’s time by introducing <span class="math inline">\(\sigma\)</span>-algebras, because this is a blog post not a bloody measure theory textbook. If such things unnerve you too greatly, I refer you whichever section of <a href="https://books.google.com.au/books?hl=en&amp;lr=&amp;id=s5LHBgAAQBAJ&amp;oi=fnd&amp;pg=PR7&amp;dq=schervish+theory+of+statistics&amp;ots=qZHiGJ_sA5&amp;sig=-KkG3LxmWEFikss6L9PP1wC159E#v=onepage&amp;q=schervish%20theory%20of%20statistics&amp;f=false">Mark Schervish’s very excellent textbook</a> will allow you to feel love again.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Again… if you feel inclined to instruct me on the difference between <span class="math inline">\(P(x|\theta)\)</span> and <span class="math inline">\(\mathcal{L}(\theta|x)\)</span>… don’t. Go take it up with Fisher’s ashes. He’s the one who tried to misuse the ordinary natural language meaning of the word “likelihood” by inappropriately attaching probabilistic connotations to a score function that frequentists are explicitly forbidden to interpret as a probability<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>No seriously. Go visit his ashes. Fisher retired to St Mark’s college in Adelaide, and his ashes are kept in St Peter’s Cathedral in North Adelaide, a short walk from the University. The staff there are very friendly and will gladly show you to them.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>IF YOU EMAIL ME TO TALK ABOUT UNCOUNTABLY INFINITE SETS OR TRY TO DISCUSS LEBESGUE MEASURABLE FUNCTIONS IN MY PRESENCE I WILL HUNT YOU DOWN, CUT YOU INTO INFINITESMALLY THIN HORIZONTAL SLICES AND FEED THE SLICES TO MY CHILDREN.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>Something about metric MDS rather than nonmetric MDS… don’t @ me<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>FOR REASONS<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>That’s not to say I think there is no role for orthodox inference, nor that controlling error rates is a thing we should just not think about anymore. I just don’t think that this is a sensible idea to build an entire theory of inference around<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div quarto-reuse="quarto-reuse" class="quarto-appendix-contents"><a rel="license" href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</a></div></section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{navarro2023,
  author = {Danielle Navarro},
  title = {A Personal Essay on {Bayes} Factors},
  date = {2023-04-12},
  url = {https://blog.djnavarro.net/posts/2023-04-12_bayes-factors},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-navarro2023" class="csl-entry quarto-appendix-citeas" role="doc-biblioentry">
Danielle Navarro. 2023. <span>“A Personal Essay on Bayes
Factors.”</span> April 12, 2023. <a href="https://blog.djnavarro.net/posts/2023-04-12_bayes-factors">https://blog.djnavarro.net/posts/2023-04-12_bayes-factors</a>.
</div></div></section></div></main> <!-- /main -->
<!-- plausible -->
<script async="" defer="" data-domain="blog.djnavarro.net" src="https://plausible.io/js/plausible.js"></script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>